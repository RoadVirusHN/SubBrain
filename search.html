<!DOCTYPE html>
<html lang="kr"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>🧠SUBBRAIN | 이것이 디지털 동물의 숲이다!! 파멸편 (This is the Digital Animal Crossing!! Bad Ending.01)</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="🧠SUBBRAIN" />
<meta property="og:locale" content="kr" />
<meta name="description" content="이것이 디지털 동물의 숲이다!! 파멸편 (This is the Digital Animal Crossing!! Bad Ending.01)" />
<meta property="og:description" content="이것이 디지털 동물의 숲이다!! 파멸편 (This is the Digital Animal Crossing!! Bad Ending.01)" />
<link rel="canonical" href="http://localhost:4000/search.html" />
<meta property="og:url" content="http://localhost:4000/search.html" />
<meta property="og:site_name" content="🧠SUBBRAIN" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="🧠SUBBRAIN" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"이것이 디지털 동물의 숲이다!! 파멸편 (This is the Digital Animal Crossing!! Bad Ending.01)","headline":"🧠SUBBRAIN","url":"http://localhost:4000/search.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="🧠SUBBRAIN" /><link rel="icon" type="image/x-icon" href="/assets/img/common/favicon.ico">
</head>
<div class="scrollWrapper">
  <div class="scrollbar"></div>
  <div class="progressbar"></div>
  <div class="scrollbarButton"></div>
</div>

<link rel="stylesheet" href="/assets/css/obsidian/obs-scrollbar.css" />

<!--<div class="redirection">
  <h1 class="name">Redirection for full experience.</h1>
  <br>
  Move to <br /> <a class="to" href="#">netlify url</a><br />
  <div>after <span class="counter">10</span>secs.</div>
  press <button class="cancle">here</button> to cancle.
</div>
<div class="overlay"></div>
<script type="module" src="/assets/scripts/common/components/init_redirection.js"></script>

<link rel="stylesheet" href="/assets/css/common/redirection.css" />-->

<body><header class="site-header" role="banner">

  <div class="wrapper" style="display: flex; justify-content: space-between;"><div id="header-wrapper">
    <a class="site-title" rel="author" href="/blog">🧠SUBBRAIN</a>

    </div><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><script src="https://unpkg.com/lunr/lunr.js"></script>
<link rel="stylesheet" href="/assets/css/common/searchbar.css" />

<form id="search-form" method="get">
  <span id="search-wrapper">
    <span id="tag-holder" ></span>
    <input type="text" id="search-box" placeholder='Prefix "#" to add Tag.' autocomplete="off">
    <span class="inner-search" >🔍</span>
  </span>
</form><a class="page-link" href="/">ABOUT ME</a><a class="page-link" href="/blog">ALL ARTICLES</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
    <link rel="stylesheet" href="/assets/css/common/drawer.css" />
<button class="drawer-button open">▶️</button>
<div id="drawer" class="close">
  <button class="drawer-button close">
    ◀️
  </button>
  <div class="drawer-content">
    <div class="my-description">
      <div class="avatar-section" style="display: flex; flex-direction: row;">

        <img src="/assets/img/common/avatar.png" alt="avatar" class="avatar">
        <div style="display: flex; flex-direction: column; margin-left: 5px;">
          <a href="/about/">
            <h3 class="name">ROADVIRUSHN</h3>
          </a>
          <div class="stack-list" style="margin: 5px 0 0 5px;">
            <a title="My github page" href="https://github.com/RoadVirusHN">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#github"></use>
  </svg>
</a>
<a title="My G-mail" href="mailto:roadvirushn@gmail.com">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#gmail"></use>
  </svg>
</a>
<a title="My Blog" href="https://luminous-bubblegum-8e9be4.netlify.app">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#blog"></use>
  </svg>
</a>
          </div>
        </div>
        <!-- <h4 class="name">(JUNSEOK YUN)</h4> -->
      </div>
      <p style="margin: 5px 0 0 0;">
        풀스택 웹🌐 개발자 지망생 🧑🏽‍💻
        <br>
        ➕ 인공지능 관심 🤖
      </p>
    </div>
      <hr>
      <div class="categories">
        <h3 style="margin: 0;"><a href="/">Categories</a></h3>
        <ul class="category-list">
  
  
  
  <li>
    <strong style="font-size: larger;">┣ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/COMPUTER_SCIENCE/" class="category-drop-down">▶</a>
      
      <span class="category-link">COMPUTER_SCIENCE</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 6
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/DATABASE/">DATABASE</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/ALGORITHM/">ALGORITHM</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 16 
            📂: 1
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/OS/">OS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            📂: 1
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/NETWORK/">NETWORK</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 8 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/ETC/">ETC</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            📂: 1
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/OSSU/">OSSU</a>
        </h4>
          <span style="font-size: xx-small;">
             
            📂: 1
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">┣ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/WEB/" class="category-drop-down">▶</a>
      
      <span class="category-link">WEB</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 3
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/FRONTEND/">FRONTEND</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/BACKEND/">BACKEND</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            📂: 2
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/CI,CD/">CI,CD</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            📂: 2
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">┣ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/ETC/" class="category-drop-down">▶</a>
      
      <span class="category-link">ETC</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 2
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/ETC/ETCS/">ETCS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 8 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/ETC/SUBBRAIN 개발기/">SUBBRAIN 개발기</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 5 
            
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">┗ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/AI/" class="category-drop-down">▶</a>
      
      <span class="category-link">AI</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 9
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/AITOOLS/">AITOOLS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 3 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/CV/">CV</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/DEEP_LEARNING/">DEEP_LEARNING</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/DATA_VIS/">DATA_VIS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/GRAPH/">GRAPH</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/LIGHTWEIGHT/">LIGHTWEIGHT</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/MATH/">MATH</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/NLP/">NLP</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 3 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/STRUCTURED_DATA/">STRUCTURED_DATA</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
    </ul>
  </li>
  
</ul>
      </div>
      <hr>
      <div class="recent-view">
        <h3 style="margin: 0;">Recent views</h3>
        <ul style="margin: 0;">
          <li>
            <strong style="color:rgb(219, 219, 12);">1 <a id="recent-1"></a></strong>
          </li>
          <li>
            2 <a id="recent-2"></a>
          </li>
          <li>
            3 <a id="recent-3"></a>
          </li>
          <li>
            4 <a id="recent-4"></a>
          </li>
          <li>
            5 <a id="recent-5" style="overflow: hidden;"></a>
          </li>
        </ul>
      </div>
    </div>
    <hr>
  <div style="height: 7vh;"></div>
</div>
    <div class="wrapper">
      <script>
window.store = {
  
  "/articles/computer_science/database/Database%20SQL%20%EA%B8%B0%EC%B4%88.html": {
    title: "Database SQL 기초",
    date: " Aug 13, 2019 ",
    url: "/articles/computer_science/database/Database%20SQL%20%EA%B8%B0%EC%B4%88.html",
    tags: ["DB","CS"],
    content: "Database SQL 기초DB  체계화된 데이터들의 모임  열, 칼럼, 각 열에는 고유한 데이터 형식이 지정됨, 데이터의 속성  행, 레코드, 1개의 데이터  스키마, 틀 각 열의 요소, 즉  들어갈 데이터의 내용을 정의함    구성요소          개체, entity( 사원들)      그들이 가지는 속성, attribute (사원번호, 성명, 부서 , 직책)      개체 사이의 관계 (relation)        ORM(Object-Relational Mapping)              SQL이 몰라도 파이썬 문법으로 하는 데이터베이스 언어, 파이썬 언어(초안, 청사진)을 SQL 문으로 해석하여 객체로 이용하게 해줌, 중간 번역자    CRUD란?    데이터로 할 수 있는 4가지  C: CREATE  R: READ  U: UPDATE  D: DELETE    RDBMS(관계형 데이터베이스 관리 시스템)    개체와 개체 사이의 관계를 표현하기 위해 2차원의 표를 사용  차수 : 어트리뷰트의 수, 카디널러티 : 개체수 ??    SQLite    서버가 아닌 응용 프로그램에 넣어 사용하는 비교적 가벼운 데이터베이스(안드로이드, 임베디드 , django 등), 오픈소스이며 로컬에 간단한 DB 구성 가능,    기본 용어 정의          스키마(scheme) : 데이터 베이스에서 자료의 구조, 표현 방법, 관계 등을 정의한 구조, 명세서, 데이터 규격, 데이터 타입 정의, 데이터 베이스의 구조와 제약 조건에 관련한 전반적인 명세를 기술.      테이블: 스키마로 구현하는 데이터를 저장하는 공간, 엑셀 표 같은거, 데이터들의 집합      열(칼럼, 어트리뷰트, 속성, Column): 고유한 데이터 형식의 지정      행(레코드, 데이터, row):      PK(기본 키, Primary Key) : 각 행(레코드)의 고유한 값, 반드시 설정됨, 관리 및 관계 설정시 활용        SQL 개념              SQL(Structured Query Language)는 RDBMS의 데이터를 관리하기 위해 설계된 특수 목적 프로그래밍 언어이다, 자료의 검색, 관리, 스키마 생성과 수정, 객체 접근 조정 관리를 위해 고안    문법 종류          DDL(Data Definition Language) - 데이터 정의 언어.구조, 즉 테이블과 스키마를 정의              CREATE, DROP, ALTER 등                  DML(Data Manipulation Language) - 데이터 조작 언어. 데이터를 저장, 수정, 삭제, 조회                    INSERT(데이터삽입), UPDATE(갱신), DELETE(삭제, 행제거), SELECT(데이터, 검색)                  DCL(Data Control Language) - 데이터 제어 언어, 사용자 권한 제어를 위해 사용                    GRANT, REVOKE, COMMIT, ROLLBACK        실습        기본 sql 동작                  sqlite3.exe가 있는 폴더에서 터미널로 sqlite3 치면```sql$ sqlite3SQLite version 3.29.0 2019-07-10 17:32:03Enter \".help\" for usage hints.Connected to a transient in-memory database.Use \".open FILENAME\" to reopen on a persistent database.sqlite&#38;#62;                    상단처럼 표시된다.  명령어 앞에 `.`을 입력하면 DB 관련 명령어이다.- 예를 들어 .exit로 나갈 수 있다2. sqlite3 파일 생성법 \t1) sqlite3 test.sqlite3 입력\t2) .databases 입력```sqlsqlite3 test.sqlite3SQLite version 3.29.0 2019-07-10 17:32:03Enter &amp;#34;.help&amp;#34; for usage hints.sqlite&amp;#38;#62; .databasesmain: C:\\Users\\student\\sqlite\\test.sqlite3sqlite&amp;#38;#62;  test는 원하는 데이터 베이스 명          csv 파일 가져와서 데이터베이스로 만들기```sqlsqlite&#38;#62; .mode csvsqlite&#38;#62; .import hellodb.csv examples      - hellodb는 csv 파일 이름- .mode csv는 어떤 파일을 대상으로 할것인가?- 뒤 .import 는 그 파일을 데이터 베이스로 만들고, 테이블 이름을 examples로 만듦- 4. 데이터 베이스 눈으로 보이게 만들기```sqlSELECT * FROM examples;1,&amp;#34;길동&amp;#34;,&amp;#34;홍&amp;#34;,600,&amp;#34;충청도&amp;#34;,010-2424-1232            은 전부 다라는 표현에 가까움, examples라는 테이블에 데이터 전부(*) 반환해 라는 뜻        여기서 SELECT, FROM 등은 키워드이며, 키워드는 대문자가 관례(소문자로 되긴함)          더 예쁘게 가져오기```sqlsqlite&#38;#62; .headers onsqlite&#38;#62; .mode columnsqlite&#38;#62; SELECT * FROM examples;id          first_name  last_name   age         country     phone———-  ———-  ———-  ———-  ———-  ————-1           길동          홍           600         충청도         010-2424-1232      - .headers로 헤더를 적는 모드로, .mode column으로 각 컬럼마다 구별하는 모드로 바꾼 듯?6. 파일로 명령어 적어서 실행하기\t1) 00_intro.sql 파일 만들기\t\t&gt; 00_intro.sql 파일 내용```sql-- sql 주석처리는 --로 한줄-- 데이터베이스 생성.database-- csv파일을 읽어오기 sqlite의 기능임 sql 문법 아님, ; 필요 없음.mode csv.import hellodb.csv examples -- 외부 파일에서 스키마 가져오기 (전부 TEXT로 가져옴)-- 예쁘게 보기.headers on.mode column-- 테이블 조회 sql 문법 끝에는 꼭 ;를 붙임SELECT * FROM examples;  CAST(값 AS INTEGER) = INTEGER로 형변환으로 적적히 형변환 가능2) 터미널에서 .read 00_intro.sql 명령어로 실행( 같은 디렉토리일시)sqlite&amp;#38;#62; .read 00_intro.sqlmain: C:\\Users\\student\\sqlite\\test.sqlite3id,first_name,last_name,age,country,phone1,&amp;#34;길동&amp;#34;,&amp;#34;홍&amp;#34;,600,&amp;#34;충청도&amp;#34;,010-2424-1232id,first_name,last_name,age,country,phone1,&amp;#34;길동&amp;#34;,&amp;#34;홍&amp;#34;,600,&amp;#34;충청도&amp;#34;,010-2424-1232CRUD 동작데이터 생성 CREATE, Table 생성1) 01_DDL.sql 파일 생성 &gt; 01_DDL.sql 파일 내용 ```sql -- DDL(데이터 정의 언어) CREATE TABLE classmates (id INTEGER PRIMARY KEY,name TEXT ); -- 파이썬과 달리 마지막 요소에 &amp;#39;,&amp;#39; 남기면 오류가 남 -- 만들어진 테이블 목록 조회 .tables– 스키마 조회, 주석도 같이 나온다..schema classmates– 테이블 삭제DROP TABLE classmates;.tables1) 터미널에 .read 01_DDL.sql&gt; 터미널 입력```sqlsqlite&amp;#38;#62; .read 01_DDL.sqlclassmatesCREATE TABLE classmates (    id INTEGER PRIMARY KEY, -- INTEGER는 INT로 줄여써도 됨, 단 PRIMARY KEY는 INTEGER로 써야함, 아닐시는 INT로 써도 됨    name TEXT );데이터 추가 (INSERT)1. data 추가 (INSERT)\t: 특정 table에 새로운 행을 추가해 데이터 추가\t- INSERT INTO talbe(column1, column2,...)\t- VALUES (value1, value2, ...)2. 02_CRUD.sql 파일 생성&gt; 02_CRUD.sql 파일 내용 ```sql-- 데이터 테이블 만들기 CREATE TABLE classmates (name TEXT,age INT,address TEXT );– CREATE – ;이 등장하지 않으면 1줄로 인식함INSERT INTO classmates (name,age,address)VALUES ('윤준석', 27, '광주');.headers on.mode column– 확인SELECT * FROM classmates;3. 터미널에 .read 02_CRUD.sql 실행&gt; 터미널 내용, classmate 1개 추가```sqlsqlite&amp;#38;#62; .read 02_CRUD.sqlid          name        age         address----------  ----------  ----------  ----------1           윤준석         27          광주  한번 더 실행  터미널 내용, 같은 내용이 들어있는 classmate 1개 더 추가```sqlsqlite&#38;#62; .read 02_CRUD.sqlid          name        age         address———-  ———-  ———-  ———-1           윤준석         27          광주2           윤준석         11          광주5. 파일에서 요소 하나를 빼서 .read 02_CRUD.sql 실행&gt; 변경된 02_CRUD.sql 파일 내용```sql-- 데이터 테이블 만들기CREATE TABLE classmates (    name TEXT,    age INT,    address TEXT);-- CREATE -- ;이 등장하지 않으면 1줄로 인식함INSERT INTO classmates (name,address)VALUES (&amp;#39;윤준석&amp;#39;, &amp;#39;광주&amp;#39;);.headers on.mode column-- 확인SELECT * FROM classmates;  age 칸이 비어있는 classmate 1개 더 추가```sqlsqlite&#38;#62; .read 02_CRUD.sqlid          name        age         address———-  ———-  ———-  ———-1           윤준석         27          광주2           윤준석         11          광주3           윤준석                     광주6.  파일을 또 변경한 예시&gt; 변경된 02_CRUD.sql 파일 내용```sqlCREATE TABLE classmates (    name TEXT,    age INT,    address TEXT);INSERT INTO classmates -- 모든 열에 데이터를 넣을때는 생략 가능VALUES(&amp;#39;홍길동&amp;#39;, 30, &amp;#39;서울&amp;#39;);SELECT rowid, * FROM classmates; -- 자동으로 생성된 rowid를 같이 출력 DROP TABLE classmates; -- 테이블 삭제  터미널 결과```sqlsqlite&#38;#62; .read 02_CRUD.sqlrowid       name        age         address———-  ———-  ———-  ———-1           홍길동         30          서울7. 데이터 베이스 무결성의 원칙을 위한 NOT NULL 추가&gt; 변경된 02_CRUD.sql 파일 내용```sqlDROP TABLE classmates;.tablesCREATE TABLE classmates (    id INTEGER PRIMARY KEY NOT NULL,    name TEXT NOT NULL, -- NOT NULL을 넣으면 비운채로 넣을 수 없다.    age INT NOT NULL,    address TEXT NOT NULL);INSERT INTO classmates VALUES(1, &amp;#39;윤준석&amp;#39;, 27, &amp;#39;광주&amp;#39;),(2, &amp;#39;오창희&amp;#39;, 11, &amp;#39;광주&amp;#39;),(3, &amp;#39;박나래&amp;#39;, 25, &amp;#39;서울&amp;#39;),(4, &amp;#39;이요셉&amp;#39;, 29, &amp;#39;구미&amp;#39;),(5, &amp;#39;김철수&amp;#39;, 27, &amp;#39;대전&amp;#39;); -- 위와 같은 형식으로 여러 값을 한꺼번에 넣을 수도 있다.SELECT * FROM classmates;  만약 NOT NULL인 값을 빈 채로 쓰면 이런 에러가 뜬다.```sqlError: near line 9: NOT NULL constraint failed: classmates.address- 이를 통해 데이터가 공백이 되는걸 방지한다.&gt; 만약 id 값이 겹치면 이런 에러가 뜬다.```sqlError: near line 11: UNIQUE constraint failed: classmates.id      이를 방지하고 효율적으로 하기 위해 직접 넣지 말고 sql이 자동으로 생성되는 값을 쓰자        만약 id를 직접 집어 넣어줘야 한다면 NOT NULL 대신 AUTOINCREMENT 를 넣어서 자동으로 증가하게 해주자.(이러면 지워진 id를 새로운 열에 사용하지 않는다, SQLite에서는 비추천함(메모리 사용증가))        AUTOINCREMENT를 쓰면 그 부분은 제외 열을 나열해줘야함          AUTOINCREMENT 예시```sqlCREATE TABLE bands(  id INTEGER PRIMARY KEY AUTOINCREMENT,  name TEXT,  debut INTEGER);INSERT INTO bands (name, debut) VALUES('Queen', 1973),('Coldplay', 1998),('MCR', 2001);      &gt; CREATE TABLE classmates의 예시```sqlid INT PRIMARY KEY AUTOINCREMENT, -- 또는 아예 id 안써서 자동생성되게끔원하는 데이터만 제한해서 가져오기(READ)  select 명령문```sqlSELECT rowid, name FROM classmates; – 전체 데이터중 일부만 가져오는법– 터미널 결과examplesrowid       name———-  ———-1           윤준석2           오창희3           박나래4           이요셉5           김철수&gt;특정한 테이블에서 원하는 갯수만큼만 가져오기(LIMIT 명령어)```sqlSELECT rowid, name FROM classmates LIMIT 1; -- 전체 데이터중 1개만 가져오는법-- 터미널 결과sqlite&amp;#38;#62; .read 02_CRUD.sqlexamplesrowid       name----------  ----------1           윤준석  데이터 몇개를 스킵하고 가져오기(OFFSET 명령어)```sqlSELECT rowid, name FROM classmates LIMIT 1 OFFSET 2; – 전체 데이터중 처음 2개를 스킵하고 1개만 가져오는법– 터미널 결과sqlite&#38;#62; .read 02_CRUD.sqlexamplesrowid       name———-  ———-3           박나래- LIMIT와 함께 쓰는 경우가 많다&gt; 특정한 값 만 검색해서 가져오기(WHERE 명령어)```sqlSELECT rowid, name FROM classmates WHERE address=&amp;#39;구미&amp;#39;; -- 전체 데이터중 주소가 &amp;#39;구미&amp;#39;인 사람만 가져오기-- 터미널 결과sqlite&amp;#38;#62; .read 02_CRUD.sqlexamplesrowid       name----------  ----------4           이요셉  검색 등에 활용          특정 table 중복 제거결과 출력```sql– 전체 나이 데이터를 가져오되, 중복은 없앰SELECT DISTINCT age FROM classmates;– 터미널 결과, 마지막 27살이 없어짐name        age         address———-  ———-  ———-윤준석         27          광주오창희         11          광주박나래         25          서울이요셉         29          구미김철수         27          대전age———-27112529      #### 데이터 삭제(DELETE)```sqlDELETE FROM classmates WHERE address=&amp;#39;광주&amp;#39;; -- 주소가 광주인 사람만 지우기-- 터미널 결과name        age         address----------  ----------  ----------박나래         25          서울이요셉         29          구미김철수         27          대전  중복이 불가능한(UNIQUE한) id를 기준으로 지우면 특정 열만 지울 수 있다.  지워진 id는 다음에 새로 추가한 열값에 할당된다 이를 막으려면 id값을 선언할 때AUTOINCREMENT를 써야함(SQLite에서는 비추천)데이터 수정(UPDATE)  UPDATE와 SET, WHERE로 일부만 수정하기UPDATE classmates SET name=&amp;#39;홍길동&amp;#39;, address=&amp;#39;제주&amp;#39; WHERE age&amp;#38;#62;=50 AND address=&amp;#34;마포&amp;#34;; -- 나이가 50살 이상이고 고향이 마포인 사람 사람의 이름과 주소 바꾸기-- 터미널 결과rowid       name        age         address----------  ----------  ----------  ----------1           윤준석         27          광주2           오창희         11          광주3           박나래         25          서울4           이요셉         29          구미5           홍길동         54          제주 -- 박철용, 마포가 홍길동, 제주로 바뀜  address==”마포” 여도 괜찮음, 괄호 처리도 괜찮음  CONUT()를 이용하여 숫자세기```sqlSELECT COUNT(*) FROM users WHERE age &#38;#62;= 30 AND last_name = '김';– 나이가 30이상이고 성이 김씨인 유저의 수는?&gt; AVG()를 이용한 평균내기(INT 형만 가능)```sqlSELECT AVG(age) FROM users WHERE age &amp;#38;#62;= 30 AND last_name = &amp;#39;김&amp;#39;;-- 나이가 30이상이고 성이 김씨인 유저들의 평균 나이는?  MAX()를 이용한 최대값구하기```sqlSELECT MAX(balance), first_name FROM users;– 가장 잔액이 많은 사람의 잔액과 이름은?- MIN()은 최소값을 구하는데 사용&gt; 와일드카드 패턴 (LIKE)```sqlSELECT * FROM users WHERE age LIKE &amp;#39;2_&amp;#39;;-- users 중 20대인 사람 출력 2&amp;#37;로하면 2살, 200살도 나옴      해당 정보에서 포함이 되어있는가? 등을 찾음        구글 검색 등에서도 사용 가능하다 (?? : 특정한 문자 갯수 랜덤 문자, *: 갯수와 상관없이 아무 문자나)                            표시          예시          의미                                      %          2%          2로 시작하는 값                                     %2          2로 끝나는 값                                     %2%          2가 들어가는 값                          -          _2%          아무값이나 들어가고 두번째가 2로 시작하는 값                                     1____          1로 시작하고 4자리인 값                                     2_%_%          2로 시작하고 적어도 3자리인 값                    데이터 정렬(ORDER)  ORDER```sqlSELECT * FROM users ORDER BY age DESC, last_name ASC LIMIT 10;– 나이가 많은 상위 10명의 성순으로 10명 오름차순 정보- ORDER BY column1, column2 [ASC|DESC], ASC = 올림차순 (기본값)  작은것 부터, DESC= 내림차순, 큰것부터 작은것1. 테이블명 수정&gt; 04_DDL_a.sql 파일 내용```sqlDROP TABLE articles;DROP TABLE news;CREATE TABLE articles(    title TEXT NOT NULL,    content TEXT NOT NULL);INSERT INTO articles VALUES(&amp;#39;윤준석 굶어죽다&amp;#39;, &amp;#39;백수 생활 길어 잔액 부족해... 충격&amp;#39;);SELECT * FROM articles;ALTER TABLE articles RENAME TO news; -- 테이블 이름 바꾸기-- created_at이라는 DATETIME 타입의 COLUMN을 새로 넣으려고 시도ALTER TABLE news-- ADD COLUMN created_at DATETIME NOT NULL; -- 비어있는 값의 새로운 column이 생기면 안되므로 에러 발생ADD COLUMN created_at DATETIME NOT NULL DEFAULT 1; -- DEFAULT 값 정해주었으므로 에러 안남.tables  ADD COLUMN 대신 RENAME COLUMN  바꿀컬럼 TO 바뀔이름 으로 컬럼 이름 바꿀 수 있음  ALTER TABLE “table_name” Change “column 1” “column 2” [“Data Type”]; 한꺼번에 형변환"
  }
  , 
  
  "/articles/web/frontend/CSS%20%EA%B0%9C%EB%85%90.html": {
    title: "CSS 개념",
    date: " Nov 2, 2019 ",
    url: "/articles/web/frontend/CSS%20%EA%B0%9C%EB%85%90.html",
    tags: ["CSS","FE","CRUDE"],
    content: "CSSCSS의 개념과 기본 사용법HTML =&gt; 정보와 구조화CSS =&gt; styling의 정의이 둘은 각자 문법이 다른 별개의 언어, 하지만 HTML이 없으면 CSS는 무의미h1 &amp;#123; \tcolor:blue;\tfont-size:15px; &amp;#125;  h1 : 셀렉터(selector),  color:blue; : 선언          color: 프로퍼티(property), blue: 값(value)        세미콜론(;)으로 각각의 값을 구분  CSS의 주석 설정은 /* 내용 */CSS 활용CSS 활용하기 1. Inline(인라인)title: 00_intro.html 바디 부분&lt;body&gt;    &lt;h1 style=\"color:red;\"&gt;CSS intro&lt;/h1&gt;&lt;/body&gt;  html 태그 안에 인라인으로 CSS를 적용해본 것, 유지보수가 힘드므로 금지된다.CSS 활용하기 2. CSS 나누기title: 00_intro.html 파일 헤더와 바디 부분&lt;head&gt;    &lt;style&gt;     h2{ &lt;!--보통 여러값이 나오므로 CSS 사용시 여러줄로 나눔--&gt;        color: blue;        font-size:50px;    }    &lt;/style&gt;&lt;/head&gt;&lt;body&gt;    &lt;h2&gt;CSS is awesome&lt;/h2&gt;&lt;/body&gt;CSS 활용하기 3.파일 분리하기title: 같은 폴더 내의 00__intro css 파일p{    color: green;}title: 00_intro.html 파일 헤더와 바디 부분&lt;head&gt;    &lt;link rel=\"stylesheet\" href=\"00_intro css\"&gt;&lt;/head&gt;&lt;body&gt;    &lt;p&gt;Lorem ipsum dolor sit amet.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;title: vscode 기능lorem ipsum 기능: 랜덤의 의미없는 문장 만들어줌lorem{단어수}값값 : 키워드, 크기 단위, 색깔 등이 들어갈 수 있음  키워드 예시 : display: block, visiblity: hidden 등…키워드의 경우 개발자 도구로 확인 가능  주황색 영역 : 설정된 크기 영역  파란색 영역 : 실제로 해당 요소가 차지하고 있는 부분title: CSS 값 설정 예시 (01_css_val css 파일)/* CSS 주석 #으로 시작하는 것은 아이디이다.*/#hello {    font-size: 50px;}#welcome{    font-size: 50%; /* 기존의 크기 대비해서 바꿈*/}#lunch{    font-size: 5em; /* 기본설정의 x배 커지게 함  %와 마찬가지로 곱셈됨*/}/*div는 부모*/div{    width: 50%;}/*h1은 자식, 부모의 width를 상속받아서 결국은 0.5 * 0.5 의 값을 갖는다.*/h1{    width: 50%;}#snack {    font-size: 0.5rem;}#menu { /*화면 크기에 상대적으로 크기가 바뀌는 반응형 단위*/    background-color: red;    width: 50vw;    height: 50vh;    /*프로퍼티값 앞에 콜론을 붙이고 한칸 띄우는게 국룰*/}title: CSS 값 설정 예시 (01_css_val.html 파일)&lt;link rel=\"stylesheet\" href=\"01_css_val css\"&gt;&lt;/head&gt;&lt;body&gt;    &lt;P id=\"hello\"&gt;안뇽&lt;/P&gt;&lt;!--px 단위--&gt;    &lt;P id=\"welcome\"&gt;반갑습니다&lt;/P&gt; &lt;!--% 단위--&gt;    &lt;h1&gt;        &lt;P id=\"lunch\"&gt;점심시간입니다.&lt;/P&gt;&lt;!--em 단위--&gt;    &lt;/h1&gt;    &lt;h1&gt;        &lt;p id=\"snack\"&gt;오늘 간식은 도시락&lt;/p&gt;&lt;!--rem 단위--&gt;    &lt;/h1&gt;    &lt;div&gt;        &lt;h1&gt;배고파 &lt;/h1&gt;    &lt;/div&gt;    &lt;h1 id=\"menu\"&gt;오늘의 메뉴&lt;/h1&gt;&lt;!--viewport 단위--&gt;&lt;/body&gt;px  한 픽셀은 RGB로 색을 구분하여 표현하며, 매우 작은 사각형  디바이스별로 픽셀의 크기는 제각각, 대부분 브라우저는 1px를 1/96인치로 지정%백분율 단위의 상대 단위이다.요소에 지정된 사이즈(상속된 사이즈나 디폴트 사이즈)에 상대적인 사이즈를 설정한다.부모와 상속, 즉 곱셈 됨emem은 배수 단위로 상대 단위이다. 요소에 지정된 사이즈에 상대적인 사이즈를 설정한다. %와 마찬가지로 곱셈, 즉 상속됨remem과 비슷하나 상대 단위가 아닌, 절대 단위, 즉 2배로 설정하면 다른 값을 다 곱해도 2배로 설정됨  보통 1rem은 16pxviewport 단위디바이스마다 다른 크기의 화면을 가지고 있기 때문에 상대적인 단위인 viewport를 기준으로 표현하는 단위, 인터넷창이나 모니터크기를 바꾸면 동적으로 바뀐다. 반응형 단위IE 8 이하는 지원하지 않으며 IE 9~11, Edge는 지원이 완전하지 않다색상 표현 단위HEX 표현: 16진수의 두자리씩 RGB 표현 \\#24f4cd, \\#ffffff(흰색)RGB 표현: 빨강, 초록, 파랑이 섞이는 정도를 표현 (0~255)RGBA 표현: RGB + 투명도 (alpha)Box modelmargin : 내용과 내용사이의 여백border: 테두리 영역padding: 테두리 안쪽의 내부 여백, 요소에 적용된 요소들이 내용과 함께 적용됨contents: 내용, 역시 스타일 적용됨title: Box model 예시(02_box.html)&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;    &lt;meta charset=\"UTF-8\"&gt;    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt;    &lt;title&gt;Document&lt;/title&gt;    &lt;link rel=\"stylesheet\" href=\"02_box css\"&gt;&lt;/head&gt;&lt;body&gt;    &lt;div class=\"margin border\"&gt;&lt;/div&gt;    &lt;div class=\"margin padding\"&gt;&lt;/div&gt;    &lt;div class=\"margin border\"&gt;&lt;/div&gt;    &lt;div class=\"margin\"&gt;&lt;/div&gt;    &lt;div class=\"margin padding\"&gt;&lt;/div&gt;    &lt;div class=\"margin-1\"&gt;&lt;/div&gt;    &lt;div class=\"margin-2\"&gt;&lt;/div&gt;    &lt;div class=\"margin-3\"&gt;&lt;/div&gt;    &lt;div class=\"margin-4\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;title: Box model 예시 CSS (02_box css)div{    width: 100px;    height: 100px;    background-color: rgb(196, 61, 61);}.margin {    margin-top: 10px;    margin-bottom: 10px;    margin-left: 10px;    margin-right: 10px;}.padding{    padding-top: 20px;    padding-bottom: 10px;}.border{    /* border-width: 2px;    border-color: blue;    border-style: dotted; */    border: 3px blue dotted; /*위의 축약형*/}.margin-1{    margin: 10px;     /* 상하 좌우 전부 마진 10 */}.margin-2{    margin: 10px 20px;    /* 위 아래 마진 10px, 20px */}.margin-3{    margin: 10px 20px 30px;    /*위 10 양옆 20 아래 30px 마진 생김*/}.margin-4{    margin: 10px 20px 30px 40px;    /* 위 마진 부터 시계방향으로 10, 20, 30, 40 px 마진 생김 */}/*border, padding에도 적용됨*/margin      id 속성 특정 id 속성을 가진 태그 하나에만 적용할때 쓰는 속성, CSS에서 앞에 #을 붙여 표현    class 속성 같은 클래스 속성의 태그를 전부 적용할때 쓰는 속성, CSS에서 앞에 .(점)을 붙여 표현          각기 다른 class를 한 태그에 스페이스바(공백)으로 나누어 적용할 수 있다.        마진 상쇄현상: 다른 존재의 마진과 상쇄됨    padding  bordershorthandDisplaydisplay 프로퍼티를 이용해 block과 inline 그리고 inline-block, none을 정해줄 수 있다. 원래 기본이 block이여도 inline으로 바꿀 수 잇음title: 03_display css 파일 예시div {    background-color: cornflowerblue;    margin-left: auto;    margin-right: auto;}.half {    width: 50%;}.b {    display: block;}.i {    display: inline;}.bi {    display: inline-block;    margin-top: 20px;}.bg {    background-image: url(https://picsum.photos/200/300);}.text {    font-family: 'Saira Stencil One', cursive;}title: Display 예시 (03_display.html)&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;    &lt;meta charset=\"UTF-8\"&gt;    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt;    &lt;title&gt;Document&lt;/title&gt;    &lt;link href=\"https://fonts.googleapis.com/css?family=Fira+Code|Saira+Stencil+One&amp;display=swap\" rel=\"stylesheet\"&gt;    &lt;link rel=\"stylesheet\" href=\"03_display.css\"&gt;&lt;/head&gt;&lt;body&gt;    &lt;div&gt;        &lt;h1&gt;div는 block입니다.&lt;/h1&gt;    &lt;/div&gt;    &lt;div class=\"half i\"&gt;        &lt;span&gt;test&lt;/span&gt; &lt;!-- &lt;h1&gt;여기는 절반만!&lt;/h1&gt; --&gt;    &lt;/div&gt;    &lt;!-- &lt;h1&gt;여기는 h1입니다.&lt;/h1&gt; --&gt;    &lt;div class=\"half i\"&gt;        &lt;span&gt;test&lt;/span&gt; &lt;!-- &lt;h1&gt;여기는 절반만!&lt;/h1&gt; --&gt;    &lt;/div&gt;    &lt;span class=\"bi\"&gt;여기는 span&lt;/span&gt;    &lt;input class=\"b\" type=\"text\"&gt;    &lt;input type=\"date\"&gt;    &lt;h1 class=\"bg\"&gt;hello.&lt;/h1&gt;    &lt;h1 class=\"text\"&gt;ooh yeah.&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;block  항상 새로운 라인에서 시작한다.  너비가 정해지면 마진으로 나머지 영역을 채움(margin-right: auto;(기본값)) margin-left: auto; 도 같이 하면 중앙 정렬이 됨  화면 크기 전체의 가로폭을 차지한다(width: 100%)  block 레벨 요소 내에 inline  block 레벨 요소의 예    inline    span, input 등  새로운 라인에서 시작하지 않으며 문장의 중간에 들어갈 수 있따.  content의 너비만큼 가로폭을 차지한다.  width, height, margin-top, margin-bottom 같은 프로퍼티를 지정할 수 없다.  상, 하 여백은 line-height로 지정한다.inline-block  두 요소를 모두 가지고 있는 성질, 마진을 줄 수도 있음  inline 레벨 요소처럼 한줄에 표시되면서 마직 속성을 모두 정해줄 수 있다.    none    적용하면 보이지않음, 동적으로 웹을 만들어 사라지게 만들고 싶을때 씀 (공간조차 사라짐0)visibility 속성visible  보이게 한다    hidden    안보이게 한다, none과 달리 사라지진 않고 보이지만 않는다. 영역을 그대로 차지하고 있음background-image 속성  url(https://picsum.photos/200/300);을 지정하면 그 공간만큼의 사진을 보여줌font-size, font-family, letter-spacing, white space, text-align 등이 있음position 속성  left: , right: , bottom, top 요소로 얼마나 떨어져야 하는 가를 크기로 정해줌  즉 얼마나 떨어져야하는가 ex) left: 50px == 왼쪽으로 부터 50px 떨어짐  만약 left: 0px, right: 0px 처럼 서로 충돌하면 왼쪽, 그리고 위쪽이 우선됨  각 div의 중심은 div가 그 해당 모서리에서 가장 가까운 쪽으로 결정되는듯?  Ctrl + enter로 코드 중간에 줄바꿈 가능title: Position 속성 예시 (04_postion css)div {    height: 100px;    width: 100px;}.blue {    background-color: blue;    position: static;}.red {    background-color: red;    position: relative;    left: 50px;    bottom: 50px;}.green {    background-color: green;    position: absolute;        left: 0px;     bottom: 0px;}.yellow {    background-color: yellow;    position: fixed;    right: 0px;    top: 0px;}.parent {       height: 200px;    width: 100%;    background-color: gray;    position: relative;}.children {    background-color: pink;    position: absolute;    left: 0px;    bottom: 0px;}title: position 예시 (04_position.html)&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;    &lt;meta charset=\"UTF-8\"&gt;    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt;    &lt;title&gt;Document&lt;/title&gt;    &lt;link rel=\"stylesheet\" href=\"04_position css\"&gt;&lt;/head&gt;&lt;body&gt;    &lt;div class=\"blue\"&gt;&lt;/div&gt;    &lt;div class=\"red\"&gt;&lt;/div&gt;    &lt;div class=\"green\"&gt;&lt;/div&gt;    &lt;div class=\"parent\"&gt;        &lt;div class=\"children\"&gt;&lt;/div&gt;    &lt;/div&gt;    &lt;div class=\"yellow\"&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;static  아무것도 안적은 기본값  안움직임, absolute의 부모가 될 수 없음relative  left: , right: , bottom, top 요소로 정한 크기만큼 원래 있어야하는 위치를 중심으로 여백을 줌  전체 영역 (마진, 보더 등)이 전부 움직임, 원래 위치에도absolute  부모(body가 부모일 경우 화면의 왼쪽 상단)를 중심으로 left: , right: , bottom, top 요소 크기만큼 여백을 줌, 그만큼 떨어짐  부모를 중심으로 이동함,  부모는 position 속성이 static이 아니여야 함, 부모 바깥으로 나갈 수도 있음fixed(고정위치)  부모 자식 관계없이 브라우저 뷰포트(우측 상단)를 중심으로 이동함  스크롤을 움직여도 따라 움직임, 상단 메뉴바 같은거 만들 때"
  }
  , 
  
  "/articles/web/frontend/HTML%20%EA%B0%9C%EB%85%90.html": {
    title: "HTML 개념",
    date: " Nov 2, 2019 ",
    url: "/articles/web/frontend/HTML%20%EA%B0%9C%EB%85%90.html",
    tags: ["HTML","FE"],
    content: "웹과 HTML월드 와이드 웹(WWW)은 인터넷에 연결된 컴퓨터들을 통해 사람들이 정보를 공유할 수 있는 전 세계적인 정보 공간을 말한다.웹서비스(WEB service)란? :https://www.fasdjflksd/index.htmlhttps://서버주소/파일이름크롬 브라우저 : html 문서를 읽어 보여주는 역할도 함IP(Internet Protocol) (172.217.27.78)8비트(0 ~ 255)까지의 숫자로 구성된 숫자의 집합, 각자가 가지고 있는 주소와 동일하다.V4, V6 , 컴퓨터가 너무 많아지면 할당값이 조금씩 늘어나서 V6를 쓰게 될것임도메인(Domain) (google.com)네트워크상의 컴퓨터를 식별하는 호스트명URL(Uniform Resuorce Locator)https://www.google.co.kr/search?q=구글도메인 + 경로, 실제로 해당 서버에 저장된 자료의 우치static web :미리 만들어놓은 HTML 파일을 찾아서 줌&lt;-&gt; dynamic web : 서버 사용자가 요청하면 템플릿으로 HTML 파일 생성W3C : 웹표준 규약 , HTML CSS JSWHATWG : 다른 웹표준 규약 최근 승리함HTML : Hyper Text Markup Language : 하이퍼 링크를 통해 문서간의 연결 뼈대를 만듬, 웹페이지를 작성하기 위한 역할 표시 언어HTML 파일 : HTML 로 작성된 문서파일  doctype 요소  HTML 요소          head 요소      body 요소                  브라우저 화면에 나타나는 정보로 실제 내용에 해당                    HTTPS(Hyper Text Transfer Protocol) : Hyper Text를 주고 받는 규 칙Cascading Style Sheet (CSS): 모양이나 위치를 바꾸는 역할, 꾸며줌JavaScript :  HTML 문서의 기본 구조&amp;#38;#60;!DOCTYPE html&amp;#38;#62; &amp;#38;#60;!--DOCTYPE 선언부 사용하는 문서의 종류를 선언, 보통 html을 사용한다.--&amp;#38;#62;&amp;#38;#60;html lang=&amp;#34;ko&amp;#34;&amp;#38;#62; &amp;#38;#60;!--html 요소, html 문서의 최상위 요소로 문서의 root를 뜻한다. head와 body 부분으로 구분된다.--&amp;#38;#62;&amp;#38;#60;head&amp;#38;#62; &amp;#38;#60;!--head 요소 문서 제목, 문자코드(인코딩)와 같이 해당 문서 정보를 담고 있으며, 브라우저에 나타나지 않는다. CSS 선언 혹은 외부 로딩 파일 지정 등을 작성합니다. og와 같은 메타태그 선언이 이뤄집니다.--&amp;#38;#62;\t&amp;#38;#60;meta charset=&amp;#34;UTF-8&amp;#34;&amp;#38;#62;\t&amp;#38;#60;title&amp;#38;#62;Document&amp;#38;#60;/title&amp;#38;#62;&amp;#38;#60;/head&amp;#38;#62;&amp;#38;#60;body&amp;#38;#62;&amp;#38;#60;!--body 요소 브라우저 화면에 나타나는 정보로 실제 내용에 해당한다.--&amp;#38;#62;&amp;#38;#60;/body&amp;#38;#62;&amp;#38;#60;/html&amp;#38;#62;  html 문서 예시 (intro.html)&amp;#38;#60;!-- 주석 내용 ctrl+/로 자동 생성 가능--&amp;#38;#62;&amp;#38;#60;!-- 요소(Element) : 태그와 내용으로 구성되어 있으며, 대소문자 구별안하지만,    보통 소문자로 작성, 요소간의 중첩도 가능하다 --&amp;#38;#62;&amp;#38;#60;!-- 여는태그-&amp;#38;#62; &amp;#38;#60;h1&amp;#38;#62;contents&amp;#38;#60;/h1&amp;#38;#62;&amp;#38;#60;- 닫는 태그 --&amp;#38;#62;&amp;#38;#60;!-- self-closing element &amp;#38;#60;img src=&amp;#34;url&amp;#34;/&amp;#38;#62; 닫는 태그가 없는 태그,     이미지, 다른 스크립트 불러오는 태그 등이 있다.--&amp;#38;#62;    &amp;#38;#60;!-- 속성(Attribute) 태그에는 속성이 지정될수 있다. href = 속성명, &amp;#34;google.com&amp;#34; = 속성값 &amp;#38;#60;a href=&amp;#34;google.com&amp;#34;/&amp;#38;#62; 이건 하이퍼 텍스트 예시 속성명=&amp;#34;속성값&amp;#34;은 띄어쓰지 않는다.        속성값은&amp;#34;&amp;#34;(큰 따옴표) 안에 사용!, id : 유일한 식별자, class: 스타일 시트에  --&amp;#38;#62;&amp;#38;#60;!-- DOM트리 : body태그와 h1태그는 부모와 자식 관계 (내부에 속해 있음)h1태그와 ul태그(순서없는 리스트용)는 형제 관계(같은 곳에 속해 있음) --&amp;#38;#62;&amp;#38;#60;body&amp;#38;#62;\t&amp;#38;#60;h1&amp;#38;#62;웹문서&amp;#38;#60;/h1&amp;#38;#62;\t&amp;#38;#60;ul&amp;#38;#62;\t\t&amp;#38;#60;li&amp;#38;#62;HTML&amp;#38;#60;/li&amp;#38;#62;\t\t&amp;#38;#60;li&amp;#38;#62;CSS&amp;#38;#60;/li&amp;#38;#62;\t&amp;#38;#60;/ul&amp;#38;#62;&amp;#38;#60;/body&amp;#38;#62;&amp;#38;#60;!-- 시맨틱태그 : ex) div 태그 : 영역을 나누는 태그 &amp;#38;#60;div&amp;#38;#62; &amp;#38;#60;/div&amp;#38;#62; 공간 분할 의미 외에는 없지만, 컴퓨터에게 의도한 바를 알려줄 수 있다.개발자가 의도한 요소의 의미가 보임, 유지보수가 쉬워짐, SEO : 검색 엔진 최적화 에 쓰임 article, footer, header 등이 추가로 있음--&amp;#38;#62;&amp;#38;#60;!-- h1태그는 왠만하면 한페이지에 1개만 쓰자 글자 크기를 바꾸고싶으면 CSS를 활용하자--&amp;#38;#62;&amp;#38;#60;h1&amp;#38;#62;여기는 h1 중요한 제목입니다.&amp;#38;#60;/h1&amp;#38;#62;&amp;#38;#60;p&amp;#38;#62;상세한 본문은 &amp;#38;#60;b&amp;#38;#62;여기에 굵게&amp;#38;#60;/b&amp;#38;#62; 표시 됩니다&amp;#38;#60;/p&amp;#38;#62; &amp;#38;#60;!--p : 본문용 태그 b(시멘틱 태그 의미 없음, 단순 강조), strong(시멘틱 태그 의미 추가, SEO가 분석함): 굵은 글씨로--&amp;#38;#62;&amp;#38;#60;ol&amp;#38;#62;순서가 있는 ol 태그 &amp;#38;#60;!--ol&amp;#38;#62;li*3 하면 자동으로 3개가 속해있는 태그가 생성됨, ol + li*3라고 하면 ol과 li*3가 병렬적으로 생성됨 Emmet.io 참조--&amp;#38;#62;    &amp;#38;#60;li&amp;#38;#62;원소1&amp;#38;#60;/li&amp;#38;#62;&amp;#38;#60;!--ol 태그 내에 요소를 적음--&amp;#38;#62;    &amp;#38;#60;li&amp;#38;#62;원소2&amp;#38;#60;/li&amp;#38;#62;&amp;#38;#60;/ol&amp;#38;#62;&amp;#38;#60;ul&amp;#38;#62;순서가 없는 ul 태그     &amp;#38;#60;li style=&amp;#34;border-style: dotted&amp;#34;&amp;#38;#62;원소1&amp;#38;#60;/li&amp;#38;#62; &amp;#38;#60;!-- 내부 속성 태그로 스타일 등을 바꿀 수 있음 주로 CSS가 해주는 역할임--&amp;#38;#62;    &amp;#38;#60;li&amp;#38;#62;원소2&amp;#38;#60;/li&amp;#38;#62;&amp;#38;#60;/ul&amp;#38;#62;&amp;#38;#60;h2&amp;#38;#62;222&amp;#38;#60;/h2&amp;#38;#62;&amp;#38;#60;h3&amp;#38;#62;3333&amp;#38;#60;/h3&amp;#38;#62;&amp;#38;#60;a href=&amp;#34;https://naver.com&amp;#34;&amp;#38;#62;&amp;#38;#60;img src=&amp;#34;http://imgnews.naver.net/image/5002/2017/09/03/0000977925_001_20170903211801431.jpg&amp;#34; alt=&amp;#34;&amp;#34;&amp;#38;#62;&amp;#38;#60;/a&amp;#38;#62;&amp;#38;#60;!--a : 하이퍼 링크 태그href=연결될 경로를 적어야함 닫는 태그 있어야함--&amp;#38;#62;&amp;#38;#60;!--미디어 태그들--&amp;#38;#62;&amp;#38;#60;!-- 이미지 태그는 src 경로의 이미지를 불러옮--&amp;#38;#62;&amp;#38;#60;!-- &amp;#38;#60;video src=&amp;#34;&amp;#34;&amp;#38;#62;&amp;#38;#60;/video&amp;#38;#62; &amp;#38;#60;iframe&amp;#38;#62;     등--&amp;#38;#62;  시맨틱태그의 종류            태그      설명                  header      헤더 (문서 전체나 섹션의 헤더)              nav      내비게이션              aside      사이드에 위치한 공간으로, 메인 콘텐츠와 관련성이 적은 콘텐츠에 사용              section      문서의 일반적인 구분으로 컨텐츠의 그룹을 표현하며, 일반적으로 h1~h6 요소를 가짐.              article      문서, 페이지, 사이트 안에서 독립적으로 구분되는 영역(포럼/신문 등의 글 또는 기사)              footer      푸터 (문서 전체나 섹션의 푸터)        HTML 예시 2 (index.html)&amp;#38;#60;!DOCTYPE html&amp;#38;#62;&amp;#38;#60;html lang=&amp;#34;en&amp;#34;&amp;#38;#62;&amp;#38;#60;head&amp;#38;#62;    &amp;#38;#60;meta charset=&amp;#34;UTF-8&amp;#34;&amp;#38;#62;    &amp;#38;#60;meta name=&amp;#34;viewport&amp;#34; content=&amp;#34;width=device-width, initial-scale=1.0&amp;#34;&amp;#38;#62;    &amp;#38;#60;meta http-equiv=&amp;#34;X-UA-Compatible&amp;#34; content=&amp;#34;ie=edge&amp;#34;&amp;#38;#62;    &amp;#38;#60;title&amp;#38;#62;Document&amp;#38;#60;/title&amp;#38;#62;&amp;#38;#60;/head&amp;#38;#62;&amp;#38;#60;body&amp;#38;#62;    &amp;#38;#60;h1&amp;#38;#62;프로그래밍 교육&amp;#38;#60;/h1&amp;#38;#62;    &amp;#38;#60;a href=&amp;#34;#python&amp;#34;&amp;#38;#62;&amp;#38;#60;img src=&amp;#34;https://cdn.inflearn.com/wp-content/uploads/python-1.png&amp;#34; width=&amp;#34;50&amp;#34; height=&amp;#34;50&amp;#34;&amp;#38;#62;&amp;#38;#60;/a&amp;#38;#62; &amp;#38;#60;!--inline 줄바꿈을 안함--&amp;#38;#62;    &amp;#38;#60;a href=&amp;#34;#web&amp;#34;&amp;#38;#62;&amp;#38;#60;img src=&amp;#34;images/web.png&amp;#34; width=&amp;#34;50&amp;#34; height=&amp;#34;50&amp;#34;&amp;#38;#62;&amp;#38;#60;/a&amp;#38;#62; &amp;#38;#60;!--하이퍼링크 태그로 해당 ID 값으로 이동할 수 있음--&amp;#38;#62;    &amp;#38;#60;a href=&amp;#34;intro.html&amp;#34;&amp;#38;#62;웹사이트&amp;#38;#60;/a&amp;#38;#62;&amp;#38;#60;!--다른 문서 경로도 이동 가능--&amp;#38;#62;    &amp;#38;#60;hr&amp;#38;#62; &amp;#38;#60;!--수평선 긋기 쓰면 안좋음, div로 구별하는것이 좋음--&amp;#38;#62;    &amp;#38;#60;h2 id=&amp;#34;python&amp;#34;&amp;#38;#62;&amp;#38;#60;a href=&amp;#34;https://docs.python.org/ko/3/tutorial/index.html&amp;#34; target=&amp;#34;_blank&amp;#34;&amp;#38;#62;파이썬&amp;#38;#60;/a&amp;#38;#62;&amp;#38;#60;/h2&amp;#38;#62;&amp;#38;#60;!--태그의 순서는 의미 없음--&amp;#38;#62;    &amp;#38;#60;h3&amp;#38;#62;Number type&amp;#38;#60;/h3&amp;#38;#62;    &amp;#38;#60;p&amp;#38;#62;파이썬에서 숫자형은 아래와 같이 있다.&amp;#38;#60;/p&amp;#38;#62;&amp;#38;#60;!--block 한줄을 전부 포함한 범위--&amp;#38;#62;    &amp;#38;#60;ol&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;int&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;float&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;complex&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;&amp;#38;#60;del&amp;#38;#62;str&amp;#38;#60;/del&amp;#38;#62;&amp;#38;#60;/li&amp;#38;#62; &amp;#38;#60;!--삭선 긋기--&amp;#38;#62;    &amp;#38;#60;/ol&amp;#38;#62;    &amp;#38;#60;h3&amp;#38;#62;Sequence&amp;#38;#60;/h3&amp;#38;#62;    &amp;#38;#60;p&amp;#38;#62;파이썬에서 시퀀스는 아래와 같이 있다.&amp;#38;#60;/p&amp;#38;#62;    &amp;#38;#60;p&amp;#38;#62;&amp;#38;#60;strong&amp;#38;#62;시퀀스는 for문을 돌릴 수 있다!!&amp;#38;#60;/strong&amp;#38;#62;&amp;#38;#60;/p&amp;#38;#62;    &amp;#38;#60;ol&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;str&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;list&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;tuple&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;range&amp;#38;#60;/li&amp;#38;#62;    &amp;#38;#60;/ol&amp;#38;#62;    &amp;#38;#60;hr&amp;#38;#62;    &amp;#38;#60;h2 id=&amp;#34;web&amp;#34;&amp;#38;#62;&amp;#38;#60;a href=&amp;#34;https://developer.mozilla.org/en-US/&amp;#34;&amp;#38;#62;웹&amp;#38;#60;/a&amp;#38;#62;&amp;#38;#60;/h2&amp;#38;#62;    &amp;#38;#60;h3&amp;#38;#62;기초&amp;#38;#60;/h3&amp;#38;#62;    &amp;#38;#60;ul style=&amp;#34;list-style-type: circle&amp;#34;&amp;#38;#62; &amp;#38;#60;!--list의 bullet 모양 바꾸기--&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;HTML&amp;#38;#60;/li&amp;#38;#62;        &amp;#38;#60;li&amp;#38;#62;CSS&amp;#38;#60;/li&amp;#38;#62;&amp;#38;#60;/ul&amp;#38;#62;&amp;#38;#60;iframe width=&amp;#34;560&amp;#34; height=&amp;#34;315&amp;#34; src=&amp;#34;https://www.youtube.com/embed/I_2D8Eo15wE&amp;#34; frameborder=&amp;#34;0&amp;#34; allow=&amp;#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&amp;#34; allowfullscreen&amp;#38;#62;&amp;#38;#60;/iframe&amp;#38;#62;&amp;#38;#60;!-- html안에 또다른 html을 넣을때 iframe 씀 유튜브에서 만든 웹페이지를 끌어다가 놓은거임 왠만하면 쓰지말자--&amp;#38;#62;&amp;#38;#60;!--html 표만들기, table 태그 활용--&amp;#38;#62;&amp;#38;#60;table&amp;#38;#62;        &amp;#38;#60;thead&amp;#38;#62;            &amp;#38;#60;th&amp;#38;#62;&amp;#38;#60;/th&amp;#38;#62;            &amp;#38;#60;th&amp;#38;#62;월&amp;#38;#60;/th&amp;#38;#62;            &amp;#38;#60;th&amp;#38;#62;화&amp;#38;#60;/th&amp;#38;#62;            &amp;#38;#60;th&amp;#38;#62;수&amp;#38;#60;/th&amp;#38;#62;        &amp;#38;#60;/thead&amp;#38;#62;        &amp;#38;#60;tbody&amp;#38;#62;            &amp;#38;#60;tr&amp;#38;#62;                &amp;#38;#60;td&amp;#38;#62;1&amp;#38;#60;/td&amp;#38;#62;                &amp;#38;#60;td rowspan=&amp;#34;2&amp;#34;&amp;#38;#62;2&amp;#38;#60;/td&amp;#38;#62;                &amp;#38;#60;td colspan=&amp;#34;2&amp;#34;&amp;#38;#62;3&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;/tr&amp;#38;#62;            &amp;#38;#60;tr&amp;#38;#62;                &amp;#38;#60;td&amp;#38;#62;1&amp;#38;#60;/td&amp;#38;#62;                &amp;#38;#60;td&amp;#38;#62;2&amp;#38;#60;/td&amp;#38;#62;                &amp;#38;#60;td&amp;#38;#62;3&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;/tr&amp;#38;#62;        &amp;#38;#60;/tbody&amp;#38;#62;&amp;#38;#60;/table&amp;#38;#62;&amp;#38;#60;h3&amp;#38;#62;음악페스티벌 타임테이블을 만들어봅시다.&amp;#38;#60;/h3&amp;#38;#62;&amp;#38;#60;h1&amp;#38;#62;2019 타임테이블&amp;#38;#60;/h1&amp;#38;#62;&amp;#38;#60;table&amp;#38;#62;    &amp;#38;#60;thead&amp;#38;#62;        &amp;#38;#60;th&amp;#38;#62;TIME&amp;#38;#60;/th&amp;#38;#62;        &amp;#38;#60;th&amp;#38;#62;INDOOR&amp;#38;#60;/th&amp;#38;#62;        &amp;#38;#60;th colspan=&amp;#34;2&amp;#34;&amp;#38;#62;OUTDOOR&amp;#38;#60;/th&amp;#38;#62;    &amp;#38;#60;/thead&amp;#38;#62;    &amp;#38;#60;tbody&amp;#38;#62;        &amp;#38;#60;tr&amp;#38;#62;            &amp;#38;#60;td&amp;#38;#62;&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;소극장&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;잔디마당&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;대공연장&amp;#38;#60;/td&amp;#38;#62;        &amp;#38;#60;/tr&amp;#38;#62;        &amp;#38;#60;tr&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;10:00&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td  style=&amp;#34;border: 2px solid red&amp;#34; rowspan=&amp;#34;2&amp;#34;&amp;#38;#62;안녕하신가영&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;10CM&amp;#38;#60;/td&amp;#38;#62;        &amp;#38;#60;/tr&amp;#38;#62;        &amp;#38;#60;tr&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;13:00&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td rowspan=&amp;#34;2&amp;#34; style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;선우정아&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td rowspan=&amp;#34;2&amp;#34; style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;참깨와 솜사탕&amp;#38;#60;/td&amp;#38;#62;        &amp;#38;#60;/tr&amp;#38;#62;        &amp;#38;#60;tr&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;15:00&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td&amp;#38;#62; &amp;#38;#60;/td&amp;#38;#62;        &amp;#38;#60;/tr&amp;#38;#62;        &amp;#38;#60;tr&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;17:00&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;크러쉬&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td&amp;#38;#62;&amp;#38;#60;/td&amp;#38;#62;            &amp;#38;#60;td style=&amp;#34;border: 2px solid red&amp;#34;&amp;#38;#62;폴킴&amp;#38;#60;/td&amp;#38;#62;        &amp;#38;#60;/tr&amp;#38;#62;    &amp;#38;#60;/tbody&amp;#38;#62;    &amp;#38;#60;/table&amp;#38;#62;&amp;#38;#60;/body&amp;#38;#62;&amp;#38;#60;/html&amp;#38;#62;  HTML form 예시(subway.html)```HTML&lt;!DOCTYPE html&gt;                Document        FORM    주문서를 작성해주세요.            이름:                날짜:                        1. 샌드위치 선택        에그마요        비엘티        터키                2. 사이즈 선택        cm                                3. 빵 선택                    허니오트            플랫 브래드            하티 이탈리안                        4. 야채/소스        토마토        오이        할라피뇨        핫 칠리        바베큐                    ```"
  }
  , 
  
  "/articles/web/backend/Django/Django%20%EA%B8%B0%EB%B3%B8.html": {
    title: "Django 기본",
    date: " Feb 14, 2020 ",
    url: "/articles/web/backend/Django/Django%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["DJANGO","PYTHON","BE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: truedjango 프레임워크django의 개념django 폴더 참조  다소 독선적  관용적  정적인 웹을 동적인 웹으로 바꿈  기본적인 구조, 필요한 코드를 제공해줌  유튜브, 나사, 인스타그램 등MVC(Model View Controller), MTV(Model Template View) 패턴  MVC(model view controller) 패턴 : 모든 웹 서비스라면 모두 가지고 있는 디자인 패턴  MTV(model template view) : 파이썬이 바꾼 이름, 힙스터 쉑 ㅋ          모델: 데이터를 관리, 데이터 베이스 그 자체      템플릿 : 사용자가 보는 화면,      뷰 : 중간 관리자 요청을 받아서 모델에게 데이터 찾으라고 명령하고 모델에게서 데이터를 받은뒤 템플릿으로 보냄      ## django_intro 폴더 (프로젝트 폴더) 내부 파일 __init__.py : 패키지로 인식하게 해줌 wsgi.py : 웹서버 게이트웨이 인터페이스 settings.py : 여러가지 설정에 관련된 것 urls.py : flask의 app.route 역할django-admin startapp pages : pages 폴더를 만듬 내부 구조는 상당히 유사함 admin.py : 관리하는 공간 apps.py : 엡에대한 설정 models.py : 데이터베이스에 관련된 것 MㅇTV 관련 testsp.py : 테스팅용 views.py : 중간관리자 MTV 관련 장고는 프로젝트 내부에 여러가지 앱을 만드는 형식settings.py의 Installed_Apps 란에 pages를 등록하면 앱을 등록하는 행위임 (커스텀은 맨위에 놓은 것을 권장, 이름이 같으면 맨위 리스트에 있는 것을 가져옴)USE_I18N =  위에있는 Lanugate_code 를 사용할 것인가?django 생성법과 가상환경설정1. mkdir 13workshop2. cd 13workshop3. python -m venv venv4. source venv/Scripts/activate5. pip install django6. django-admin startproject classroom . (. 안찍으면 같은 이름의 폴더를 추가 후 생성)7. python manage.py runserver선택:.gitignore 파일 안에 설정django-admin startapp pages로 pages 앱 생성 (프로젝트 폴더 바깥에서?)python manage.py startapp your_app_name (앱 생성 2)가상 환경 설정 : 웹 디벨로핑에 필요한 모듈만 python에 쌓는것 (예를들어 jupyter 같은거 필요 없음)python -m venv 폴더이름 : 처음 venv는 버전 환경(version environment)의 준말 해당 디렉토리에 폴더이름이 되어있는 가상환경을 만들어줌디렉토리 안에서 source 폴더이름/Scripts/activate 하면 (또는 거기서 그 파일을 클릭) (venv)라는 키워드가 터미널에 뜸 안에서 쓸모없는 패키지가 없어짐 deactivate 하면 다시 사라짐또는 vscode에서 f1으로 select interpreter을 고른후, venv 환경의  파이썬을 고르면 터미널에서 (venv)가 활성화 되어있음 django-admin startproject django_intro . 로 현재 폴더에 프로젝트 생성  python manage.py runserver 로 서버 돌리기django-admin startapp pages로 pages 앱 생성python manage.py startapp your_app_name (앱 생성 2)django 앱 추가 방법 대략적인 흐름 : 1. urls.py에서 urlpatterns 리스트에 path(&amp;#34;url이름/&amp;#34;, views.url이름), 을 등록 , 2번째 views.url이름은 views.py에 올라가는 함수 이름임, 1번째 url 이름 뒤에 / 필수 (django가 자동으로 마지막에 /를 붙이므로 없으면 절대 접근할수 없음)2. views.py에 url이름과 같은 함수(또는 1단계에 정의한 2번째 인자)를 정의 (request는 무조건 처음 인자에 포함), 3. return으로 render(request, &amp;#39;html이름&amp;#39;, context), context는 넘겨줄 인자들의 dictionary, 4. pages 폴더의 templates폴더내에 url이름.html에 파일 만들고 html 코드 구성, 5. 인터넷에 해당 url로 가보면 완성url로 앱에 변수 넘겨주기 (variable routing)https://서버주소/greeting/윤준석/path(‘greeting//', views.greeting)이런식으로 name 문자열을 넘겨줄 수 있음,이러면 views.py에 함수를 만들때 매개변수로 name 을 문자열로 넘겨주면웹 페이지 url을 통해 해당 변수를 넘겨주면 받을 수 있음str이 아니라 int면 타입이 정수형으로 바뀜path(‘mul//', views.mul)이런식으로 여러개의 인수를 넘겨줄 수도 있으며, 해당 매개변수들을 연산하려면 views.py의 함수 내에서 해야한다.이런식으로 동적인 웹, 데이터베이스 ID에 따른 아이디에 따른 웹페이지 자동생성등에 쓴다.url namespacing  url을 html 파일에 하이퍼링크로 넘겨줄 때, DTL을 이용하여 변수화해서 넘겨줄 수 있다.          namspacing 예시와 기존의 url 할당법(밑)```html&#38;#60;a href=\"{% url 'todos:index' %}\"&#38;#62;전체글보기&#38;#60;/a&#38;#62; &#38;#60;a href=\"/todos/new/\"&#38;#62;새로 게시글 작성하기&#38;#60;/a&#38;#62;&#38;#62;      - 기존의 url 할당 법에서는 /를 맨 앞에 추가하면 root 주소 앞에 해당 주소를 추가한다는 의미이다- /가 없다면 현재 주소에 추가로 /를 붙여서 추가한다는 의미이다&gt; 위 기존 url 할당법의 /가 없는 예시```html&amp;#38;#60;a href=&amp;#34;new/&amp;#34;&amp;#38;#62;새로 게시글 작성하기&amp;#38;#60;/a&amp;#38;#62;      이렇게 하면 기존의 경로에 추가되는 형식, 이것을 신경 쓰기 힘드므로 변수화 (namespacing)한다.        네임 스페이싱을 위해 먼저 해당 앱의 앱 네임을 선언한다          앱네임 선언 예시(urls.py)```pythonfrom django.urls import pathfrom . import views      app_name = 'todos' # 일반적으로 앱 이름과 같은 이름을 넣음urlpatterns = [    path('', views.index, name=\"index\"),    path('new/', views.new, name=\"new\"),# 폼 보여주기    path('create/', views.create, name=\"create\"), # 받은 폼의 정보를 저장하기    path('&#38;#60;int:id&#38;#62;/delete/', views.delete, name=\"delete\"), # 변수로 이용하면 여기서만 / 신경쓰면 됨    # 끝에 /를 꼭 붙여주자.]- 이를 해주면 다른 앱에서 url 변수가 겹쳐도 문제가 생기지 않는다.- 또한 변수화할 path 함수에 name에 변수명을 넣어주면 된다.&gt; variable routing 이 적용된 namespacing 예시```html&amp;#38;#60;a href=&amp;#34;&amp;#123;&amp;#37; url &amp;#39;todos:delete&amp;#39; todo.id &amp;#37;&amp;#125;&amp;#34;&amp;#38;#62;삭제&amp;#38;#60;/a&amp;#38;#62;  홑따옴표 쌍따옴표를 번갈아가서 쓰지 않아도 되나, 그러면 python 문법으로 인해 보기 흉해진다.(권장)  ”{% url ‘앱이름:url변수명’ 매개변수1개, 매개변수 2개..%}”를 하이퍼링크로 넣어주면 된다.  variable routing일 경우 url 변수 넣기 알아보기 (todos의 index.html)    DTL (django template language)    : jinja랑 문법이 상당히 비슷함    for문 예시```html  {% for menu in menus %}  &#38;#60;li&#38;#62;{{forloop.counter}} : {{menu}}&#38;#60;/li&#38;#62;  {% empty %}  &#38;#60;p&#38;#62;리스트가 비어있습니다.&#38;#60;/p&#38;#62;  {% endfor %}&#38;#60;!–forloop.counter:for내부에서만 사용가능함 반복 숫자 표시해줌 enumerate 처럼 쓸수 있음, 문장은 {% 사이에 스페이스바로 띄어서 %}, 변수는 {%안띄움%}{% empty %}는 만약 해당 리스트가 비어있다면 아래 코드가 실행됨–&#38;#62;- 만약 리스트의 원소에 인덱스로 접근하고 싶으면 {{변수.인덱스}}로 접근할 수 있다.- ex) student.0- forloop + first나 last를 치면 처음 원소나 마지막 원소일때 true를 반환한다.&gt; 조건문 예시\t```html  &amp;#38;#60;h3&amp;#38;#62;조건문&amp;#38;#60;/h3&amp;#38;#62;  &amp;#123;&amp;#37; if &amp;#39;한식&amp;#39; in menus &amp;#37;&amp;#125;  &amp;#38;#60;p&amp;#38;#62;역시 한국이라면 한식이지&amp;#38;#60;/p&amp;#38;#62;  &amp;#123;&amp;#37; endif &amp;#37;&amp;#125;  조건문과 else문, elif문, for문과의 중첩 예시```html  {% for menu in menus %}&#38;#60;p&#38;#62;{{forloop.counter}}번째 반복문 도는중…&#38;#60;/p&#38;#62;  {% if '한식' == menu %}  &#38;#60;p&#38;#62;역시 한국인은 한식이지&#38;#60;/p&#38;#62;  {% elif '일식' == menu %}  &#38;#60;p&#38;#62;이것도 불매 운동 해야하나?&#38;#60;/p&#38;#62;  {% else %}  &#38;#60;p&#38;#62;{{menu}}&#38;#60;/p&#38;#62;  {% endif %}  {% endfor %}- DTL 내부에서는 괄호()를 쓸 수 없다.- 왠만하면 판단은 views에서 먼저 하자  1) 함수 안에서 다른 페이지의 폼에서 입력한 데이터 받기 (딕셔너리 형태)request.GET.get('name')- queryDict라는 새로운 형식이므로 GET을 통하여 평범한 dict로 만든 뒤, get() 메서드로 받는것- url에는 _(언더바)를 쓰면 하이퍼링크 적용시 파란 아랫줄 때문에 안보이므로 왠만하면 -(하이픈)을 쓰자&#38;#60;form action=\"/post-pong/\" method=\"POST\"&#38;#62;{% csrf_toekn %}&#38;#60;input&#38;#62; 어쩌구 저쩌구\t&#38;#60;/form&#38;#62;- method 안넣으면 기본 GET 방식, 대소문자 구분 안함- POST를 넣으면 좀더 안전하고 CSRF에 대비하므로 csrf_token 필요,- 이때는 requset.POST.get('name') 방식으로 받아와야함- {% csrf_token %}을 폼안에 넣어서 토큰 부여 하면 암호화? 같은게 됨\t- csrf_token = 랜덤한 문자열을 생성해줌 type=\"hidden\"이 되어서 랜덤 문자열을 인증해줌- 이러면 URL에 해당 데이터들이 보이지 않음, 개발자도구 헤더스에서의 formdata에서  확인 가능- GET은 보내주세요, POST는 처리해주세요 라는 의미- GET : 서버의 데이터를 변화시키지 않음- POST : 서버의 데이터를 변화시킴 (대부분의 데이터 이용에 적합)CSRF : 사이트간 요청 위조 공격,  ##  static 키워드로 파일가져오기, static 하다 : 변화가 없다&gt; base.html block 설정```html&amp;#38;#60;!DOCTYPE html&amp;#38;#62;&amp;#38;#60;html lang=&amp;#34;en&amp;#34;&amp;#38;#62;&amp;#38;#60;head&amp;#38;#62;  &amp;#38;#60;meta charset=&amp;#34;UTF-8&amp;#34;&amp;#38;#62;  &amp;#38;#60;meta name=&amp;#34;viewport&amp;#34; content=&amp;#34;width=device-width, initial-scale=1.0&amp;#34;&amp;#38;#62;  &amp;#38;#60;meta http-equiv=&amp;#34;X-UA-Compatible&amp;#34; content=&amp;#34;ie=edge&amp;#34;&amp;#38;#62;  &amp;#38;#60;title&amp;#38;#62;Document&amp;#38;#60;/title&amp;#38;#62;&amp;#38;#60;/head&amp;#38;#62;&amp;#38;#60;body&amp;#38;#62;&amp;#38;#60;h1&amp;#38;#62;Base is everywhere!&amp;#38;#60;/h1&amp;#38;#62;&amp;#123;&amp;#37; block body1 &amp;#37;&amp;#125;&amp;#123;&amp;#37; endblock &amp;#37;&amp;#125;  &amp;#123;&amp;#37; block body2 &amp;#37;&amp;#125;&amp;#123;&amp;#37; endblock &amp;#37;&amp;#125;  &amp;#38;#60;/body&amp;#38;#62;&amp;#38;#60;/html&amp;#38;#62;  파비콘(favicon) : 웹페이지 이름 옆에 있는 작은 아이콘          파비콘 추가 방법  ```html  앱 내부에 static 폴더 생성, 그 내부에 파비콘으로 쓸 이미지 파일 추가  html 파일 맨 위에 맨 위줄에{% load static %} 추가  html header에 &#38;#60;link rel=\"icon\" href=\"{% static 'static폴더 내의 파일 이름.파일확장자' %}\"&#38;#62; 추가(ico, png 확장자 지원)      &gt; static 키워드의 사용과 block 사용```html&amp;#123;&amp;#37; extends &amp;#39;base.html&amp;#39; &amp;#37;&amp;#125; # base.html에서 block을 가져온다는 의미 한 파일에 1개만 허용&amp;#123;&amp;#37; load static &amp;#37;&amp;#125; &amp;#38;#60;!--폴더 경로는 앱 내의 static폴더의 css폴더 내부--&amp;#38;#62;&amp;#123;&amp;#37; block body1 &amp;#37;&amp;#125; # body1 block 가져오기&amp;#38;#60;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;&amp;#123;&amp;#37; static &amp;#39;css/style.css&amp;#39; &amp;#37;&amp;#125;&amp;#34;&amp;#38;#62;&amp;#38;#60;h1&amp;#38;#62;본오본오&amp;#38;#60;/h1&amp;#38;#62;&amp;#38;#60;img src=&amp;#34;&amp;#123;&amp;#37; static &amp;#39;image/feelthethunder.jpeg&amp;#39; &amp;#37;&amp;#125;&amp;#34; alt=&amp;#34;&amp;#34;&amp;#38;#62;&amp;#123;&amp;#37; endblock &amp;#37;&amp;#125; # body1 block 닫기&amp;#123;&amp;#37; block body2 &amp;#37;&amp;#125;# body2 block 가져오기&amp;#123;&amp;#37; endblock &amp;#37;&amp;#125;# base.html에 2개이상 정의햇을 경우  장고 문법에서 static은 변하지 않을 정보에 붙여서 가져오는데 쓴다.  대부분 서버를 껏다 켜야지 변화가 적용됨, 예외도 있음  load static을 extends 밑에 써서 키워드를 사용할수 있게 가져와야한다.  보통 관례상 폴더 이름은 static으로 한다.3) 만약 다른 앱에도 같은 URL의 path가 있다면 어떻게 할 것인가?  master urls.py, 프로젝트 폴더에 있는 urls.py```from django.contrib import adminfrom django.urls import path, include # django.urls 패키지에 include 꺼내옴from pages import viewspath('pages/', include('pages.urls')), # pages 폴더의 서브 urls.py의 url들을 가져온다, include는 해당 폴더의 해당 파일을 가져오는 함수path('utilities/', include('utilities.urls')), # utilities 폴더의 서브 urls.py의 url들을 가져온다&gt; sub urls.py, pages 폴더(앱)안에 있는 urls.pyfrom django.urls import pathfrom . import views # . 현재 폴더urlpatterns = [    path('ping/', views.ping),    path('pong/', views.pong),    path('post-ping/', views.post_ping),    path('post-pong/', views.post_pong),    path('static-example/', views.static_example),]- 만약 pages의 ping url에 접근하고 싶으면 https://서버 주소/pages/ping/ 으로 접근 가능하다.- 마스터 urls.py가 pages의 urls.py로 권한과 요청을 넘기는 형식### SQL ORM#### DB- 데이터들의 모임- 열, 칼럼, 각 열에는 고유한 데이터 형식이 지정됨, 데이터의 속성- 행, 레코드, 1개의 데이터- 스키마, 틀 각 열의 요소, 즉  들어갈 데이터의 내용을 정의함#### ORM(Object-Relational Mapping)- SQL이 몰라도 파이썬 문법으로 하는 데이터베이스 언어, 파이썬 언어(초안, 청사진)을 SQL 문으로 해석해줌, 중간 번역자### CRUD- C: CREATE, 데이터 생성- R: READ, 데이터 읽기- U: UPDATE, 데이터 변경- D: DELETE, 데이터 지우기- 데이터로 할 수 있는 4가지#### ORM 사용법과 GIT BASH 명령어django-admin startapp posts : 앱 생성&gt; Student 모델 생성 예시 (앱 내부의 models.py)```pythonfrom django.db import models# Create your models here.&amp;#123;: #create-your-models-here&amp;#125;class Student(models.Model):# 각각 colum 정의&amp;#123;: #각각-colum-정의&amp;#125;    name = models.CharField(max_length=64)    email = models.CharField(max_length=128) # 할당할 메모리(max_length) 설정    birthday = models.DateField()    age = models.IntegerField()    def __str__(self): # str 오버로딩하면 데이터베이스에 보여줌        return f&amp;#39;이 학생의 이름은 &amp;#123;self.name&amp;#125;&amp;#39;  model을 바꾸면 다시 migrate를 해줘야 업데이트 되며 왠만하면 바꿔주지말자 (기존에 이미 추가된 데이터들이 말썽을 부린다. 보통 다 날리고 다시 만듦 먼저 모델링 하자!)python manage.py makemigrations : 모델의 0001_initial.py 생성 (번역기로 번역 보내기) (일종의 데이터 표의 헤더 만들기)Migrations for &amp;#39;posts&amp;#39;:  posts\\migrations\\0001_initial.py    - Create model Post # 이렇게 뜸`python manage.py migrate: 파이썬 코드를 SQL 언어로 해석함 (번역본 DB로 보내 명령하기) (일종의 데이터 엑셀 표 만들어 놓기 공간만들기)python manage.py shell (작은 IDE? exit()로 나감) 에서 만든 클래스를 인스턴스화  데이터베이스 데이터 추가 구문```post1 = class이름() 또는 post1 = class이름(열요소=\"내용\",열요소2=\"내용2\")&gt; 데이터베이스 데이터 추가 및 변경 구문class인스턴스.save()하면 SQL에 해당 인스턴스 저장, 또는 데이터 수정 이후 그 객체는 자신이 설정한만큼 데이터속성을 가짐&gt; 데이터 삭제 구문class인스턴스.delete() 하면, 해당 데이터 삭제됨Post.objects.all() : Post 클래스의 모든 객체를 출력 all(1)하면 QuerySet의 1번째 요소를 가져옴Post.objects.get() : 1개의 데이터 객체만 가져옴, get()매개변수에 id=1하면 id가 1인 객체, title=\"내용\" 하면 title안에 \"내용\"인 데이터 객체, 일치하는 객체가 2개 이상 있으면 에러냄Post.objects.filter() : 괄호안에 조건과 일치하는 모든 데이터를 가져옴 ex) title=\"hello\", 인 모든 객체 가져옴SQlite 익스텐션 깔고 SQLite explorer에서 클릭하여 데이터베이스를 클릭할 수 있음django_extensions 패키지 = 알아서 데이터베이스의 정보를 임포트 해옴id는 지워져도 해당 아이디는 다시 쓰이지 않음,query= list의 성질과 거의 비슷함### admin 생성하기python manage.py createsuperuser유저 네임, 이메일 어드레스, 패스워드 입력, 패스워드 확인 후http://서버주소/admin이 곳에서 들어가서 로그인 하면 관리자 페이지로 감from .models import 대상 모델```python# admin.py에서 admin.site.register(대상 모델)이후 다시 관리자 페이지로 들어가면 대상 데이터 베이스가 올라가 있고, 내용 확인 가능from django.shortcuts import redirectreturn redirect(&amp;#39;/url/&amp;#39;)# 가고싶은 서버내 웹페이지로 보냄MODEL.objects.order_by(‘요소’).all() : 해당 요소의 정렬 순으로, 반대로 원할 경우 ‘-요소’로 넣기QnA 달기foreign key = 댓글이 가지고 있는 외래 아이디, 질문의  id를 가지고 있어서 해당 질문에 속하게 만들 수 있다  answer 모델 예시```class Answer(models.Model):    content = models.CharField(max_length=100)    # CASCADE : question이 사라지면 그 밑에도 모두 지우겠다는 의미    question = models.ForeignKey(Question, on_delete=models.CASCADE)- restfull한 url : 현재 경로와 자원이 표시되어있는 url&gt; main 페이지```html&amp;#123;&amp;#37; extends &amp;#39;base.html&amp;#39; &amp;#37;&amp;#125;&amp;#123;&amp;#37; block body &amp;#37;&amp;#125;&amp;#123;&amp;#37; for question in questions &amp;#37;&amp;#125;&amp;#38;#60;h1&amp;#38;#62;&amp;#123;&amp;#123;question.title&amp;#125;&amp;#125;&amp;#38;#60;/h1&amp;#38;#62;&amp;#38;#60;p&amp;#38;#62;&amp;#123;&amp;#123;question.user&amp;#125;&amp;#125;&amp;#38;#60;/p&amp;#38;#62;&amp;#38;#60;p&amp;#38;#62;&amp;#123;&amp;#123;question.content&amp;#125;&amp;#125;&amp;#38;#60;/p&amp;#38;#62;&amp;#38;#60;form action=&amp;#34;/questions/&amp;#123;&amp;#123;question.id&amp;#125;&amp;#125;/answers/create/&amp;#34;&amp;#38;#62;  &amp;#38;#60;input type=&amp;#34;text&amp;#34; name=&amp;#34;content&amp;#34;&amp;#38;#62;  &amp;#38;#60;input type=&amp;#34;submit&amp;#34;&amp;#38;#62;&amp;#38;#60;/form&amp;#38;#62;&amp;#123;&amp;#37; for answer in question.answer_set.all &amp;#37;&amp;#125; &amp;#38;#60;!--models.ForeignKey가 만들어준 answer_set 함수, 이를 통해 이 question 모델을 foreignkey로 가지고 있는 모든 answer 모델을 찾아올 수 있다.--&amp;#38;#62;&amp;#38;#60;p&amp;#38;#62;&amp;#123;&amp;#123;answer.content&amp;#125;&amp;#125;&amp;#38;#60;/p&amp;#38;#62;&amp;#123;&amp;#37; endfor &amp;#37;&amp;#125;&amp;#38;#60;hr&amp;#38;#62;&amp;#123;&amp;#37; endfor &amp;#37;&amp;#125;&amp;#123;&amp;#37; endblock &amp;#37;&amp;#125;ERD ( Entity Relationship Diagram)vscode = U 아직 깃에 안올라갔다. M : 무언가 바뀌었다. A: add 됬다.git add (올릴파일이나 폴더)git status (commit한 상태 보기)git restore –staged (add 취소할 파일)Nosql :if 문을 이용한 restfull한 웹코딩  urls.py```python…path('add/', views.add, name=\"add\" ), # if 분기로 하나의 경로로 2가지 방식의 일을 처리(중요)path('&#38;#60;int:id&#38;#62;/update/', views.update, name=\"update\"),&gt; restfull code 예시 (create와 new path를 합친 add path)```pythondef add(request): # if 분기로 하나의 경로로 2가지 방식의 일을 처리, form에 action=&amp;#39;&amp;#39;이면 자기자리로 돌아가는 것을 이용(restfull한 코드))(중요)    if request.method == &amp;#34;POST&amp;#34;: # method가 post로 보내지면 create로             author = request.POST.get(&amp;#39;author&amp;#39;)        title = request.POST.get(&amp;#39;title&amp;#39;) # url로 보내는 것은 무조건 get 방식         content = request.POST.get(&amp;#39;content&amp;#39;)        due_date = request.POST.get(&amp;#39;due-date&amp;#39;)           todo = Todo.objects.create(author=author, title=title, content=content, due_date=due_date)        return redirect(&amp;#39;todos:index&amp;#39;) # /있으면 루트주소에 추가, 없으면 현재 주소 앞에 추가    else:   # Get방식이면 데이터를 form으로 받는 new로        return render(request, &amp;#39;add.html&amp;#39;)  django는 보내는 method로 delete와 put은 지원 안함(HTTP5도 지원 안함)  다른 프레임워크에서 일단 post로 보내고 delete put을 넣을 때는 input type= hidden에  name=’_method’, value=”DELETE” 로 숨겨서 보냄 (HTTP5가 지원 안하므로)  restfull한 코딩을 하려면 주소창에 동사는 빼고 명사로만 쓰며, 동사는 method 방법을 바꿔서 해결  다른 프레임 워크는 http verb라는 개념도 있다.          add.html```html{% extends 'base.html' %}      {% block body %}&#38;#60;form action=\"\" method=\"POST\"&#38;#62;   {% csrf_token %}  작성자 : &#38;#60;input type=\"text\" name=\"author\"&#38;#62;  제목 : &#38;#60;input type=\"text\" name=\"title\"&#38;#62;  내용 : &#38;#60;input type=\"text\" name=\"content\"&#38;#62;  마감일 : &#38;#60;input type=\"date\" name=\"due-date\"&#38;#62;  &#38;#60;input type=\"submit\" value=\"저장\"&#38;#62;&#38;#60;/form&#38;#62;{% endblock %}&#38;#60;!–먄약 기본값 value를 정하고 싶으면 &#38;#60;input type=\"date\" name=\"due-date\" value=\"{{todo.due_date|date:'Y-m-d'}}\"&#38;#62; 이런식으로 –&#38;#62;- post, get 방식을 이용해 이 폼(add.html) 하나를 수정할 때 쓰는 폼으로도 사용할 수 있다.## django 이미지 업로딩과 이미지 리사이징### 이미지 보여주기1. 위 static 부분의 favicon 부분 참조#### fontawesome 이용하기1. 먼저 fontawesome 홈페이지에가서 로그인한다.2. 키를 발급받고 script를 html 폴더의 header안에 넣는다.```html&amp;#38;#60;script src=&amp;#34;https://kit.fontawesome.com/e87731a046.js&amp;#34; crossorigin=&amp;#34;anonymous&amp;#34;&amp;#38;#62;&amp;#38;#60;/script&amp;#38;#62;  원하는 곳에 원하는 아이콘 태그를 삽입한다.```html&#38;#60;i class=\"fas fa-chess-knight\"&#38;#62;### 이미지 업로딩1. 먼저 앱 내부의 models.py에서 데이터베이스 모델을 만든다.&gt; models.py 파일```pythonfrom django.db import models# Create your models here.class Feed(models.Model):    content = models.CharField(max_length=150);     created_at = models.DateTimeField(auto_now_add=True) # 자동으로 현재 날짜 입력    image = models.ImageField() -이를 makemigrations하려하면 다음과 같은 오류가 뜰 수 도 있다.```git bashYou are trying to add a non-nullable field ‘img’ to todo without a default; we can’t do that (the databaseneeds something to populate existing rows).Please select a fix: 1) Provide a one-off default now (will be set on all existing rows with a null value for this column) 2) Quit, and let me add a default in models.pySelect an option:- 이 때는 db에서 해당 모델을 날리고 다시 makemigrations하면 된다. 앱 내부의 migrations 폴더 내부의 (ex)0001_initial.py)1) 2. ### 이미지 리사이징인스톨 필요한 패키지 : django-imagekit, Pillow, IPython## django form을 이용한 자동 폼 생성### django form 선언#### models.py```pythonfrom django.db import models# Create your models here.class Movie(models.Model):    title = models.CharField(max_length=50)    title_en = models.CharField(max_length=50)    audience = models.IntegerField()    open_date = models.DateField()    genre = models.CharField(max_length=50)    watch_grade = models.CharField(max_length=50)    score = models.FloatField()       poster_url = models.TextField()    description = models.TextField()class Comment(models.Model):    content = models.TextField()    movie = models.ForeignKey(Movie,on_delete=models.CASCADE)    class Meta:        ordering = (&amp;#39;-id&amp;#39;,) # 정렬을 id의 역순(-)으로 출력 , ,꼭 필요form.pyfrom django import formsfrom .models import Movie, Commentclass MovieForm(forms.Form): # forms.Form 상속    title = forms.CharField(max_length=50)    title_en = forms.CharField(                    max_length=50,                    label=&amp;#34;영문 제목&amp;#34;, # 기본값에서 원하는 라벨이름 바꿔줌                    widget=forms.TextInput(                        attrs=&amp;#123;                            &amp;#39;placeholder&amp;#39;: &amp;#39;영문제목을 입력해주세요&amp;#39;,                        &amp;#125;                    )                )    audience = forms.IntegerField()    open_date = forms.DateField(widget=forms.DateInput(attrs=&amp;#123;&amp;#39;type&amp;#39;:&amp;#39;date&amp;#39;&amp;#125;))    genre = forms.CharField(max_length=50)    watch_grade = forms.CharField(max_length=50)    score = forms.FloatField()       poster_url = forms.CharField(widget=forms.Textarea) # textarea로 바꿔줌, django가 charfield를 기본으로 잡고 있어서 이거 써야함    description = forms.CharField(widget=forms.Textarea)    # widget: 여러가지 커스터마이징을 하게 해줌 attrs, html tag에 값을 추가해줌class MovieModelForm(forms.ModelForm): # Model에 관계있는 Form으로 상속, 자동으로 모델 내부의 column 만큼 폼을 만들어줌    open_date = forms.DateField(widget=forms.DateInput(attrs=&amp;#123;&amp;#39;type&amp;#39;:&amp;#39;date&amp;#39;&amp;#125;)) # 이런 방법으로 일부는 다른 방식으로 만들어낼 수 있음 오버라이딩?        class Meta: # 속성으로 만들면 column으로 인지하므로 class 객체로 만들어야 함        model = Movie # 모델 입력        fields = &amp;#39;__all__&amp;#39; # 전부class CommentModelForm(forms.ModelForm):    class Meta:        model = Comment        fields = (&amp;#39;content&amp;#39;,) # 출력할 부분만 tuple로 넣어주면 됨, 끝에 &amp;#39;,&amp;#39; 무조건 넣어야함  이것을 이용하여 form을 자동 생성할 수 있다.  ModelForm을 이용하면 form input 요소 또한 자동으로 만들어 준다.    django form 이용    form.html    ```html{% extends 'base.html' %}{% load bootstrap4 %}{% block body %}{% if request.resolver_match.url_name  == \"create_model_form\" %} {% comment %} 현재 사이트로 정보를 보낸 views.py에서 정의한 함수 이름을 가져옴 {% endcomment%}  &#38;#60;h1&#38;#62;Create&#38;#60;/h1&#38;#62;{% else %}  &#38;#60;h1&#38;#62;Update&#38;#60;/h1&#38;#62;{% endif %}&#38;#60;form action=\"\" method=\"POST\"&#38;#62;  {% csrf_token %}  {% bootstrap_form form %}  {% buttons submit=\"제출\" %}  {% endbuttons %}    {% comment %}  # html {{form.as_p}}하면 p태그로 묶여서 출력됨  # as_ul 하면 ul 태그로 묶여서 출력됨  # as_table * table 태그안에 넣으면 * 테이블 형태로 출력  # {{form.title.label_tag}} 하면 해당 요소 이름의 label이 생김    &#38;#60;table&#38;#62;          {{form.as_table}}  &#38;#60;/table&#38;#62;  &#38;#60;input type=\"submit\"&#38;#62;  {% endcomment %}    &#38;#60;/form&#38;#62;  {% endblock %}  #### base.html```python&amp;#123;&amp;#37; load bootstrap4 &amp;#37;&amp;#125; &amp;#123;&amp;#37; comment &amp;#37;&amp;#125; 현재 페이지에서만 동작함 다른 block은 extends 아래 block위에서 할 것 &amp;#123;&amp;#37; endcomment&amp;#37;&amp;#125;&amp;#38;#60;!DOCTYPE html&amp;#38;#62;&amp;#38;#60;html lang=&amp;#34;ko&amp;#34;&amp;#38;#62;&amp;#38;#60;head&amp;#38;#62;  &amp;#38;#60;meta charset=&amp;#34;UTF-8&amp;#34;&amp;#38;#62;  &amp;#38;#60;meta name=&amp;#34;viewport&amp;#34; content=&amp;#34;width=device-width, initial-scale=1.0&amp;#34;&amp;#38;#62;  &amp;#38;#60;meta http-equiv=&amp;#34;X-UA-Compatible&amp;#34; content=&amp;#34;ie=edge&amp;#34;&amp;#38;#62;  &amp;#38;#60;title&amp;#38;#62;formmovie&amp;#38;#60;/title&amp;#38;#62;  &amp;#123;&amp;#37; bootstrap_css &amp;#37;&amp;#125;&amp;#38;#60;/head&amp;#38;#62;&amp;#38;#60;body&amp;#38;#62;  &amp;#38;#60;a href=&amp;#34;&amp;#123;&amp;#37; url &amp;#39;movies:index&amp;#39; &amp;#37;&amp;#125;&amp;#34;&amp;#38;#62;홈&amp;#38;#60;/a&amp;#38;#62;  &amp;#38;#60;a href=&amp;#34;&amp;#123;&amp;#37; url &amp;#39;movies:create&amp;#39; &amp;#37;&amp;#125;&amp;#34;&amp;#38;#62;글쓰기&amp;#38;#60;/a&amp;#38;#62;  &amp;#38;#60;a href=&amp;#34;&amp;#123;&amp;#37; url &amp;#39;movies:create_model_form&amp;#39; &amp;#37;&amp;#125;&amp;#34;&amp;#38;#62;글쓰기(모델폼)&amp;#38;#60;/a&amp;#38;#62;  &amp;#38;#60;div class=&amp;#34;container&amp;#34;&amp;#38;#62;    &amp;#123;&amp;#37; block body &amp;#37;&amp;#125;    &amp;#123;&amp;#37; endblock &amp;#37;&amp;#125;  &amp;#38;#60;/div&amp;#38;#62;  &amp;#123;&amp;#37; bootstrap_javascript jquery=&amp;#39;full&amp;#39; &amp;#37;&amp;#125;&amp;#38;#60;/body&amp;#38;#62;&amp;#38;#60;/html&amp;#38;#62;  pip install django-bootstrap4 이후, settings.py의 INSTALLED_APPS에 ‘bootstrap4’,를 추가하고, {% load bootstrap4 %}를 추가하여 사용한다.  이외에도 {% bootstrap_javascript jquery=’full’ %}, {% bootstrap_css %}를 각각 body와 header에 추가하여야 한다.  {% comment %} 를 이용하면 DTL이 무시하는 주석을 달 수 있다.{% endcomment%}    detail.html    ```python{% extends 'base.html' %}{% load bootstrap4 %}{% block body %}&#38;#60;br&#38;#62;{{movie.title}}{{movie.title_en}}{{movie.audience}}{{movie.open_date}}{{movie.poster_url}}&#38;#60;form action=\"{% url 'movies:delete' movie.id %}\" method=\"POST\"&#38;#62;{% csrf_token %}&#38;#60;input type=\"submit\" value=\"삭제(post)\"&#38;#62;&#38;#60;/form&#38;#62;{% comment %}&#38;#60;a href=\"{% url 'movies:delete' movie.id %}\"&#38;#62;삭제해라 애송이&#38;#60;/a&#38;#62;요것은 주석 다는방법{% endcomment %}&#38;#60;a href=\"{% url 'movies:update' movie.id %}\"&#38;#62;수정&#38;#60;/a&#38;#62;&#38;#60;a href=\"{% url 'movies:update_model_form' movie.id %}\"&#38;#62;수정(모델폼)&#38;#60;/a&#38;#62;{% for comment in movie.comment_set.all %}{{comment.content}}&#38;#60;a href=\"{% url 'movies:comment_delete' movie.id comment.id %}\"&#38;#62;삭제&#38;#60;/a&#38;#62;&#38;#60;br&#38;#62;{% endfor %}&#38;#60;form action=\"{% url 'movies:comment_create' movie.id %}\" method=\"POST\"&#38;#62;{% csrf_token %}{% bootstrap_form comment_form %} {% comment %}forms.py에서 field=(content,) 도 가능하지만이방법도 됨{% bootstrap_form comment_form exclude='movie'%}{% endcomment %}{% buttons submit=\"제출\" %}{% endbuttons %}&#38;#60;/form&#38;#62;{% endblock %}  #### views.py```pythonfrom django.shortcuts import render, redirect, get_object_or_404from .forms import MovieForm, MovieModelForm, CommentModelFormfrom IPython import embed # 웹 디버깅용from .models import Movie, Comment# Create your views here.def index(request):    movies = Movie.objects.all().order_by(&amp;#39;-id&amp;#39;) # all() 생략가능    context = &amp;#123;        &amp;#39;movies&amp;#39;: movies    &amp;#125;    return render(request, &amp;#39;index.html&amp;#39;, context)def create(request):    if request.method == &amp;#34;POST&amp;#34;:        form = MovieForm(request.POST)               if form.is_valid(): # 백엔드에서 한번 더 검증            movie = Movie() # form.cleaned_data : 검증이 완료, 가공이 완료된 데이터 (앞의 빈공간을 지우는 등의 행동)            movie.title = form.cleaned_data.get(&amp;#39;title&amp;#39;)            movie.title_en = form.cleaned_data.get(&amp;#39;title_en&amp;#39;)            movie.audience = form.cleaned_data.get(&amp;#39;audience&amp;#39;)            movie.open_date = form.cleaned_data.get(&amp;#39;open_date&amp;#39;)            movie.genre = form.cleaned_data.get(&amp;#39;genre&amp;#39;)            movie.watch_grade = form.cleaned_data.get(&amp;#39;watch_grade&amp;#39;)            movie.score = form.cleaned_data.get(&amp;#39;score&amp;#39;)             movie.poster_url = form.cleaned_data.get(&amp;#39;poster_url&amp;#39;)            movie.description = form.cleaned_data.get(&amp;#39;title_en&amp;#39;)            movie.save()            return redirect(&amp;#39;movies:index&amp;#39;)    else:        form = MovieForm()    context = &amp;#123;        &amp;#39;form&amp;#39; : form    &amp;#125;    return render(request, &amp;#39;form.html&amp;#39;, context)def detail(request, id):    # movie = Movie.objects.get(id=id)    movie = get_object_or_404(Movie, id=id) # 잘못된 접근이면 404 에러를 뜨게 해줌    comment_form = CommentModelForm()    context = &amp;#123;        &amp;#39;movie&amp;#39;: movie,        &amp;#39;comment_form&amp;#39;: comment_form,    &amp;#125;    return render(request, &amp;#39;detail.html&amp;#39;, context)def delete(request, id):    movie = get_object_or_404(Movie, id=id)    if request.method == &amp;#34;POST&amp;#34;: # POST 요청일 시에만 삭제가 가능하도록 만듦 (즉 url을 통한 접근이 아닌 정상적인 접근일시)        movie.delete()        return redirect(&amp;#34;movies:index&amp;#34;)    else:        return redirect(&amp;#34;movies:detail&amp;#34;, id)def update(request, id):    movie = get_object_or_404(Movie, id=id)    if request.method == &amp;#34;POST&amp;#34;:        form = MovieForm(request.POST)        if form.is_valid():            movie.title = form.cleaned_data.get(&amp;#39;title&amp;#39;)            movie.title_en = form.cleaned_data.get(&amp;#39;title_en&amp;#39;)            movie.audience = form.cleaned_data.get(&amp;#39;audience&amp;#39;)            movie.open_date = form.cleaned_data.get(&amp;#39;open_date&amp;#39;)            movie.genre = form.cleaned_data.get(&amp;#39;genre&amp;#39;)            movie.watch_grade = form.cleaned_data.get(&amp;#39;watch_grade&amp;#39;)            movie.score = form.cleaned_data.get(&amp;#39;score&amp;#39;)             movie.poster_url = form.cleaned_data.get(&amp;#39;poster_url&amp;#39;)            movie.description = form.cleaned_data.get(&amp;#39;title_en&amp;#39;)            movie.save()            return redirect(&amp;#39;movies:detail&amp;#39;, id)    else:        form = MovieForm(initial=movie.__dict__) # initial과 해당 객체의 __dict__를 이용해 기존 데이터를 가져올 수 있음    context = &amp;#123;        &amp;#39;form&amp;#39;: form,    &amp;#125;    return render(request, &amp;#39;form.html&amp;#39;, context)def create_model_form(request):    if request.method == &amp;#34;POST&amp;#34;:                form = MovieModelForm(request.POST)        if form.is_valid():            movie = form.save() # form.save()는 movie 객체를 리턴한다.            return redirect(&amp;#39;movies:detail&amp;#39;, movie.id)    else:        form = MovieModelForm()    context = &amp;#123;        &amp;#39;form&amp;#39;: form    &amp;#125;    return render(request, &amp;#39;form.html&amp;#39;, context)def update_model_form(request, id):    movie = get_object_or_404(Movie, id=id) # Movie 모델의 id값이 id인 객체를 가져와라    if request.method == &amp;#34;POST&amp;#34;:        form = MovieModelForm(request.POST, instance=movie) # 이런 방법으로 과거의 정보와 수정된 이후의 정보도 가져올 수 있음        if form.is_valid(): # 이를 통해 그 두가지 정보를 검증할 수 있음            form.save()            return redirect(&amp;#39;movies:detail&amp;#39;, id)    else:        form = MovieModelForm(instance = movie) # modelForm을 상속 받은 form은 instanace로 해야함    context = &amp;#123;        &amp;#39;form&amp;#39;: form    &amp;#125;    return render(request, &amp;#39;form.html&amp;#39;, context)def comment_create(request, movie_id):    movie = get_object_or_404(Movie, id=movie_id)    if request.method == &amp;#34;POST&amp;#34;:        form = CommentModelForm(request.POST)        if form.is_valid():            comment = form.save(commit=False) # 바로 save하지 않고 기다림            comment.movie = movie # movie 정보 주입            comment.save() # save함            return redirect(&amp;#39;movies:detail&amp;#39;, movie_id)        else:            return redirect(&amp;#39;movies:detail&amp;#39;, movie_id)    else:        return redirect(&amp;#39;movies:detail&amp;#39;, movie_id)def comment_delete(request, movie_id, comment_id):    comment = Comment.objects.get(id=comment_id)    comment.delete()    return redirect(&amp;#39;movies:detail&amp;#39;, movie_id)#  POST 접근과 GET 접근의 차이를 이용하여 valid한 접근 구별가능  Form을 이용하여 update구현"
  }
  , 
  
  "/articles/web/backend/Django/Django%20WSGI%20%EA%B8%B0%EB%B3%B8.html": {
    title: "Django WSGI 기본",
    date: " Feb 15, 2020 ",
    url: "/articles/web/backend/Django/Django%20WSGI%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["DJANGO","PYTHON","BE"],
    content: "WSGI(Web Server Gateway Interface)WSGI란?Django 같은 Python 계열 웹 어플리케이션 서버에서 웹 서버와 통신할 때, 미들웨어로 작용하며,      웹서버와 웹 어플리케이션 서버의 호환성 관계없이 통신할 수 있게 해주고,        여러 워커를 통해 요청을 동기적, 혹은 비동기적으로 빠르게 처리하게 도와준다.        특히 Python으로 돌아가는 서버의 경우 python 코드를 불러와 로딩하는데 많은 시간이 걸리는데, 이를 프리 포킹(pre-forking)를 통해 미리 불러오고 공유하여 성능을 향상시킨다.        확장된 기능으로 Nginx와 같은 웹서버와 같은 역할을 맡기도 한다.  Gunicorn과 uWSGI가 존재하며, 추가로 비동기적 처리를 지원하는 ASGI(Asynchronous Server Gateway Interface)가 존재한다.Setting up Django and your web server with uWSGI and nginx 참조uwsgi 예제pip install uwsgi위 코드를 이용해 virutalenv 설정 후 다운로드한 후 아래와 같이 uwsgi.ini 설정 파일을 만든다.[uwsgi]socket = /srv/docker-server/movie_backend.sock // 생성할 소켓파일 위치 되도록이면 절대 위치 기입master = true // 마스터 서버인가?processes = 1threads = 2chdir = /srv/docker-server/// 프로젝트 위치module = movie_backend.wsgi// django의 wsgi 연결 모듈 위치logto = /var/log/uwsgi/uwsgi.log// 로깅 장소log-reopen = true vaccum = true// 종료 시 관련한 생성 파일 삭제  기타 추가적인 설정은 Configuring uWSGI — uWSGI 2.0 documentation 확인위 uwsgi.ini파일의 주석을 전부 지우고 프로젝트 폴더에 넣는다.uwsgi --ini uwsgi.ini.ini의 파일 경로를 제대로 설정하자.Attaching to django, nginxdjango  | [uWSGI] getting INI configuration from uwsgi.inidjango와 제대로 연결되었다면 상단과 같은 메시지가 나타나게 된다."
  }
  , 
  
  "/articles/etc/etcs/Nodemon%20%ED%95%99%EC%8A%B5.html": {
    title: "Nodemon 학습",
    date: " Feb 16, 2020 ",
    url: "/articles/etc/etcs/Nodemon%20%ED%95%99%EC%8A%B5.html",
    tags: ["JS","NODE","TOOL"],
    content: "Nodemonnode.js 기반 응용프로그램의 코드가 변경시 자동으로 서버를 재시작해주는 도구.주로 개발 단계에서 일일이 서버를 재시작 하는 수고를 덜어주어 디버깅과 개발을 용이하게 해준다. npm install --save-dev nodemon # or using yarn: yarn add nodemon -D 위와 같이 설치 후, 아래와 같이 기존의 서버 실행 스크립트에 씌워주는 형식이다.nodemon node index.js localhost 8080보통은 package.json의 scripts 항목에 추가하여 간단하게 사용한다.// package.json의 일부 예시...&amp;#34;scripts&amp;#34;: &amp;#123;    &amp;#34;gen-env&amp;#34;: &amp;#34;gen-env-types .env -o src/env.d.ts -e .&amp;#34;,    &amp;#34;build&amp;#34;: &amp;#34;tsc&amp;#34;,    &amp;#34;watch&amp;#34;: &amp;#34;tsc -w&amp;#34;,    &amp;#34;dev&amp;#34;: &amp;#34;nodemon dist/index.js&amp;#34;,    &amp;#34;dev2&amp;#34;: &amp;#34;nodemon --exec ts-node src/index.ts&amp;#34;,    &amp;#34;start&amp;#34;: &amp;#34;nodemon dist/index.js&amp;#34;,    &amp;#34;start2&amp;#34;: &amp;#34;ts-node src/index.ts&amp;#34;,    &amp;#34;test&amp;#34;: &amp;#34;echo \\&amp;#34;Error: no test specified\\&amp;#34; &amp;#38;&amp;#38; exit 1&amp;#34;,    &amp;#34;create:migration&amp;#34;: &amp;#34;mikro-orm migration:create&amp;#34;  &amp;#125;,...만약 수동적으로 재시작하고 싶다면, 콘솔에 rs를 입력하면 nodemon이 재시작 된다.추가적인 Nodemon 설정 (재시작 지연시간, 변경 무시 파일) 등은 nodemon.json이라는 따로 설정 파일을 놓을 수 있으나, 한꺼번에 node.js 설정을 관리하고 싶으면, 아래와 같이, package.json에 nodemonConfig 항목을 추가하여 설정해줄 수 있다.&amp;#123;  &amp;#34;name&amp;#34;: &amp;#34;nodemon&amp;#34;,  &amp;#34;homepage&amp;#34;: &amp;#34;http://nodemon.io&amp;#34;,  &amp;#34;...&amp;#34;: &amp;#34;... other standard package.json values&amp;#34;,  &amp;#34;nodemonConfig&amp;#34;: &amp;#123;    &amp;#34;ignore&amp;#34;: [&amp;#34;test/*&amp;#34;, &amp;#34;docs/*&amp;#34;],    &amp;#34;delay&amp;#34;: 2500  &amp;#125;&amp;#125;--watch 플래그를 통해 여러 경로의 파일의 변화를 감시할 수 있다.nodemon --watch app --watch libs app/server.js다음과 같은 형태로 node.js 기반이 아니어도 실행이 가능하다.nodemon --exec &amp;#34;python -v&amp;#34; ./app.py"
  }
  , 
  
  "/articles/web/backend/Django/Django%20CRUD.html": {
    title: "Django CRUD",
    date: " Feb 16, 2020 ",
    url: "/articles/web/backend/Django/Django%20CRUD.html",
    tags: ["DJANGO","PYTHON","BE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueDjango CRUD(Create, Read, Update, Delete ) 만들기Django 프로젝트 생성과 가상 환경 설정  mkdir 대상폴더이름, cd 대상폴더 이름으로 폴더를 생성하고 이동한다  python -m venv venv로 해당 디렉토리에 폴더이름의 가상환경을 만들어줌  source venv/Scripts/activate로 터미널을 가상환경에 들어가게 한다. (deactivate로 나감)          또는 vsCode에서 f1으로 select interpreter를 통하여 가상환경 들어가기 가능        pip install django로 가상환경 내에 django를 설치한다.  django-admin startproject 프로젝트명 . 으로 현재 폴더 내에 프로젝트를 만든다          마지막 .이 없으면 프로젝트명으로 된 폴더를 만들고 그 내부에 만들게 된다.        python manage.py runserver로 서버를 돌리고 확인해본다.          http://127.0.0.1:8000/ 가 기본 주소 , 터미널에서 Ctrl + C로 서버를 닫는다      터미널에 code .을 치면 해당 디렉토리로 VS Code IDE가 켜짐(버그 있음)        Django 앱 생성과 추가              대상 폴더(보통 프로젝트 폴더와 동일 위치)에서 django-admin startapp 앱이름 으로 앱 생성          python manage.py startapp 앱이름 으로도 가능        프로젝트명 폴더 내부의 settings.py의 INSTALLED_APPS 리스트에 앱이름을 추가          보통 커스텀 app은 맨 위에 놓으며 동명일시 맨 위의 app이 우선이다.(‘,’ 추가 주의)                  myCRUD 프로젝트 폴더 내부의 settings.py의 일부 ```python # Application definition                      INSTALLED_APPS = [ 'CRUD', # 앱 이름 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles',]  3. 동일한 폴더의 urls.py 파일 내부의 urlpatterns 리스트에 path()함수를 이용해 해당 앱 URL 추가\t- 보통 path('앱이름/', include('앱이름.urls')), 를 추가한다 (뒤의 / 추가 필수!)\t- 굳이 앱이름을 쓸필요 없지만 관례와 효율 상 그렇게 한다.\t- include()함수는 django.urls에서 임포트 해온다.(from django.urls import path, include)\t&gt; 프로젝트 폴더의 urls.py의 일부```pythonfrom django.contrib import adminfrom django.urls import path, includeurlpatterns = [    path(&amp;#39;admin/&amp;#39;, admin.site.urls),    path(&amp;#39;CRUD/&amp;#39;, include(&amp;#39;CRUD.urls&amp;#39;)),]  해당 앱 폴더 내부에 urls.py 파일을 생성하고, urlpatterns 리스트를 만든다  필요 모듈들 (django.urls의 path함수, 해당 폴더(.)의 views 파일 등)을 임포트          앱 폴더 내부의 새로만든 urls.py 예시```pythonfrom django.urls import pathfrom . import views      app_name = 'CRUD' # 일반적으로 앱 이름과 같은 이름을 넣음urlpatterns = [    path('create/', views.create, name=\"create\"),    # variable routing + namespacing    path('&#38;#60;int:id&#38;#62;/update/', views.update, name=\"update\"),    # 기타 필요한 path 추가]## model에 데이터베이스 모델 추가1. 해당 앱 폴더 내부의 models.py 파일에 원하는 이름의 모델 클래스를 만든다.\t- 이 클래스는 models.Model을 상속받아야한다.2. 내부의 클래스 변수로 각각의 정보 필드를 설정한다.\t- models.CharField(max_length=)는 보통 짧은 내용의 문자열 정보에 사용하며, max_length라는 최대 저장 공간을 미리 할당해줘야 한다.\t- models.TextFiled()는 보통 본문 같은 긴 길이의 문자열 정보에 사용, max_length 할당이 필요 없다\t- models.DateField()는 날짜와 시간 정보에 사용한다.\t- models.BooleanField()는 참, 거짓으로 구분하는 정보에 사용한다.&gt; 해당 앱 폴더의 models.py 예시```pythonfrom django.db import models# Create your models here.class TODO(models.Model): # models.Model 상속해주기 필수    title = models.CharField(max_length=50)    content = models.TextField()    due_date = models.DateField()    check = models.BooleanField()        def __str__(self): # __str__을 오버로딩하면 admin의 데이터베이스에 가시성좋게 보여줄 수 있다.        return f&amp;#39;&amp;#123;self.due_date&amp;#125;까지 &amp;#123;self.title&amp;#125;일을 해야합니다. 내용 : &amp;#123;self.content&amp;#125;, 행동 여부 : &amp;#123;self.check&amp;#125;&amp;#39;  해당 데이터필드의 종류를 잘 설정해줘야, 나중에 form 자동 생성기능을 사용할 때 적절하게 생성된다.          모델이 완성되면 터미널에서 python manage.py makemigrations를 통하여 모델을 SQL 번역기로 보낸다. (일종의 데이터표 헤더 만들기)              이때 0001_initial.py가 생성되며, 지우면 다시 만들어줘야 한다.                  python manage.py migrate를 통해 SQL로 해석한다. (일종의 데이터 표 그리기)                    이때 모델링을 신중히하여 나중에 모델을 수정하는 일을 없도록 하자.      모델링 수정시 해당 모델 파일(0001_initial.py)를 지우고 다시 만들어야 한다.      만약 기존에 데이터베이스에 해당 모델의 정보가 있으면 문제를 일으킨다. (다 지우고 새로 바꿔주는게 나음)        기본 html 파일 설정                  해당 앱 폴더 내부에 templates 라는 이름으로 새 폴더를 만든다.          내부 폴더에 base.html을 만들고 !+tab으로 기본 형식을 잡는다.          &lt;body&gt;태그 사이에 {% block body %}와 {% endblock %} DTL 문법을 넣는다.                    이때 원한다면 bootstrap을 추가하거나 여러가지를 꾸민다.                  base.html 파일 예시```html&#38;#60;!DOCTYPE html&#38;#62;&#38;#60;html lang=\"en\"&#38;#62;&#38;#60;head&#38;#62;&#38;#60;meta charset=\"UTF-8\"&#38;#62;&#38;#60;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&#38;#62;&#38;#60;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&#38;#62;&#38;#60;title&#38;#62;MyOwnSuckyBulletin&#38;#60;/title&#38;#62;&#38;#60;/head&#38;#62;&#38;#60;body&#38;#62;{% block body %}{% endblock %}{% block body2 %}{% endblock %}&#38;#60;!–이런 식으로 2개 이상 놓을 수도 있다.–&#38;#62;&#38;#60;!–html 주석 달기, 하지만 DTL과 충돌이 일어날 수 도 있다. –&#38;#62;&#38;#60;/body&#38;#62;&#38;#60;/html&#38;#62;                    4. base.html과 같은 폴더에 index.html 파일을 만들어준다.5. {% extends 'base.html' %}, {% block body %}, {% endblock %}를 통하여 base.html의 템플릿을 가져온다.\t- body라는 이름을 바꾸면 다른 위치의 템플릿을 가져올 수 있다.\t- 하지만 2개이상의 템플릿을 가져오는 것은 할 수 없다.6. 해당 폴더의 urls.py의 urlpatterns에 메인 페이지 표시를 위한 path를 추가한다.7.  해당 폴더의 views.py에 index 함수를 만들고 추가한 모델들을 정렬 후 변수에 집어넣는다.8. index 함수로 index.html 파일로 데이터를 넘겨주고 출력하는 html 코드를 만든다.&gt; urls.py의 일부(앱 내부 폴더)```pythonurlpatterns = [    path(&amp;#39;&amp;#39;, views.index, name=&amp;#34;index&amp;#34;), #todos의 메인 페이지 url을 의미    ]  views.py (앱 내부 폴더)```pythonfrom django.shortcuts import renderfrom .models import TODO  이용할 모델을 models.py에서 가져와야 한다.  Create your views here.  def index(request): # request는 요청자가 보낸 정보들이 담겨져있다.    todos = TODO.objects.order_by('due_date').all() # 날짜순으로 정렬    context = { # context라는 딕셔너리 형태로 넣어주어야 한다.        'todos': todos,    }    return render(request, 'index.html', context)  render 함수는 정보와 html 코드, 가공된 정보를 가지고 사람들에게 보여줄 페이지를 만들어준다.- 모델 클래스 내부의 함수들\t- 모델.objects.all(count) : 클래스의 모든 객체를 출력, count를 빈칸으로 놓지 않으면, count번째 요소를 가져온다.\t\t- 모델.objects.get(id=count, title=\"content\"): 1개의 데이터 객체만 가져온다. id에 값을 지정하면, id가 1인 객체 하나, content를 지정하면 해당 content를 title로 가지고 있는 데이터 객체를 가져온다. 일치하는 객체가 2개 이상 있으면 에러가 발생한다.\t\t- 모델.objects.filter(title=\"hello\") : 괄호안에 조건과 일치하는 모든 데이터를 가져옴 ex) title=\"hello\", 인 모든 객체 가져옴\t\t- 모델 객체.delete() : 해당 모델 객체 삭제\t\t- 모델객체 = 모델이름(열요소=내용,열요소2=내용2) : 모델 객체 생성\t\t- &gt; 모델 객체 생성 2\t- ```python\t  모델객체 = 모델이름()\t  모델객체.열요소1 = 내용\t  모델겍체.열요소2 = 내용\t  모델객체.save() # 모델 객체 적용\t  - 이 방법으로 모델객체 생성을 다른 타이밍에 할 수 있다. &gt; index.html (앱 내부 templates 폴더 내부) ```html &amp;#123;&amp;#37; extends &amp;#39;base.html&amp;#39; &amp;#37;&amp;#125; &amp;#38;#60;!--base.html의 템플릿을 가져옴--&amp;#38;#62; &amp;#123;&amp;#37; block body &amp;#37;&amp;#125;\t&amp;#38;#60;!-- base.html의 구멍을 선택--&amp;#38;#62;&amp;#123;&amp;#37; for todo in todos &amp;#37;&amp;#125; &amp;#38;#60;!-- DTL for문 구문--&amp;#38;#62;제목 : &amp;#38;#60;h1&amp;#38;#62;todo.title&amp;#38;#60;/h1&amp;#38;#62; &amp;#38;#60;!-- 각 todo 객체의 내용물 보여주기--&amp;#38;#62;내용 : &amp;#38;#60;h2&amp;#38;#62;todo.content&amp;#38;#60;/h2&amp;#38;#62;날짜 : &amp;#38;#60;h3&amp;#38;#62;todo.due_date&amp;#38;#60;/h3&amp;#38;#62;체크 : &amp;#38;#60;h3&amp;#38;#62;todo.check&amp;#38;#60;/h3&amp;#38;#62;&amp;#123;&amp;#37; endfor &amp;#37;&amp;#125;&amp;#38;#60;!-- for문, 꼭 끝을 알릴것 --&amp;#38;#62; &amp;#123;&amp;#37; endblock &amp;#37;&amp;#125; &amp;#38;#60;!-- block의 끝 --&amp;#38;#62;## Django Create 생성1. 앱의 urls.py의 urlpatterns 리스트에 url이름, views의 함수, url 변수명을 할당해 넣는다.&gt; 앱 폴더 내부의 urls.py의 urlpatterns 예시```pythonurlpatterns = [    path(&amp;#39;create/&amp;#39;, views.create, name=&amp;#34;create&amp;#34;),    #뒤의 name은 namespacing으로, url대신 쓸 이름    # 기타 필요한 path 추가]  해당 앱 폴더 내부에 views.py 파일로 가서 위 path함수의 2번째 인자인 함수명을 만든다. (일반적으로 url 이름과 같다.)          앱 폴더 내부의 views.py의 create 함수 예시```pythonfrom django.shortcuts import render, redirect # redirect와 render를 사용하기 위해 임포트 해야한다.from .models import TODO # 사용할 모델 또한 불러와야함      def create(request):    if request.method == \"POST\": # 만약 form의 method가 Post인채로 보내지면?    # 그렇다면 정보를 저장하고 데이터베이스에 올리고 메인 페이지로 돌려보냄        check = request.POST.get('check') # check 되면 'on'을 리턴함        if check == 'on':            check = True        else:            check = False        title = request.POST.get('title')        content = request.POST.get('content')        due_date = request.POST.get('due-date')        # todo = TODO()        # todo.check = check        # todo.title = title        # todo.content = content        # todo.due_date = due_date        # todo.save() # 똑같은 기능을 하는 코드        # 더 길지만 대신 save()를 이용해 todo 객체의 생성 타이밍을 원하는 시간에 할 수 있음        todo = TODO.objects.create(check=check, title=title, content=content, due_date=due_date)        return redirect('CRUD:index') # /있으면 루트주소에 추가, 없으면 현재 주소 앞에 추가    else: # form의 method가 Get이라면?    # 그렇다면 todo 정보를 입력하는 페이지로 보내야함        return render(request, 'create.html')- 기본 정보 method는 GET방식이다. 사용자가 새로운 글을 쓰기 위해 create url로 들어오면 GET방식이므로 crete.html이 만드는 페이지로 가서 정보 입력창으로 간다.- 정보를 모두 입력하고 사용자가 method를 POST 방식으로 설정한 폼에서 submit 버튼을 누르면, POST 방식으로 정보가 들어오므로 TODO 객체를 만들고 메인페이지로 돌아간다.- 이런 식으로 하나의 url 주소에서 받은 정보 유형에 따라 다르게 동작하게 만들어 효율적인 웹 코딩을 구현한 것이 restfull한 웹 코딩이라고 부른다.- 다른 프레임워크는 delete와 put 형식의 method도 지원하나 django와 HTML5는 공식적으로 지원하지 않고 있다. 이외에 인기있는 http verb 라는 개념도 지원안함- HTML5도 공식적으로 지원하지 않으므로 delete와 put형식으로 보낼 때는 새로 input 태그를 만들어, type= hidden에  name='_method', value=\"DELETE\" 로 숨겨서 보냄3. templates 폴더에 create.html을 만든다.&gt; create.html 예시```html&amp;#123;&amp;#37; extends &amp;#39;base.html&amp;#39; &amp;#37;&amp;#125;&amp;#123;&amp;#37; block body &amp;#37;&amp;#125;&amp;#38;#60;form action=&amp;#34;&amp;#123;&amp;#37; url &amp;#39;CRUD:create&amp;#39; &amp;#37;&amp;#125;&amp;#34; method=&amp;#34;POST&amp;#34;&amp;#38;#62; # 대소문자 구별안함  &amp;#123;&amp;#37; csrf_token &amp;#37;&amp;#125; # 폼 내부에 이게 있어야지 인증이되서 에러 안남  달성여부 : &amp;#38;#60;input type=&amp;#34;checkbox&amp;#34; name=&amp;#34;check&amp;#34;&amp;#38;#62; # check하면 &amp;#39;on&amp;#39;을 보냄  제목 : &amp;#38;#60;input type=&amp;#34;text&amp;#34; name=&amp;#34;title&amp;#34;&amp;#38;#62; # 각자 모델 정보명으로 보냄  내용 : &amp;#38;#60;input type=&amp;#34;text&amp;#34; name=&amp;#34;content&amp;#34;&amp;#38;#62;  마감일 : &amp;#38;#60;input type=&amp;#34;date&amp;#34; name=&amp;#34;due-date&amp;#34;&amp;#38;#62;  &amp;#38;#60;input type=&amp;#34;submit&amp;#34; value=&amp;#34;저장&amp;#34;&amp;#38;#62;&amp;#38;#60;/form&amp;#38;#62;&amp;#123;&amp;#37; endblock &amp;#37;&amp;#125;  form의 action 요소를 DTL 문법으로 namespacing을 이용하여 보낼 수 있다.  ”{% url ‘앱이름:url변수명’ 매개변수1, 매개변수2, …%}” 매개변수 또한 보낼 수 있으며, 앱이름을 설정하지 않으면 ‘앱이름:’부분은 빼도 된다. 그러면 다른 앱에 겹치는 url 변수명이 서로 충돌할 수 있다.  POST 방식으로 보내면 url 주소에 정보 값이 보이질 않고, csrf로 암호화되서 보내지며, 이를 제대로 해독해서 받으려면 form 태그 내부에 {% csrf_token %}을 지정하여 토큰도 같이 보내야 한다. 이를 통해 restfull 한 웹 코딩도 가능하다.    Django Read(detail) 생성          먼저 앱 폴더 내부의 urlpatterns 리스트에 새로 path를 추가한다.                  새로 추가한 detail path```pythonpath('&#38;#60;int:id&#38;#62;/', views.detail, name=\"detail\"), # 자세히 볼 id를 variable routing 해야함                    2. views.py에 detail 함수를 추가한다.&gt; 새로 추가한 detail 함수```pythondef detail(request, id): #자세히 볼 todo의 id를 html파일로 넘겨주어야 한다.    todo = TODO.objects.get(id=id) # 해당 id인 TODO 객체 찾기    context = &amp;#123; # render로 넘겨줄 때 3번째 인자로, 딕셔너리 형태로 넘겨줘야한다.        &amp;#39;todo&amp;#39;: todo,    &amp;#125;    return render(request, &amp;#39;detail.html&amp;#39;, context)  deatil.html 파일을 만들어 templates 폴더에 추가한다.          detail.html```python{% extends 'base.html' %}{% block body %}&#38;#60;h1&#38;#62;{{todo.title}}&#38;#60;/h1&#38;#62;&#38;#60;h2&#38;#62;{{todo.content}}&#38;#60;/h2&#38;#62;&#38;#60;h3&#38;#62;{{todo.due_date}}&#38;#60;/h3&#38;#62;&#38;#60;h3&#38;#62;{{todo.check}}&#38;#60;/h3&#38;#62;&#38;#60;a href=\"\"&#38;#62;삭제&#38;#60;/a&#38;#62;&#38;#60;a href=\"\"&#38;#62;수정&#38;#60;/a&#38;#62;&#38;#60;a href=\"{% url 'CRUD:index' %}\"&#38;#62;뒤로&#38;#60;/a&#38;#62;{% endblock %}      - 메인 페이지로 돌아가나는 뒤로와 나중에 추가될 삭제와 수정 버튼을 넣었다.4. 기존에 있던 index.html에 해당 모델 객체의 detail 페이지로 갈 수 있는 링크를 추가한다.&gt; index.html 의 일부```html&amp;#38;#60;a href=&amp;#34;&amp;#123;&amp;#37; url &amp;#39;CRUD:detail&amp;#39; todo.id &amp;#37;&amp;#125;&amp;#34;&amp;#38;#62;자세히&amp;#38;#60;/a&amp;#38;#62;  namspacing을 이용한 url 할당에 todo의 id를 매개변수로 보내고 있다.  보내야될 변수가 2개 이상이면 앞에서 부터 차례대로 적용.    Django Update 생성          앱 폴더 내부의 urls.py의 urlpatterns 리스트에 update path를 추가한다.                  url.py의 일부```python  path('&#38;#60;int:id&#38;#62;/update/', views.update, name=\"update\"),                    2. 앱 폴더 내부의 views.py에 update 함수를 추가한다.&gt; views.py의 update 함수```pythondef update(request, id):    todo = TODO.objects.get(id=id) # get일 때도, post일 때도 todo를 쓰므로 외부에서 구한다.    if request.method == &amp;#34;POST&amp;#34;:                todo.check = request.POST.get(&amp;#39;check&amp;#39;)        if todo.check == &amp;#39;on&amp;#39;:            todo.check = True        else:            todo.check = False        todo.title = request.POST.get(&amp;#39;title&amp;#39;)        todo.content = request.POST.get(&amp;#39;content&amp;#39;)        todo.due_date = request.POST.get(&amp;#39;due-date&amp;#39;)        return redirect(&amp;#39;CRUD:index&amp;#39;) # 값을 수정하고 메인 페이지로    else:        context = &amp;#123;        &amp;#34;todo&amp;#34;: todo,        &amp;#125;        return render(request, &amp;#39;update.html&amp;#39;, context)  update.html 파일을 templates 폴더 안에 추가한다.          update.html```html{% extends 'base.html' %}{% block body %}&#38;#60;form action=\"{% url 'CRUD:update' %}\" method=\"POST\"&#38;#62;   {% csrf_token %}  달성여부 : &#38;#60;input type=\"checkbox\" name=\"check\"&#38;#62;  제목 : &#38;#60;input type=\"text\" name=\"title\" value=\"{{todo.title}}\"&#38;#62;  내용 : &#38;#60;input type=\"text\" name=\"content\" value=\"{{todo.content}}\"&#38;#62;  마감일 : &#38;#60;input type=\"date\" name=\"due-date\" value=\"{{todo.due_date}}\"&#38;#62;  &#38;#60;input type=\"submit\" value=\"저장\"&#38;#62;&#38;#60;/form&#38;#62;{% endblock %}      ## Django Delete 생성## bootstrap 꾸미기## git1. gitignore.io에 가서 자신의 환경을 입력한 뒤 깃헙에 올리지 않을 설정을 받아 .gitignore에 넣기2. github에 가서 새로운 레포지토리 만들기3. 해당 폴더에 가서 터미널 안에  git init으로 git 환경으로 만든 뒤, 4. 터미널에 git commit -m \"message\"로 첫 커밋 남기기5. 터미널에 git remote add origin 레포지토리주소로 연결하기6. git push origin master로 현재 상태 업로드하기7. git add ., git commit -m \"message\", git push origin master로 업데이트, 커밋 남기기## admin 생성하기, 로그인 하기1. 터미널에 python manage.py createsuperuser 입력2. 터미널에 유저네임, 이메일주소, 패스워드 입력, 패스워드 확인을 물어봄\t- 패스워드 입력시 입력결과가 눈에 보이지 않는다.2. http://서버주소/admin 에 들어가서 로그인하면 관리자 페이지로 들어갈 수 있다.3. 해당 앱 폴더 내부의 admin.py에서 admin.site.register(대상 모델)로 대상 모델을 업데이트하면 관리 페이지로 들어가면 볼 수 있다.\t- 그 전에 from .models import 모델이름 으로 불러온다.## 댓글 달기 기능### django ORM#### READ&gt; 게시물 가져오는 법```pythonQuestion.objects.get(id=1)  없는 id면 에러 발생          게시물 불러오기```pythonIn [6]: question = Question.objects.get(id=1)      In [7]: questionOut[7]: &#38;#60;Question: Question object (1)&#38;#62;- 게시물 불러오기&gt; 댓글 저장하기```pythonIn [7]: questionOut[7]: &amp;#38;#60;Question: Question object (1)&amp;#38;#62;In [8]: AnswerOut[8]: questions.models.AnswerIn [9]: answer = Answer()In [10]: answerOut[10]: &amp;#38;#60;Answer: Answer object (None)&amp;#38;#62; # 아직 정보가 없으므로 NoneIn [11]: answer.contentOut[11]: &amp;#39;&amp;#39;In [12]: answer.content = &amp;#34;이것은 댓글입니다.&amp;#34;In [13]: answer.contentOut[13]: &amp;#39;이것은 댓글입니다.&amp;#39;In [14]: answer.question = questionIn [15]: answer.questionOut[15]: &amp;#38;#60;Question: Question object (1)&amp;#38;#62;In [16]: answer.save() # 정보 다 안채운채로 하면 에러 발생In [17]: answerOut[17]: &amp;#38;#60;Answer: Answer object (1)&amp;#38;#62;In [18]: Answer.objects.create(content=&amp;#34;두번째 댓글&amp;#34;, question=question)Out[18]: &amp;#38;#60;Answer: Answer object (2)&amp;#38;#62;댓글 정보In [19]: answer.idOut[19]: 1In [20]: answer.pkOut[20]: 1In [21]: answer.question_id # answer가 가지고 있는 id값, **이게 더 빠름**Out[21]: 1In [23]: answer.question.id # answer가 가지고있는 question이 가지고 있는 id값Out[23]: 1  answer.id == answer.pk(primary key) 아이디랑 같음  question_id와 answer.question.id 값은 같지만 접근하는데 question을 찾지 않으므로 question_id가 더욱 빠르다.```pythonIn [26]: answer.question.contentOut[26]: 'worst mankind on the planet earth'In [27]: question.answer_set # foreignkey를 가지고 있는 answer에 의해 자동으로 생성된 question의 요소Out[27]: &#38;#60;django.db.models.fields.related_descriptors.create_reverse_many_to_one_manager.&#38;#60;locals&#38;#62;.RelatedManager at 0x28b04474808&#38;#62;In [28]: question.answer_set.all()Out[28]: &#38;#60;QuerySet [&#38;#60;Answer: Answer object (1)&#38;#62;, &#38;#60;Answer: Answer object (2)&#38;#62;]&#38;#62; # answer 객체가 2개 들어있고 이에 접근 가능- answer가 가지고 있는 question의 내용에 접근 가능#### 1:N&gt; Question(1)=&gt;Answer(N) : answer_set 으로 접근 가능```pythonIn [28]: question.answer_set.all()Out[28]: &amp;#38;#60;QuerySet [&amp;#38;#60;Answer: Answer object (1)&amp;#38;#62;, &amp;#38;#60;Answer: Answer object (2)&amp;#38;#62;]&amp;#38;#62;  question.answer 로는 가져올 수 없다. (1개여도)  항상 데이터가 여러개라고 생각해야한다.  question에는 answer에 대한 정보를 넣지 않는다.(없을 수도 있으므로) dir(객체)로 확인 가능          Answer(N)=&gt;Question(1): question 으로 접근 가능```pythonIn [15]: answer.questionOut[15]: &#38;#60;Question: Question object (1)&#38;#62;      ```"
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%83%81.html": {
    title: "알고리즘-상",
    date: " Aug 13, 2020 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%83%81.html",
    tags: ["알고리즘","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘 정리파이썬 SW문제해결 기본List101_알고리즘알고리즘 개요  알고리즘이란 유한한 단계를 통해 문제를 해결하기 위한 절차나 방법1) 컴퓨터 용어로 쓰이며, 컴퓨터가 어떤 일을 수행하기 위한 단계적 방법2) 어떠한 문제를 해결하기 위한 절차  알고리즘 표현법슈도 코드(의사 코드): 특정 프로그래밍 언어의 문법을 따라 쓰여진 것이 아니라, 일반적인 언어로 코드를 흉내 내어 알고리즘을 써 놓은 코드# 코드 예시 (1~100까지 더하는 코드의 2가지 방법)&amp;#123;: #코드-예시-1~100까지-더하는-코드의-2가지-방법&amp;#125;def calcSum(n) :\tsum = 0\tfor i in range(1, n+1):\t    sum = sum + i\treturn sum    def calcSum2(n):\treturn n*(n+1)//2\tprint(calcSum(100))print(calcSum2(100))  의사 코드로 흉내만 냈으며, 컴퓨터에서 실행 불가능  알고리즘을 대략적으로 모델링하는데 쓰임순서도: 프로그램, 작업의 진행 흐름을 순서에 따라 여러 가지 기호나 문자로 나타낸 도표흐름도: 프로그램의 논리적인 흐름, 데이터의 처리과정을 표현하느데 사용,  프로그램을 작성하기 전에 프로그램의 전체적인 흐름과 과정 파악을 위해 거쳐야하는 작업  알고리즘의 성능 분석1) 정확성 : 얼마나 정확하게 동작하는가?2) 작업량 : 얼마나 적은 연산으로 원하는 결과를 얻어내는가?3) 메모리 사용량 : 얼마나 적은 메모리를 사용하는가?4) 단순성 : 얼마나 단순한가?5) 최적성 : 더 이상 개선할 여지 없이 최적화되었는가?          많은 문제에서 알고리즘의 성능 분석 기준으로 알고리즘의 작업량을 비교              실제 걸리는 시간을 측정, 또는 실행되는 명령문의 개수(주로 쓰임)를 계산    - 예를 들어 위의 예시 코드 첫번째는 99번의 연산 (덧셈 99번)    - 2번째 코드를 사용하면 3번의 연산 ( 덧셈 1번, 곱셈 1번, 나눗셈 1번) 더 효율적임    * 시간복잡도    - 실행되는 명령문의 개수를 계산                  알고리즘 1```pythondef calcSum(n) : sum = 0# 1번 for i in range(1, n+1):# n 갯수마다 1번  sum = sum + i# n 갯수마다 1번 return sum          시간 복잡도 : 1 + n * 2= 2n+1          {: #시간-복잡도-1-n-2-2n-1}print(calcSum(100))                    &gt; 알고리즘 2```pythondef calcSum(n):    return n*(n+1)//2#3번# 시간 복잡도 : 3&amp;#123;: #시간-복잡도-3&amp;#125;print(calcSum(100))      시간 복잡도의 표기법으로 __빅-오 표기법__이 있다.                  시간 복잡도 함수 중에서 가장 큰 영향력을 주는 n에 대한 항만을 표시                    계수(Coefficient)는 생략하여 표시                    ex) 위의 O(2n+1)은 O(2n) (최고차항만 선택) = O(n) (계수 제거) 과 같음                    요소 수가 증가함에 따라 각기 다른 시간복잡도의 알고리즘은 아래와 같은 연산 수를 보임                                        0.1초에 1억개의 연산을 처리할 수 있는 기계를 가정했을 때임                    시간복잡도에 따라 몇 초에서 몇 년까지 걸리기도 함            02 ListPython 소개      파이썬 : 1991년 귀도 반 로섬이 개발한 프로그래밍 언어    1) 인터프리터 언어로 독립적인 플랫폼 (실행시 마다 기계어를 해석함, 플랫폼에 관계 없음)    2) 객체지향 언어 : 파이썬에서는 모든 자료가 객체, 변수의 선언 따로 없음, 초기화시 생성          하나의 변수에 다른 타입의 값을 변수에 저장할 수 있음        3) 업그레이드 중, 파이썬 3버전은 2버전과 호환성 없음    4) 라즈베리파이, 빅데이터 자료분석 등에서 관심이 높아짐    5) 프로그램의 실행속도가 크게 차이 났었음, 실행속도가 느렷었음    6) 하지만 개발 속도와 생산성이  파이썬이 좋으며, 컴퓨터 하드웨어의 발달로 실행속도 차이가 줌        이하 파이썬의 특징은 PythonTIL jupyter notebook 참조  Exhaustive Search(완전 검색)      문제의 해법으로 생각할 수 있는 모든 경위의 수를 나열해보고 확인하는 기법        Brute-force 혹은 Generate-and-Test 기법이라고도 불림        모든 경우의 수를 테스트한 후, 최종 해법을 도출함        일반적으로 경우의 수가 상대적으로 작을 때 유용함        모든 경우의 수를 생성하고 테스트하기 때문에 수행속도는 느리지만, 해답을 찾아내지 못할 확률이 작음        주어진 문제를 풀 때, 우선 완전 검색으로 접근하여 해답을 도출한 후, 성능 개선을 위해 다른 알고리즘을 사용하고 해답을 확인하는 것이 바람직함        예시 : Baby-gin game : AlgorithmTIL 참조          고려할 수 있는 모든 경우의 수 생성하기, 6개의 숫자로 만들 수 있는 모든 숫자 나열(중복 포함), 이후 해답 테스트하여 판단해서 찾음      순열(Permutation)      서로 다른 것들 중 몇 개를 뽑아서 한 줄로 나열하는 것        서로 다른 n개 중 r개를 택하는 순열은 다음과 같이 표현 (nPr)        nPr은 다음과 같은 식이 성립, n(n-1)(n-2)…(n-r+1)        nPn은 n!이라고 표기하며 Factorial이라 부름          n! = n(n-1)(n-2)…2*1      Greedy Algorithm(탐욕 알고리즘)      최적 해를 구하는데 사용되는 근시안적인 방법        여러 경우 중 하나를 결정해야 할 때마다 그 순간에 최적이라고 생각되는 것을 선택해 나가는 방식으로 진행하여 최종적인 해답에 도달함        각 선택의 시점에서 이루어지는 결정은 지역적으로는 최적이지만, 그것들을 계속 수집하여 최종적인 해답을 만들었다고 하여, 그것이 최적이라는 보장은 없음        일반적으로 머릿속에 떠오르는 생각을 검증 없이 바로 구현하면 Greedy 접근이 됨  탐욕 알고리즘의 수행과정1) 해 선택 : 현재 상태에서 부분 문제의 최적 해를 구한 뒤, 이를 부분 해 집합(Solution Set)에 추가함2) 실행 가능성 검사: 새로운 부분 해 집합이 실행 가능한지 확인, 문제의 제약 조건을 위반하지 않는지를 검사3) 해 검사 : 새로운 부분 해 집합이 문제의 해가 되는지를 확인, 아직 전체 문제의 해가 완성되지 않았다면 해 선택부터 다시 시작함      ex) AlgorithmTIL Baby-gin, Algorithm PPT 참고        일부 해를 구하고 나머지 해를 구하는 방식?        탐욕 알고리즘 접근이 해답을 찾아내지 못하는 경우도 있음 주의  Sort(정렬)      2개 이상의 자료를 특정 기준에 의해 작은 값부터 큰값(ascending) 또는 그 반대의 순서대로(descending) 재배열 하는 것        키란 자료를 정렬하는 기준이 되는 특정 값  대표적인 정렬 방식의 종류버블 정렬(Bubble Sort)      인접한 두 개의 원소를 비교하며 자리를 계속 교환하는 방식        첫 번째 원소부터 인접한 원소끼리 계속 자리를 교환하면서 맨 마지막 자리까지 이동        한 단계가 끝나면 가장 큰 원소 또는 가장 작은 원소가 마지막 자리로 정렬됨        교환하며 자리를 이동하는 모습이 물 위에 올라오는 거품모양 같아서 버블이라고 지음        시간 복잡도 O(n^2)        버블정렬의 예시는 Algorithm PPT 참고        비교와 교환 방식, 코딩이 가장 손쉬움, 평균과 최악 전부 O(n^2)          리스트를 활용한 버블정렬 슈도코드        ```pythondef BubbleSort(a):# 정렬할 List  for i in range(len(a)-1,0,-1):# 범위의 끝 위치      for j in range(0,i):          if a[j] &#38;#62; a[j+1]:              a[j], a[j+1] = a[j+1], a[j]  ##### 카운팅 정렬 (Counting Sort)- 항목들의 순서를 결정하기 위해 집합에 각 항목이 몇 개씩 있는지 세는 작업을 하여, 선형 시간에 정렬하는 효율적인 알고리즘. - 정렬과정: 정수나 정수로 표현할 수 있는 자료에 대해서만 적용 가능- 카운트들을 위한 충분한 공간을 할당하려면 집합 내의 가장 큰 정수를 알아야 함- 시간 복잡도 O(n + k): n은 리스트의 개수, k는 정수의 최대값- __비교환 방식, n이 비교적 작을 때만 가능. 최악, 평균 경우 모두 :O(n + k)__- 정렬 과정 예시 Algorithm PPT 참고    &gt; 리스트를 활용한 카운팅 정렬 슈도코드  ```pythondef CountingSort(A, B, k):# A[1 .. n] -- 입력 리스트 사용된 숫자(1 ~ k)&amp;#123;: #a-1-n-입력-리스트-사용된-숫자-1-~-k&amp;#125;# B[1 .. n] -- 정렬된 리스트.&amp;#123;: #b-1-n-정렬된-리스트&amp;#125;# C[1 .. k] -- 카운트 리스트.&amp;#123;: #c-1-k-카운트-리스트&amp;#125;\tC = [0]*k      for i in range(0, len(B)):\t    C[A[i]] += 1      for i in range(1, len(C)):\t    C[i] += C[i-1]      for i in range(len(B)-1, -1, -1):        B[C[A[i]]-1] = A[i]        C[A[i]] -= 1a = [0,4,1,3,1,2,4,1]b = [0]*len(a)ContingSort(a, b, 5)print(b)List 22차원 List 구조      1차원 List를 묶어놓은 List        2차원 이상의 다차원 List는 차원에 따라 Index를 선언        2차원 List의 선언: 세로길이(행의 개수), 가로길이(열의 개수)를 필요로 함        파이썬에서는 데이터 초기화를 통해 변수 선언과 초기화가 가능함  List 초기화      리스트 원소 나열, 리스트 축약형 등을 사용하여 초기화 가능        PythonTIL jupyter notebook 참조  2차원 List 입력 받기  for문, 원소 등 여러가지 방법이 있다.2차원 List의 순회  n X m List의 n*m 개의 모든 원소를 빠짐없이 조사하는 방법          행 우선 순회            List의 행을 우선으로 List의 원소를 조사하는 방법          행 우선 순회 예시        ```pythonarr = [[0,1,2,3],[4,5,6,7],[8,9,10,11]]    i : 행의 좌표, n = len(arr)    {: #i-행의-좌표-n-len-arr}    j : 열의 좌표, m = len(arr[0])    {: #j-열의-좌표-m-len-arr-0}for i in range(len(arr)):  for j in range(len(arr[i])):      arr[i][j]# 필요한 연산 수행  2. 열 우선 순회- List의 열부터 먼저 조사하는 방법    ```python  arr = [[0,1,2,3],[4,5,6,7],[8,9,10,11]]# i : 행의 좌표, n = len(arr)&amp;#123;: #i-행의-좌표-n-len-arr&amp;#125;# j : 열의 좌표, m = len(arr[0])&amp;#123;: #j-열의-좌표-m-len-arr-0&amp;#125;  for j in range(len(arr)):# 행과 열 요소를 반대로    for i in range(len(arr[i])):        arr[i][j]# 필요한 연산 수행    지그재그 순회          List의 행을 좌우로 조사하는 방법        arr = [[0,1,2,3],[4,5,6,7],[8,9,10,11]]# i : 행의 좌표, n = len(arr)&amp;#123;: #i-행의-좌표-n-len-arr&amp;#125;# j : 열의 좌표, m = len(arr[0])&amp;#123;: #j-열의-좌표-m-len-arr-0&amp;#125;  for i in range(len(arr)):    for j in range(len(arr[0])):        arr[i][j + (m-1-2*j)*(i&amp;#37;2)]# 필요한 연산 수행  델타를 이용한 2차 List 탐색      2차 List의 한 좌표에서 네 방향의 인접 List 요소를 탐색할 때 사용하는 방법        델타 값은 한 좌표에서 네 방향의 좌표와 x, y의 차이를 저장한 List로 구현        델타 값을 이용하여 특정 원소의 상하좌우에 위치한 원소에 접근 가능        이차원 List의 가장자리 원소들은 상하좌우 네 방향에 원소가 존재하지 않을 경우가 있으므로, Index를 체크하거나 Index의 범위를 제한해야 합니다.          델타를 이용한 2차 List 탐색 예시        ```python    arr[0…n-1][0…n-1] : 2차원 List    {: #arr-0-n-1-0-n-1-2차원-list}dx = [0, 0, -1, 1]# 상하좌우dy = [-1, 1, 0, 0]    for x in range(len(arr)) :    for y in range(len(arr[x])):        for i in range(4) :            testX = x + dx[i]            testY = y + dy[i]            print(arr[testX][testY])  #### 전치 행렬- 행과 열의 값이 반대인 행렬을 의미- 모든 좌표에 대하여 행과 열의 값을 바꾸게 되면 본래의 모습으로 되돌아오기 때문에 이를 주의해야 함    &gt; 전치 행렬 예시  ```python# i : 행의 좌표, len(arr)&amp;#123;: #i-행의-좌표-len-arr&amp;#125;# j : 열의 좌표, len(arr[0])&amp;#123;: #j-열의-좌표-len-arr-0&amp;#125;arr = [[1,2,3], [4,5,6], [7,8,9]]# 3*3 행렬 for i in range(3):    for j in range(3):        if i&amp;#38;#60;j:            arr[i][j], arr[j][i] = arr[j][i], arr[i][j ]zip 함수 활용zip 함수는 주어진 배열들의 각각 위치의 서브 배열을 tuple로 묶은 1차원 리스트로 만들 수 있다.이를 이용해 배열을 조작할 수 있음.  zip 함수를 활용한 배열 회전lists = []#  정사 배열 생성&amp;#123;: #정사-배열-생성&amp;#125;for i in range(1,10):            temp = [[j+k*i for j in range(i)] for k in range(i)]    lists.append(temp)# 배열 90도 회전&amp;#123;: #배열-90도-회전&amp;#125;def spin(arr):    return list(zip(*arr[::-1]))# 배열 -90도 회전&amp;#123;: #배열-90도-회전&amp;#125;def spin_rev(arr):    return list(reversed(list(zip(*arr))))# 배열 전용 출력&amp;#123;: #배열-전용-출력&amp;#125;def print_arr(rev_arr,origin, arr, arr2):    print(&amp;#34;*&amp;#34;*20)    for i in range(len(arr)):        print(rev_arr[i], end=&amp;#34;   &amp;#34;)        print(origin[i], end=&amp;#34;   &amp;#34;)        print(arr[i], end=&amp;#34;   &amp;#34;)   x     print(arr2[i])    print(&amp;#34;arr: &amp;#34;, len(arr), &amp;#34;X&amp;#34;, len(arr))    print(&amp;#34;*&amp;#34;*20)# after spin&amp;#123;: #after-spin&amp;#125;for arr in lists:    print_arr(spin_rev(arr), arr, spin(arr), spin(spin(arr)))print(*[[1,2],[3,4]])# *은 내부 element를 각각의 부분 배열로 만들어준다.부분 집합부분 집합 문제예시) 유한개의 정수로 이루어진 집합이 있을 때 이 집합의 부분 집합 중에서 그 집합의 원소를 모두 더한 값이 0이 되는 경우가 있는가?      완전 검색 기법으로 문제를 풀기 위해서는 모든 부분집합을 생성후 각 부분 집합의 합을 계산함        부분집합의 총 개수는 집합의 원소의 개수가 n 개일때 공집합을 포함해서 2^n개        각 원소를 부분집합에 포함시키거나 포함시키지 않는 경우를 1과 0으로 표현해서 알고리즘으로 표현 가능    Loop를 이용하여 확인하고, 부분 집합을 생성하는 방법bit = [0, 0, 0, 0]# 비트 리스트: 각 원소를 포함할지 말지 결정하는 Listfor i in range(2):    bit[0] = i# 0번째 원소\tfor j in range(2):\t    bit[1] = j# 1번째 원소\t\t    for k in range(2):\t\t\t\tbit[2] = k# 2번째 원소\t\t\t    for l in range(2):\t\t\t        bit[3] = l# 3번째 원소print(bit)# 생성된 부분집합 출력 부분 집합 문제 알고리즘 2      비트 연산자 : 0과 1로 이루어진 이진수에 대한 연산을 수행하는 연산자        비트연산자 종류 : C나 C++ 정리 참조        ex) 1« n:2n^n == 원소가 n개일 경우의 모든 부분 집합의 수를 의미함        ex) i &amp; (i«j): 1 == i의 j번 비트가 1인가? (비트번호는 0번부터 시작)        만약 0이면 and 연산자에 의해 0, 1이면 둘다 1이므로 1이 나온다.(특정비트 검사)    비트 연산을 이용한 간결한 부분집합 생성arr = [3, 6, 7, 1, 5, 4]n = len(arr)# n:원소의 개수for i in range(1&amp;#38;#60;&amp;#38;#60;n) :# 1&amp;#38;#60;&amp;#38;#60;n: 부분 집합의 개수    for j in range(n) :# 원소의 수만큼 비트를 비교함        if i&amp;#38;(1&amp;#38;#60;&amp;#38;#60;j) :# i의 j번째 비트가 1이면 j번째 원소 출력\t        print(arr[j], end=&amp;#34;,&amp;#34;)    print()검색검색 개요      저장되어 있는 자료 중에서 원하는 항목(목적하는 탐색키(Search key : 자료를 구별하여 인식할 수 있는 키)를 가진 항목)을 찾는 작업        순차검색, 이진검색, 인덱싱 등의 방법이 있음  순차 검색      일렬로 되어 있는 자료를 순서대로 검색하는 방법        List나 연결 List 등 순차구조로 구현된 자료구조에서 유용함        구현이 쉽지만 검색대상이 많은 경우 수행시간 증가로 비효율적    정렬 되지 않은 경우 :1) 첫번째 원소부터 순서대로 검색대상과 키 값이 같은 원소가 있는지를 비교하여 찾음2) 키 값이 동일한 원소를 찾으면 그 원소의 인덱스를 반환3) 자료구조의 마지막에 갈 때까지 검색 대상을 찾지 못하면 검색 실패      찾고자 하는 원소의 순서따라 비교횟수 결정, 평균 비교 횟수는 n+1/2 복잡도 O(n)    정렬 된 경우:1) 자료가 오름차순으로 정렬된 상태에서 검색을 실시한다고 가정2) 자료를 순차적으로 검색하면서 키값을 비교함3) 원소의 키 값이 검색 대상의 카 값보다 크면 원소가 없다는 것이므로 더 이상 검색하지 않고 검색을 종료함  정렬되어 있으므로 검색 실패를 반환하는 경우 평균 비교횟수가 반으로 줄어듦  시간 복잡도 O(n)이진 검색      효율적인 검색 방법, 자료의 가운데 항목의 키 값과 비교하여 다음 검색의 위치를 결정하고 검색을 계속하는 방법        목적 키를 찾을 때까지 이진 검색을 순환적으로 반복 수행함으로써 검색 범위를 반으로 줄여가면서 빠르게 검색을 수행함        이진 검색을 하기 위해서는 __자료가 정렬된 상태__여야함        정렬된 데이터 n개가 있는 경우의 시간 복잡도 : O(logN)1) 자료의 중앙에 있는 원소를 선택  2) 중앙 원소의 값과 찾고자 하는 목표 값을 비교3) 목표값 &lt; 중앙 원소 값 : 자료의 왼쪽 반에 대해서 새로 검색을 수행   목표값 &gt; 중앙 원소 값 : 자료의 오른쪽 반에 대해서 새로 검색을 수행4) 찾고자 하는 값을 찾을 때까지 [ 1 ~ 3 ]의 과정을 반복      시작점과 종료점을 이용해 검색을 반복 수행        자료에 삽입이나 삭제 발생시, List의 상태를 항상 정렬 상태로 유지 작업 필요  인덱스      데이터베이스에서 유래 테이블에 대한 동작 속도를 높임, 룩업 테이블 등으로도 불림        인덱스를 저장하는데 필요한 디스크 공간은 보통 테이블 저장에 필요한 디스크 공간보다 작음 (인덱스는 키-필드만 갖고 있고, 테이블의 다른 세부 항목은 없음)        List를 사용한 인덱스 (대량의 데이터를 매번 정렬하면, 프로그램의 반응은 느려질 수 밖에 없음, 이러한 대량 데이터의 성능 저하 문제를 해결하기 위해 List 인덱스를 사용할 수 있음)        원본 데이터에 데이터가 삽입될 경우, 상대적으로 크기가 작은 인덱스 List를 정렬하기 때문에 속도가 빠름  정렬셀렉션 알고리즘  저장되어 있는 자료로부터 k번째로 큰 혹은 작은 원소를 찾는 방법  최소값, 최대값, 혹은 중간값을 찾는 알고리즘을 의미하기도 함셀렉션 선택 과정1) 정렬 알고리즘을 이용하여 자료를 정렬2) 원하는 순서에 있는 원소 가져오기  k번째로 작은 원소를 찾는 알고리즘   def select(list, k):    for i in range(0,k):        minIndex = i        for j in range(i+1, len(list)):            if list[minIndex] &amp;#38;#62; list[j]:                minIndex = j        list[i], list[minIndex] = list[minIndex], list[i]    return list[k-1]         1번부터 k번째까지 작은 원소들을 찾아 List의 앞쪽으로 이동시키고, List의 k번째를 반환        k가 비교적 작을 때 유용하며 O(kn)의 수행시간을 필요로 함  선택 정렬      주어진 자료들 중 가장 작은 값의 원소부터 차례대로 선택하여 위치를 교환하는 방식    셀렉션 알고리즘을 전체 자료에 적용한 것정렬 과정1) 주어진 List 중에서 최소값을 찾음2) 그 값을 List의 맨 앞에 위치한 값과 교환3) 맨 처음 위치를 제외한 나머지 List를 대상으로 위의 과정을 반복      시간 복잡도 ,평균, 최악 전부 O(n^2), 비교와 교환 방식, 버블 삽입정렬보다 나음          선택 정렬 알고리즘        ```pythondef selectionSort(a) :  for i in range(0, len(a)-1):      min = i      for j in range(i+1, len(a)):          if a[min] &#38;#62; a[j]:              min = j      a[i], a[min] = a[min], a[i]  ### String 1#### 문자 표현#### 컴퓨터에서의 문자표현- 메모리는 숫자만을 저장할 수 있기 때문에 A라는 글자의 모양 그대로 비트맵으로 저장하는 방법을 사용하지 않는 한 (메모리 낭비 심각) 각 문자에 대해서 대응되는 숫자를 정해놓고 이것을 메모리에 저장하는 방법이 사용됨- 코드 체계    - 영어가 대소문자 합쳐서 52이므로 6(64가지) 비트면 모두 표현 가능  - 네트워크 발전 이후 표준화를 위해 ASCII 인코딩 표준 제정  1) ASCII(아스키) - 문자 인코딩 표준, 7bit 인코딩으로 128문자를 표현하며 33개의 출력 불가능한 제어 문자들과 공백을 비롯한 95개의 출력 가능한 문자들로 이루어짐2) 확장 아스키- 표준 문자 이외의 악센트 문자, 도형문자, 특수문자, 특수 기호 등  128개를 추가- 1Byte 내의 8bit를 모두 사용하여 추가적인 문자 표현 가능- 컴퓨터 생산자와 소프트웨어 개발제에게 할당된 확장 부호는 표준 아스키와 다르게 서로 다른 프로그램이나 컴퓨터 사이에 교환 불가능- 프로그램이나 컴퓨터/프린터가 그것을 해독할 수 있도록 설계되어 있어야만 올바로 해독 가능3) 유니 코드- 다국어 처리를 위한 표준, 각 국가들은 자국의 문자를 표현하기 위해 코드체계 제작- 각국의 코드체계가 다른 국가에서 잘못 해석되는 문제 발생- 다국어 처리를 위해 표준을 마련한것이 유니코드, - CharacterSet: 유니코드를 저장하는 변수의 크기를 정의, UCS-2, UCS-4 등- 유니코드 인코딩 (UTF:Unicode Transformation Format)    - UTF-8(in web), MIN: 8bit, MAX: 32bit(1 Byte * 4)  - UTF-16(in windows, Java), MIN: 16bit, MAX: 32bit(2 Byte * 2)  - UTF-32(in unix), MIN: 32bit, MAX: 32bit(4 Byte * 1)- 파이썬은 2.x 버전 때는 ASCII, 3.x버전은 유니코드 UTF-8로 인코딩- 2.X 버전은#-*- coding:utf-8 -*- 로 변환해야했다.- 3.x 버전은 생략해도 되고 바꾸고 싶을때 기입하여 인코딩 방식 전환 가능#### 문자열의 분류- 문자열(String)- Fixed length : 고정길이 문자열, Variable length: 가변길이 문자열- Length controlled : java 언어에서의 문자열, 맨앞에 문자열길이 저장- Delimited : C언어에서의 문자열, 문자열 끝을 알리는 부호를 맨 끝에 저장(\\0)- C : 아스키 코드로 저장, java: 유니코드 UTF-16으로 저장, python : UTF_8- 파이썬은 문자 하나와 문자열을 동일하게 취급함, 시퀀스 자료형- 인덱싱, 슬라이싱 연산들을 사용할 수 있음- 문자열을 뒤집기, 비교, 숫자로 변환 등을 할줄 알아야함### 02 패턴 매칭#### 패턴 매칭 알고리즘- 패턴매칭: 본문에서 특정한 문자열 찾는 것  #### 고지식한 알고리즘(Brute Force)- 본문 문자열을 처음부터 끝까지 차례대로 순회하면서 패턴 내의 문자들을 일일이 비교하는 방식, M = 문자열 패턴의 길이, n = 총 문자열 길이- 최악의 경우 시간 복잡도는 텍스트의 모든 위치에서 패턴 비교하여 O(MN)이 됨  &gt; 고지식한 알고리즘 예시  ```pythonp = &amp;#34;is&amp;#34;# 찾을 패턴t = &amp;#34;This is a book~!&amp;#34;# 전체 텍스트M = len(p)# 찾을 패턴의 길이N = len(t)# 전체 텍스트의 길이def BruteForce(p, t):    i = 0# t의 인덱스    j = 0# p의 인덱스    while j &amp;#38;#60; M and i &amp;#38;#60; N :        if t[i] != p[j]:            i = i - j            j = -1        i = i + 1        j = j + 1\t    if j == M : return i - M# 검색 성공\t    else: return -1# 검색 실패KMP 알고리즘      불일치가 발생한 텍스트 문자열의 앞 부분에 어떤 문자가 있는지를 미리 알고 있으므로, 불일치가 발생한 앞 부분에 대하여 다시 비교하지 않고 매칭을 수행        패턴을 전처리하여 배열 nextM을 구해서 잘못된 시작을 최소화함        시간 복잡도 O(M+N)  보이어-무어 알고리즘      오른쪽에서 왼쪽으로 비교, 대부분의 상용 소프트웨어에서 채택하고 있는 알고리즘        앞부분 보단 끝부분에서 불일치가 일어날 확률이 높다는 성질 이용        패턴에 오른쪽 끝에 있는 문자가 불일치하고 이 문자가 패턴 내에 존재하지 않는 경우, 이동거리는 패턴의 길이 만큼이 됨        시간복잡도 최악의 경우 O(MN)이지만, 일반적으로 O(n)보다 적음  01 Stack 자료구조의 개념Stack의 특성      프로그램에서 중요성과 활용도가 매우 높은 자료구조    1) 물건을 쌓아 올리듯 자료를 쌓아 올린 형태의 자료구조임2) 스택에 저장된 자료는 선형구조(자료 간의 관계가 1대 1의 관계)를 가짐          비선형 구조 : 자료 간의 관계가 1대 N의 관계를 가짐(예: 트리)3) 스택에 자료를 삽입하거나 스택에서 자료를 꺼낼 수 있음4) 마지막에 삽입한 자료를 가장 먼저 꺼냄(후입선출, LIFO)      1, 2, 3 순으로 자료를 삽입한 후 꺼내면 역순으로 3, 2, 1 순으로 꺼내짐      Stack의 구현      자료를 선형으로 저장할 저장소로, C에서는 배열, Python에서는 리스트를 사용하며        스택에서 마지막 삽입된 원소의 위치를 top이라고 부름        연산에 관련하여                  삽입 : 저장소에 자료를 저장하고 보통 push라고 부름                    삭제 : 저장소에서 자료를 꺼냄, 꺼낸 자료는 삽입한 자료의 역순으로 꺼냄, pop이라고 부름                    isEmpty : 스택이 공백인지 아닌지를 확인하는 연산                    peek : 스택에 top에 있는 item의 값을 반환            Stack의 연산  빈스택에 원소를 집어넣으면 나중에 넣은 것이 맨위 (Push 연산)  push 알고리즘def push(item):    s.append(item)      python의 리스트는 크기의 제한이 없으므로, overflow 문제 고려안해도 됨,        top 변수 또한 필요 없음, 자동으로 append를 통해 맨 마지막에 삽입 가능    pop 알고리즘def pop():    if len(s) == 0:  # underflow        return    else:        return s.pop(-1)      리스트를 사용하면 구현이 용이하지만, 리스트 크기 변경 작업이 큰 overhead 발생 작업으로 많은 시간이 소요        리스트의 크기가 변동되지 않도록 배열처럼 크기를 미리 정해놓거나, 동적 연결리스트를 이용하여 저장소를 동적으로 할당하여 스택을 구현하면 된다.        그러면 구현이 어려운 대신 리스트 크기 변경작업이 잦을 때 성능이 좋다.  Stack 1Stack의 운용괄호검사  알고리즘 TIL의 bracketExamination 참조함수 호출 관리(Function call)      프로그램에서의 함수 호출과 복귀에 따른 수행 순서를 관리        가장 마지막에 호출된 함수가 가장 먼저 실행을 완료하고 복귀하는 후입선출 구조의 스택을 이용하여 수행순서 관리        함수 호출 발생시 호출한 함수 수행에 필요한 지역변수, 매개변수 및 수행 후 복귀할 주소 등의 정보를 스택 프레임에 저장하여 시스템 스택에 삽입        함수의 실행이 끝나면 시스템 스택의 top 원소(스택 프레임)을 삭제(pop)하면서 프레임에 저장되어있던 복귀주소를 확인하고 복귀        함수 호출과 복귀에 따라 이 과정을 반복하여 전체 프로그램 수행이 종료되면 시스템 스택은 공백 스택이 됨  재귀호출      자기 자신을 호출하여 순환 수행되는 것 ex) factorial, fibonacci        함수에서 실행해야 하는 작업의 특성에 따라 일반적인 호출방식보다 재귀 호출 방식을 사용하여 함수를 만들면 프로그램의 크기를 줄이고 간단하게 작성할 수 있음        디버깅이 어렵고 잘못작성하게 되면 수행시간이 많이 소요됨  extra) 재귀호출의 기본      특징                  자기 자신을 호출하지만 사용하는 메모리 영역이 구분되므로 다른 함수를 호출 하는 것과 같음                    정해진 횟수만큼, 혹은 조건을 만족할 때 까지 호출을 반복함                  호출 횟수에 대한 정보는 인자로 전달, 정해진 회수에 다다르면 호출 중단                    Memoization피보나치 수열  피보나치 수열을 재귀함수로 작성 가능하다def fibo(n):    if n &amp;#38;#60; 2:        return n    else:        return fibo(n-1) + fibo(n-2)  이러한 방식은 중복 호출이 상당히 많아서 비효율적이다. 예를 들어 fib(6)은 같은 값을 가지는 fib(1) = 1을 8번 구한다.Memoization(메모이제이션)      컴퓨터 프로그램을 실행할 때 이전에 계산한 값을 메모리에 저장해서 매번 다시 계산하지 않도록 하여 전체적인 실행속도를 빠르게 하는 기술        DP(동적 계획법)의 핵심이 되는 기술          memoization 방법을 적용한 알고리즘      # memo를 위한 리스트를 생성하고,&amp;#123;: #memo를-위한-리스트를-생성하고&amp;#125;# memo[0]을 0으로 memo[1]는 1로 초기화 한다&amp;#123;: #memo-0-을-0으로-memo-1-는-1로-초기화-한다&amp;#125;def fibo1(n):    global memo    if n &amp;#38;#62; = 2 and len(memo) &amp;#38;#60;= n:         memo.append(fibo1(n-1)+fibo1(n-2))    return memo[n]  memo = [0, 1]DP(동적계획법) 알고리즘DP(동적계획법)      Dynamic Programming의 약자, 그리디 알고리즘과 같이 최적화 문제를 해결하는 알고리즘        먼저 입력 크기가 작은 부분 문제들을 모두 해결한 후에 그 해들을 이용하여 보다 큰 크기의 부분 문제들을 해결, 최종적으로 원래 주어진 입력의 문제를 해결  DP 피보나치 수 함수1) 문제를 부분 문제로 분할2) 부분 문제로 나누는 일을 끝냈으면 가장 작은 부분 문제부터 해를 구함3) 결과를 테이블에 저장 후, 테이블에 저장된 부분 문제 해로 상위 문제 해 구함4) 부분 문제의 답으로 본 문제의 답을 얻을 수 있는 최적 부분구조로 이뤄져야 가능  DP 적용 피보나치 수 함수def fibo2(n):    f = [0, 1]       for i in range(2, n+1):        f.append(f[i-1] + f[i-2])       return f[n]DP 구현방식1) recursive (재귀함수) 방식 : fibo1()  재귀적 구조는 내부에 시스템 호출 스택을 사용하는 overhead가 발생할 수 있음2) iterative (반복문) 방식 : fibo2()  Memoization을 재귀적 구조에 사용하는 것보다 반복적 구조로 DP를 구현하는 것이 성능 면에서 보다 효율적DFS(깊이 우선 탐색)이란?DFS(깊이 우선 탐색)      비선형 구조인 그래프 구조는 표현된 모든 자료를 빠짐없이 검색하는 것이 중요          그러한 알고리즘으로는 아래가 있다.1) 깊이 우선 탐색 (Depth First Search, DFS)2) 너비 우선 탐색 (Breadth First Search, BFS)            깊이 우선 탐색은 stack을 이용하여 시작 정점의 한 방향으로 갈 수 있는 경로가 있는 곳까지 깊이 탐색 후, 더이상 갈 곳이 없으면 가장 마지막에 만났던 갈림길 간선이 있는 정점으로 되돌아옴, 그후 다른 방향으로 정점을 탐색하고 결국 모든 정점을 방문        가장 마지막에 만났던 갈림길의 정점으로 되돌아가서 다시 깊이 우선 탐색을 반복하므로, 후입선출 구조의 스택을 사용          DFS 알고리즘의 슈도코드      visited[],stack[] 초기화DFS(v)    v방문;    visited[v]&amp;#38;#60;-true;    do &amp;#123;        if (v의 인접 정점 중 방문 안 한 w 찾기)            push(v);        while(w)&amp;#123;            w 방문;            visited[w] &amp;#38;#60;- true;            push(w);            v&amp;#38;#60;-w;            v의 인접 정점 중 방문 안 한 w 찾기        &amp;#125;        v &amp;#38;#60;- pop(stack);    &amp;#125; while(v)end DFS()stack2계산기      문자열 계산식을 스택을 이용하여 결과값 계산하는 방법으로 중위 표기법 수식을 후위표기법으로 변경하는 방법이 있다.        중위 표기법(infix notation): 연산자를 피연산자의 가운데 표기하는 방법 (일반적으로 우리가 사용하는 방법 ex)A+B)        후위 표기법(postfix notation): 연산자를 피연산자 뒤에 표기하는 방법 (컴퓨터가 연산할때 자주 사용하는 방법 ex)A B + )  중위표기식을 후위표기식으로 변환      중위표기식의 후위표기식으로 변환 방법 1    1) 수식의 각 연산자에 대해서 우선순위에 따라 괄호를 사용하여 다시 표현    -((A*B)-(C/D))    2) 각 연산자를 그에 대응하는 오른쪽 괄호의 뒤로 이동          ((A B) * (C D)/)-        3) 괄호 제거          AB*CD/-            중위표기식 후위표기식 알고리즘 (스택이용)    1) 입력 받은 중위표기식에서 토큰을 읽음2) 토큰이 피연산자이면 토큰을 출력 (stack 거쳐가지 않음)3) 토큰이 연산자(괄호포함)일 경우 (stack 거쳐감)          우선선위가 높으면 -&gt; 스택에 push      우선순위가 안 높으면 -&gt; 연산자의 우선순위가 토큰의 우선순위보다 작을 때까지 스택에서 pop한 후 토큰의 연산자를 push ( ‘(‘ 는 ‘)’를 만날 때까지 pop 되면안되므로 우선순위가 가장 작다. 나머지는 동)      만약에 top에 연산자가 없으면 -&gt; push4) 토큰이 오른쪽 괄호 ‘)’일 경우      스택 top에 왼쪽 괄호 ‘(‘가 올 때까지 스택에 pop 연산을 수행      pop한 연산자를 출력      왼쪽 괄호를 만나면 pop만 하고 출력하지는 않음( (, ) 둘다 안쓰고 버려짐)5) 중위 표기식에 더 읽을 것이 없다면 중지, 더 읽을 것이 있다면 1부터 반복 6) 스택에 남아있는 연산자를 모두 pop하여 출력      스택 밖의 왼쪽 괄호는 우선 순위가 가장 높으며,      스택 안의 왼쪽 괄호는 우선 순위가 가장 낮음              토큰 : 수식에서 의미 있는 최소 단위                    icp(in-coming priority)                    isp(in-stack priority)        if (icp &gt; isp) push()  else pop()                  토큰 우선순위                                                    토큰 (낮을 수록 우선순위가 낮다)              ISP(스택 안에서)              ICP(외부에서)                                                          )              -              -                                      *, /              2              2                                      +, -              1              1                                      (              0              3                                                                                                               자세한 예시는 APS 기본 참조        후위표기법 알고리즘1) 피연산자를 만나면 스택에 push함2) 연산자를 만나면 필요한 만큼의 피연산자를 스택에서 pop하여 연산하고, 연산결과를 다시 스택에 push함3) 수식이 끝나면, 마지막으로 스택을 pop하여 출력      계산시 주의사항 : 후위 표기식을 계산 시, 피연산자를 스택에 쌓아 계산      파이썬 내장함수인 eval() 함수로 처리할 수도 있음  백트래킹백트래킹 기법의 정의      백트래킹(Backtracking): 해를 찾는 도중에 막히면, 즉 해가 아니면 되돌아가서 다시 해를 찾아가는 기법        어떤 노드의 유망성을 점검 후, 유망하지 않다고 결정되면 그 노드의 부모로 되돌아가 다음 자식 노드로 감        어떤 노드를 방문했을 때 그 노드를 포함한 경로가 해답이 될 수 없으면 그 노드는 유망하지 않다고 함, 반대로 해답 가능성이 있으면 유망하다고 함        가지치기(Pruning): 유망하지 않은 노드가 포함되는 경로는 더 이상 고려 안함        최적화(Optimization) 문제 해결 가능        결정(Decision) 문제 해결 가능        문제의 조건을 만족하는 해가 존재하는지의 여부를 ‘yes’ 또는 ‘no’로 답하는 문제        미로 찾기, n-Queen 문제, Map coloring, 부분 집합의 합(Subset Sum ) 문제 등  백트래킹 기법의 특징      어떤 노드에서 출발하는 경로가 해결책으로 이어질것 같지 않으면 더 이상 그 경로를 따라가지 않음으로써 시도의 횟수를 줄임(가지치기, Prunning)        불필요한 경로의 조기 차단, 깊이 우선탐색은 모든 경로와 후보를 추적함        백트래킹을 가하면 경우의 수가 줄어들지만 최악의 경우는 깊이 우선탐색과 동일함  1) 상태 공간 Tree의 깊이 우선 탐색을 실시2) 각 노드가 유망한지를 점검3) 만일 그 노드가 유망하지 않으면, 그 노드의 부모 노드로 돌아가 검색을 계속일반 백트래킹 알고리즘      주로 nqueen 문제에 씀, n*n의 정사각형 안에 n개의 queen을 배치하는 문제로, 모든 queen은 자신의 일직선 상 및 대각선 상에 아무것도 놓이지 않아야 함          일반 백트래킹 대략적인 슈도 코드```pythondef checknode (v):# node  if promissing(v):      if there is a solution at v:          write the solution      else:          for u in each child of v :              checknode(u)      ##### Power Set- 어떤 집합의 공집합과 자기자신을 포함한 모든 부분집합- 구하고자 하는 어떤 집합의 원소 개수가 n일 경우 부분집합의 개수는 2^n이 나옴    &gt; Power Set을 구하는 백트래킹 알고리즘    ```python  def backtrack(a, k, input):    global MAXCANDIDATES    c = [0]*MAXCANDIDATES      if k == input:        process_solution(a,k)# 답이면 원하는 작업을 한다    else:        k+=1        ncandidates = construct_candidates(a, k, input, c)        for i in range(ncandidates):            a[k] = c[i]            backtrack(a, k, input)    def construct_candidates(a, k, input, c):# 후보군 구하는 함수      c[0] = True      c[1] = False      return 2    def process_solution(a, k) :      print(&amp;#34;(&amp;#34;, end=&amp;#34;&amp;#34;)      for i in range(k+1):          if a[i]:              print(i, end=&amp;#34; &amp;#34;)      print(&amp;#34;)&amp;#34;)    MAXCANDIDATES = 100  NMAX = 100  a = [0]*NMAX  backtrack(a, 0, 3)    순열을 구하는 백트래킹 알고리즘  def backtrack(a, k, input):    global MAXCANDIDATES    c = [0] * MAXCANDIDATES      if k == input:        for i in range(1, k+1):            print(a[i], end=&amp;#34; &amp;#34;)        print()    else:        k += 1        ncandidates = construct_candidates(a, k, input, c)        for i in range(ncandidates):            a[k] = c[i]            backtrack(a, k, input)  def construct_candidates(a, k, input, c):    in_perm = [False] * NMAXfor i in range(1, k):    in_perm[a[i]] = Truencandidates = 0for i in range(1, input+1):    if in_perm[i] == False:        c[ncandidates] = i        ncandidates += 1    return ncandidates#### 분할 정복##### 분할정복 알고리즘1. 분할(Divide) : 해결할 문제를 여러 개의 작은 부분으로 나눔2. 정복(Conquer) : 나눈 작은 문제를 각각 해결3. 통합(Combine) : (필요하다면) 해결된 해답을 모음   &amp;#38;#62; 보통의 거듭 제곱(Exponentiation) 알고리즘 : O(n)   ```python   def Power(Base, Exponent):    if Base == 0 : return 1    result = 1# Base^0은 1이므로    for i in range(Exponent):        result *= Base    return result  분할 정복 기반의 알고리즘 : O(log2n)def Power(Base, Exponent): if Exponent == 0 or Base == 0:     return 1 if Exponent &amp;#37; 2 == 0:     NewBase = Power(Base, Exponent/2)     return NewBase * NewBase else :     NewBase = Power(Base, (Exponent-1)/2)     return (NewBase * NewBase) * Base  거듭 제곱을 반으로 나눈 후 합치는 방식퀵 정렬  퀵정결과 합병 정렬 비교                   합병 정렬      퀵 정렬                  공통점      주어진 리스트를 두개로      분할하고, 각각을 정렬              차이점      분할할 때, 단순하게 두 부분으로 나눔      기준 아이템(Pivot Item)을 중심으로, 이보다 작은 것은 왼편, 큰 것은 오른편에 위치시킴                     각 부분 정렬이 끝난 후,  합병 이란 후처리 작업 필요      후처리 작업 필요 없음        퀵 정렬 알고리즘def quickSort(a, begin, end):if begin &amp;#38;#60; end:    p = partition(a, begin, end)    quickSort(a, bdgin, p-1)    quickSort(a, p+1, end)  주어진 리스트에서 피봇을 구하는 알고리즘def partition(a, begin, end):    pivot = (begin + end) // 2    L = begin    R = end    while L &amp;#38;#60; R:        while(a[L]&amp;#38;#60;a[pivot] and L&amp;#38;#60;R):L+=1        while(a[R]&amp;#38;#62;=a[pivot] and L&amp;#38;#60;R):R-=1        if L &amp;#38;#60; R:            if L==pivot : pivot = R            a[L], a[R] = a[R], a[L]    a[pivot], a[R] = a[R], a[pivot]    return R  퀵정렬의 최악의 시간 복잡도는 O(n^2)로 합병정렬에 비해 좋지않지만  평균 시간 복잡도는 nlogn으로 오히려 좋다  정렬 알고리즘의 특성 비교            알고리즘      평균 수행시간      최악 수행시간      알고리즘 기법      비고                  버블 정렬      O(n^2)      O(n^2)      비교와 교환      코딩이 가장 손쉬움              카운팅 정렬      O(n+K)      O(n+K)      비교환 방식      n이 비교적 작을 때만 가능              선택 정렬      O(n^2)      O(n^2)      비교와 교환      교환의 회수가 버블, 삽입정렬보다 작음              퀵 정렬      O(n logn)      O(n^2)      분할 정복      최악의 경우 O(n^2)이지만 평균적으로 가장 빠름              삽입 정렬      O(n^2)      O(n^2)      비교와 교환      n의 개수가 작을 때 효과적              병합 정렬      O(n logn)      O(n logn)      분할 정복      연결 List의 경우 가장 효율적인 방식, 메모리 사용량 큼      Queue01 QueueQueue 자료 구조의 개념1) 삽입, 삭제의 위치가 제한적인 자료구조  뒤에서 삽입, 앞에서는 삭제만 이루어짐  삽입 : enQueue, 삭제: deQueue2) 선입선출구조(FIFO: First In First Out)  큐에 삽입한 순서대로 원소가 저장  가장 먼저 삽입(First in)된 원소는 가장 먼저 삭제(First Out)됨3) 큐의 예: 서비스 대기 행렬 (줄서기)Queue의 구조 및 기본 연산  큐의 주요 연산| 연산          | 기능                                               || ————- | ————————————————– || enQueue(item) | 큐의 뒤쪽(rear 다음)에 원소를 삽입하는 연산        || deQueue()     | 큐의 앞쪽(front)에서 원소를 삭제하고 반환하는 연산 || createQueue() | 공백 상태의 큐를 생성하는 연산                     || isEmpty()     | 큐가 공백상태인지를 확인하는 연산                  || isFull()      | 큐가 포화상태인지를 확인하는 연산                  || Qpeek()       | 큐의 앞족(front)에서 원소를 삭제없이 반환하는 연산 |Queue의 연산과정1) 공백 큐 생성 : createQueute();  front = rear = -1-1: 없음(가상의 공간), 0: [], 1: [], 2: []front, rear = -12) 원소 A 삽입: enQueue(A); -1: 없음(가상의 공간), 0: [A], 1: [], 2: []  front= -1 , rear + 1, rear = 0 3) 원소 B 삽입: enQueue(B); -1: 없음(가상의 공간), 0: [A], 1: [B], 2: []  front= -1 , rear + 1, rear = 14) 원소 반환/삭제: deQueue(); -1: 없음(가상의 공간), 0: [], 1: [B], 2: []      front= 0 , front + 1, rear = 1    A가 반환됨5) 원소 C 삽입: enQueue(C); -1: 없음(가상의 공간), 0: [], 1: [B], 2: [C]  front= 0 , rear + 1, rear = 26) 원소 반환/삭제: deQueue(); -1: 없음(가상의 공간), 0: [], 1: [], 2: [C]      front= 1 , front + 1, rear = 2    B가 반환됨7) 원소 반환/삭제: deQueue(); -1: 없음(가상의 공간), 0: [], 1: [], 2: []      front= 2 , front + 1, rear = 2        C가 반환됨    front값과 rear값이 같아짐 == 큐가 비어있다고 판단 가능Queue의 종류      선형큐: 간단하고 기본적인 형태 (리스트로 구현)        원형큐: 선형에서 발전된 형태 (리스트로 구현)        연결큐: 연결 리스트 형식을 이용        우선순위 큐: 이들을 응용한 큐  02 Queue의 종류선형 Queue선형 큐의 특징  1차원 리스트를 이용한 큐          큐의 크기 = 리스트의 크기            front: 저장된 첫 번째 원소의 인덱스    rear: 저장된 마지막 원소의 인덱스          상태 표현            초기 상태 : front = rear = -1        공백 상태 : front = rear    포화 상태 : rear = n-1(n: 리스트의 크기, n-1: 리스트의 마지막 인덱스)          문제점        1) 잘못된 포화 상태 인식:          리스트의 크기를 고정, 사용할 큐의 크기만큼을 미리 확보, 메모리의 낭비 발생      삽입, 삭제 반복시, 리스트 앞부분 공간이 있어도 rear=n-1이면 포화상태로 인식하여 더 이상의 삽입을 수행할 수 없음      ex) n = 5, front= 3, rear=4이면 포화상태, 실제로는 3메모리 더 남음            장점: 삽입, 삭제의 처리속도 빠름, 대신 메모리 낭비가 심함                  원형 큐 사용으로 메모리 절약 가능                    파이선 리스트는 동적이므로 메모리 절약 가능, 대신 연산 수행 시간 소모                    단순 연결 리스트로 구현한 큐 사용으로 메모리 동적 확보 가능                    큐 라이브러리를 사용하여 구현도 가능            선형큐의 구현      크기 n인 1차원 리스트 생성, front, rear = -1로 초기화        위 Queue의 연산과정 참조          큐의 삽입(enQueue(item)) 코드      def enQueue(item):    global rear    if isFull() : print(&amp;#34;Queue_Full&amp;#34;)    else :        rear += 1        Q[rear] = item  큐의 반환 및 삭제(deQueue()) 코드def deQueue(item):    glbal front    if isEmpty():print(&amp;#34;Queue_Empty&amp;#34;)    else:        front += 1        return Q[front]  공백상태 및 포화상태 검사: isEmpty(), isFull()def isEmpty():    return front == rear  def isFull():      return rear == len(Q) - 1      front == rear일 경우 공백 상태        rear == n-1일 경우 포화 상태          값 검색: Qpeek()            가장 앞에 있는 원소를 검색하여 반환하는 연산        현재 front의 한자리 뒤(front+1)에 있는 원소, 즉 큐에 첫 번째에 있는 원소를 반환  def Qpeek():    if isEmpty(): print(&amp;#34;Queue_Empty&amp;#34;)    else: return Q[front+1]      가장 앞에 있는 원소를 검색하여 반환하는 연산        현재 front의 한자리 뒤(front+1)에 있는 원소, 즉 큐의 첫 번째에 있는 원소를 반환  원형 Queue  1차원 리스트를 사용하되, 논리적으로 리스트의 처음과 끝이 연결되어 원형 형태의 큐를 이룬다고 가정하고 사용함, 즉, 인덱스 0과 인덱스 n-1이 연결됨원형 Queue의 특징1) 초기 공백 상태  front = rear = 02) index의 순환  front와 rear의 위치가 리스트의 마지막 인덱스인 n-1을 가리킨 후, 논리적 순환을 이루어 리스트의 처음 인덱스인 0으로 이동해야함  이를 위해 나머지 연산자 %를 사용3) front 변수      공백 상태와 포화 생태 구분을 쉽게 하기 위해 front가 있는 자리는 사용하지 않고 항상 빈자리로 둠          원형 큐와 선형 큐의 삭제 위치 비교                  테이블 인덱스      삽입 위치      삭제 위치                  선형 큐      rear = rear + 1      front = front + 1              원형 큐      rear = (rear + 1) % n      front = (front + 1) % n      원형 큐의 기본 연산과정1) 큐 생성: front, rear 값이 0으로 초기화 (n = 4)2) 원소 A 삽입: enQueue(A); : front = 0, rear = (rear+1) % len(cQ), rear = 1 Q[1] = A3) 원소 B 삽입: enQueue(B); : front = 0, rear = (rear+1) % len(cQ), rear = 2 Q[2] = B4) 큐 삭제: front = (front + 1) % len(cQ), front = 1, rear = 2 Q[1] = None,5) 원소 C 삽입: enQueue(C); : front = 0, rear = (rear+1) % len(cQ), rear = 3 Q[3] = C6) 원소 D 삽입: enQueue(D); : front = 0, if rear &gt; 3: rear = 0,rear = (rear+1) % len(cQ) Q[4] = D  front가 있는 자리는 사용하지 않으므로 Q는 포화 상태원형 큐의 구현      초기 공백큐 생성은 선형 큐와 다르지 않되, front, rear = 0          공백 상태 및 포화 상태 검사: isEmpty(),isFull()        ```pythondef isEmpty():  return front == rear    def isFull():    return (rear+1) % len(cQ) == front  - 공백 상태 : front = rear- 포화 상태 : 삽입할 rear의 다음 위치 = 현재 front- (rear + 1) % n = front    &gt; 삽입: enQueue(item)    ```python  def enQueue(item):    global rear    if isFull():        print(&amp;#34;Queue_Full&amp;#34;)    else:        rear = (rear + 1) &amp;#37; len(cQ)        cQ[rear] = item        마지막 원소 뒤에 새로운 원소를 삽입하기위해,    1) rear 값을 조정하여 새로운 원소를 삽입할 자리를 마련함          rear &lt;- (rear +1) % n;        2) 인덱스에 해당하는 리스트 원소 cQ[rear]에 item을 저장          삭제: deQueue(), delete()        ```python   def deQueue():   global front   if isEmpty():    print(\"Queue_Empty\")   else:    front = (front + 1) % len(cQ)    return cQ[front]    def delete():        global front      if isEmpty() :          print(\"Queue_Empty\")      else:          front = (front + 1) % len(cQ)    가장 앞에 있는 원소를 삭제하기 위해1) front 값을 조정하여 삭제할 자리를 준비함2) 새로운 front 원소를 리턴함으로써 삭제와 동일한 기능을 함리스트의 특성을 사용한 Queue3) 파이썬의 리스트 특성을 사용한 큐  리스트는 크기를 동적으로 변경할 수 있음  메모리 절약  삽입, 삭제 시 복사, 데이터를 이동시키는 연산을 수행하는데 많은 시간 소모4) 리스트의 메서드  append와 pop을 이용3) front는 리스트의 맨 앞: -14) rear는 리스트의 맨 뒤: len(queue) -1&#38;#62; 리스트 메서드 큐      def isEmpty():    return len(queue) == 0def enQueue(item):    queue.append(item)def deQueue():    if isEmpty():        print(“Queue_Empty”)    else:        return queue.pop(0)def Qpeek():    if isEmpty():        print(“Queue_Empty”)    else:        return queue[0]##### 연결 Queue&amp;#38;#62; 연결 큐의 특징1. 단순 연결 리스트(Linked List)를 이용한 큐   - 큐의 원소: 단순 연결 리스트의 노드   - 큐의 원소 순서: 노드의 연결 순서, 링크로 연결되어 있음   - front: 첫 번째 노드를 가리키는 링크   - rear: 마지막 노드를 가리키는 링크2. 상태 표현   - 초기 상태: front = rear = None   - 공백 상태: front = rear = None###### 연결 큐의 연산 과정1) 큐 생성 createLinkedQueue();   - front와 rear의 값을 None으로 쵝화2) 원소 A 삽입 : enQueue(A); front, rear 값 모두 A 위치 가리킴3) 원소 B 삽입 : enQueue(B); front는 A를 가리킴, A의 링크, rear 값은 B를 가리킴4) 원소 삭제: deQueue(); front와 rear가 B를 가리킴, A 삭제5) 원소 C 삽입: enQueue(C); front는 B, B의 링크와 rear는 C를 가리킴6) 원소 삭제: deQueue(); front와 rear가 C를 가리킴 B 삭제7) 원소 삭제: deQueue(); front와 rear가 None을 가리킴 C 삭제##### 연결 큐의 구현8) createLinkedQueue(): 리스트 노드 없이 포인터 변수만 생성함   - front = None, rear = None으로 초기화9) isEmpty(): 공백상태: front = rear = None   ```python   def isEmpty():    return front == None10) enQueue(item):1) 새로운 노드 생성 후 데이터필드에 item 저장2) 연결 큐가 공백인 경우, 아닌 경우에 따라 front, rear 변수 지정      &gt; enQueue(item)      ```python   def enQueue(item):# 연결 큐의 삽입 연산   global front, rear   newNode = Node(item)# 새로운 노드 생성   if isEmpty():# 큐가 비어있다면    front = newNode   else :    rear.next = newNode   rear = newNode11) deQueue():        1) old가 지울 노드를 가리키게 하고, front 재설정        2) 삭제 후 공백 큐가 되는 경우, rear도 None로 설정        3) old가 가리키는 노드를 삭제하고 메모리 반환              &gt; deQueue()              ```python       def deQueue():# 연결 큐의 삭제 연산       global front, rear       if isEmpty():        print(&amp;#34;Queue_Empty&amp;#34;)        return None              item = front.item       front = front.next       if isEmpty():        rear = None       return item         파이썬으로 구현한 연결 큐class Node:    def __init__(self, item, n=None):        self.item = item        self.next = n    def enQueue(item):# 연결 큐의 삽입 연산        global front, rear        newNode = Node(item)# 새로운 노드 생성        if front == None:# 큐가 비어있다면            front = newNode        else:            rear.next = newNode        rear = newNode    def isEmpty():        return front == None    def deQueue():# 연결 큐의 삭제 연산        global front, rear        if isEmpty():            print(&amp;#34;Queue_Empty&amp;#34;)            return None        item = front.item        front = front.next        if front == None :            rear = None        return item    Queue 라이브러리큐 모듈  큐 모듈에 정의된 클래스            클래스      내용                  queue.Queue(maxsize)      선입선출(FIFO First-In, First-Out)큐 객체를 생성              queue.LifoQueue(maxsize)      스택(Stack)개념의 후입선출 큐 객체 생성              queue.PriorityQueue(maxsize)      우선순위 큐 객체를 생성, 입력되는 아이템의 형식은 (순위, 아이템)의 튜플로 입력되며, 우선순위는 숫자가 작을수록 높은 순위를 가짐            maxsize는 최대 아이템수, 지정하지 않거나 음수이면 내용만큼 늘어남        제시된 3개의 클래스는 다음과 같은 메설드를 동일하게 가짐              메서드      내용                  qsize()      큐 객체에 입력된 아이템의 개수를 반환              put(item[, block[, timeout]])      큐 객체에 아이템을 입력              get([block[, timeout]])      생성된 큐 객체 특성에 맞추어, 아이템 1개를 반환              empty()      큐 객체가 비어있으면 True 리턴              full()      큐 객체가 꽉차있으면 True 리턴            클래스의 정렬방식에 따라 get 계열의 메서드 결과가 달라짐        import queue로 사용  03 Queue의 활용우선순위 Queue      우선순위를 가진 항목들을 저장하는 큐        FIFO 순서가 아니라 우선순위가 높은 순서대로 먼저 나가게 됨        우선순위가 같은 경우, 선입선출 방식을 사용        시뮬레이션 시스템, 네트워크 트래픽 제어, 운영체제의 테스크 스케줄링 등에 쓰임        구현을 위해 리스트를 이용하거나 Queue 라이브러리 사용        삽입시 우선순위에 맞는 위치에 삽입하고 삭제시 가장 앞에있는 원소 삭제  리스트를 이용한 우선순위 큐의 구현1) 리스트를 이용하여 자료 저장2) 원소를 삽입하는 과정에서 우선순위를 비교하여 적절한 위치에 삽입하는 구조3) 가장 앞에 최고 우선순위의 원소가 위치하게 됨4) 단점으로 삽입이나 삭제 연산시 원소 재배치로 인한 시간 소요가 걸림5) 연결리스트로 할시 조금 나으나, PriorityQueue 클래스 또는 힙 자료구조가 직빵버퍼      데이터를 한 곳에서 다른 한 곳으로 전송하는 동안 일시적으로 그 데이터를 보관하는 메모리 영역을 의마하며 버퍼링 : 버퍼를 활용하는 방식 또는 채우는 동작을 의미        일반적으로 입출력 및 네트워크와 관련된 기능에서 이용, 순서대로 입출력,전달해야하므로 선입 선출 방식의 자료구조인 큐가 활용됨  04 BFS(너비 우선 탐색)BFS(너비 우선 탐색) 특징      그래프 탐색 방법으로는 DFS(깊이 우선 탐색)와 BFS(너비 우선 탐색)가 있다        DFS(Depth First Search, 깊이 우선 탐색)          Stack 활용            BFS(Breadth First Search, 너비 우선 탐색)                  큐 활용                    시작점의 인접한 정점들을 모두 차례로 방문후 방문했던 정점들을 시작점으로 하여 다시 인접한 정점들을 차례로 방문하는 방식                    인접한 정점들을 탐색한 후, 찰례로 너비 우선 탐색을 진행해야 하므로 선입선출 형태의 자료구조인 큐 활용                  너비 우선탐색 알고리즘                    def BFS(G, v):# 그래프 G, 탐색 시작점 v    visited = [0]*n# n: 정점의 개수    queue = []# 큐 생성    queue.append(V)# 시작점 V를 큐에 삽입    while queue:# 큐가 비어있지 않은 경우      t = queue.pop(0)# 큐의 첫번째 원소 반환      if not visited[t]:# 방문되지 않은 곳이라면          visited[t] = True# 방문한 것으로표시          visit(t)      for i in G[t]:# t와 연결된 모든 선에 대해          if not visited[i]:# 방문되지 않은 곳이라면              queue.append(i)# 큐에 넣기LinkedList01 Linked ListList(리스트)      순서를 가진 데이터의 묶음- 같은 데이터의 중복 저장 가능        시퀀스 자료형 - 인덱싱, 슬라이싱, 연산자, 메서드 사용 가능        크기 제한 없음, 타입 제한 없음, 배열과의 차이점임          배열을 기반으로 구현된 것을 순차 리스트 (파이썬 리스트)        메모리의 동적할당을 기반으로 구현된 것을 연결 리스트라고 함순차 List      변수에 값을 초기화하는 것으로 리스트 생성 가능        시퀀스 자료형으로 인덱스를 이용해 원하는 위치의 데이터를 변경 및 참조 가능        자료의 삽입, 삭제 연산시 원소의 이동작업이 필요 이때 성능이 떨어짐        리스트 복사에는 여러가지 방법이 있으며 각각 수행시간과 의미가 달라짐          리스트 복사                         코드      설명                  1      new_list = old_list      주소의 복사, 얕은 복사              2      new_list = old_list[:]      슬라이싱, 깊은 복사              3      new_list = [] , new_list.extend(old_list)      extend() : 리스트를 추가하는 함수 깊은 복사              4      new_list = list(old_list)      list(), 깊은 복사              5      import copy, new_list = copy.copy(old_list)      copy 활용, 깊은 복사              6      new_list = [i for i in old_list]      리스트 함축, 깊은 복사              7      import copy, new_list = copy.deepcopy(old_list)      리스트 원소까지도 깊은 복사, 가장 느림, 깊은 복사      연결 List  리스트의 단점을 보완한 자료 구조          자료의 논리적인 순서와 메모리 상의 물리적인 순서가 일치하지 않고, 개별적으로 위치하고 있는 원소의 주소를 연결하여 하나의 전체적인 자료구조를 이룸            링크를 통해 원소에 접근하므로, 순차 리스트에서 물리적인 순서를 맞추기 위한 작업이 필요하지 않음        자료구조의 크기를 동적으로 조정할 수 있어, 메모리의 효율적인 사용이 가능        탐색- 순차 탐색          연결 리스트 사용을 위한 주요 함수                  함수명      기능                  addtoFirst()      연결 리스트의 앞쪽에 원소를 추가하는 연산              addtoLast()      연결 리스트의 뒤쪽에 원소를 추가하는 연산              add()      연결 리스트의 특정 위치에 원소를 추가하는 연산              delete()      연결 리스트의 특정 위치에 있는 원소를 삭제하는 연산              get()      연결 리스트의 특정 위치에 있는 원소를 리턴하는 연산            노드란? : 연결 리스트에서 하나의 원소에 필요한 데이터를 갖고 있는 자료단위          데이터 필드 : 원소의 값을 저장하는 자료구조      링크 필드 : 다음 노드의 주소를 저장하는 자료구조            헤드란? : 리스트의 처음 노드를 가리키는 레퍼런스        단순 연결 리스트 : 노드가 하나의 링크 필드에 의해 다음 노드와 연결되는 구조를 가짐                  헤드가 가장 앞의 노드를 가리키고, 각 노드의 링크 필드가 연속적으로 다음 노드를 가리킴                    최종적으로 None을 가리키는 노드가 리스트의 가장 마지막 노드임                  Node class 예시                ```pythonclass Node:def __init(self, data, link):  self.data = data  self.link = link        def addtoLast(data):# 마지막에 데이터 삽입  global Head  if Head == None:# 빈 리스트이면      Head = Node(data, None)  else:      p = Head      while p.link != None :# 마지막 노드 찾을 때까지          p = p.link      p.link = Node(data, None)        def add(pre, data):# pre 다음에 데이터 삽입  if pre == None:      print('error')  else:      pre.link = Node(data, pre.link)'''또는 def add(data, idx):  global pHead  p = pHead  n = 0  while n&#38;#60;idx-1:      p = p.link      n += 1  t = p.link  p.link = Node(data, t)  return'''        def get(idx):# idx의 데이터 리턴            ###### 단순 연결 리스트의 삽입 연산- A, C, D를 원소로 갖고 있는 리스트의 두 번째에 B 노드를 삽입할 때1. 메모리를 할당하여 새로운 노드 new 생성임2. 새로운 노드 new의 데이터 필드에 B 저장3. 삽입될 위치의 바로 앞에 위치한 노드의 링크 필드를 new에 복사4. new의 주소를 앞 노드의 링크 필드에 저장&gt; 첫번째 노드로 삽입하는 알고리즘```pythondef addtoFirst(data):# 첫 노드에 데이터 삽입    global Head    Head = Node(data, Head)# 새로운 노드 생성  가운데 노드로 삽입하는 알고리즘def add(pre, data):# pre 다음에 데이터 삽입    if pre == None:        print(&amp;#39;error&amp;#39;)    else:        pre.link = Node(data, pre.link)      노드 pre의 다음 위치에 노드 삽입          마지막 노드로 삽입하는 알고리즘        ```pythondef addtoLast(data):# 마지막에 데이터 삽입  global Head  if Head == None:# 빈 리스트이면      Head = Node(data, None)  else:      p = Head      while p.link != None :# 마지막 노드 찾을 때까지          p = p.link      p.link = Node(data, None)  ###### 단순 연결 리스트의 삭제 연산 (A, B, C, D 리스트의 B 노드를 삭제 시)1. 삭제할 노드의 앞 노드(선행노드) 탐색2. 삭제할 노드의 링크 필드를 선행노드의 링크 필드에 복사      &gt; 첫 번째 노드를 삭제하는 알고리즘      ```python   def deletetoFirst():# 처음 노드 삭제    global Head    if Head == None:        print(&amp;#39;error&amp;#39;)    else:        Head = Head.link     노드를 삭제하는 알고리즘      노드 pre의 다음 위치에 있는 노드 삭제    def delete(pre):# pre 다음 노드 삭제    if pre==None or pre.link==None:        print(&amp;#39;error&amp;#39;)    else:        pre.link = pre.link.link  이중 연결 리스트      양쪽 방향으로 순회할 수 있도록 노드를 연결한 리스트        두 개의 링크 필드와 한 개의 데이터 필드로 구성 (prev, data, next)          이중연결 리스트 class        ```pythonclass Node:  def init(self, data, pre, link):# 이중 연결 리스트      self.data = data      self.pre = pre      self.link = link  1) cur이 가리키는 노드 다음에 D값을 가진 노드를 삽입하는 과정      1. 메모리를 할당하여 새로운 노드 new를 생성하고 데이터 필드에 'D'를 저장   2. cur의 next를 new의 next에 저장하여 cur의 다음 노드를 삽입할 노드의 다음 노드로 연결   3. new의 값을 cur의 next에 저장하여 삽입할 노드를 cur의 다음 노드로 연결   4. cur의 값을 new의 prev 필드에 저장하여 cur를 new의 이전 노드로 연결   5. new의 값을 new가 가리키는 노드의 다음 노드의 prev 필드에 저장하여 삽입하려는 노드의 다음 노드와 삽입하려는 노드를 연결2) 이중 연결 리스트의 삭제 연산 (cur이 가리키는 노드를 삭제하는 과정)      1. 삭제할 노드의 다음 노드의 주소를 삭제할 노드의 이전 노드의 next 필드에 저장하여 링크를 연결      2. 삭제할 노드의 다음 노드의 prev 필드에 삭제할 노드의 이전 노드의 주소를 저장하여 링크를 연결      3. cur이 가리키는 노드에 할당된 메모리를 반환      #### 02 삽입 정렬      ##### 삽입 정렬의 특징- 자료 배열의 모든 원소들을 앞에서부터 차례대로 이미 정렬된 부분과 비교해 자신의 위치를 찾아냄으로써 정렬을 완성, 시간 복잡도 O(n^2)  ###### 삽입 정렬의 정렬 과정1. 정렬할 자료를 두 개의 부분집합 S와 U로 가정      - 부분집합 S : 정렬된 앞부분의 원소들   - 부분집합 U : 아직 정렬되지 않은 나머지 원소들2. 정렬되지 않은 부분집합 U의 원소를 하나씩 꺼내서 이미 정렬되어있는 부분집합 S의 마지막 원소부터 비교하면서 위치를 찾아 삽입 이를 반복3. 삽입 정렬을 반복하면서 부분집합 S의 원소는 하나씩 늘리고 부분집합 U의 원소는 하나씩 감소하게 함4. 부분집합 U가 공집합이 되면 삽입정렬이 완성됨   #### 03 병합 정렬   ##### 병합 정렬의 특징- 여러 개의 정렬된 자료의 집합을 병합하여 한 개의 정렬된 집합으로 만드는 방식1. 분할 정복 알고리즘 활용, 시간 복잡도 :O(n log n)      - 자료를 최소 단위의 문제까지 나눈 후에 차례대로 정렬하여 최종 결과를 얻어냄      - Top-Down 방식     ##### 병합 정렬의 과정2. 분할 단계 : 전체 자료 집합에 대하여, 최소 크기의 부분집합이 될 때까지 분할 작업을 계속함3. 병합 단계 : 2개의 부분집합을 정렬하면서 하나의 집합으로 병합   ##### 병합 정렬 알고리즘   ###### 분할 과정의 알고리즘   &gt; 분할 과정 알고리즘 파이썬 코드```pythondef merge_sort(m):    if len(m) &amp;#38;#60;= 1:# 사이즈가 0이거나 1인 경우, 바로 리턴        return m# 1. DIVIDE 부분    mid = len(m) // 2    left = m[:mid]    right = m[mid:]# 리스트의 크기가 1이 될 때까지 merge_sort 재귀 호출    left = merge_sort(left)    right = merge_sort(right)# 2. CONQUER 부분 : 분할된 리스트들 병합    return merge(left, right)병합 과정의 알고리즘def merge(left, right):    result = []# 두 개의 분할된 리스트를 병합하여 result를 만듦    while len(left) &amp;#38;#62; 0 and len(right) &amp;#38;#62; 0 :# 양쪽 리스트에 원소가 남아있는 경우 # 두 서브 리스트의 첫 원소들을 비교하여 작은 것부터 result에 추가함        if left[0] &amp;#38;#60;= right[0]:            result.append(left.pop(0))        else:            result.append(right.pop(0))    if len(left) &amp;#38;#62; 0 :# 왼쪽 리스트에 원소가 남아있는 경우        result.extend(left)    if len(right) &amp;#38;#62; 0 :# 오른쪽 리스트에 원소가 남아있는 경우        result.extend(right)    return result      연결 리스트를 사용하면 리스트의 삽입, 삭제, 크기 변경에서 일어나는 성능 저하를 줄일 수 있음          정렬 알고리즘의 특성 비교                  알고리즘      평균 수행시간      최악 수행시간      알고리즘 기법      비고                  버블 정렬      O(n^2)      O(n^2)      비교와 교환      코딩이 가장 손쉬움              카운팅 정렬      O(n+K)      O(n+K)      비교환 방식      n이 비교적 작을 때만 가능              선택 정렬      O(n^2)      O(n^2)      비교와 교환      교환의 회수가 버블, 삽입정렬보다 작음              퀵 정렬      O(n logn)      O(n^2)      분할 정복      최악의 경우 O(n^2)이지만 보통 가장 빠름, 데이터가 자주 추가 삽입시 비효율              삽입 정렬      O(n^2)      O(n^2)      비교와 교환      n의 개수가 작을 때 효과적              병합 정렬      O(n logn)      O(n logn)      분할 정복      연결 List의 경우 가장 효율적인 방식, 메모리 사용량 큼      04 Linked List의 활용List를 이용한 Stack      스택의 원소: 리스트의 노드        스택 내의 순서는 리스트의 링크를 통해 연결됨        Push: 리스트의 마지막에 노드 삽입        Pop: 리스트의 마지막 노드 반환/삭제        변수 Top : 리스트의 마지막 노드를 가리키는 변수, 초기상태는 None  List를 이용한 Stack의 연산  Push와 Pop 연산 구현          None 값을 가지는 노드를 만들어 스택 초기화            원소 A 삽입: push(A)        원소 B 삽입: push(B)        원소 C 삽입: push(C)        원소 반환: pop    Push / Pop 연산의 알고리즘def push(i):# 원소 i를 스택 top(맨앞) 위치에 push    global top    top = Node(i, top)# 새로운 노드 생성   def pop():# 스택의 top을 pop    global top       if top == None:# 빈 리스트이면        print(&amp;#34;error&amp;#34;)    else:        data = top.data        top = top.link# top이 가리키는 노드를 바꿈    return data우선순위 Queue우선순위 큐의 구현과 기본 연산      우선순위 큐의 구현 : 연결 리스트를 이용한 우선순위 큐        기본 연산 : 삽입: enQueue, 삭제: deQueue  순차 리스트를 이용한 우선순위 큐 구현      순차 리스트를 이용하여 자료 저장        원소를 삽입하는 과정에서 우선순위를 비교하여 적절한 위치에 삽입하는 구조        가장 앞에 최고 우선순위의 원소가 위치하게 됨        문제점 : 배열을 사용하므로 삽입, 삭제 연산 시 원소 재배치로 인한 시간과 메모리 낭비가 큼  연결 리스트를 이용한 우선순위 Queue 구현      연결 리스트를 이용하여 자료 저장        원소를 삽입하는 과정에서 리스트 내 노드의 원소들과 비교하여 적절한 위치에 노드를 삽입하는 구조        리스트의 가장 앞쪽에 최고 우선순위가 위치하게 됨        배열 대비 장점: 삽입/삭제 연산 이후 원소의 재배치가 필요 없음, 메모리의 효율적인 사용이 가능함  Tree01 TreeTree의 특성      비선형 구조로 원소들 간에 1:n 관계를 가지는 자료구조    1) 원소들 간에 계층관계를 가지는 계층형 자료구조2) 상위 원소에서 하위 원소로 내려가면서 확장되는 Tree(나무) 모양의 구조      1. 한 개 이상의 노드로 이루어진 유한 지합          루트(Root) : 노드 중 최상위 노드      단말 노드 : 가장 마지막, 자식이 없는 끝 노드      나머지 노드들: n(&gt;= 0)개의 분리 집합 T1, …, TN으로 분리될 수 있음        이들 T1, …, TN은 각각 하나의 트리가 되며(재귀적 정의) 루트의 서브트리(SubTree)라고 함Tree의 구성요소노드(node)      트리의 원소, 트리 내부에 속해있는 원소들                  루트 노드(Root node) : 트리의 시작 노드 (문제 안에서 부모가 없는 노드)                    형제 노드(Sibling node) : 같은 부모 노드의 자식 노드들                    조상 노드(Ancestor node) : 간선을 따라 루트 노드까지 이르는 경로에 있는 모든 노드들                    서브 트리(Sub Tree) : 부모 노드와 연결된 간선을 끊었을 때 생성되는 트리                    자손 노드(Descendent node) : 서브트리에 있는 하위 레벨의 노드들( 자식의 자식들 포함)            간선(edge)  노드를 연결하는 선, 부모 노드와 자식 노드를 연결차수(degree)      노드에 연결된 자식 노드의 수 (B의 차수 = 2, C의 차수 = 1)        트리의 차수는 트리에 있는 노드의 차수 중에서 가장 큰 값  단말 노드 (리프 노드, 맆 노드?)  차수가 0인 노드, 자식 노드가 없는 노드높이      노드의 높이          루트(Root) : 노드 중 최상위 노드      노드의 높이(레벨)는 루트 노드로 부터의 거리            트리의 높이          트리에 있는 노드의 높이 중에서 가장 큰 값, 최대 레벨      02 Binary Tree-Binary Tree의 특징이진 트리      모든 노드들이 2개의 서브트리를 갖는 특별한 형태의 트리          즉 레벨 i에서의 노드의 최대 개수는 2^i개      높이가 h인 이진트리가 가질 수 있는 노드의 최소 개수는 (h+1)개, 최대 개수는 (2^(h+1)-1)개가 됨            노드가 자식 노드를 최대한 2개 까지만 가질 수 있는 트리                  왼쪽 자식 노드 (Left child node)                    오른쪽 자식 노드 (Right child node)            Binary Tree의 종류포화 이진 트리 (Full binary Tree)      모든 레벨에 노드가 포화 상태로 차 있는 이진 트리                  최대의 노드 개수인 (2^(h+1)-1)의 노드를 가진 이진 트리                    루트를 1번으로 하여 (2^(h+1)-1)까지 정해진 위치에 대한 노드 번호를 가짐            완전 이진 트리 (Comlete binary Tree)      높이가 h이고 노드 수가 n개일 때 (단, 2^h &lt;= n &lt; (2^(h+1)-1)), Full 이진 트리의 노드 번호 1번부터 n번까지 빈자리가 없는 이진 트리        즉 계속 그리다가 만 포화 이진 트리 같은 느낌  편향 이진 트리 (Skewed binary Tree)      높이 h에 대한 최소 개수의 노드를 가지면서 한쪽 방향의 자식 노드 만을 가진 이진 트리        왼쪽 평향 이진 트리와 오른쪽 평향 이진트리가 있다, 일직선으로 뻗어나가는 느낌        편향 이진 트리는 메모리 공간 낭비와 비효율을 만드므로, 완전이진트리로 변향하는 알고리즘을 쓴다(알아보자)  Binary Tree - 순회(traversal)      순회란, 트리의 각 노드를 중복되지 않게 전부 방문(Visit) 하는 것을 말하는데, 트리는 비 선형 구조이기 때문에 선형구조에서와 같이 선후 연결 관계를 알 수 없음        Segment tree에 대해서도 알아보기  3가지의 기본적인 순회 방법V: 루트 노드  L: 루트의 자손 노드 중 왼쪽 서브 트리  R: 루트의 자손 노드 중 오른쪽 서브 트리  이라고 하였을 때,      전위 순회 (Preorder traversal)                  VLR 순                    자손 노드보다 루트 노드를 먼저 방문                  전위 순회 알고리즘                    def preorder_traverse(T) :# 전위순회\tif T :# T is not None\t    visit(T)# print(T.item)\t    preorder_traverse(T.left)\t    preorder_traverse(T.right)      중위 순회 (Inorder traversal)                  LVR 순                    왼쪽 자손, 루트, 오른쪽 자손 순으로 방문                  중위 순회 알고리즘                    def inorder_traverse(T) :# 중위 순회\tif T:# T is not None\t\tinorder_traverse(T.left)\t    visit(T)# print(T.item)\t    inorder_traverse(T.right)  후위 순회 (Postorder traversal)          LRV 순      루트노드보다 자손을 먼저 방문        후위 순회 알고리즘def postorder_traverse(T) :# 후위 순회\tif T:# T is not None\t    postorder_traverse(T.left)\t    postorder_traverse(T.right)\t    visit(T)# print(T.item)03 Expression TreeList를 이용한 Binary Tree의 표현리스트를 이용한 이진 트리의 표현      이진 트리에 각 노드 번호를 다음과 같이 부여        루트의 번호를 1로 함        레벨 n에 있는 노드에 대하여 왼쪽부터 오른쪽으로 2n 부터 2^(n+1) - 1 까지 번호를 차례로 부여  노드 번호의 성질      노드 번호가 i인 노드의 부모 노드 번호는 i//2        노드 번호가 i인 왼쪽 자식 노드 번호는 2*i        노드 번호가 i인 오른쪽 자식 노드 번호는 2*i+1  이진트리의 리스트 표현 인덱스      노드 번호를 리스트의 인덱스로 사용        높이가 h인 이진 트리를 위한 리스트의 크기는 2^(h+1), 마지막 인덱스는 2^(h+1) - 1  리스트를 이용한 이진 트리 표현의 단점      편향 이진 트리의 경우에 사용하지 않는 리스트 원소에 대한 메모리 공간 낭비 발생        이를 보완하기 위해 연결 리스트를 이용하여 트리를 표현 가능,        또는 연결 자료구조를 이용하면 이진 트리는 최대 2개의 자식 노드를 가지므로 쉽게 구현 가능          리스트(배열)을 이용한 이진트리 표현 예시        ```pythonV = 13# 간선 수 = V - 1E = V - 1t = [1, 2, 1, 3, 2, 4, 3, 5, 6, 4, 7, 5, 8, 5, 9, 6, 10, 6, 11, 7, 12, 11, 13]  배열의 모든 순회 알고리즘 예시def preorder(n):      if n &#38;#62; 0:          print(n, end=' ')          preorder(ch1[n])          preorder(ch2[n])def inorder(n):      if n &#38;#62; 0:          inorder(ch1[n])          print(n, end=' ')          inorder(ch2[n])def postorder(n):      if n &#38;#62; 0:          postorder(ch1[n])          postorder(ch2[n])          print(n, end=' ')조상 찾기def f(n):# n의 조상 출력하기      while(par[n] != 0):# n의 부모가 있으면          print(par[n], end=' ')          n = par[n]# 부모를 새로운 자식으로 해서 부모의 부모를 찾으러 감1. 배열 방식부모를 인덱스로 자식을 저장하는 방법ch1 = [0] * (V+1)# 부모를 인덱스로 자식 저장  ch2 = [0] * (V+1)for i in range(E) :      p = t[2 * i]      c = t[2 * i + 1]      if ch1[p] == 0:# 아직 ch1 자식이 없으면          ch1[p] = c      else:          ch2[p] = c배열 방식 2번째자식을 인덱스로 부모를 저장하는 방법ch1 = [0] * (V+1)# 부모를 인덱스로 자식 저장  ch2 = [0] * (V+1)  par    = [0] * (V+1)# 자식을 인덱스로 부모 저장for i in range(E) :      p = t[2 * i]      c = t[2 * i + 1]      if ch1[p] == 0:# 아직 ch1 자식이 없으면          ch1[p] = c      else:          ch2[p] = c      par[c] = p```링크드 리스트를 이용한 이진 트리 표현  배열을 이용한 이진트리 표현의 단점(편향 이진 트리 일시 메모리, 퍼포먼스 낭비, 트리 중간에 새로운 노드를 삽입 하거나 기존의 노드 삭제시 배열 크기 변경 어려움)을 보완하지만 대신 구현과 구조가 복잡해진다는 단점이 생김04 Binary Search TreeBinary search Tree의 특징      탐색작업을 효율적으로 하기 위한 자료구조        모든 원소는 서로 다른 유일한 키를 가짐        키 기준으로, key(왼쪽 서브트리) &lt; key(루트 노드) &lt; key(오른쪽 서브트리)        왼쪽 서브트리와 오른쪽 서브트리도 이진 탐색 트리임        중위 순회시, 오름차순으로 정렬된 값을 얻을 수 있음  Binary search Tree의 연산탐색 연산      루트에서 시작        탐색할 키값 x를 루트 노드의 키값과 비교    1) 키값 x == 루트 노드의 키값일 경우, 원하는 원소를 찾았으므로 성공2) 키값 x &lt; 루트 노드의 키값일 경우, 왼쪽 서브트리에 대해서 탐색 연산 수행3) 키값 x &gt; 루트 노드의 카값일 경우, 오른쪽 서브트리에 대해 탐색 연산 수행        서브트리에 대해서 순환적으로 탐색 연산을 반복  삽입 연산      먼저 탐색 연산을 수행          삽입할 원소와 같은 원소가 트리에 있으면 삽입할 수 없으므로, 같은 원소가 트리에 있는지 탐색하여 확인      탐색에서 탐색 실패가 결정되는 위치가 삽입위치가 됨            탐색 실패한 위치에 원소를 삽입  Binary search Tree의 성능  탐색(searching), 삽입(insertion), 삭제(deletion) 시간은 트리의 높이에 좌우 됨          시간 복잡도는 O(h), h: BST의 깊이        평균의 경우          이진 트리가 균형적으로 생성되어 있는 경우, O(log n)        최악의 경우          한쪽으로 치우친 평향 이진 트리의 경우 O(n)        순차탐색과 시간 복잡도가 같음검색 알고리즘 성능 비교  리스트에서의 순차 검색: O(N)  정렬된 리스트에서의 순차 검색: O(N)  정렬된 리스트에서의 이진 검색: O(logN)  이진 탐색 트리에서의 평균: O(logN)          최악의 경우: O(N)      완전 이진 트리 또는 균형 트리로 바꿀 수 있다면 최악의 경우를 없앨 수 있음                  새로운 원소를 삽입할 때 삽입 시간을 줄임          평균과 최악의 시간이 같음 O(logn)                      해쉬 검색: O(1), 대신 메모리가 크게 듦05 HeapHeap의 특징      완전 이진 트리에 있는 노드 중에서 키값이 가장 큰 노드나 키값이 가장 작은 노드를 찾기 위해 만든 자료 구조          최대 힙(Max heap)                  키 값이 가장 큰 노드를 찾기 위한 완전 이진 트리                                    {부모 노드의 키값 &gt; 자식 노드의 키값}            루트 노드: 키값이 가장 큰 노드                  최소 힙(Min heap)                            키 값이 가장 작은 노드를 찾기 위한 완전 이진 트리                    {부모 노드의 키값 &lt; 자식 노드의 키값}            루트 노드: 키값이 가장 작은 노드      Heap의 연산  노드의 값이 중복되어 있으면 힙의 정의에 맞지않는 이진트리이다.Heap의 삽입 연산      삽입 연산의 경우 새로 넣을 값을 노드의 맨 마지막에 새로 자리를 만들어 넣고 부모노드와 값을 비교해 유효한지 확인한다.        유효할 경우 그대로 종료, 유효하지 않을 경우, 부모 노드와 위치를 바꾼다. 이를 유효할 때까지 반복  Heap의 삭제 연산  힙에서는 루트 노드의 원소만을 삭제할 수 있음  루트 노드의 원소만을 삭제하여 반환  힙의 종류에 따라 최대값 또는 최소값을 구할 수 있음          이를 이용하여 우선순위 큐를 힙으로 구현 가능     - 루트 노드의 원소를 삭제하고, 마지막 노드를 루트 노드 위치로 이동, 이후 삽입 노드와 자식 노드를 비교하며 유효할 때까지 자리를 바꿔나가면 된다     - 최소값 힙 구현 algorithm TIL 190924 확인      "
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%A4%91.html": {
    title: "알고리즘-중",
    date: " Aug 19, 2020 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%A4%91.html",
    tags: ["알고리즘","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘 응용02 알고리즘 복잡도알고리즘      유한한 단계를 통해 문제를 해결하기 위한 절차나 방법        주로 컴퓨터 용어로 쓰이며, 컴퓨터가 어떤 일을 수행하기 위한 단계적 방법        어떠한 문제를 해결하기 위한 절차, 알고리즘을 쓰면 메모리, 재사용성, 성능 측면에서 유리        1~100까지의 합을 구하는 방법: 공식 사용 vs 일일이 하기 정도의 차이        실행에 필요한 자원을 분석하여 효율성 제시, 복잡도(Complexity)로 표현, 높을수록 안좋음       1) 공간적 효율성 : 얼마나 많은 메모리 공간을 요하는가?   2) 시간적 효율성 : 얼마나 많은 시간을 요하는가?   - 하드웨어, 소프트웨어 환경에 따라 처리시간 달라짐, 시간으로는 표현 힘듬   - 보통 알고리즘 효율성은 입력크기가 너무 클때 문제가 됨              같은 일을 해도 알고리즘에 따라 300년걸릴 일일을 5분만에 끝낼 수 있음      점근적 표기      시간 (또는 공간) 복잡도의 점근적 표기          입력 크기에 대한 함수로 표기, 함수는 다항식              단순한 함수로 표현하기 위해 점근적 표기(Asymptotic Notation) 사용    1. O(Big-Oh) 표기            복잡도의 점근적 상한(즉 효율이 최악의 상황일 때의 표기)      다항식의 최고차항만 계수없이 표기함      f(n) = 2n^2^ - 7n + 4라면 f(n)의 O-표기는 O(n^2^)      실행시간이 입력시가 n일 경우 n^2^에 비례하는 알고리즘을 의미함      가장 자주 사용됨      상수시간, 로그 시간, 선형 시간, 로그 선형 시간, 제곱시간, 세제곱시간 등이 있음            Ω(Big-Omega) 표기          복잡도의 점근적 하한( 즉 효율이 최고로 좋은 상황일 때의 표기)      위와 동(최소한 이만한 시간은 걸린다)            Θ(Theta) 표기                  복잡도의 평균 ( 위 두 표기가 서로 같을 경우에 사용)                    전과 동 ()            03 비트연산비트 연산자                              다른 연산자들에 비해 실행 시간이 적게 소요, &amp;,          , ^, ~, «, » 등이 있음 (Python 정리 참조)                          연산 속도를 향상 키거나 메모리 절약 가능, ex ) 홀수 짝수 판별로 모듈러 연산 M%2 이나 N &amp;1로 마지막 비트값이 0인지 1인지 검사 가능        1«n 하면 2^n^의 값을 가지며, 부분집합 등을 구하는 데 사용함        i &amp; (1 « j): 특정 비트값이 1인지 0인지 판별하는데 사용          비트 연산 1: 특정 위치의 비트값을 확인하는 수식에 대한 예제      def BitPrint(i):\tfor j in range(7, -1, -1):\t    print(&amp;#39;1&amp;#39; if (i&amp;#38;(1&amp;#38;#60;&amp;#38;#60;j)) else &amp;#39;0&amp;#39;, end=&amp;#34;&amp;#34;)# print(&amp;#34;&amp;#37;d&amp;#34; &amp;#37; ((i&amp;#38;#60;&amp;#38;#60;j)&amp;#38;1), end=&amp;#34;&amp;#34;)for i in range(-5, 6):\tprint(&amp;#34;&amp;#37;2d =&amp;#34; &amp;#37; i, end=&amp;#34;&amp;#34;)# 십진수 출력    BitPrint(i)# 이진수 출력    print()  비트 연산 2: 4바이트 크기의 인트형 변수에 저장된 값들을 한 바이트씩 읽어서 비트 형태로 출력하는 예제a = 0x10x = 0x01020304print(&amp;#34;&amp;#37;d=&amp;#34; &amp;#37; a, end=&amp;#34;&amp;#34;)BitPrint(a)print()print(&amp;#34;&amp;#37;08x= &amp;#34; &amp;#37; x, end=&amp;#34;&amp;#34;)for i in range(0, 25, 8):    BitPrint(x&amp;#38;#62;&amp;#38;#62;i)    print(end=&amp;#34; &amp;#34;)엔디안(Endianness)  컴퓨터의 메모리와 같은 1차원의 공간에 여러 개의 연속된 대상을 배열하는 방법을 의미하며 HW 아키텍처마다 다름  속도 향상을 위해 바이트 단위와 워드 단위를 변환하여 연산할 대 올바로 이해하지 않으면 오류가 발생할 수 있음1) 빅 엔디안(Big-endian) : 보통 큰 단위가 앞에 나옴, 네트워크 프로그래밍 0x1234 = 12 342) 리틀 엔디안(Litte=endian): 작은 단위가 앞에 나옴, 대다수 PC 0x1234 = 34 12    XOR          비트 연산 4: 비트 연산자 ^를 두 번 연산하면 처음 값을 반환```pythona = 0x86key = 0xAAprint(\"a ==&#38;#62; \", end=\"\")BitPrint(a)print()print(\"a^=key ==&#38;#62;\", end=\"\")a ^= keyBitPrint(a)print()print(\"a^=key ==&#38;#62;\", end=\"\")a ^= keyBitPrint(a)print()      ### 04 진수#### 진법 변환* 문제 해결을 위해서 진수 변환이 필요한 경우- 2진수, 8진수, 10진수, 16진수 등이 있으며 원하는 타진법의 수로 나눈 뒤, 나머지를 거꾸로 읽어서 만듦- 타진수를 10진수로 변환은 각 자릿값을 해당 진수의 값을 곱해서 구함  #### 음의 정수 표현* 컴퓨터에서의 음의 정수 표현 방법- Python 정리의 음의 정수 표현방법 참조  ### 05 실수  #### 2진 실수* 컴퓨터에서의 실수의 표현 방법    - 각 자리수는 2^-1^.... 2^-n^으로 감소하여 구함    #### 부동 소수점 표기법- 컴퓨터에서 실수를 표현하기 위해 소수점의 위치를 고정시켜 표현하는 방식 6.02 X 10^23^- 32비트는 단정도 실수, 배정도 실수는 64비트로 저장 Python,C 정리 참조   #### 유효 숫자- 8비트의 255 가지수의 중간(127)을 0으로 놓고 그 위를 양수, 그 아래를 음수로 표현하는 Excess 표현법을 사용함- 즉 10진수 값 255(2진수값 11111111)은 실제로 128지수를 의미하며 0(00000000)은 -127을 의미- 컴퓨터는 실수를 근사적으로 표현하므로 작은 오차가 생기게 됨  ## 02 완전검색  ### 1. 완전 검색 기법- 문제의 해를 얻기 위해 가능한 모든 경우들을 나열해 보고 확인하는 기법- 고지식한 방법(Brute-force), 생성 및 테스트 라고도 불리우며- 문제 해결하기 위한 간단하고 쉬운 접근법으로, 빠른 시간 안에 문제 해결(알고리즘 설계) 가능- 대부분의 문제에 적용 가능, 문제의 자료의 크기가 작을 경우 유용  #### 고지식한 검색( 순차 검색, Sequential Search)- 자료들의 리스트에서 키 값을 찾기 위해 첫 번째 자료부터 비교하면서 진행    &gt; 예시 코드  ```pythondef sequentialSearch(a, n, key):    i = 0    while i &amp;#38;#60; n and a[i] != key :        i = i + 1    if i &amp;#38;#60; n : return i# 성공    else: return -1# 실패  리스트의 키값이 존재하지 않을 시, 모든 자료들에 대해 비교 작업이 진행되므로 가장 느림완전 검색으로 시작하기      문제 해결을 위한 가장 단순한 방법이기 때문에 문제의 크기가 커지면 시간 복잡도가 매우 크게 증가        모든 경우의 수를 생성하고 테스트하기 때문에 수행 속도는 느리지만, 해답을 찾아내지 못할 확률이 적음          완전검색을 통해 입력의 크기를 작게 해서 빠르게 답을 구하는 알고리즘 설계            그리디 기법이나 동적 계획법을 이용해서 효율적인 알고리즘 찾음        학술적 또는 교육적 목적을 위해 알고리즘의 효율성을 판단하기 위한 척도로 사용        검정 등에서 주어진 문제를 풀 경우, 완전 검색으로 시작하여 답을 구한 후, 성능 개선을 위해 다른 알고리즘을 사용하는 방식으로 이용함  2. 조합적 문제1) 완전 검색과 조합적 문제      많은 종류의 문제들이 특정 조건을 만족하는 경우나 요소를 찾는 검색        순열, 조합, 부분집합 같은 조합적 문제들과 관련됨, 조합적 문제에 대한 고지식한 방법  2) 순열      서로 다른 것들 중 몇 개를 뽑아 한 줄로 나열하는것, 서로 다른 n개 중 r개를 택하는 순열 표현        nPr = n!, n Factorial이라고도 부름        다수의 알고리즘 문제들이 순서화된 요소들의 집합에서 최선의 방법을 찾는 것과 관련됨        N의 크기가 증가할 수록 실행 시간이 기하 급수적으로 증가함, 완전 검색은 비현실적인 방법  단순한게 순열을 생성하는 방법  {1, 2, 3}을 포함하는 모든 순열을 생성하는 함수for i1 in range(1, 4):    for i2 in range(1, 4):        if i2 != i1:            for i3 in range(1, 4):                if i3 != i1 and i3 != i2:                    print(i1, i2, i3)              일일이 포문을 생성하기 힘듬          사전식 순서(Lexicographic-Order)              요소들이 오름차순으로 나열된 형태가 시작하는 하나의 순열            최소 변경을 통한 방법(Minimum-exchange requirement)                  각각의 순열들은 이전의 상태에서 단지 두 개의 요소들 교환을 통해 생성                    {1, 2, 3,} 순열에서 1과 3을 교환해서 {3, 2, 1} 순열을 만듦                    Johnson-Trotter 알고리즘                    N이 1~4이고, 서로 바꾸는 순서를 명시할 때,                    (1,2) (2,3), (3,4) (1,2) (3,4) (2,3) (1,2) (3,4) 무한 반복 하면 언젠가는 1,2,4,3으로 끝나 생성                    두 원소의 교환을 통해 생성, 트리 구조를 가짐, 재귀호출로 순열 생성 가능              재귀 호출을 통한 순열 생성# a[]: 데이터가 저장된 리스트# n: 원소의 개수, k: 현재까지 선택된 원소의 수def perm(n,k) :\tif k == n:# 하나의 순열이 생성됨.\t    print(a)# 원하는 작업 수행    else:\t    for i in range(k, n):\t        a[k], a[i] = a[i], a[k]# 교환을 통한 선택            perm(n, k + 1)# 재귀 호출            a[k], a[i] = a[i], a[k]# 이전 상태로 복귀      파이선의 라이브러리 itertools의 permutations를 통하여 순열 생성 가능        permutations(list, r) : list에 원소들의 리스트를, r에 원하는 집합의 크기, 생략시 list와 동 크기        repeat=숫자를 적으면 그 수만큼 중복을 허용함 기본값은 1  3. 부분집합      집합에 포함된 원소들을 선택하는 것        다수의 중요 알고리즘들이 원소들의 그룹에서 최적의 부분 집합을 찾는 것        N 개의 원소를 포함한 집합은 2^n^개가 존재함          단순하게 모든 부분 집합 생성하는 방법 ( 4개 원소의 Power set)      arr = [2, 3, 4, 5]# 실제 집합bit = [0]*len(arr)for i in range(2):    bit[0] = i# 0번째 원소    for j in range(2):        bit[1] = j# 1번째 원소        for k in range(2):            bit[2] = k# 2번째 원소            for l in range(2):                bit[3] = l# 3번째 원소                print([arr[x] for x in range(len(bit)) if bit[x]])# 생성된 부분집합 출력  이런식으로 일일이 for문을 쓰지 않고 비트연산이나 재귀로 생성 가능바이너리 카운팅을 통한 사전적 순서      부분집합을 생성하기 위한 가장 자연스럽고 간단한 방법        바이너리 카운팅(Binary Counting)은 사전적 순서로 생성하기 위한 가장 간단한 방법        원소 수에 해당하는 N개의 비트 열을 이용, i 번째 비트 값이 1이면 i 번째 원소가 포함됬음          바이너리 카운팅을 통한 부분집합 생성 코드      arr = [2, 3, 4, 5]n = len(arr)# n: 원소의 개수for i in range(1&amp;#38;#60;&amp;#38;#60;n):# 1&amp;#38;#60;&amp;#38;#60;n: 부분집합의 개수    for j in range(n):# 원소의 수만큼 비트를 비교함        if i &amp;#38; (1&amp;#38;#60;&amp;#38;#60;j):# i의 j번째 비트가 1이면 j번재 원소 출력            print(arr[j], end=&amp;#34;,&amp;#34;)    print()#또는for i in range(1&amp;#38;#60;&amp;#38;#60;len(arr)):# 1&amp;#38;#60;&amp;#38;#60;n: 부분집합의 개수    print([arr[j] for j in range(len(arr)) if i &amp;#38; (i &amp;#38;#60;&amp;#38;#60; j)])4. 조합(Combination)  서로 다른 n개의 원소 중 r개를 순서없이 골라낸 것  nCr = n! / ((n-r)! r!) 로 표현, nCr = n-1Cr-1 + n-1Cr nC0 = 1, nCn = 1 (재귀적 표현)          재귀 호출을 이용한 조합 생성 알고리즘```python      listnum: 집합의 크기      combinum: 조합 대상의 크기      def comb(listtarget, combitarget):    global combi    global eles    global result    global cntcnt += 1if combitarget == 0:# 만약 모든 원소가 채워지면    result.append(list(combi))# 완성된것임elif listtarget &amp;#38;#60; combitarget:# 넣을 수 있는 원소의 수보다 넣어야 하는 조합의 크기가 더 크면 안됨, 2번째 루트가 원소수는 줄이는데 조합수는 줄이지 않으므로 꼭필요함,    returnelse:    combi[combitarget-1] = eles[listtarget-1]# 해당 조합 자리에 해당 원소를 집어 넣음    comb(listtarget-1, combitarget-1)# 그 다음 원소를 그 다음번 조합 자리에 넣는 방향    comb(listtarget-1, combitarget)# 현제 그 자리에 그 원소 대신 다른 원소를 넣는 방향, 2번째 루트 # 선택 정렬과 비슷한 방식이네 eles = [1, 2, 3, 4] result = [] cnt = 0 for i in range(4+1):combi = [None for _ in range(i)]comb(4, i) print(result) print(cnt)- 파이썬에서 itertools 라이브러리 객체의 combinations()를 통하여 조합 생성 가능- cominations_with_replacement()를 통하여 중복 조합 생성 또한 가능  ## 03 탐욕 알고리즘  ### 01 탐욕 알고리즘- 최적해를 구하는 데 사용되는 근시안적인 알고리즘 주로 최적(최대, 최소)값을 구할 때에 쓰임- 여러 해가 있을 수도 있고 일반적으로 머리 속에 떠오르는 생각을 그대로 쓰면 나타남- 1) 여러 경우 중 하나 선택, 2) 선택 마다 최적이라고 생각되는 것을 선택, 3) 최종적인 해답에 도달- 한번 선택된 것은 번복되지 않으므로, 단순하며, 제한적인 문제들에 적용, 지역적으로만 최적  #### 1. 동작 과정 (동전 거스름돈 문제 예시)* 동전 거스름돈 문제- 거스름돈으로 거내주어야할 동전의 수를 최소화하기 문제1. 해 선택      - 현재 상태에서 부분 문제의 최적해를 구한 뒤, 부분해 집합에 추가   - 하나의 선택이 이루어지면 새로운 부분 문제 발생   - 현재 고를 수 있는 가장 단위가 큰 동전을 골라 거스름돈에 추가2. 실행 가능성 검사 실시      - 새로운 부분 해 집합의 실행가능 여부 확인   - 문제의 제약 조건 위반을 검사   - 거스름 돈이 손님에게 내드려야 할 액수를 초과하는지 확인, 초과하면 1로 돌아가 한 단계 작은 단위의 동전을 추가하여 반복3. 해 검사      - 새로운 부분 해 집합이 문제의 해가 되는지 확인   - 전체 문제의 해가 완성되지 않았다면 1의 해 선택부터 다시 시작   - 손님에게 드려야 하는 거스름돈의 액수와 같은지 확인, 액수가 모자라면 1. 해 선택부터 다시- 반드시 최적해를 구한다는 보장할 수 없다. 알고리즘의 정확성을 증명하는 과정 필요- DP(동적 프로그래밍)이나 완전 검색 기법으로 풀어야 한다.- 무게와 가치를 고려하여 정해진 무게만큼 담는 배낭 문제 (0-1 Knapsack)  등도 비슷하게 푼다.- 활동 선택 문제(회의시간이 겹치는 회의들을 가능한 많이 회의실에 배정)와 fractional Knapsack(물건을 나눌 수 있는 배낭 문제),Baby-gin 문제(run : 같은 카드 3장, triplet : 연속된 3숫자를 이루는 카드를 가지고 있는가?) 등은 탐욕 기법으로 구해도 해를 구해짐  #### 2. 탐욕 기법 검증1) 탐욕적 선택 속성(Greedy choice property)- 탐욕적 선택은 최적해로 갈 수 있음, 즉 항상 그 선택은 최적해로 가는 안전한 길임2) 최적 부분 구조(Optimal substructure property)- 최적화 문제를 정형화, 하나의 선택을 하면 풀어야 할 하나의 하위 문제가 남음- [원 문제의 최적해 = 탐욕적 선택 + 하위 문제의 최적해] 임을 증명    &gt; 탐욕 알고리즘이 최적해를 구한다는 것에 대한 증명, 활동 선택 문제 예시  1. 문제에서 종료시간이 가장 빠른 활동 am을 선택하는 것은 항상 안전한가?- 전체 회의들의 집합 S에서 서로 겹치지 않는 최대 크기의 부분 집합 A가 있음(최적해?)- ak는 A에 속한 종료 시간이 가장 빠른 회의- 만약 ak = am이면 최대 크기 부분집합에 포함- 만약 ak != am이면 A에서 ak를 제거하고 am을 추가해도 A 부분집합의 다른 회의와 겹치지 않게 됨,- 그러므로 종료 시간이 가장 빠른 활동을 선택하는 것은 항상 안전2. 활동 선택 문제의 해 = 탐욕적 선택 + 하위 문제의 최적해- S에서 am을 선택하면 하위 문제 S(i~m)와 S(m~j) 존재- S(i~m)은 공집합이므로 am을 선택한다는 것은 부분 문제 S(m~j)만이 고려해야할 활동들이 존재하는 유일한 부분 문제임을 보임(am이 가장 빠르므로 S(m~j)는 회의가 없다)- S(m~j)에 회의가 있다면 이는 am 보다 빠르므로 am이 가장 빠른 회의라는 선택에 모순- 그러므로 S(m~j)는 공집합이다.  #### 3. 탐욕 기법과 동적 계획법의 비교&gt; 비교표| 탐욕기법                                      | 동적 계획법                           || ----------------------------------------- | -------------------------------- || 매 단계에서, 가장 좋아 보이는 것을 빠르게 선택한다. (지역 최적 선택) | 매 단계의 선택은 해결한 하위 문제의 해를 기반으로 한다. || 하위 문제를 풀기 전에 탐욕적 선택이 먼저 이루어짐              | 하위 문제가 우선 해결 됨                   || Top-down 방식                               | Bottom-up 방식                     || 일반적으로, 빠르고 간결하다.                          | 좀더 느리고, 복잡하다.                    |#### 04 Baby-Gin 다시 보기1. 6개의 숫자는 6자리의 정수 값으로 입력 되며 카운트 리스트에 저장   - 카운트 리스트는 0에서 9까지의 숫자의 빈도수를 저장하는 리스트2. Counts 리스트의 각 원소를 체크하여 Run과 Triplet 및 Baby-Gin 여부 판단   - Counts 리스트의 모든 원소를 1씩 뺀 후에도 Counts의 값이 3 이상이면 Run   - Triplet은 한 리스트에 숫자가 3개 연속으로 0이 아니면 Triplet3. 탐욕 알고리즘 방법 적용   - 카운트 리스트에서 Run과 Triplet 중에 가능한 것을 조사   - 조사에 사용한 데이터는 삭제   - 남은 데이터를 다시 Run과 Triplet 중에 가능한 가를 조사&gt; 대표적인 탐욕 기법의 알고리즘들| 알고리즘           | 목적                               | 설명                                                   |     || -------------- | -------------------------------- | ---------------------------------------------------- | --- || Prim           | N개의 정점으로 구성된                     | 정점을 하나씩 선택하는 과정에서 트리를 확장하면서 MST를 찾음                  | 그래프 || Kruskal        | 최소 신장트리(MST)를 찾음                 | 싸이클이 없는 서프 그래프들을 확장하면서 MST를 찾음                       | 그래프 || Dijkstra       | 주어진 정점에서 다른 정점들에 대한 최단 경로를 찾음    | 주어진 정점에서 가장 가까운 정점을 선택하면서 출발점에서 다른 모든 정점들의 최단 경로를 찾음 | 그래프 || Huffman coding | 문서의 압축을 위해 문자들의 빈도수에 따라 코드 값을 부여 | 출현 빈도가 낮은 문자부터 선택해 이진 트리를 완성하고 코드값을 부여함              | 문자열 |## 04 분할 정복### 1. 소개, 설계 전략- Topdown 방식, 작은 부분 부터 구한 뒤 큰 부분을 구하는 방법 ex) 거듭제곱 알고리즘 1. 분할(Divde) : 해결할 문제를 여러 개의 작은 부분 문제들로 분할2. 정복(Conquer) : 나눈 작은 문제를 각각 해결3. 통합(Combine) : 필요 시 해결된 해답을 모음      &gt; 분할 정복 기반 거듭제곱 함수   ```pythondef Recursive_Power(C, n):    if n == 1:        return C    if n &amp;#37; 2 == 0:# even        y = Recursive_Power(C, n/2)        return y*y    else:# odd        y = Recursive_Power(C, (n-1)/2)        return y*y*C  n번 곱하는 방식은 O(n) 인 시간 복잡도를 O(log2n)으로 줄일 수 있음2. 병합 정렬1. 소개      병합정렬은 여러 개의 정렬된 자료의 집합을 병합하여 한 개의 정렬된 집합으로 만드는 방식        분할 정복 알고리즘을 활용하여 자료를 최소 단위의 문제까지 나눈 후, 차례대로 정렬하여 최종 결과 획득, Top-Down 방식 O(n log n)  2. 병합 정렬 과정      먼저 분할 단계에서 전체 자료 집합에 대하여 최소 크기의 부분 집합이 될 때 까지 계속 분할        병합 단계에서 2개의 부분 집합을 정렬하면서 하나의 집합으로 병합 (이 과정에서 1개의 원소 끼리 크기 비교 하여 정렬한다.)          알고리즘 : 분할 과정      def merge_sort(m):    if len(m) &amp;#38;#60;= 1:# 사이즈가 0이거나 1인 경우, 바로 리턴        return m# 1. Divide 부분    mid = len(m)//2    left = m[:mid]    right = m[mid:]   # 리스트의 크기가 1이 될 때까지 merge_sort 재귀 호출    left = merge_sort(left)    right = merge_sort(right)   # 2. CONQUER 부분: 분할된 리스트들 병합    return merge(left, right)  알고리즘 : 병합 과정def merge(left, right):    result = []# 두 개의 분할된 리스트를 병합하여 result를 만듦       while len(left) &amp;#38;#62; 0 and len(right) &amp;#38;#62; 0 :# 양쪽 리스트에 원소가 남아있는 경우, 두 서브 리스트의 첫 원소들을 비교하여 작은 것부터 result에 추가함        if left[0] &amp;#38;#60;= right[0]:            result.append(left.pop(0))        else:            result.append(right.pop(0))    if len(left) &amp;#38;#62; 0 :# 왼쪽 리스트에 원소가 남아있는 경우        result.extend(left)    if len(right) &amp;#38;#62; 0 :# 오른쪽 리스트에 원소가 남아있는 경우        result.extend(right)    return result  리스트 사용시 분리, 병합 과정에서 자료의 비교연산과 이동 연산이 발생하여 비효율 적이므로 링크드 리스트를 사용하거나 미리 크기가 정해진 result 리스트에 append, pop 대신 조회 후 값을 대입하는 방법으로 하자3. 퀵 정렬퀵 정렬이란?      주어진 리스트를 두 개로 분할하고 각각을 정렬, 병합정렬과 비슷해보이나 차이가 있음                  병합 정렬과 달리 기준 피봇을 중심으로 이보다 작은 것은 왼편 큰 것은 오른편에 위치시킴                    병합정렬은 다시 합병하는 후처리 작업이 필요하지만 퀵정렬은 아니다.            퀵 정렬 알고리즘  동작 과정과 예시 코드# A: 리스트, l: 시작 인덱스, r: 끝 인덱스# 작은 것부터 큰것 순으로 정렬def quickSort(A, l, r):\tif l &amp;#38;#60; r:# 시작 인덱스는 끝 인덱스보다 작아야함\t    s = partition(A, l, r)# 피봇을 반환\t    quickSort(A, l, s-1)#왼쪽 부분 재귀 호출\t    quickSort(A, s+1, r)# 오른쪽 부분 재귀 호출Hoare 파티션 알고리즘 아이디어      P(피봇)값들 보다 큰 값은 오른쪽, 작은 값들은 왼쪽 집합에 위치시킴        이동 후, 피봇을 두 집합의 가운데에 위치시킴(피봇을 정렬 상태일 시, 자기가 있어야할 위치로)        피봇 값은 다음 정렬 과정에서 제외    Hoare-Partition 알고리즘def partition(A, l, r):    p = A[l]# p: 피봇 값    i = l + 1# 첫번째 값은 피봇으로 사용함    j = r    while i &amp;#38;#60;= j:        while(i &amp;#38;#60;= j and A[i] &amp;#38;#60;= p) : i += 1        while(i &amp;#38;#60;= j and A[j] &amp;#38;#62;= p) : j -= 1        if i &amp;#38;#60;= j:            A[i], A[j] = A[j], A[i]    A[l], A[j] = A[j], A[l]    return j  피봇 선택 시 왼쪽 끝, 오른쪽 끝, 임의의 세개 값 중에 중간 값을 고르면 좋다. 피봇 값에 따라 효율이 떨어질 수도 있기 때문에Lomuto 파티션      로무토 파티션은 위와 달리  i,  j 두개의 변수가 모두 증가하면서 작업을 수행하게 됨.        위 보다 속도는 같지만, 더 간단히 구현 가능          Lomuto 파티션 알고리즘      def partition(A, l, r):    x = A[r]# 오른쪽 끝값이 피봇    i = l - 1# 시작위치 -1, i의 인덱스    for j in range(l, r):# 이 루프를 통해서 i번째까지 피봇보다 크기순서 상관 없이 작은 값들이 모이게 됨(당연히 i+2부터는 피봇보다 순서와 관계없이 큰 값임)        if A[j] &amp;#38;#60;= x :# == if A[j] &amp;#38;#60;= A[r] :             i += 1            A[i], A[j] = A[j], A[i]    A[i+1], A[r] = A[r], A[i+1]    return i+1이진 검색      자료의 가운데에 있는 항목의 키 값과 비교하여 다음 검색의 위치를 결정하고, 검색을 계속 진행하는 방법        목적 키를 찾을 때까지 이진 검색을 순환적으로 반복 수행해 검색 범위를 반으로 줄여가면서 보다 빠르게 검색 수행.        단 자료가 미리 정렬된 상태여야 함,  순차검색 보다 효율적임.  이진 검색의 검색 과정      자료의 중앙에 있는 원소를 고른다.        중앙 원의 값과 찾고자 하는 목표 값을 비교한다.        목표 값이 중앙 원소의 값보다 작으면 왼쪽 반에 대해서만 새로 검색 크면 반대로 오른쪽 반에서        찾고자 하는 값을 찾을 때 까지 반복  알고리즘 :반복 구조      자료 삽입, 삭제 발생 시, 리스트의 상태를 항상 정렬 상태로 유지하는 추가 작업 필요          이진 검색 알고리즘      # a : 검색할 리스트# key : 검색하고자 하는 값def binarySearch(a, key):    start = 0    end = len(a) - 1    while start &amp;#38;#60;= end:        middle = start + (end - start) // 2        if key == a[middle]:# 검색 성공            return middle        elif key &amp;#38;#60; a[middle]:            end = middle - 1        else :# a[middle] &amp;#38;#60; key :            start = middle + 1    return -1# 검색 실패  재귀 구조 이진 검색# a : 검색할 리스트# key : 검색하고자 하는 값def binarySearch2(a, low, high, key):    if low &amp;#38;#62; high :# 검색 실패        return -1# 검색 실패 False    else:        middle = (low + high) // 2        if key == a[middle]:# 검색 성공            return middle# True        elif key &amp;#38;#60; a[middle]:            return binarySearch2(a, low, middle-1, key)        else:# a[middle] &amp;#38;#60; key:            return binarySearch2(a, middle+1, hight, key)분할 정복 사례      병합 정렬 : 외부 정렬의 기본이 되는 정렬 알고리즘          멀티코어 CPU, 다수의 프로세서에서 정렬 알고리즘 병렬화를 위해 병합 정렬 알고리즘 활용            퀵 정렬 : 매우 큰 입력 데이터에 대해서 좋은 성능을 보이는 알고리즘          생물 정보 공학 등에서 특정 유전자 찾는데 사용, 문자열에서도 접이어 배열 학습            최근접 점의 쌍 문제 :  2차원 평면상의 n개의 점이 입력으로 주어질 때, 거리가 가장 가까운 한쌍의 점을 찾는 문제          컴퓨터 그래픽스, 컴퓨터 비전, 지리 정보 시스템, 항공 그래픽 제어, 마케팅 등에 사용      05 백트래킹1. 백트래킹소개      N-Queen 문제, 순열, 동전 거스름돈 문제 등에 사용됨 (190919 참조)        해를 찾는 도중에 막히면(즉 해가 아니면) 되돌아가서 다시 해를 찾아가는 기법        최적화 문제와 결정 문제를 해결 할 수 있다.          문제의 조건을 만족하는 해가 존재하는지의 여부를 yes 또는 no로 답하는 문제            초기 상태에서 목표 상태로 가는 경로를 탐색하는 기법          여러가지 선택지(옵션)들이 존재하는 상황에서 한가지 선택      선택이 이루어지면 새로운 선택지들의 집합 생성      선택을 반복하면 최종 상태에 도달            보통 재귀 함수로 구현하며, 반복구조로도 구현 가능하나 재귀 함수가 백트래킹하기에 더 쉽다.  상태 공간 트리      해를 찾기 위한 선택의 과정을 트리로 표현        트리의 내부 노드는 최종 상태로 가는 중간 상태를 나타냄        트리의 단말 노드는 하나의 후보해에 대한 최종 상태가 됨          후보해 : 루트 노드에서 단말 노드로 가는 경로,            상태 공간 트리를 탐색하는 것은 모든 후보해들을 탐색하는 것        트리를 깊이 우선 탐색 하는 방법이 백트래킹 알고리즘의 기본 형태  백트래킹과 깊이 우선 탐색과의 차이      백트래킹 기법은 어떤 노드에서 출발한 경로가 해결책으로 이어질 것 같지 않으면 더 이상 그 경로를 따라가지 않음으로써 시도의 횟수를 줄임( Prunning 가지치기)        깊이 우선 탐색은 그러하지 않으므로 경우의 수가 너무 많아 처리가 불가능        백트래킹 또한 최악의 경우 깊이 우선 탐색과 같을 수 있음        즉 백트래킹은 모든 후보해를 검사하지 않고 유망하지 않다(해답이 될 수 없다)고 결정되면 노드의 부모로 돌아가 다음 자식 노드로 감,  -예를 들어 N-Queen 문제에서 1번째 퀸과 2번째 퀸이 서로 경로상에 존재하면 3번째 퀸은 그냥 놓아보지 않고 2번째 퀸을 다른 곳에 다시 놓는다.        트리에 관한 이야기는 알고리즘 기초 참조          N-Queen 슈도 코드      f(i, N)    if i==N# 모든 줄에 퀸을 놓으면        cnt += 1    else        for j : 0 -&amp;#38;#62; N-1            if col[j] == 0 and right[i+j]==0 and left[i-j+N-1]==0# 다른 줄에 j번 열에 퀸이 없어야 하고\t    # 왼쪽 대각선과 오른쪽 대각선에 퀸이 없어야 한다.                board[i][j] = 1                col[j] = 1# 현재 줄에서 j열을 사용함으로 표시                right[i+j] = 1                left[i-j+N-1] = 1                f(i+1, N)# j열에 놓을 수 있으면 다음 줄로 이동                col[j] = 0                right[i+j] = 0                left[i-j+N-1] = 0백트래킹을 이용한 알고리즘의 진행 절차      상태 공간 트리에 대한 깊이 우선 탐색 실시        방문하는 노드가 유망한지 여부 점검          노드의 유망성 판단 방법은 해를 찾으려는 문제에 따라 달라짐            만일 선택한 노드가 유망하지 않을 경우, 해당 노드의 부모 노드로 돌아가서 검색 계속 진행          일반적인 백트래킹 알고리즘 코드      def checknode(v):# node\tif promising(v):         if there is a solution at v:            write the solution        else:            for u in each child of v:            checknode(u)power set 생성 방법      power set이란 어떤 집합의 공집합과 자기 자신을 포함한 모든 부분집합,        구하고자 하는 어떤 집합의 원소 개수가 n일 경우 부분집합의 개수는 2^n^        algorithmTIL 190919 참조    powerset 생성법# listnum: 집합의 크기# combinum: 조합 대상의 크기def comb(listtarget, combitarget):    global combi    global eles    global result    global cnt    cnt += 1    if combitarget == 0:# 만약 모든 원소가 채워지면        result.append(list(combi))# 완성된것임    elif listtarget &amp;#38;#60; combitarget:# 넣을 수 있는 원소의 수보다 넣어야 하는 조합의 크기가 더 크면 안됨, 2번째 루트가 원소수는 줄이는데 조합수는 줄이지 않으므로 꼭필요함,        return    else:        combi[combitarget-1] = eles[listtarget-1]# 해당 조합 자리에 해당 원소를 집어 넣음        comb(listtarget-1, combitarget-1)# 그 다음 원소를 그 다음번 조합 자리에 넣는 방향        comb(listtarget-1, combitarget)# 현제 그 자리에 그 원소 대신 다른 원소를 넣는 방향, 2번째 루트# 선택 정렬과 비슷한 방식이네\teles = [1, 2, 3, 4]\tresult = []\tcnt = 0\tfor i in range(4+1):\t    combi = [None for _ in range(i)]\t    comb(4, i)\tprint(result)\tprint(cnt)순열 생성 알고리즘과 슈도 코드  슈도 코드```python  order[] : 순열의 순서를 저장하는 리스트  def permutation(order, k, n):    if k == n:        print_order_array(order, n)    else:        check = [False] * n        for i in range(k):            check[order[i]] = True    for i in range(n):        if check[i] == False:            order[k] = i            permutation(order, k+1, n)&gt; python 순열 알고리즘```pythondef f(n, path, used):    global pathes    for k in range(idx):        if used[k] == 0:            used[k] = 1            path[n] = locates[k]            if n == idx-1:                print(path)            f(n+1, path, used)            used[k] = 0idx = 5locates = [i for i in range(idx)]visited = [0 for j in range(idx)]soonyeal = [0 for j in range(idx)]f(0, soonyeal, visited)06 그래프의 기본과 탐색그래프 기본소개      그래프란 : 객체( 사물 또는 추상적 개념)들과 객체들 사이의 연결 관계 표현        정점(Vertex)들의 집합과 정점을 연결하는 간선(Edge)들의 집합으로 구성된 자료 구조정점 : 각 객체의 표현, 간선 : 이들 사이의 관계        선형 자료구조나 트리 자료구조로 표현 어려운 N:N 관계를 가지는 원소들을 표현하기에 용이        인접행렬은 만들기 쉽고 직관적이나 쓸데없는 메모리와 효율 낭비가 심함          무향 그래프(Undirected Graph) : 서로 대칭적인 관계를 연결해서 나타낸 그래프            유향 그래프(Directed Graph): 간선을 화살표로 표현하고 방향성의 개념(서로 대칭적 아님)        가중치 그래프(Weighted Graph): 각 간선간에 비용이 있음, 이동 소요 시간, 비용 등 *인접(Adjacency) : 두 개의 정점이 간선에 의해 연결해 있는 경우          완전 그래프에 속한 임의의 두 정점들은 모두 인접해 있음,      부분 그래프 : 완전 그래프에서 일부의 정점이나 간선을 제외한 그래프     * 경로: 간선들을 순서대로 나열한 것, 단순경로는 경로 중 한 정점을 최대한 한번만 지나는 경로            사이클: 시작한 정점에서 끝나는 경로 (ex) 1 - 3 - 5 - 1)        0 - 2 - 4 - 6 (경로)        사이클이 없는 유향 그래프 (DAG, Directed Acyclic Graph)  그래프 표현  간선의 정보를 저장하는 방식, 메모리나 성능을 고려해서 결정          인접 행렬(Adjacent matrix) : 정점수 * 정점수 크기의 2차원 리스트를 이용해 간선 정보 저장              두 정점을 연결하는 간선의 유무를 행렬 형태로 표현      행 번호와 열 번호는 그래프의 정점에 대응, 두 정점이 인접되있으면 1 아니면 0      무향 그래프는 i번째 행의 합과 열의 합이 같으며 해당 정점의 차수(연결된 간선수)이다.      유향 그래프는 행의 합(가로줄)은 진출 차수(나가는 간선), 열의 합은 진입 차수(반대)이다.      인접행렬의 단점 : 1) 행과 열이 같은 부분은 자기 자신으로 가는 간선을 의미하므로 0으로 놓게 되고 쓸데 없는 메모리 낭비가 된다.2) 정점의 개수 n이 커지면 인접 행렬에 필요한 메모리 크기는 n^2^에 비례해서 커진다.3) 어떤 정점의 인접 정점을 찾을 때 마다, 천 개의 슬롯을 조사해야 함      그래프에 포함된 간선의 수가 많지 않은 경우 메모리 사용을 줄이기 위해 간선들의 정보를 나열해서 저장하는 것이 나음                  간선 리스트에서 어떤 정점의 인접 정점은 일치하는 시작 정점을 찾으면 됨                      인접 리스트(Adjacent List) : 각 정점마다 인접 정점으로 나가는 간선의 정보 저장          정점의 개수에 비해 상대적으로 간선의 수가 적을 때, 간선들을 리스트에 연속으로 저장해 사용            인접리스트는 구현이 힘듬 대신 메모리와 효율 이 괜찮음 10000개 이상의 정점이 있을시 쓸것,        각 정점에 대한 인접 정점들을 순차적으로 표현        정점의 개수만큼 메모리 주소를 저장하는 리스트 존재,        정점의 번호에 대응하는 곳은 연결 리스트의 첫번째 노드에 대한 주소 저장        각 정점의 연결 리스트는 인접 정점의 개수만큼 노드를 연결  그래프 탐색그래프 순회  비선형 구조인 그래프로 구현된 모든 자료(정점)을 빠짐없이 탐색, 그래프 탐색의 대표적인 방법깊이 우선 탐색(DFS, Depth First Search)1) 시작 정점에 갈 수 있는 한 방향을 선택해서 다음 정점으로 이동2) 선택된 정점에서 다시 1)과 같은 작업을 반복 수행하면서 갈 수 있는 경로가 있는 곳까지 깊이 탐색, 이때 이미 방문했던 정점은 재방문하지 않음3) 더 이상 갈곳이 없으면, 가장 최근에 방문한 갈림길이 있는 정점으로 되돌아와서 다른 방향의 정점으로 탐색을 계속 반복하여 결국 모든 정점을 방문하는 순회 방법      가장 마지막에 만났던 갈림길의 정점으로 되돌아가서 다시 깊이 우선 탐색 반복        후입선출 구조의 스택 사용하거나 재귀 호출을 이용해서 구현          DFS 알고리즘 - 재귀      # G: 그래프, v: 시작 정점# visited: 정점의 방문 정보 표시, False로 초기화# G[v]: 그래프 G에서 v의 인접 정점 리스트def DFS_Recursive(G,v):    visited[v] = True    visit(v)    for w in G[v]:        if not visited[w]:            DFS_Recursive(G,w)  DFS 알고리즘 - 반복# G: 그래프, S: 스택, v: 시작 정점# visited: 정점의 방문 정보 표시, False로 초기화# G[v] : 그래프 G에서의 v의 인접 정점 집합def DFS_Iterative(S, v):    S = [v]\twhile stack:\t    v = S.pop()\t    if v not in visited:\t        visited.append(v)\t\t    visit()\t        S.extend(G[v] - set(visited))\t    return visited-반복 구조 stack DFS 시, push와 함께 visited를 표시하자 그러면 조금 줄일 수 있다.너비 우선 탐색(BFS, Breadth First Search)      탐색 시작점의 인접한 정점들을 먼저 모두 차례로 방문        방문했던 정점들을 다시 시작점으로 하여 앞의 과정을 반복 수행                  이미 방문한 정점은 재방문하지 않음                    선입 선출 구조인 큐를 활용함                  너비우선 탐색 알고리즘 - 재귀                    # G : 그래프, Q : 큐, v : 시작 정점# visited: 정점의 방문 정보 표시, False로 초기화# G[v]: 그래프 G에서의 v의 인접 정점 리스트def BFS(Q, v):\tQ.append(v)    visited[v] = True    visit(v)    while Q:\t    v = Q.pop(0)        for w in G[v]:\t        if not visited[w]:\t            Q.append(w)                visited[w] = True                visit(w)# 시장정점에서 거리를 구하는 확장 알고리즘 D[]: 최단 거리, P[]: 최단 경로def BFS2(Q, v):\tD[v] = 0    P[v] = v    Q.append(v)    visited[v] = True    visit(v)    while Q:\t    v = Q.pop(0)        for w in G[v]:\t        if not visited[w]:\t            Q.append(w)                visited[w] = True                visit(w)                D[w] = D[v] + 1                P[w] = v  BFS 정점으로 부터의 거리 ,visited 표시할 때, V[i]=1 대신, V[i] = V[n] + 1로 하면 시작점과 해당 정점까지의 거리를 구할 수 있다. 대상 V[대상 노드 인덱스] - 시작점 V[시작점] = 거리, 오직 시작점과의 거리만 구할 수 있음, 0이면 경로가 없는거위상 정렬(Topological Sort)      DAG만 가능(Directed Acyclic Graph) : 방향성 있는 비 사이클 그래프 O(N+M)        제일 앞에 오는 정점은 Indegreer가 0이여야 한다.        위상정렬을 한 후 Indegree가 0인 정점은 앞으로 옮겨도 정렬이 유지됨  알고리즘Indegree : 이 노드로 들어오는 간선 수, Outdegree: 노드에서 나가는 간선수  Indegree가 0인 정점들을 큐에 추가한다.  큐가 빌 때까지 다음을 반복한다.(O(N))          1) 큐에서 정점을 하나 뽑아 배치한다.O(1)      2) 해당 정점에서 뻗어나가는 간선(=이 노드에서 갈 수 있는 다른 노드들로 향하는 간선)들을 삭제한다.(O(M)) (3번도 이때 같이 처리하면 O(N)이 사라짐)      3) 이로 인해 Indegree가 0이 된 정점들을 큐에 추가한다.(O(N))        배치가 안된 정점이 남아있다면, 주어진 그래프에 사이클이 존재하는 것이다.(DAG 아님)          큐 대신 스택으로 쓰면 정답이지만 경우의 수가 다른 정답을 내놓는다.      (위상 정렬은 여러가지 경우의 수의 정답을 가진 경우가 많음).import sys, heapqinput = sys.stdin.readlinestudentNum, compNum = map(int, input().split())indegree = [[] for _ in range(studentNum+1)]outdegree = [[] for _ in range(studentNum+1)]for _ in range(compNum):\tfr, bk = map(int,input().split())    indegree[bk].append(fr)    outdegree[fr].append(bk)hq = []for i in range(1, studentNum+1):    if len(indegree[i]) == 0:        hq.append(i)while hq:    student = heapq.heappop(hq)    print(student, end=&amp;#34; &amp;#34;)    for taller in outdegree[student]:        indegree[taller].remove(student)        if len(indegree[taller]) == 0:            heapq.heappush(hq, taller)상호배타 집합들서로소 또는 상호배타 집합들  서로 중복 포함된 원소가 없는 집합들로 교집합이 없음,  집합에 속한 하나의 특정 원소(대표자, Representative)를 통해 각 집합들을 구분  구현이 간단하고 동작속도가 빠르기 때문에 그래프 영역에서 많이 사용되고 다른 알고리즘의 일부로 활용, ex) 그래프의 연결성 확인하기, KRUSKAL MST 알고리즘 각 집합에 속한 원소 수 관리  상호배타 집합을 표현하는 방법으로 연결리스트와 트리구조가 있다.  문제를 풀 때 양반향인지 단방향인지 주의, 시작할 때 p[rep(n2)] = repr(n1) 식으로 사이클을 줄이고 루트 최적화를 할 수 있다.  배열로 만들수 있지만 배열로 만들면 O(N)으로 느리다  트리로 만들면 좀더 빠르다(다만 집합의 부모를 집합의 대표로 바꿔주는 pathcompression 작업 필수O(n^2)에서 O(nlog*n) 로 바꿔줌)  a(N) = log* n(ackermann 함수) = log n보다 적음, n이 100억이여도 4, 상수시간 수준 보통 5이하임  상호배타 집합 연산의 종류          Make-Set(x) - 원소 x 만으로 구성된 집합을 생성하는 연산      Find-Set(x) - 임의의 원소 x가 속한 집합을 알아내기 위해 사용하며, 집합의 대표자를 알기 위한 연산      Union(x, y) - x 원소가 속한 집합과 y 원소가 속한 집합을 하나의 집합으로 합치는 연산              합쳐진 새로운 집합의 대표자는 아무거나 골라도 됨이외에 Size함수로 합쳐질때마다 size 값이 갱신되도록 하여 전체 노드의 갯수를 알 수 있게 가능이 를 통하여 가장 큰 집합 추적, 노드 개수가 몇개 이상이 되는 시점을 찾을 수 있음      DFS 시 재귀는 백트래킹을 놓기 쉽고 스택을 이용한 반복구조는 빠르다 즉 모든 구조를 돌아야할 때는 반복구조로, 가지치기가 가능하면 재귀구조로 짜는게 빠름( 반복구조는 append와 pop을 쓰지 말고 top포인터로 쓰면 빠르다)연결리스트 표현      같은 집합의 원소들은 하나의 연결 리스트로 관리        연결리스트의 첫 번째 원소를 집합의 대표 원소로 선택        각 원소는 집합의 대표 원소를 가리키는 링크를 가짐        두 집합을 합칠 때는 크기가 작은 집합을 큰 집합의 뒤에 연결  트리 표현      연결리스트보다 더욱 효율적임        하나의 집합(a disjoint set)을 하나의 트리로 표현        자식 노드가 부모 노드를 가리키며 루트 노드가 대표자가 됨        Union 함수 사용시, 해당 루트의 부모를 다른 루트노드로 바꾸면 됨        find-set 시, 자기 자신을 부모로 가리키는 원소를 찾을때 까지 탐색(즉, 루트노드)        상호 배타 집합을 트리로 표현하기 위해 리스트 사용,                  각 원소의 부모에 대한 정보 저장 형태, 자기 자신을 가리키면 대표자가 됨                  트리 표현 시 상호배타 집합 연산들에 대한 알고리즘                    def Make_Set(x):# 유일한 원소 x를 포함하는 새로운 집합을 생성하는 연산    p[x] = x        def Find_Set(x):# x를 포함하는 집합을 찾는 연산    if x == p[x] : return x    else: return Find_Set(p[x])    def Union(x, y):# x와 y를 포함하는 두 집합을 통합하는 연산    p[Find_Set(y)] = Find_Set(x)      문제점 : 집합 Union하는 과정에서 편향된 트리 구조 생성, 이로 인해 함수에서 여러 재귀 호출이 필요함, 이를 해결하기 위해 모든 원소들의 부모를 루트 노드로 하면 1번만 호출해도 부모 찾아감        연산 효율을 높이기 위해                  Rank를 이용한 Union                  각 노드는 자신을 루트로 하는 Subtree의 높이를 랭크(Rank)라는 이름으로 저장          두 집합을 합칠 때 Rank가 낮은 집합을 Rank가 높은 집합에 붙임          두 집합의 Rank값이 같으면 랭크값이 1 증가됨                            Path compression                              Find-Set을 행하는 과정에서 만나는 모든 노드들이 직접 Root를 가리키도록 부모 정보를 변경                          효율성이 고려된 상호배타 집합 연산들                                          # p[x]: 노드 x의 부모 저장# rank[x]: 루트 노드가 x인 트리의 랭크 값 저장# 유일한 멤버 X를 포함하는 새로운 집합을 생성하는 연산def Make_Set(x):\tp[x] = x# 자기자신\trank[x] = 0  # x를 포함하는 집합을 찾는 오퍼레이션  def Find_Set(x):\tif x!= p[x]:# x가 루트가 아닌 경우\t    p[x] = Find_Set(p[x])# Path Compression    return p[x]  # 특정 노드에서 루트까지의 경로에 존재하는 노드가 루트를 부모로 가리키도록 갱신  # x와 y를 포함하는 두 집합을 통합하는 오퍼레이션  def Union(x, y):    Link(Find_Set(x), Find_Set(y))  def Link(x,y):    if rank[x]&amp;#38;#62;rank[y]:        p[y] = x    else:        p[x] = y    if rank[x] == rank[y]:        rank[y] += 1Indexed Tree(Segment tree)  Sum Indexed Tree (도식화)      숫자의 갱신과 주어진 구간([a:b])에 대해서 원하는 연산 (총합, 최대값, 최솟값)을 O(logN)의 시간복잡도로 해결할 수 있는 자료구조, (tree 구조를 기반)        높이는 log(N)+1, 부모 노드의 값은 두 자식 노드 값의 계산 결과(합이면 두 노드의 합, 최대값이면  둘 중 더욱 큰거)        자식노드가 바뀌면 부모노드와 조상노드들을 바꿔줘야함, 갱신 시간복잡도: O(logN)        같은 높이의 3개 이상의 노드가 사용될 일이 없다( 비둘기집의 원리) 연산 시간복잡도 O(logN)        1차원 배열로 구현 가능, 루트가 인덱스 1부터 시작  알고리즘      1차원 배열의 단말부분에 내가 쓰고 싶은 부분을 넣는다.          노드 갯수의 4배를 하면 된다.((노드갯수+ 연산을 위한 빈 노드들) + 내부(조상 노드들)갯수 (앞 두 합의 2배) )      연산을 위해 인덱스를 구하는 방법 : 트리의 단말 노드 갯수 - 1 이 가장 마지막 레벨이 낮고 마지막에 나오는 부모 노드(매직 넘버)이 이후로가 단말노드의  첫번재 인덱스            그 밑(부모들)에는 두 단말노드의 연산값, 그 부모 노드 둘의 연산값들을 넣는다.( O(N)으로 생성가능)(전체 갯수가 최대 2N개이므로 ( 단말노드 N, 내부노드 N-2~N-2))        빈 배열에 넣을 값은 용도에 따라 다른데, 예를 들어 합이 목적이면 0을 넣으면 된다. 최솟값이 목적이면 해당 자료형의 최소값(연산에 대한 항등원)을 넣으면 된다.        left와 right 두 포인터를 가지고 left의 인덱스가 홀수면 left++, right 인덱스가 짝수면 right–(둘다 누적 한뒤 실행한다.)                  모든 홀수번 정점은 오른쪽 자식이고, 모든 짝수번 정점은 왼쪽 자식이므로                    이후 left와 right의 부모로 간뒤(나누기2) 이후 누적이나 연산한 후, 무한 반복                    그러다 left가 right 보다 크면 모든 연산이 끝난 것이다.            import sysinput = sys.stdin.readlinenumCount= int(input())# segment tree nums = [0 for _ in range(numCount*2)] + list(map(int, input().split())) + [0 for _ in range(numCount)] for i in range(len(nums), -1, -1):    if (2*i+1 &amp;#38;#60; len(nums)):        nums[i] = nums[2*i]+nums[2*i+1]left, right = map(int, input().split())left += numCount*2 - 1right += numCount*2 - 1result = 0while(left &amp;#38;#60;= right):    if (left&amp;#38;1):        result += nums[left]        left += 1    if not (right&amp;#38;1):        result += nums[right]        right -= 1            left = left//2    right = right//2print(result)# 배열의 값이 바뀌는 경우가 아니면 왠만하면 DP로 푸는게 더빠르고 쉽다.07 그래프의 최소 비용 문제1. 최소 신장 트리      그래프의 최소 비용 문제는 보통    1) 최소 신장 트리 문제          가중치 그래프에서 모든 정점들을 연결하는 간선들의 가중치의 합이 최소가 되는 트리를 찾는 문제2) 최단 경로 문제      시작 정점에서 목표 정점까지 가는 간선의 가중치의 합이 최소가 되는 경로를 찾는 문제      최소 신장 트리 소개      신장 트리(Spanning Tree): n개의 정점을 포함하는 무향 그래프에서 n개의 정점과 n-1개의 간선으로 구성된 트리, 그래프에 존재하는 신장 트리의 수는 정점의 개수와 간선의 수에 비례해 증가        최소 신장 트리(Minimum Spanning Tree) : 가중치 그래프에서 신장 트리를 구성하는 간선들의 가중치의 합이 최소인 신장 트리  2. 프림 알고리즘프림 알고리즘이란?  한 정점에서 연결된 간선들 중 하나씩 선택하면서 최소 신장 트리를 만들어 가는 방식1) 임의의 정점을 하나 선택해서 시작2) 선택한 정점들과 인접하는 정점들 중에 최소 비용의 간선이 존재하는 정점 선택  heap으로 짜면 빠르다. O(logN)3) 사이클이 있는지 검사 후 없으면 그 정점을 그대로 최소 신장트리에 추가  검사 방법은 이미 추가 했었는가?(visited) 또는 위상정렬 또는 서로소 집합 부모 확인4) 모든 정점이 선택될 때 까지 두 번째 과정 반복  정점의 수가 n개라면 n-1개의 간선이 선택되며 하나의 트리를 구성하게 됨프림 알고리즘의 동작1) 트리 정점들(Tree vertices) : 최소 신장 트리를 만들기 위해 선택된 정점들2) 비트리 정점들(Non-tree vertices) : 선택되지 않은 정점들  프림 알고리즘 코드def MST_PRIM(G, s):# G: 그래프, s: 시작 정점    key = [INF]*N# 1. 가중치를 무한대로 초기화    pi = [None]*N# 2. 트리에서 연결될 부모 정점 초기화    visited = [False]*N# 3. 방문 여부 초기화    key[s] = 0# 4. 시작 정점의 가중치를 0으로 설정    for _ in range(N):# 5. 정점의 개수만큼 반복        minIndex = -1        min = INF        for i in range(N):# 6. 방문 안한 정점중 최소 가중치 정점 찾기            if not visited[i] and key[i] &amp;#38;#60; min:                min = key[i]                minIndex = i        visited[minIndex] = True# 7. 최소 가중치 정점 방문처리        for v, val in G[minIndex]:# 8. 선택 정점의 인접한 정점            if not visited[v] and val &amp;#38;#60; key[v]:# 9 방문하지 않고 갱신될 수 있는 정                key[v] = val# 10. 가중치 갱신                pi[v] = minIndex# 11. 트리에서 연결될 부모 정점3. 크루스칼 알고리즘크루스칼 알고리즘 소개      사이클이 생기지 않도록 최소 가중치 간선을 하나씩 선택해서 최소 신장 트리를 찾는 알고리즘                  N개의 정점을 포함하는 그래프에서 n-1개의 간선을 선택하는 방식                    간선을 선택해 나가는 과정에서 여러 개의 트리들이 존재                    초기 상태는 n개의 정점들이 각각 하나의 트리 ( 상호 배타 집합)이 존재하며                    간선을 선택하면 간선의 두 정점이 속한 트리가 연결되어 하나의 집합으로 변함                    선택한 간선의 두 정점이 이미 연결된 트리에 속한 정점들일 경우 사이클이 생기므로 두 정점에 대해 같은 집합의 원소 여부 검사            크루스칼 알고리즘의 동작      최초, 모든 간선을 가중치에 따라 오름차순으로 정렬        가중치가 가장 낮은 간선부터 선택하면서 트리 증가시킴          사이클이 존재하면 다음으로 가중치가 낮은 간선 선택            n-1개의 간선이 선택될 때까지 두 번째 과정을 반복          크루스칼 알고리즘의 동작과정      def MST_KRUSKA(G):    mst = []# 1. 최소 거리 간선 집합으로 공집합 생성       for i in range(N):# 2. 각 노드를 집합으로 만들기        Make_Set(i)    G.sort(key = lambda t:t[2])# 3. 가중치 기준으로 정렬       mst_cost = 0# 4. MST 가중치       while len(mst) &amp;#38;#60; N-1:# 5. 필요한 간선의 갯수(노드 갯수 - 1)만큼        u, v, val = G.pop(0)# 6. 최소 가중치 간선 가져오기        if Find_Set(u) != Find_Set(v):# 7 만약 to node와 from node가 같은 집합에 있지 않다면            Union(u, v)           # 8 두 노드를 같은 집합으로 합치기            mst.append((u,v))   # 9 최소 거리 간선 집합에 (u, v) 추가            mst_cost += val           간선 선택 과정에서 생성되는 트리를 관리하기 위해 상호 배타 집합 사용          트리에 속한 노드들은 자신을 루트로 하는 서브트리의 높이를 랭크 이름으로 저장            선택한 간선으로 두 개의 트리가 한 개의 트리로 합쳐질 때 각 트리에 해당하는 상호 배타 집합을 Union연산으로 합침          랭크 값이 작은 트리를 랭크 값이 큰 트리의 서브트리로 포함시킬 경우 트리에 포함된 노드들의 랭크 값 수정 불필요      4. 최단 경로최단 경로란?      간선의 가중치가 있는 유향 그래프에서 두 정점 사이의 경로들 중 가중치의 합이 최소인 경로        엄밀히말하면 한 정점에서 다른 정점들 까지 가는 최소거리이긴 하다.                  단일 시작점 최단 경로 문제                  출발점에서 다른 모든 정점들에 이르는 최단 경로를 구하는 문제                          다익스트라(Dijkstra) 알고리즘 : 음의 가중치를 허용하지 않음              벨만-포드(Bellman-Ford) 알고리즘 : 음의 가중치 허용, 가중치 합이 음인 사이클은 허용하지 않음                                                  모든 쌍 최단 경로 문제                  모든 정점 쌍 간의 최단 경로를 구하는 것으로 플로이드-워샬 알고리즘 이용                    5. 다익스트라 알고리즘다익스트라 알고리즘이란?      시작 정점에서 거리가 최소인 정점부터 선택해 나가면서 최단 경로를 구하는 방식        탐욕 기법을 사용한 알고리즘으로 최소 신장 트리를 구하는 프림 알고리즘과 유사        시장 정점(r)에서 끝 정점(t)까지의 최단 경로에 정점 x 가 존재,        최단 경로는 r에서 x까지의 최단 경로와 x에서 t까지의 최단 경로로 구성        시작점에서의 최단 경로를 찾은 정점들의 집합(S)을 관리        집합 S에 포함되지 않은 정점들 중에 출발점에 가장 가까운 정점 선택  대략적인 알고리즘1) 기본버전 알고리즘  시작 노드 주위 이외의 최단거리를 무한대로 놓는다          최단 거리가 확정되지 않은 노드(거리가 무한대) 중 Dis[i]값이 가장 작은 노드(v)를 고른다(O(N))      그 노드로부터 이어진 간선들을 이용하여 Dis[w]들을 갱신한다.(O(N))      모든 노드가 확정되기 까지 1,2 과정을 반복(N번 반복)-&gt; N^2 +N^2 = O(N^2)      2) 빠른 버전 알고리즘(힙을 사용한 최솟값 구하기, 인접 행렬을 인접 리스트(간선을 저장)로 바꿈)  의미있는 값만 가지므로 메모리와 시간을 아낄 수 있음  시작 노드 주위 이외의 최단거리를 무한대로 놓는다.  최단 거리가 확정되지 않은 노드(거리가 무한대) 중 Dis[i] 값이 가장 작은 노드(v)를 고른다(최솟값 찾기(Heap):O(logM))  그 노드로부터 이어진 간선들을 이용하여 Dis[w]들을 갱신한다.          인접리스트에서 갱신하기: O(OutDegree(v)) // outdegree: 나가는 간선의 수(outdegree를 다합치면 간선의 수가 된다)       3. 모든 노드가 확정되기 까지 1,2 과정을 반복(N번 반복)        1번 작업은 O(NlogM)    2번 작업은 MN(간선의 수)        O(NlogM)+ O(M) = O(NlogM+M)  다익스트라 알고리즘의 동작 과정, O(N^2)# D: 출발점에서 각 정점까지의 최단 경로 가중치 합을 저장# P: 최단 경로 트리 저장def Dijkstra(G,r):# G: 그래프, r: 시작 정점    D = [INF]*N# 시작점부터 각 노드로 가능 최단 거리를 무한대로 설정    P = [None]*N# 2 시작지점 부터 해당지점까지의 최단 거리를 구할 수 있게 끔 도와주는 노드    visited = [False] * N# 3 방문 여부를 모두 False로 설정    D[r] = 0# 4 시작 지점부터 시작 지점까지의 거리 0으로 설정    for _ in range(N):# 5 각 노드 연산 때마다 거리값을 갱신        minIndex = -1        min = INF        for i in range(N):            if not visited[i] and D[i] &amp;#38;#60; min:# 아직 최소거리를 못구했고(visited) 가장 최단경로일 경우, 힙으로 구현시 더빠름                min = D[i]# 최소 거리 노드로 설정                minIndex = i        visited[minIndex] = True# 7 이제 최소거리를 구할 것이므로 visited 설정        for v, val in G[minIndex]:# 8 해당 node의 간선들에 대해서            if not visited[v] and D[minIndex] + val &amp;#38;#60;D[v]:# 9 해당 노드의 기존의 최소 거리보다 더 낮은 비용으로 도달할 수 있다면                D[v] = D[minIndex] + val# 10# 해당 값으로 변경                P[v] = minIndex# 11 node v의 부모 노드(한번에 한해 어디 노드로부터 이 노드로 오는것이 최단경로인가?) 저장        # P 집합을 역순으로 올라가서 시작정점까지 가는 경로가 시작 노드 -&amp;#38;#62; 해당 노드로 가능 최단 거리  더 빠른 버전,  O(NlogM+M)INF = 999999999def Dijkstra(G, r):# G: 그래프, r: 시작 정점, N: 노드의 수    N = len(G)    D = [INF]*N# 1 시작점부터 각 노드로 가능 최단 거리를 무한대로 설정    P = [None]*N# 2 시작지점 부터 해당지점까지의 최단 거리를 구할 수 있게 끔 도와주는 배열    visited = [False] * N# 3 방문 여부를 모두 False로 설정    D[r] = 0# 4 시작 지점부터 시작 지점까지의 거리 0으로 설정    min_heap = [&amp;#123;&amp;#34;weight&amp;#34;: 0, &amp;#34;node&amp;#34;: r&amp;#125;]# 5 시작 지점 설정    while min_heap:# 6 더이상 갱신할 노드가 없을때 까지 실행# 최소 거리 노드로 설정        min_node = heapq.heappop(min_heap)# 7 힙을 통해 새로 갱신된 최소 거리 노드 값 구함        visited[min_node[&amp;#34;node&amp;#34;]] = True# 8 이제 최소거리를 구할 것이므로 visited 설정        for node in G[min_node[&amp;#34;node&amp;#34;]]:# 9 해당 최소 거리 노드와 연결된 노드들에 대해                       if not visited[node[&amp;#34;node&amp;#34;]] and D[min_node[&amp;#34;node&amp;#34;]] + node[&amp;#34;weight&amp;#34;] &amp;#38;#60; D[min_node[&amp;#34;node&amp;#34;]]:# 10 해당 노드의 기존의 최소 거리보다 더 낮은 비용으로 도달할 수 있다면                D[node[&amp;#34;node&amp;#34;]] = D[min_node[&amp;#34;node&amp;#34;]] + node[&amp;#34;weight&amp;#34;]# 11 새로운 최소 거리 경신       # 12 node의 부모 노드(한번에 한해 어디 노드로부터 이 노드로 오는것이 최단경로인가?) 저장                P[node[&amp;#34;node&amp;#34;]] = min_node[&amp;#34;node&amp;#34;]       # P 집합을 역순은 시작정점까지 가는 경로가 시작 노드 -&amp;#38;#62; 해당 노드로 가능 최단 거리    return D, P# 13 더이상 갱신할 것이 없는 경우 구한 최소 거리와 최소 거리 경로를 도출#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;using namespace std;struct edge{    int to;    int weight;};vector&lt;edge&gt; edges[300001];int main(){        cin.tie(NULL);    cout.tie(NULL);    ios::sync_with_stdio(false);        int nodeNum, edgeNum;    int startNode;    cin&gt;&gt;nodeNum&gt;&gt;edgeNum&gt;&gt;startNode;    int max= 0 ;    for (int i =0; i &lt;edgeNum; i++){        int from,to,weight;        cin&gt;&gt;from&gt;&gt;to&gt;&gt;weight;        edges[from].push_back(edge{to,weight});        max += weight;    }     max = max * 10;    int distant[nodeNum+1];    for (int i = 0; i &lt;= nodeNum; i++){        distant[i] = max;    }    priority_queue&lt;pair&lt;int,int&gt;&gt; heap; // 자동으로 1번째 인자를 비교하여 정렬되는듯하다    distant[startNode] = 0;    heap.push({0, startNode});    while(!heap.empty()){        int minIndex = heap.top().second;         heap.pop();          for (int k = 0; k&lt;edges[minIndex].size(); k++){            edge betweennode = edges[minIndex][k];            if ((distant[minIndex] +betweennode.weight) &lt; distant[betweennode.to]){                distant[betweennode.to] = distant[minIndex] +betweennode.weight;                heap.push({distant[betweennode.to]*-1, betweennode.to}); // -1을 곱해줘서 max heap을 min heap으로             }         }    }    for (int i = 1; i &lt;= nodeNum; i++){        if (distant[i] &lt; max){            cout&lt;&lt;distant[i]&lt;&lt;\"\\n\";        }        else{            cout&lt;&lt;\"INF\\n\";        }    }    return 0;}벨만-포드 알고리즘      음의 가중치를 포함하는 그래프에서 최단 경로를 구함        가중치의 합이 음인 사이클은 허용하지 않음, 다익스트라로 최단경로를 구할 수 있다면 벨만-포드로 가능        출발점에tj 각 정점까지 간선 하나로 구성된 경로만 고려해서 최단 경로를 구함        그 다음 최대 간선 두개 까지 고려해서 최단경로를 구해나가서 최대 간선 n-1개 까지 고려한 경로들에서 최단 경로를 구함(n은 정점의 개수), 동적 계획법 적용        만약, 모든 간선과 노드를 고려하여 업데이트한 그래프가 또 다시 최적화될 수 있다면, 그것은 무한 루프를 포함한다는 의미이다.(음의 가중치를 가진 간선에 의해 경로 가중치가 음수로 계속 최적화 되는 경우)        다익스트라에 비해 많은 시간 소요(O(n^2))    &#38;#62; python 구현  import sysinput = sys.stdin.readlineINF = 9999999999nodeNum, edgeNum = map(int, input().split())edges = [list(map(int,input().split())) for i in range(edgeNum)]distant = [INF for i in range(nodeNum+1)]distant[1] = 0# set distance to the first city to the first city to zero  for _ in range(nodeNum):# Update distance to each node once with edge information.     for edge in edges:        fr, to, wt = edge        if (distant[fr] &amp;#38;#62;= INF):continue# if the from node of the edge is unreachable, continue. # because Infinity - weight == Infinity        if (distant[to] &amp;#38;#62; distant[fr] + wt):            distant[to] = distant[fr] + wtINFLOOP=Falsefor edge in edges:    fr, to, wt = edge    if distant[fr] &amp;#38;#62;= INF: continue    if (distant[to] &amp;#38;#62; distant[fr] + wt):# if you can update distance once more, # that means, there is an infinity loop in the graph. \t    print(-1)        INFLOOP=True        breakif not INFLOOP:    [print(-1 if i &amp;#38;#62;= INF else i) for i in distant[2:]]  C++ 구현#define INF 987654321#include&lt;iostream&gt;struct edge{    int from;    int to;    int weight;};using namespace std;int main(){    cin.tie(NULL);    cout.tie(NULL);    ios::sync_with_stdio(false);    int cityNum, lineNum;    cin&gt;&gt;cityNum&gt;&gt;lineNum;    edge edges[lineNum];    for(int i = 0; i&lt;lineNum; i++){        int from, to, weight;        cin&gt;&gt;from&gt;&gt;to&gt;&gt;weight;        edges[i] = edge({from,to,weight});    }    long long distant[cityNum+1];    for (int i =0; i&lt;=cityNum; i++){        distant[i] = INF;    }    distant[1] = 0;      for (int i = 0; i&lt;cityNum; i++){        for (int j =0; j&lt;lineNum; j++){            if (distant[edges[j].from]&gt;=INF) {                continue;            }            if (distant[edges[j].to] &gt; distant[edges[j].from] + edges[j].weight){                distant[edges[j].to] = distant[edges[j].from] + edges[j].weight;            }        }    }      for (int i =0; i &lt; lineNum; i++){        if (distant[edges[i].from] &gt;= INF) continue; // 무한 루프가능 인지 확인할때도 INF 확인 필수        if (distant[edges[i].to] &gt; distant[edges[i].from] + edges[i].weight){            cout&lt;&lt;-1&lt;&lt;\"\\n\";            return 0;        }    }    for(int i = 2; i &lt;= cityNum; i++){        long long result = distant[i] == INF ? -1 : distant[i];        cout&lt;&lt;result&lt;&lt;\"\\n\";    }    return 0;}플로이드 워셜 알고리즘      가능한 모든 쌍에 대해 최단 거리를 구하는 알고리즘 O(N^3)        모든 쌍에 대한 최단거리를 음의 가중치를 가지는 그래프에서도 가능        대신 매우 느리다.        각 시작점과 경유지, 끝점을 모두 돌면서 최단거리 업데이트        경유지를 loop문의 가장 위로 해야 된다는 점을 주의!  import sysinput = sys.stdin.readlineINF = 999999999citynum = int(input())edgenum = int(input())graph = [[INF for j in range(citynum + 1)] for i in range(citynum+1)]  for _ in range(edgenum):# Create edge graph\tfr, to, wt = map(int, input().split())    if (graph[fr][to] &amp;#38;#62; wt):# if the edge has the same start and destination, Update it to a smaller weight.        graph[fr][to] = wt  for start in range(1, citynum+1):     for end in range(1, citynum+1):        if (start==end):# set weight to zero, if start == end            graph[start][end] = 0  # loop all of the nodes.  for mid in range(1, citynum+1):# Make sure loop stopover first.    for start in range(1, citynum+1):        for end in range(1, citynum+1):            if (graph[start][end] &amp;#38;#62; graph[start][mid] + graph[mid][end]):                graph[start][end] = graph[start][mid] + graph[mid][end]  for i in graph[1:]:    for j in i[1:]:        if j&amp;#38;#62;=INF:            print(&amp;#34;0&amp;#34;, end=&amp;#34; &amp;#34;)        else:            print(j, end=&amp;#34; &amp;#34;)    print()#include&lt;iostream&gt;#define INF 987654321using namespace std;int main(){    ios::sync_with_stdio(false);    cin.tie(NULL);    cout.tie(NULL);    int cityNum, busNum;    cin&gt;&gt;cityNum&gt;&gt;busNum;    long long weights[cityNum+1][cityNum+1];    for (int i = 1; i &lt;=cityNum; i++){        for (int j = 1; j&lt;=cityNum; j++){            if (i==j){                weights[i][j] = 0;                  continue;            }            weights[i][j] = INF;          }    }    for (int i=0; i&lt;busNum; i++){        int from, to, weight;        cin&gt;&gt;from&gt;&gt;to&gt;&gt;weight;        weights[from][to] = weights[from][to] &gt; weight ? weight : weights[from][to];    }    for (int k = 1; k &lt;=cityNum; k++){        for (int i = 1; i &lt;= cityNum; i++){            for (int j = 1; j &lt;= cityNum; j++){                weights[i][j] = weights[i][k] + weights[k][j] &lt; weights[i][j] ? weights[i][k] + weights[k][j] : weights[i][j];            }        }    }    for (int i = 1; i &lt;= cityNum; i++){        for (int j = 1; j &lt;= cityNum; j++){            if (weights[i][j]==INF){                cout&lt;&lt;0&lt;&lt;\" \";                continue;            }            cout&lt;&lt;weights[i][j]&lt;&lt;\" \";        }                cout&lt;&lt;\"\\n\";    }    return 0;  }08 문자열 탐색1. 해싱소개      파일 시스템에서 파일 등을 찾는 데에 사용        특정 항목 검색 시, 탐색 키에 대한 산술적 연산으로 키가 있는 위치를 계산하여 바로 찾아가는 방법 (시간 복잡도 O(1)만들 때는 O(S*T)문자열의 길이 + 단어의 길이 )  해싱의 방법1) 직접 번지 테이블 (배열)  자료를 직접 번지 테이블에 저장하는 방법  전체 키들의 집합이 작은 경우 효율적  아마 배열에 A~Z중 사용되는 B, D의 위치를 배열에 저장? 하고 나머지는 빈공간에 놓는 방식인듯  전체 키 집합이 크면 컴퓨터 메모리 공간을 어마어마하게 잡아먹음  전체 키 집합 U에 비해 실제 키 집합 K가 매우 작으면 배열 메모리 공간 낭비가 심하기 때문,2) 해시 테이블      집합 U에 비해 실제 사용되는 키들의 집합 K가 작을 때 사용                                직접 번지 테이블보다 메모리 공간이 적게 필요, 세타(          k          )만큼 필요                            세타는 최적의 경우 시간복잡도            키 값 k의 자료를 저장할 위치를 계산하는 해시 함수(h) 사용        키 값 k인 자료를 h(k)의 위치에 저장        모든 키들의 집합 U를 해시 테이블 T의 위치에 대응시킴        키 두개가 동일한 위치가 될 수 있는 데 이를 충돌이라고 함.        다른 키값을 해시 함수로 적용해도 반환된 해시 함수는 같을 수 있음    해시 함수가 해시 주소를 공평하게 분배해도 해시 테이블에 저장되는 키에 해당하는 자료의 수가 증가하면 충돌은 불가피함해싱 충돌 피하는 방법3) 체이닝(Chaning)  해시 테이블의 구조를 변경하여 각 버킷에 하나 이상의 키 값을 가지는 자료가 장될 수 있도록 하는 방법  하나의 버킷에 여러 개의 키값을 저장하도록 하기 위해 연결 리스트를 활용함&#38;#62; 해싱과 체이닝 예시 코드(실습 소스) hash.c/* author: Jung,JaeJoon(rgbi3307@nate.com, http://www .kernel.bz/) comments: hash 알고리즘*/#include#include#define HASHSIZE 256//구조체선언struct nlist &amp;#123; struct nlist *next; //연결구조체(리스트) char *name;                                       //key char *phone;&amp;#125;;//구조체테이블정의(해시테이블)static struct nlist *hashtab [HASHSIZE];//hash 함수: 문자열s를위한해시 값산출unsigned hash (char *s)&amp;#123; unsigned hashval;for (hashval = 0; *s != &amp;#39;&amp;#39;&amp;#39;&amp;#39;; s++)  //문자열s의길이만큼반복       hashval = *s + 31 * hashval;  //문자*s에의존하는난수적인(random) 값 return hashval &amp;#37; HASHSIZE;  //HASHSIZE 이내의값(0 &amp;#38;#60;= 반환값&amp;#38;#60; HASHSIZE)&amp;#125;//해시함수를통하여해시테이블의요소에접근후//연결된구조체리스트를따라가며반복탐색struct nlist* lookup (char *s)&amp;#123; struct nlist *np; for (np = hashtab[hash(s)];  np != NULL; np = np-&amp;#38;#62;next)  if (strcmp(s, np-&amp;#38;#62;name) == 0)                      return np;     //found (찾은위치포인터) return NULL;           //not found&amp;#125;//문자열s를메모리에할당한포인터반환char *str_mcopy (char *s)&amp;#123; char *p; p = (char *) malloc(strlen(s)+1); //+1 for &amp;#39;&amp;#39;&amp;#39;&amp;#39; if (p != NULL)  strcpy (p, s); return p;&amp;#125;//해시테이블에구조체할당(저장)struct nlist* install (char *name, char *phone)&amp;#123;  struct nlist *np; unsigned hashval; if ((np = lookup (name)) == NULL) &amp;#123; //not found (새롭게할당)           np = (struct nlist *) malloc(sizeof(*np));           if (np == NULL || (np-&amp;#38;#62;name = str_mcopy (name)) == NULL)   return NULL;           hashval = hash (name);           np-&amp;#38;#62;next = hashtab[hashval]; //현재해시값에있는구조체는다음(뒤)으로연결           hashtab[hashval] = np; &amp;#125; else            //이미존재하면      free ((void *) np-&amp;#38;#62;phone);              //phone 해제 if ((np-&amp;#38;#62;phone = str_mcopy (phone)) == NULL)    //phone 다시할당                  return NULL; return np;&amp;#125;int main ()&amp;#123; char *name[] = &amp;#123;&amp;#34;John Smith&amp;#34;, &amp;#34;Lisa Smith&amp;#34;, &amp;#34;Sam Doe&amp;#34;, &amp;#34;Sandra Dee&amp;#34;, &amp;#34;Ted Baker&amp;#34;, &amp;#34;JaeJoon Jung&amp;#34;, &amp;#34;James Dean&amp;#34;&amp;#125;; char *phone[] = &amp;#123;&amp;#34;521-1234&amp;#34;, &amp;#34;521-8976&amp;#34;, &amp;#34;521-5030&amp;#34;, &amp;#34;521-9655&amp;#34;, &amp;#34;418-4165&amp;#34;, &amp;#34;520-3307&amp;#34;, &amp;#34;425-1020&amp;#34;&amp;#125;; int i, n = sizeof(name) / sizeof(name[0]); struct nlist *head, *ptr; //연결리스트에저장 for (i = 0; i &amp;#38;#60; n; i++) &amp;#123;  printf (&amp;#34;&amp;#37;d: &amp;#37;sn&amp;#34;, hash (name[i]), name[i]);  //구조체에할당  install (name[i], phone[i]); &amp;#125; //해시테이블에연결된리스트출력 printf (&amp;#34;nHash Table List n&amp;#34;); for (i = 0; i &amp;#38;#60; HASHSIZE; i++) &amp;#123;  head = hashtab [i];  for (ptr = head; ptr != NULL; ptr = ptr-&amp;#38;#62;next) &amp;#123;             printf(&amp;#34;&amp;#37;d: &amp;#37;s, &amp;#37;sn&amp;#34;, i, ptr-&amp;#38;#62;name, ptr-&amp;#38;#62;phone);  &amp;#125; &amp;#125; printf (&amp;#34;nHash Table Search n&amp;#34;); ptr = lookup (&amp;#34;JaeJoon Jung&amp;#34;) ; printf(&amp;#34;found: &amp;#37;s, &amp;#37;sn&amp;#34;, ptr-&amp;#38;#62;name, ptr-&amp;#38;#62;phone); printf (&amp;#34;nPress any key to end...&amp;#34;); getchar(); return 0;&amp;#125;(실행 결과) hash .c174: John Smith46: Lisa Smith57: Sam Doe23: Sandra Dee44: Ted Baker210: JaeJoon Jung10: James DeanHash Table List10: James Dean, 425-102023: Sandra Dee, 521-965544: Ted Baker, 418-416546: Lisa Smith, 521-897657: Sam Doe, 521-5030174: John Smith, 521-1234210: JaeJoon Jung, 520-3307Hash Table Searchfound: JaeJoon Jung, 520-3307Press any key to end...-출처 : 테크월드(http://www.epnc.co.kr)2) 개방 주소법(Open Adderess)      해시 함수로 구한 주소에 빈 공간이 없어 충돌이 발생하면 그 다음 위치에 빈 공간이 있는지 조사하는 방법        빈 공간이 있을 경우 : 탐색키에 대한 항목 저장        빈 공간이 없을 경우 : 공간이 나올 때까지 탐색 반복  2. 문자열 탐색  문자열 매칭(패턴 매칭)이란, 텍스트 문자열(t)에 패턴 문자열(p)가 포함되 있는지 찾는것고지식한 패턴 검색 알고리즘( 단순 검색)      본문 문자열을 처음부터 끝까지 차례대로 순회하면서 패턴 내의 문자들을 일일이 비교하는 방식의 알고리즘        패턴 길이 : m, 텍스트 길이: n일 때 O(m*n)만큼의 시간이 걸림        불일치가 발생할 시, 다음 검색 지점은 이전 시작의 다음 위치, 패턴은 항상 처음에 시작          고지식한 패턴 검색 알고리즘      // p[ ]: 패턴, t[ ]: 텍스트// M: 패턴의 길이, N: 텍스트의 길이BruteForce(int p[], int t[])&amp;#123;    int i = 0;    int j = 0;    while(j &amp;#38;#60; M and i &amp;#38;#60;N)&amp;#123;        if (t[i] != p[j])&amp;#123;            i = i - j;            j = -1;        &amp;#125;        i++;        j++;    &amp;#125;    if (j == M)&amp;#123;    return i - M;    &amp;#125;    else return i;  &amp;#125;카프-라빈 알고리즘      문자열 검색을 위해 해시 함수를 이용하는 알고리즘        패턴의 해시 값과 텍스트 내의 패턴 길이만큼의 문자열에 대한 해시 값 비교    최악의 시간 복잡도는 패턴 길이 : m, 텍스트 길이: n일 때 O(mn)이나 평균적으로는 선형임1) 주의할 점      해시 값을 구할 때, 새로 추가되는 문자와 그 전에 계산한 값을 이용해서 구함        지나간 수는 버리고 새로 추가될 값을 비교하여 구함        처음 해시 값을 구할 때 찾고자 하는 문자열에서 패턴 길이만큼 읽어서 구함        패턴의 길이가 커지면 길이를 일정 자릿수로 맞추기위해 %(모듈러) 연산함        해시 값이 일치할 경우, 실제 문자열이 일치하는 지 검사해야함(해시 특성상 문자열이 다른데, 결과값이 같을 수도 있음)          내가 구현한 카프라빈 알고리즘      #define _CRT_SECURE_NO_WARNINGS#include &amp;#38;#60;stdio.h&amp;#38;#62;#include &amp;#38;#60;Windows.h&amp;#38;#62;#include &amp;#38;#60;time.h&amp;#38;#62;#define HASH2int hash(char* str,int len);int hash2(char* str, int len);int strlen(char* str);int strcmp(char* str, char* src, int len);int main() &amp;#123;    clock_t start, end;    start = clock();        char src[6000];    freopen(&amp;#34;string.txt&amp;#34;,&amp;#34;r&amp;#34;,stdin);    gets(src, sizeof(src));        char* str = &amp;#34;Syrup&amp;#34;;    // hash : 최대 24글자 정도 사용 가능, hash2 : 훨씬 긴 문자열 가능하지만 해쉬 충돌이 많이 일어나 속도가 조금 느릴듯?    int len = strlen(str);#ifdef HASH2    int strhash = hash2(str, len);    int thash = hash2(src, len);#else    int strhash = hash(str, len);    int thash = hash(src, len);#endif // HASH@    int idx = 0;    while (*(src+idx+len-1)) &amp;#123;        if (thash == strhash) &amp;#123;            if (strcmp(str, src + idx, len)) &amp;#123;                printf(&amp;#34;&amp;#37;d\\n&amp;#34;, idx);                printf(&amp;#34;&amp;#37;s\\n&amp;#34;, (src + idx));                break;            &amp;#125;            idx++;        &amp;#125;        else &amp;#123;#ifdef HASH2            thash -= *(src+idx) + *(src+idx) * 31;            thash += *(src + idx + len) + *(src + idx + len) * 31;#else            thash -= *(src+idx) &amp;#38;#60;&amp;#38;#60; (len-1);            thash = thash &amp;#38;#60;&amp;#38;#60; 1;            thash += *(src+idx+len);#endif // HASH@            idx++;        &amp;#125;    &amp;#125;    printf(&amp;#34;done\\n&amp;#34;);    end = clock();    printf(&amp;#34;타이머: &amp;#37;f&amp;#34;, ((float)(end - start) / CLOCKS_PER_SEC));    return 0;&amp;#125;int strcmp(char* str, char* src, int len) &amp;#123;    for(int i =0; i&amp;#38;#60;len; i++) &amp;#123;        if (*str != *src ) &amp;#123;            return 0;        &amp;#125;        if (*str == 0 || *src == 0) &amp;#123;            return 0;        &amp;#125;        str++;        src++;    &amp;#125;    return 1;&amp;#125;int strlen(char* str) &amp;#123;    int idx = 0;    while (*str != 0) &amp;#123;        str++;        idx++;    &amp;#125;    return idx;&amp;#125;int hash(char* str, int len) &amp;#123;    int hashVal;    int idx = len-1;    for (hashVal = 0; idx &amp;#38;#62;= 0; idx--) &amp;#123;        hashVal += (*str++ &amp;#38;#60;&amp;#38;#60; idx);     &amp;#125;    return hashVa&amp;#37; 1000000007l; // 해쉬 꿀팁! : m(모듈러로 나누는 값, 여기서는 1000000007)은 d(d^x승으로 곱해주는 값, 여기서는 2)와 서로소이며 아주 큰 수이면 좋다.  unsigned long long으로 지정하면, overflow로 인하여 m은 2^64처리가 된다, d는 257, 259 ... 5384같은 큰값을 하면 된다.&amp;#125;int hash2(char* str, int len) &amp;#123;    int hashVal = 0;    for (int idx = 0; idx &amp;#38;#60; len; idx++) &amp;#123;        hashVal +=*str + *str*31;        str++;    &amp;#125;        return hashVal;&amp;#125;KMP 알고리즘  (Knuth-Morris-Pratt)의 앞글자를 따서 만듦  불일치가 발생한 텍스트 문자열의 앞에 어떤 문자가 있는지 알고 있으므로 다시 비교하지 않고 매칭 수행  불일치가 발생하면 다음 비교 위치를 미리 계산, 불필요한 시작을 최소화함          패턴의 모든 위치마다 불일치가 발생하면 이동할 위치를 계산하여 저장      패턴의 길이만큼의 배열 필요        시간 복잡도는 패턴 길이 : m, 텍스트 길이: n일 때 O(m+n)  아호코라식 이라는 상위호환 알고리즘이 있다. 이것이 기반1) 아이디어  패턴의 접두어와 실패한 문자열에서 슬라이싱한 패턴의 접미어를 비교하여 이동  실패 함수(f(i), Failure function)          f(i)는 패턴 P의 P[1, i]에서 가장 긴 접두사와 접미사가 같은 것의 길이  abacaba 의 경우 0, 0,1,0,1,2,3이다. 각각  a는 기본 0, b는 ‘ab’에서 공통된 접두사와 접미사의 길이가 0,  a는 aba에서 가장 긴 접두사 a와 접미사 a 길이가 1  c는 ‘abac 가장 긴접두사와 접미사의 길이가 0  마지막 a의 경우 abacaba에서 접미사aba와 접두사aba가 같으므로 3        i번째(1부터 시작) 패턴이 틀리면 (i-1) - f(i-1) 칸 만큼 이동      - 왜? : abacaba에서 c 부문이 틀렸다고 가정하면 c의 이전 문자(마지막까지 매치된 문자) a의 f(3)는  1이다, 그냥 3칸을 이동시키면 일치했던 aba 중, 마지막 a가 3번째 a와 일치하므로 그 부분부터 비교를 해야한다. 그러므로 3칸 이동 후, 접미사 길이 만큼인 1만큼 빼서 이동하는 것이다.           -   매칭이 실패했을 때 돌아갈 곳을 계산한 뒤, 그것을 next 배열에 저장한다.  next 배열에는 공통되는 접두어와 접미어 수만큼 저장하는 듯  그리고 마지막은 0  공통되는 접두어와 접미어가 있는지 확인하여 저장한다.  맨 처음 글자의 next는 -1인데, 패턴 처음 위치 불일치 시, 고지식한 패턴 검색과 유사하게 동작하기 위함  만약 1글자만 찾고 끝나는게 아니라 전체에서 단어의 수를 찾고 싶으면 찾을 문자열 패턴 마지막에 .이나 유니크한 다른 문자를 추가한다. (ex)abacabc=&gt;abacabc.)          앞의 모든 문자열을 비교하고 .을 비교할 때가 되면 그 문자열은 일치하는 것이므로 count를 증가시켜주거나 위치를 배열에 저장한다. 그리고 .은 무조건 없으므로 계속 탐색하게 된다.        KMP 알고리즘 전처리 함수(next 배열(실패 함수) 구하기)// p[] : 패턴, M : 패턴의 길이// next[] : 불일치가 발생하면 이동할 위치를 저장void KMP_Preprocess(char p[], int next[])&amp;#123;    int i = 0;// 최초 글자는 -1로 시작    int j = -1;// j : 일치한 문자의 수    next[i] = -1;    while(i&amp;#38;#60;M)&amp;#123; // 패턴의 길이만큼        while(j &amp;#38;#62;= 0 &amp;#38;&amp;#38; p[i] != p[j])            j = next[j];//패턴i,j의 위치값 비교        ++i;        ++j;        next[i] = j;    &amp;#125;&amp;#125;  실패 함수 구하기 O(M) :  KMP 알고리즘 패턴 매칭// t[]:텍스트, N: 텍스트 길이, p[]: 패턴, M: 패턴 길이// next[]: 불일치가 발생하면 이동할 위치를 저장int KMP_Search(char t[], char p[], int next[])&amp;#123;    int i = 0;    int j = 0;    while(i &amp;#38;#60; N-M+1)&amp;#123;        while (j &amp;#38;#62;= 0 &amp;#38;&amp;#38; t[i] != p[j])            j = next[j];        ++i;        ++j;        if (j==M) return i-j; // 패턴 찾음 첫글자 인덱스 리턴    &amp;#125;    return -1;//일치 하는 것이 없음&amp;#125;접두어와 접미어란?      접두어와 접미어는 문자열의 길이만큼 존재        모든 부분 문자열은 모든 접미어들의 모든 접두어          문자열이  ‘abcdabc’ 일 때, 접두어와 접미어 예시                  접두어      접미어                  a      c              ab      bc              abc      abc              abcd      dabc              abcda      cdabc              abcdab      bcdabc              abcdabc      abcdabc      보이어-무어 알고리즘      대부분의 상용 소프트웨어에서 채택하고 있는 알고리즘        오른쪽 끝에서 왼쪽으로 문자열을 비교하는 알고리즘        앞부분보다 끝부분에 불일치가 일어날 확률이 높은 성질을 이용        패턴의 오른쪽 끝 문자에서 불일치 발생 시, 텍스트의 문자가 패턴 내에 없으면 이동거리는 패턴의 길이가 됨        오른 쪽 끝 문자가 불일치 한데, 이 문자가 패턴내에 존재할 경우    1) 패턴에서 일치하는 문자를 찾아 서로 일치되도록 점프2) 어느 정도 점프해야하는지 미리 배열을 만들어 놓아야함        이를 스킵배열이라고함(오른쪽 끝 문자 불일치, 그리고 해당 문자가 패턴내에 없을시, 이동해야하는 칸수 적어놓음)        시간 복잡도는 최악의 경우 O(mn) 하지만 보통 세타(n)보다 적음  3. 트라이(Trie)  트라이 도식화      prefix(접두어) tree 라고도 함,  문자열의 집합을 표현하는 트리, 문자열을 찾을때 사용함        정보검색(Retrieval)에서 이름을 따옴 (트라이라고 발음함)        트라이의 각 간선은 하나의 문자에 대응        같은 노드에 나오는 간선들은 같은 라벨(같은 문자)을 갖지 않음        루트에서 단말 노드까지 이른 경로가 하나의 문자열        즉 단말 노드는 문자열 하나        각 노드에 최종 단말노드의 갯수(자식들의 합)를 적어놓을 수 있다.          단어가 끝나는 단말노드에 색칠(다른 표시)를 통해 단어의 끝임을 표현해줘야함(필요할 시)      알고리즘      문자열 추가 시, 단말 노드를  추가하고, 각 노드에 적혀있는 단말노드 갯수를 적어놓은 것을 추가 해줘야한다.          트라이 예시      #define NM 1000000 // 정적으로 미리 해놓지 않으면(정적 할당) 메모리를 새로 할당하는데 시간이 엄청나게 걸린다. (새로 공간을 만들어놓고 통으로 복사함)  struct NODE &amp;#123;    int next[26]; //자식을 포인터(주소값)로 넣으면 cache freindly 하지 않으므로 새로 자식을 추가할 때, 노드 생성 시기가 다르므로, 메모리상 주소가 제각각 위치가 되어 메모리가 참조하는데 엄청나게 느려진다. 그래서 번호로 기억하게 만들면 10배 이상 빨라진다.    int cnt;    void init() &amp;#123;        for (int i = 0; i &amp;#38;#60; 26; i++)        next[i] = -1; // 초기에는 어떤 자식도 없다        cnt = 0; // 어떠한 단어도 밑에 가지고 있지 않다.    &amp;#125;  &amp;#125;trie[NM];  int root = 0, trieN;void init(void) &amp;#123;    root = 0; // 루트 날리기    trieN = 0; // 트라이 날리기(모든 노드의 갯수)    trie[0].init();&amp;#125;//재귀적으로 단어 추가void myInsert(int trieIdx, int bufIdx, int buffer_size, char *buf) &amp;#123;    trie[trieIdx].cnt++;    if (bufIdx == buffer_size) return;// 단어를 끝까지 갔다는 뜻, 추가로 리턴대신 색칠 가능(단말노드로 표시)    int ch = buf[bufIdx] - &amp;#39;a&amp;#39;;    if (trie[trieIdx].next[ch] == -1) &amp;#123;// 추가로 갈 수 있는 간선이 없다면(글자가 없다면)        trie[trieIdx].next[ch] = ++trieN;// 추가로 정점을 추가해주고 가르키게 해줌        trie[trieN].init(); // 새로운 노드 생겼으므로 초기화    &amp;#125;    myInsert(trie[trieIdx].next[ch], bufIdx + 1, buffer_size, buf);    // 있으면 다음 글자를 찾아가라&amp;#125;void insert(int buffer_size, char *buf) &amp;#123;    myInsert(root, 0, buffer_size, buf);&amp;#125;// 문자열 찾기 알고리즘int query(int buffer_size, char *buf) &amp;#123;    int trieIdx = root;    for (int i = 0; i &amp;#38;#60; buffer_size; i++) &amp;#123;        int ch = buf[i] - &amp;#39;a&amp;#39;;        trieIdx = trie[trieIdx].next[ch];        if (trieIdx == -1) return 0; // 문자열이 없다면 0을 리턴    &amp;#125;    return trie[trieIdx].cnt; // 있다면 해당 단말노드 번호를 리턴&amp;#125;접미어 트라이  문자열의 모든 접미어를 Trie로 표현함  길이가 n인 문자열 A = a0~an-1, 접미어도 n-1개가 존재  부분 문자열 검사(ba가 abac의 부분인가?)          한 문자씩 루트에서 대응되는 간선 따라가면 됨        두 접미어의 최장 공통 접두어 찾기 (abac, ac의 최장공통 접두어)          두 접미어의 끝 글자에 대응하는 노드 선택 뒤, 가장 가까운 공통 조상을 찾으면 공통 접두어가 됨        사전적 순서로 정렬된 k번째 접미어는?          깊이 우선 탐색(BFS) 문자열은 사전적 순서로 정렬됨      생성된 문자열이 아닌 인덱스 값 저장(보통 접미어 시작위치)        등에 접미어 트라이가 유용함    4. 접미어 트리(Suffix Tree)    하나의 문자열의 모든 접미어를 포함하는 트라이(Trie)의 표현  1973년, Weiner에 최초로 소개되었고, 이후 공간 복잡도를 줄이기 위해 접미어 배열이 알려짐  문자열 연산에 필요한 알고리즘을 빠르게 구현 가능  문자열 매칭, 부분 문자열 매칭, 최장 공통 부분문자열에 사용 가능    압축된 트라이(Compressed Trie)    노드와 간선을 부분 문자열로 압축함  하나의 노드를 하나의 간선으로 표현함, 원래 한 간선에 1글자지만 그냥 eef 가 한 간선으로 표현    6.압축    2. Run-Length Decoding 알고리즘    동일한 값(코드)이 몇 번 반복 되는가를 나타내는 방식(bmp 확장자 방식)  예) ABBBBBCC = A1B7C2          Run-Length Encoding 알고리즘```c// MAXCOUNT = 255 최대 표현 가능 숫자void RLEncoding(char input[]){  char count = 1; // 연속적으로 나타나는 코드 값의 횟수 저장(1~255까지 표현)  char prev = '\\0'; // 바로전에 나온 코드값 저장  while (remains(input)){      char curr = getnext();// 코드값 하나 1바이트씩 읽음      if (prev == curr &#38;&#38; count &#38;#60; MAXCOUNT)          ++count;      else{          output(prev, count);          count = 1;      }      prev = curr;// 이전 코드값을 prev에 저장함  }  output(curr, count);// 마지막 코드 값을 저장      }&gt; Run-Length Decoding 알고리즘```cvoid RLDecoding()&amp;#123;    int curr = 코드값;    int count = 갯수;    while(인코딩된 데이터가 남아있으면)&amp;#123;        curr = 다음 코드 읽기;        count = 그 코드의 갯수 읽기; //ex)A6의 6        while(count &amp;#38;#62; 0)            output(curr); // 현재 코드 갯수만큼 출력            count--;    &amp;#125;&amp;#125;3. 허프만 코드(Huffman Code)  기호의 빈도와 허프만 트리가 주요 개념1) 기호의 빈도 : 전체 데이터 안에서 기호가 차지하는 비율2) 허프만 트리 : 각 기호에 이진 코드를 부여하기 위해 생성하는 이진 트리고정길이 코드와 접두어 코드3) 고정 길이 코드란? : 기호에 대응하는 코드값의 길이가 똑같은 코드 체계(ex) ASCII)4) 접두어 코드(Prefix Code) : 가변 길이 코드의 한 종류로 어느 코드가 다른 코드의 접두어 되지 않는 코드 체계 Ex) 코드집합  예를 들어 a~f까지 6글자가 5000~45000개씩 포함되어있는 파일에서 빈도수가 가장 높은 것을 가변길이 코드 용량이 낮은것 순으로 할당하면 효율적이게 저장 가능허프만 코드의 생성과 허프만 트리      탐욕 알고리즘 기법에 의해 허프만 트리 생성        단말 노드는 기호와 빈도를 저장, 부모 노드는 빈도만 저장  09 동적 계획법의 소개귀납 기본(Induction base) n = 1(혹은 n = 0)에 대해 등식이 성립함을 증명귀납 가정(Induction hypothesis) 임의의 n에 대해 등식이 성립한다고 가정귀납 단계(Induction step) 등식이 n+1에 대해서도 성립함을 증명비둘기 집의 원리를 통해서 동적계획법이 필요한 문제인지 증명 가능(중복되는 값이 많은가)인덱스트리를 통해서 좀더 빠르게 짤 수 있음표를 만든다 라는 느낌10 동적 계획법의 적용11 동적 계획법의 활용12 NP-Complete13 근사 알고리즘14 정수론과 최적화  stack 메모리(함수나 {} 중괄호 안의 변수는 이곳에 할당 된 뒤, 종료시 삭제됨 아주 작지만 빠름)  정적 메모리,heap,(전역변수 함수 바깥에 있는 메모리는 이곳에 할당됨, 크기가 아주 큼, 조금 느림, 함수 내부 변수도 static 키워드를 통해 이곳에 할당 가능)"
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%ED%95%98.html": {
    title: "알고리즘-하",
    date: " Aug 23, 2020 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%ED%95%98.html",
    tags: ["알고리즘","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘 전문Array 활용미트 인 더 미들 알고리즘 (Meet in the middle Algorithm)  분할 정복(Divide and Conquer algorithm)과 비슷하게 주어진 input을 2개로 나눈 뒤, 두개의 연산 결과를 이용해 값을 찾아내는 알고리즘  예를 들어 n개의 수의 부분집합을 찾아내는 것은 시간 복잡도 O(2^n)의 연산이 필요하지만 20개씩 2개의 부분집합을 찾아낸 뒤 합치는 것으로 O(2^(n/2)) 으로 줄일 수 있다.투 포인터 알고리즘 (Two Pointers Algorithm)  정렬된 배열 내에서 값의 쌍을 탐색하는 알고리즘# 수의 배열에서 합이 sumX가 되는 수의 쌍의 갯수를 구하기# (i, j) 와 (j, i)는 하나로 간주한다.import sysinput = sys.stdin.readlinelistSize = int(input())nums = list(map(int, input().split()))sumX = int(input())nums.sort()answer = 0start = 0end = listSize -1while(start &amp;#38;#60; end):\tsumOfPair = nums[start] + nums[end]    if sumOfPair &amp;#38;#62; sumX:\t    end -= 1    elif sumOfPair &amp;#38;#60; sumX:        start += 1    else:        start += 1        end -= 1        answer += 1print(answer)  주로 배열의 정렬과 두 포인터 인덱스를 기준으로 만든 배열을 이용해 문제를 해결한다.스위핑      배열의 값을 차례대로 처리하면서 만든 값을 비교해가며 다음 값을 구하는 방법        동적 계획법(Dynamic Programming, DP)과 비슷하나 보통 먼저 정렬을 한 뒤에 처리를 하는 방법.  기하Convex Hull  보통 좌표가 주어지고 해당 좌표로 다각형을 만드는 알고리즘CCW(Counter Clock Wise)  벡터의 외적(Cross Product)를 이용해 하나의 정점을 기준으로 다른 점과 점 간의 상대적 위치를 알아낼 수 있음세점 (a, b, c)가 존재할 때벡터의 외적 = (b.x-a.x)*(c.y-a.y) - (b.y-a.y)*(c.x-a.x)또는벡터의 외적 = (a.x * b.y + b.x * c.y + c.y * a.y) - (a.y * b.x + b.y * c.x + c.y * a.x)벡터의 외적이 양수이면 ABC 순으로 반시계반대 방향으로 벡터가 변화,(ab벡터가 ac벡터보다 더 반시계방향으로 꺾여있음)음수이면 ABC 순으로 시계방향으로 벡터가 변화,(ab벡터가 ac벡터보다 더 시계방향으로 꺾여있음)0이면 평행임을 의미한다.(ab,ac벡터가 같은 방향임)BC    A위 예시로 따지면 A,B,C 순서로 시계반대방향 관계(양수), A,C,B 순서로 시계방향(음수) 관계이다.  두 벡터의 시작점을 맞추고 그 둘의 각도를 통해 두 벡터로 만든 삼각형 넓이 구하기 가능  각 꼭지점을 반시계방향으로 점을 향한 벡터를 저장하여 단순 다각형 저장 가능  CCW 생성 가능 , sin∂ 방향으로 외적의 음수 양수를 정의해야함 A, B벡터와 B, A벡터는 부호가 다름  a,A가 시작점  음수 양수 뿐만 아니라 나오는 값의 절대값도 다르지만 이것으로 각도나 위치 등을 판단하지 말자,          좌표가 다름에도 같은 값이 나올 때가 있다.      (A:(0,0), B:(0,1) 이 일 때, C:(1,0) 과 C:(1,1)을 넣어보면 똑같이 -1이 나온다. )      int CCW(ax,ay,bx,by,cx,cy)&amp;#123;    int t = (bx-ax)*(cy-ay)-(by-ay)*(cx-ax);    if (t&amp;#38;#62;0) return 반시계;    if (t&amp;#38;#60;0) return 시계;    return 일직선;&amp;#125;그라함 스캔 알고리즘  시간 복잡도 $O(n \\log n)$import sys, functoolsinput = sys.stdin.readlinedef ccw(a, b, c):    t = (b[&amp;#39;x&amp;#39;]-a[&amp;#39;x&amp;#39;])*(c[&amp;#39;y&amp;#39;]-a[&amp;#39;y&amp;#39;]) - (c[&amp;#39;x&amp;#39;] - a[&amp;#39;x&amp;#39;])*(b[&amp;#39;y&amp;#39;] - a[&amp;#39;y&amp;#39;])    if t &amp;#38;#62; 0:        return 1    elif t &amp;#38;#60; 0:        return -1    else :        return 0def comp(a, b):    global firstPoint    return -ccw(firstPoint,a,b)def comp0(a, b):    if a[&amp;#39;y&amp;#39;] == b[&amp;#39;y&amp;#39;]:        return 1 if a[&amp;#39;x&amp;#39;] &amp;#38;#62; b[&amp;#39;x&amp;#39;] else -1     else:        return 1 if a[&amp;#39;y&amp;#39;] &amp;#38;#62; b[&amp;#39;y&amp;#39;] else -1# def getFuther(std, a, b):#    lenToA = (a[&amp;#39;x&amp;#39;] - std[&amp;#39;x&amp;#39;])**2 + (a[&amp;#39;y&amp;#39;] - std[&amp;#39;y&amp;#39;])**2#    lenToB = (b[&amp;#39;x&amp;#39;] - std[&amp;#39;x&amp;#39;])**2 + (b[&amp;#39;y&amp;#39;] - std[&amp;#39;y&amp;#39;])**2#    return a if lenToA &amp;#38;#62; lenToB else bpointNum = int(input())points = []firstPoint = &amp;#123;&amp;#39;x&amp;#39;:0, &amp;#39;y&amp;#39;:9999099&amp;#125;for _ in range(pointNum):    x, y = map(int, input().split())    points.append(&amp;#123;&amp;#39;x&amp;#39;:x,&amp;#39;y&amp;#39;:y&amp;#125;)points.sort(key=functools.cmp_to_key(comp0))firstPoint = points[0]points.sort(key=functools.cmp_to_key(comp)) # cmp to key function to use comparerst = [points[0], points[1]]for point in points[2:]:    while(len(st) &amp;#38;#62;=2):        if (ccw(st[-2], st[-1], point) &amp;#38;#62; 0):\t        break        st.pop()    st.append(point)print(len(st))기타SCC (Strongly Connected Component, 강한 연결 요소)  방향 그래프에서 집합 내에서 서로 왕복가능한 정점들의 최대 크기 집합을 의미함.  한 SCC 내의 모든 정점은 다른 모든 정점에 들럿다가 다시 돌아올 수 있다.  자기 자신으로 돌아올 수 있는 1개짜리 정점도 포함한다.  색칠한 부분이 각각의 SCC이다. https://www.acmicpc.net/problem/2150 (백준 알고리즘)      그래프가 주어졌을 때 SCC 집합의 존재 여부나 갯수, 소속 집합 정보 등이 필요할 때가 많다.        주로 DFS(Depth first search, 깊이 우선 탐색)기반의 알고리즘을 사용한다.  코사라주 알고리즘  방향 그래프, 해당 방향그래프의 역방향 그래프, 스택을 이용하여 SCC를 구하는 알고리즘  SCC는 한 노드에서, 다른 노드로 어느 방향이로든 갈 수 있으며 역방향으로 바꾸어도 마찬가지인 것을 이용한 알고리즘  아래의 타잔 알고리즘 보다 구현과 생각이 쉽지만 메모리를 좀더 먹는다.  시간 복잡도는 O(V+E)import syssys.setrecursionlimit(987654321)# kosaraju algorithminput = sys.stdin.readlinenodeCount, edgeCount = map(int, input().split())edges = [[i] for i in range(nodeCount+1)]revEdges = [[i] for i in range(nodeCount+1)]visited = [False for _ in range(nodeCount+1)]st = []SCCs = []SCCIdx = 0for _ in range(edgeCount):    fr, to = map(int, input().split())    edges[fr].append(to)    revEdges[to].append(fr)def dfs(i):    visited[i] = True    for node in edges[i]:        if not visited[node]:            dfs(node)    st.append(i)def func(node, SccIdx):    visited[node] = True    SCCs[SccIdx].append(node)    for adj in revEdges[node]:        if not visited[adj]:            func(adj, SCCIdx)# 정방향 그래프를 돌며 dfs 함수가 끝날때 스택에 집어넣는다.# 결과적으로 가장 처음에 dfs를 실행한 노드가 스택에 마지막으로 들어가고, SCC의 대표노드가 된다.for i in range(1, nodeCount+1):    if not visited[i]:        dfs(i) visited = [False for _ in range(nodeCount+1)]# 스택에 마지막에 들어간 노드가 제일 먼저 튀어나오며 역방향으로 갈 수 있는 모든 노드를 찾아 SCC로 포함시킨다.# 이때 SCC에 포함될 수 없지만 대표노드에 연결되었던 노드는 역방향 그래프로 바뀌었으므로 진입할 수 없어 SCC에 들어가지지 않는다.# 역방향이 되어 대표노드에서 갈 수 있게된 노드는 이전 dfs 때 스택에 나중에 들어가 이번에 먼저 튀어나오므로 visited가 True가 되있어 들어가지 않는다.while st:    now = st.pop()    if not visited[now]:        SCCs.append([])        func(now, SCCIdx)        SCCIdx += 1for SCC in SCCs:    SCC.sort()SCCs.sort()print(len(SCCs))for SCC in SCCs:    SCC.append(-1)    [print(i, end=&amp;#39; &amp;#39;) for i in SCC]    print()타잔 알고리즘  위상정렬을 기반으로 하는 SCC 알고리즘  스택에 노드를 집어넣고, 가장 처음 집어넣은 노드를 대표노드로 삼은 뒤, dfs를 돌려 갈 수 있는 노드를 모두 스택에 넣은 후, dfs가 대표노드로 돌아오면 처음 설정한 대표노드가 나올때 까지 pop하여 해당 노드들을 같은 SCC로 묶는 원리.  코사라주 알고리즘 보다 메모리적으로 효율적이다.  시간 복잡도는 똑같이 O(V+E)import syssys.setrecursionlimit(987654321)# tarjan algorithminput = sys.stdin.readlinenodeCount, edgeCount = map(int, input().split())edges = [[i] for i in range(nodeCount+1)]discover = [-1 for _ in range(nodeCount+1)]SCCs = [-1 for _ in range(nodeCount+1)]discoverIdx = 0SCCIdx = 0st = []res = []def dfs(now):    global discoverIdx, SCCIdx    discover[now] = discoverIdx    discoverIdx += 1    discoverNow = discover[now]    st.append(now)    for adj in edges[now]:        if discover[adj] == -1:            discoverNow = min(discoverNow, dfs(adj))        elif SCCs[adj] == -1:            discoverNow = min(discoverNow, discover[adj])    if discoverNow == discover[now]:        SCC = []        while True:            target = st.pop()            SCCs[target] = SCCIdx            SCC.append(target)            if target == now: break        SCC.sort()        res.append(SCC)        SCCIdx += 1    return discoverNowfor _ in range(edgeCount):    fr, to = map(int, input().split())    edges[fr].append(to)for i in range(1, nodeCount+1):    if discover[i] == -1:        dfs(i)res.sort()print(len(res))for SCC in res:    SCC.append(-1)    [print(i, end=&amp;#39; &amp;#39;) for i in SCC]    print()    avl tree 알아보기lcp : https://blog.naver.com/dark__nebula/220419358547suffix array : https://plzrun.tistory.com/entry/Suffix-Array-ONlogNlgN%EA%B3%BC-ONlogN-%EA%B5%AC%ED%98%84-%EB%B0%8F-%EC%84%A4%EB%AA%85"
  }
  , 
  
  "/articles/web/CI,CD/AWS%EB%A1%9C%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C%20%EC%BB%B4%ED%93%A8%ED%8C%85%20%EC%A0%95%EB%A6%AC.html": {
    title: "AWS로 시작하는 클라우드 컴퓨팅 정리",
    date: " Sep 8, 2020 ",
    url: "/articles/web/CI,CD/AWS%EB%A1%9C%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C%20%EC%BB%B4%ED%93%A8%ED%8C%85%20%EC%A0%95%EB%A6%AC.html",
    tags: ["AWS","요약","CLOUD"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueAWS로 시작하는 클라우드 컴퓨팅01 아마존 웹 서비스 Cloud 개요클라우드 컴퓨팅(Cloud Computing)      클라우드 컴퓨팅이란, 인터넷 기반 컴퓨팅의 일종으로 구성 가능한 컴퓨팅 자원에 대해 어디서나 접근이 가능한, 주문형접근을 가능케 하는 모델이며, 최소한의 관리 노력으로 빠르게 예비 및 릴리스를 가능케 함        On-premise는 사용자가 직접 인프라, 플랫폼, 애플리케이션을 관리하는 모델이며, 가장 일반적인 유형의 클라우드 서비스이며, 서비스를 제공하는 곳에서 인프라, 소프트웨어까지 모두 제공, 즉 어플리케이션 레벨까지 서비스로 제공되는 것은 Saas(Software as a Service)이다.        이외에도 IaaS, PaaS 등이 존재한다.        클라우드 컴퓨팅과 스토리지 솔루션들은 사용자와 기업들에게 개인 소유나 타사 데이터 센터의 데이터를 저장, 가공하는 다양한 기능을 제공, 전세계로 위치했으며, 전기망 등의 일관성 및 규모의 경제를 달성하기 위해 자원의 공유에 의존                            클라우드 서비스          설명                                      On-premise(Traditional IT)          사용자가 직접 인프라, 플랫폼, 애플리케이션을 관리하는 모델, 규모있는 업체라면 직접 IDC를 구축하고, 일반적인 경우 IDC에 공간을 할당 받아 물리서버를 설치하고 하드웨어, 운영체제, 서버 어플리케이션을 모두 관리함                          IaaS(Infrastructure as a Service)          AWS EC2, 가상 서버, 데이터 스토리지 및 호스팅 컴퓨터, 네트워크 등 IT 인프라를 지원해주는 서비스로 하드웨어를 서비스로 제공하는 클라우드 모델, OS와 애플리케이션을 관리함                          PaaS(Platform as a Service)          AWS Elastic Beanstalk, 기본적인 IaaS는 물론 개발툴과 기능, 애플리케이션 배포까지 제공하며 개발자가 애플리케이션을 개발하고 배포하는데 필요한 모든 것을 제공함, 개발자는 애플리케이션과 서비스로 제공되는 기능을 연결하는 로직을 작성해야 함                          SaaS(Software as a Service)          Google Apps, Office365, 가장 일반적인 유형의 클라우드 서비스이며, 서비스를 제공하는 곳에서 인프라와 소프트웨어까지 모두 제공, 즉 애플리케이션 레벨까지 서비스로 제공됨, 개발자보다는 실사용자에게 바로 제공함                      아마존 웹 서비스(Amazon Web Services)  1. AWS를 이용한 클라우드 컴퓨팅      전세계적으로 분포한 데이터 센터에서 165개가 넘는 완벽한 기능의 서비스를 제공하는 세계적으로 가장 포괄적이며, 널리 채택되고 있는 클라우드 플랫폼        빠르게 성장하는 스타트업, 가장 큰 규모의 엔터프라이즈, 주요 정부 기관을 포함해 수백만 고객이 인프라 강화하고 민첩성을 향상시키며, 비용을 절감 중        웹, 모바일, 빅데이터 프로젝트, 소셜 게임, 모바일 앱에 이르기까지 거의 모든 물리적인 컴퓨팅 자원을 클라우드를 통해 실행 가능한 인프라 및 애플리 케이션 서비스 집합을 제공중,        컴퓨팅, 스토리지, 데이터베이스, 네트워킹, 분석, 기계학습, AI, IoT, 보안, 애플리케이션 개발, 배포 및 관리 가능, 포괄적일 뿐만아니라 심층적인 기능 지원,        쉽고 빠른 확장성, 안전성, 보안성 검증됨, 선 비용, 장기 약정 없이 사용하는 만큼만 지불        이를 통해 조직이 더 빠르고 저비용으로 움직이며 성장 가능    AWS 주요 서비스    1. 클라우드 제품        분석, 데이터 베이스, 마이그레이션 및 전송, 개발자 도구, 모바일, 로보틱스, 애플리케이션 통합, 최종 사용자 컴퓨팅, 게임 기술, 증강 현실 및 가상 현실, 블록체인, 컴퓨팅, 스토리지, 위성, 기계 학습 등 수많은 세부 카테고리를 확인 가능하다.    AWS 보안    1. 보안, 자격 증명 및 규정 준수 제품        AWS에서 가장 높은 우선 순위는 클라우드 보안        AWS 고객은 위험에 가장 민감한 조직의 요구 사항에 부합하도록 구축된 데이터 센터 및 네트워크 아키텍처의 혜택을 누릴 수 있음        AWS 아티팩트 : 규정 준수 보고서        AWS Certificate Manager : SSL/TLS 인증서        AWS CloudHSM : 키 스토리지 및 관리        Amaszon Cognito : 사용자 가입 및 로그인 기능        Amazon GuardDuty : 위협 탐지        AWS Shield : DDoS 보호        등의 보안 제품 관련이 있다  AWS 아키텍처1. AWS 산업용 예측 유지 보수 기계 학습 모델      AWS IoT SiteWise 및 AWS IoT Analytics를 Amazon SNS 이상 탐지 알림과 함께 사용하여 예측 유지 보수(PdM) 기계 학습(ML)모델을 생성2. AWS 산업용 예측 유지 보수 기계 학습 모델과 이상 탐지        Amazon SageMaker와 Amazon Kinesis Data Analytics를 활용해 예측 유지 보수 가능    AWS 참조 아키텍처        유연성이 크기 때문에 원하는 방식으로 설계 가능, AWS 참조 아키텍처 데이터 시트는 AWS 클라우드를 활용하여 애플리케이션을 구축하는데 필요한 아키텍처 지침을 제공한다.        ex) 웹 애플리케이션 호스팅 : 확장성과 신뢰성이 매우 높은 웹 또는 모바일 웹 애플리케이션을 구축        ex) 콘텐츠 및 미디어 서빙 : 엄청난 양의 콘텐츠와 미디어를 서비스하는 신뢰성 높은 시스템을 구축        이외에도 일괄처리, 내결함성 및 HA, 대고뮤 컴퓨팅, 광고 지원, 로컬 애플리케이션을 위한 DR(w재해복구) 솔루션, 파일 동기화, 전자 상거래 웹 사이트 :프론트엔드, 결제서비스, 마케팅, 추천, 시계열 데이터 처리 등의 아키텍처가 있다.    02 AWS 주요 서비스 이해하기    Amazon Elastic Compute Cloud(EC2)        클라우드의 가상 서버, 안전하고 크기 조정이 가능한 컴퓨팅 파워를 클라우드에서 제공        웹 서비스 인터페이스를 통해 간편하게 필요한 용량을 얻고 구성 가능, 컴퓨팅 리소스에 대한 포괄적인 제어권을 제공,  검증된 컴퓨팅 인프라에서 실행 가능        새로운 서버 인스턴스를 획득하고 부팅하는데 필요한 시간을 단 몇 분으로 단축하므로 컴퓨팅 요구 사항의 변화에 따라 신속하게 용량을 확장하거나 축소 가능        개발자가 장애에 대한 복원력이 뛰어나고 일반적인 오류 상황에 영향을 받지않는 애플리케이션을 구축할 수 있도록 하는 도구 제공    Amazon Simple Storage Service(S3)        클라우드에서의 확장 가능한 스토리지, 어디서나 원하는 양의 데이터를 저장, 검생할 수 있도록 구축된 객체 스토리지        업체 최고의 확장성, 데이터 가용성 및 보안관 성능을 제공, 사용하기 쉬운 관리 기능, 높은 내구성    Amazon Aurora        관리형 고성능 관계형 데이터베이스, MySQL 및 PostrgreSQL 호환 관계형 데이터베이스        1/10 비용으로 상용 데이터베이스 수준의 성능 및 가용성 제공, 성능, 간편성, 가성비 뛰어남,        표준 오픈소스 데이터베이스 보다 3~5배 가량 빠르며, 보안, 가용성, 안정성이 뛰어남        자가 복구 분산 스토리지 시스템으로 최대 64TB 까지 자동으로 확장, 특정 시점 복구, 백업 가능    Amazon DynamoDB        관리형 NoSQL 데이터베이스, 어떤 규모에서든 빠르고 유연한 NoSQL 데이터 베이스 서비스        완전 관리형의 다중 리전, 다중 마스터 데이터베이스, 보안, 백업, 복원, 인 메모리 캐싱 기능 제공        10밀리초 미만의  성능을 제공하는 키-값 및 문서 데이터 베이스, 하루 10조개 요청 처리, 초당 2000만개 피크 요청 지원    Amazon RDS(Amazon Relational Database Service)        관리형의 관계형 데이터 베이스 서비스, 클릭 몇 번으로 클라우드에서 관계형 데이터베이스를 설정, 운영 및 확장 가능        하드웨어 프로비저닝, 데이터베이스 설정, 패치 및 백업과 같은 시간 소모적인 관리 작업을 자동화하면서 비용 효율적이고 크기 조정 가능한 용량을 제공        사용자가 애플리케이션에 집중하여 애플리케이션에 필요한 빠른 성능, 고가용성, 보안 및 호환성을 제공할 수 있도록 지원        여러 데이터베이스 인스턴스 유형으로 제공,        AWS Database Migration Service로 기존 데이터베이스에서 RDS로 손쉽게 마이그레이션, 복제 가능    AWS Lambda        서버 걱정없이 코드 실행, 사용한 컴퓨팅 시간에 대해서만 비용 지불        코드 실행되지 않으면 요금 부과 안됨, 모든 유형의 애플리케이션이나 백엔드 서비스에 대한 코드를 별도의 관리 없이 실행, 다른 AWS 서비스에서 자동으로 코드가 트리거 되도록 설정하거나 외부 웹 등에서 호출 가능    Amazon VPC        격리된 클라우드 리소스, 고객이 정의하는 AWS 계정 전용 가상 네트워크으로,  AWS 리소스(EC2 같은)를 구동할 수 있는 AWS 클라우드 상에 논리적으로 격리된 공간을 프로비저닝        Amazon Virtual Private Cloud는 AWS 클라우드에서 논리적으로 격리된 공간을 프로비저닝하여 고객이 정의하는 가상 네트워크에서 AWS 리소스를 시작 가능        IP 주소 범위 선택, 서브넷 생성, 라우팅 테이블 및 네트워크 게이트웨이 구성 등 가상 네트워킹 환경을 완벽하게 제어 가능        VPC에서 IPv4와 IPv6를 모두 사용하여 리소스와 애플리케이션에 안전하고 쉽게 액세스 가능    Amazon Lightsail        가상 프라이빗 서버 시작 및 관리, 저렴하고 예측가능한 요금으로 가상 서버, 스토리지, 데이터베이스 및 네트워킹을 사용        애플리케이션 또는 웹사이트를 구축하는 데 필요한 모든 것을 제공하는 사용하기 쉬운 클라우드 플랫폼        경제적이며 사용하기 쉬움, 처음 시작할 때, 간단한 웹 애플리케이션 일 때 사용하면 좋음        미리 구성된 애플리케이션(WordPress, Magento, plesk 등을 사용해 블로그, 전자 상거래 또는 개인 웹사이트를 만들기 가능)        비즈니스용 오픈 소스 및 상용 소프트웨어를 간편하게 실행 가능        상용 환경과 분리하여 몇 초 만에 개발자 또는 테스트 환경을 시작 가능    Amazon SageMaker        기계 학습 모델을 대규모로 신속하게 구출, 교육 및 배포, 머신러닝 서비스        전체 기계 학습 워크플로를 포괄하여 데이터를 분류 및 준비하고, 알고리즘을 선택하며, 모델을 학습하고 배포를 위해 조정 및 최적화하고, 예측을 수행하며, 작업을 수행하는 완전관리형 서비스,        훨씬 적은 노력과 비용으로 더 빨리 모델을 실행 가능                            구축          학습          배포                                      학습 데이터 수집 및 준비          학습 데이터 수집 및 준비          프로덕션 환경에 모델 배포                          데이터 레이블 지정 및 일반적인 문제에 대한 기본 제공 노트북          초고 성능의 인프라에서 원클릭 학습          클릭 한 번으로 배포                          ML 알고리즘 선택 및 최적화          모델 학습 및 튜닝          프로덕션 환경 조정 및 관리                          AWS Marketplace의 기본 제공 고성능 알고리즘 및 즉시 사용 가능한 수백 가지 알고리즘          한 번의 학습, 모든 위치에서 실행 및 모델 최적화          Auto Scailing 기능이 포함된 완전관리형 서비스로 75%의 비용 절감                    03 AWS 주요 제품 이해하기주요 제품 이해하기1. 주요 제품 개요2. 분석  Athena(SQL을 사용해 S3의 데이터 쿼리)  CloudSearch(관리형 검색 서비스)  EMR(호스팅된 하둡 프레임워크)  Kinesis(실시간 비디오 및 데이터 스트림 분석)  등이 있다.3. AWS 기반 데이터 레이크 및 분석  데이터 레이크 및 분석 솔루션 구축을 위한 가장 포괄적이고, 안전하고 확장 가능하며 비용 효율적인 서비스 포트폴리오  데이터 레이크는 기존의 데이터 사일로(고정적인 저장소, 표준화 안됨) 및 데이터 웨어하우스가 수행할 수 없는 방식으로 다양한 유형의 데이터와 분석 기법을 결합하여 보다 심층적인 통찰력을 얻기위해 필요한 규모, 민첩성 및 유연성을 제공  데이터(원천 데이터, 로우 데이터)는 가공 이전의 데이터를 의미,  보안 또는 거버넌스를 저하하지 않으면서 모든 관련 데이터에 쉽게 액세스 할 수 있는 가장 광범위한 분석 및 기계 학습 서비스를 고객에게 제공      데이터 이동 : 온 프레미스에서 실시간으로 데이터 수집        데이터레이크 : 기가바이트에서 엑사바이트까지 모든 유형의 데이터를 안전하게 저장        분석 : 가장 광범위한 분석 서비스를 사용하여 데이터 분석        Machine Learning : 미래의 결과 예측 및 신속한 대응 안내4. 어플리케이션 통합        Step Function(분산 애플리케이션을 위한 조정)        MQ(ActiveMQ용 관리형 메시지 브로커)        SimpleNotificationService(SNS) 게시/구독용 관리형 메시지 주제        AppSync(여러 원본의 적절한 데이터로 앱을 대규모로 구동)        등이 있다5. AWS에서의 애플리케이션 통합        마이크로 서비스, 분산 시스템 및 서버리스 애플리케이션 통합        최신 애플리케이션은 좀 더 쉽게 개발, 배포 및 유지 관리할 수 있는 작고 독립적인 빌딩 블록으로 구성됨        애플리케이션 통합서비스로 마이크로 서비스, 분산 시스템 및 서버리스 애플리케이션 내에 결합되지 않은 구성 요소 간 커뮤니케이션을 활성화하여 사용자가 확장 가능하고 복원력이 더욱 뛰어난 솔루션을 손쉽게 구축 가능6. 증강현실 및 가상현실        Amazon Sumerian(증강현실 및 가상현실 애플리케이션을 구축 및 실행)        특수 프로그래밍 또는 3D 그래프 전문 지식 없이도 가상현실, 증강현실 3D 애플리케이션을 신속하고 손쉽게 생성 및 실행 가능        모바일에서도 몰입도 높은 대화형 장면 구축 가능, 가상 강의실, 원격 건물 구축 등, 7. AWS 비용 관리        AWS 비용 및 사용량에 액세스하고 이를 이해, 제어 및 최적화하는데 도움이 되는 도구를 줌        월별 청구서            AWS 비용 관리 대시보드를 사용해 지출의 상태를 볼 수 있음  제품별 서비스 이해하기1. 블록체인  조정 가능한 블록 체인 및 원장 솔루션을 손쉽게 구축 가능한 단순한 방법 제공  주로 두가지 유형의 고객 요구사항 해결 가능          여러 당사자가 중앙의 신뢰할 수 있는 기관과 협력하여 완전하고 검증 가능한 트랜잭션 레코드를 유지하는데, 제품의 공급망 이동에 관한 투명하고 검증 가능한 정보의 레코드를 유지하는 중앙 집중식 원장에 공급업체를 연결하고자 하는 소매 고객      중앙의 신뢰할 수 있는 기관에 대한 필요 없이 여러 당사자가 분산된 방식으로 거래하는데, 중앙 집중식 기관을 연락 담당자로 사용하지 않고 자산의 교차 경계 이전을 수행하려는 은행 및 수출관 연합      1) 블록체인 제품가.  Amazon Quantum Ledger Database( 완전관리형 원장 데이터베이스, QLDB)  완전관리형 원장 데이터베이스  중앙의 신뢰할 수 있는 기관이 소유하는 투명하고, 변경 불가능하며, 암호화 방식으로 검증 가능한 트랙잭션 로그 제공  모든 애플리케이션 데이터 변경 내용을 추적하며 완전하고 검증 가능한 시간대별 변경 기록을 유지 관리나. Amazon Managed Blockchain (확장 가능한 블록체인 네트워크를 생성 및 관리)  널리 사용되는 오픈 소스 프레임 워크인 Hyperledger Fabric 및 Ethereum을 사용해 조정 가능한 블록체인 네트워크를 생성하고 관리할 수 있도록 지원하는 완전관리형 서비스  네트워크를 생성하는 데 필요한 오버헤드를 제거하며 수백만 건의 트랜잭션을 실행하는 수천 개 애플리케이션의 수요를 충족하도록 자동 조정됨다.  AWS Blockchaing Templates  자체 블록체인 네트워크를 직접 관리하면서 간편하게 설정 및 시작할 수 있는 방법을 찾는 고객을 위함  ECS 클러스터에 컨테이너로 배포하거나  Docker를 실행하는 EC2인스턴스에 직접 배포  이외에 Blockchaing Partners 등이 있음2. 비즈니스 애플리케이션1) Alexa for Business( 알렉사로 조직의 영량 강화)  Alexa를 지능형 비서로 사용하여 생산성 향상  AWS Chime(화상 회의 ) 등이 있음3. 컴퓨팅  EC2(클라우드의 가상 서버) : 크기 조정 가능 컴퓨팅 제공  EC2 Auto Scaling(수요에 맞춰 컴퓨팅 파워 조정)  EC2 Container Registry(Docker 이미지 저장 및 검색)  Elastic Container Service(Docker 컨테이너 실행 및 관리) : 확장성이 뛰어난 고성능 컨테이너 오케스트레이션 서비스, 컨테이너 기반 어플리케이션 실행,  Elastic Kubernetes Service(AWS에서 관리형 Kubernetes 실행  Lambda(서버에 대한 걱정없이 코드 실행) : 서버 프로비저닝, 관리할 필요없이 코드 실행 가능  Outposts(AWS 인프라를 온프레미스에서 실행)  VMware Cloud on AWS (사용자 지정 하드웨어 없이 하이브리드 클라우드 구축)  등이 있다.4. AWS 클라우드 컴퓨팅  가상 서버 호스팅, 컨테이너 관리 및 서버리스 컴퓨팅  모바일 앱 구축 부터 인간 게놈 분석까지 컴퓨팅을 정하고 시작함5. 고객 인게이지 먼트  Amazon Connect(클라우드 기반 콜 센터 서비스)  등6. 데이터 베이스  Amazon Aurora(고성능 관리형의 관계형 데이터베이스)  Amazon ElastiCache(인 메모리 캐싱 서비스)  Amazon RDS  Amazon Redshift( 빠르고 간단하며 비용 효율적인 데이터 웨어하우징)  등이 있다.  모든 애플리케이션 유구에 맞춰 특별히 구축된 데이터베이스7. AWS 데이터베이스  모든 어플리케이션 요구에 맞춰 특별히 구축된 데이터베이스  성능이 좋고 관계형, 비관계형 상관 안함8. 개발자 도구  AWS CodeStar 등9. 최종 사용자 컴퓨팅  Workspace 등10. 게임 기술  gamelfit(멀티플레이 게임 서버), lumberyard(게임 엔진) 등11. 사물 인터넷  1-Click: 클릭 한번으로 AWS Lambda 트리거를 생성  IoT Core : 디바이스를 클라우드에 연결  IoT Analytics: IoT 디바이스에 대한 분석  IoT Button : 클라우드 프로그래밍 가능한 대시 버튼  IoT Device Defender: IoT 보안12. 기계 학습  SageMaker : 기계 학습 모델을 대규모로 구축, 훈련 및 배포  Comprehend : 텍스트에서 통찰력 확보 및 관계 파악  Lex : 음성 및 텍스트 챗봇 구축  Polly: 텍스트를 생생한 음성으로 변환  Rekognition : 이미지 및 비디오 분석  Forecast : 기계학습을 사용하여 예측 정확도 개선  Transcribe : 자응 음성 인식  DeepLens : 딥러닝이 지원되는 비디오 카메라  TenseorFlow : 오픈소스 인공지능 라이브러리13. 관리 및 거버넌스  Auto Scailing: 수요에 맞춰 여러 리소스의 규모 조정  Config : 리소스 인벤토리 및 변경 추적  Service Catalog : 표준화된 제품을 생성 및 사용  Advisor : 성능 및 보안 최적화14. 미디어 서비스  MediConnect : 안정적이고 안전한 라이브 비디오 전송  등이 있다..15. 마이그레이션 및 전송  Migration Hub : 한곳에서 마이그레이션을 추적  DataSync : 간단하고 빠른 온라인 데이터 전송등이 있다.16. 모바일  Pinpoint : 채널 전체에서 사용자 참여  등이 있다.17. 네트워킹 및 콘텐츠 전송  VPC (격리된 클라우드 리소스)  Roiute 53( 확장 가능한 DNS)  VPN  ELB (수신 트래픽을 여러대상에 걸쳐 분산)등이 있다.18. 보안, 자격 증명 및 규정 준수  IAM (사용자 액세스 및 암호화 키 관리)  Cognito (앱을 위한 자격 증명관리)  CloudHSM(규제 준수를 위한 하드웨어 기반 키 스토리지)등이 있다.19. 스토리지  S3(클라우드에서의 확장 가능한 스토리지)  Elastic Block Store(EC2 블록 스토리지 볼륨)  S3 Glacier(클라우드상의 저렴한 아카이브 스토리지)  AWS Backup(AWS 서비스 전체에 걸쳐 중앙집중식 백업)04 AWS 서버 이해하기서버아키텍처1. 웹 서버 아키텍처AWS 서버 서비스1. Availablity Zone(AZ)      AWS의 데이터 센터는 가용 영역(AZ) 내에 편성        각 가용 영역은 하나 이상의 데이터 센터로 구성        하나의 데이터 센터가 2개의 가용 영역에 포함될 수 는 없음        각 가용 영역은 독립된 장애 영역으로 설계        가용 영역은 일반적인 대도시 리전 내에서 물리적으로 격리, 주로 서로 다른 그리드를 통해 전력을 공급받음        사용자는 시스템이 상주할 가용 영역을 선택해야하며,        여러 가용영역에 걸쳐 확장 가능하므로 재해가 발생 시 극복할 수 있도록 설계 필요        여러 개의 가용 영역에 애플리케이션을 분산하면 자연 재해나 시스템 장애 등 대부분의 장애 상황에서도 복원력을 유지 가능2. Region (리전)        가용 영역은 다시 리전으로 그룹화        각 AWS 리전은 2개 이상의 가용 영역을 포함        특정 리전에 데이터를 저장하는 경우, 해당 리전 내에서만 데이터가 복제됨        AWS는 사용자가 데이를 저장한 리전 외부로 데이터를 이동하지 않음        비즈니스에서 필요시 여러 리전에 걸쳐 데이터 복제하는 작업은 사용자 책임        각 리전이 위치한 국가 및 지역에 대한 정보를 제공하고 규정, 네트워크 지연 시간 등에 따라 사용자가 선택해야함        리전간 통신은 퍼블릭 인터넷 인프라로 이루어지므로 암호화 해야한다.        리전에 따라 사용가능한 AWS 제품과 서비스가 달라지므로 주의해야한다.3. Virtual Private Cloud(VPC)        사용자의 AWS 계정 전용 가상 네트워크        AWS 클라우드에서 다른 가상 네트워크와 논리적으로 분리        Amazon EC2 인스턴스와 같은 AWS 리소스를 VPC에서 실행할 수 있는 클라우드의 네트워크 환경이며, 다양한 리소스를 시작하고, 환경민 리소스 상호격리에 대한 탁원한 제어 기능을 제공        리전 내에 존재하며, 해당 리전 외부에는 리소스가 없음, 단, 같은 리전 내 다른 가용 영역에 있는 리소스는 같은 VPC에 존재할 수 있음        인터넷 게이트웨이, NAT 또는 방화벽 프로시를 사용하지 않고 VPC 엔드포인트를 통해 AWS 서비스에 비공개료 연결 가능4. Subnet(서브넷)        VPC의 IP 주소 범위를 의미        지정된 서브넷으로 AWS 리소스 시작 가능        인터넷 연결이 필요하면 퍼블릭 서브넷, 아니면 프라이빗 서브넷을 이용ex) Network Address Translation(NAT)을 사용하여 인터넷에서 주소로 직접 액세스하지 못하게 할 인스턴스에 대해서는 프라이빗 서브넷을 사용, 이 경우 퍼블릭 서브넷의 NAT 게이트웨이를 통해 트래픽을 라우팅하여 프라이빗 IP 주소를 노출하지 않고 인터넷에 액세스 가능    Amazon EC2(Elastic Compute Cloud)        AWS 클라우드에서 확장 가능한 컴퓨팅 제공        이를 통해 하드웨어에 선투자없이 빠르게 애플리케이션 개발, 배포 가능        EC2를 통해 원하는 만큼 가상 서버를 구축하고 보안 및 네트워크 구성과 스토리지 관리 가능        요건이나 갑작스러운 접속 증가 등 변동 사항에 따라 확장하거나 축소 가능하여 트래픽 예측 필요성 감소1. EC2의 기능        가상 컴퓨팅 환경 : 컴퓨팅 및 메모리 기능에 따라 목적에 맞는 인스턴스 생성        Amazon Machine Image(AMI) : 서버에 필요한 운영체제와 여러 소프트웨어들이 적절히 구성된 상태로 제공되는 템플릿으로 인스턴스 생성 가능하며 인스턴스를 위한 CPU, 메모리, 스토리지, 네트워킹 용량의 여러 가지 구성을 제공        인스턴스 스토어 볼륨 : 임시 데이터를 저장하는 스토리지 볼륨으로 인스턴스 종료 시 삭제됨        Amazon Elastic Block Store(Amazon EBS) : EBS 볼륨을 사용해 영구 스토리지 볼륨에 데이터를 저장 가능        Security Group (보안 그룹)을 사용해 인스턴스에 연결할 수 있는 프로토콜, 포트, 소스 IP 범위를 지정하는 방화벽 기능이 제공        Elastic IP(EIP) : 동적 클라우드 컴퓨팅을 위한 고정 IPv4 주소        태그 : 사용자가 생성하여 Amazon EC2 리소스에 할당할 수 있는 메타데이터        새로 서버 인스턴스를 획득하고 부팅하는데 단 몇분만에 만듦, 장애에 대한 복원력 뛰어남, 비용 절감 가능        기능 예시 : 인스턴스(CPU, 메모리, 스토리지 등), AMI(아마존 머신 이미지, 일종의 인스턴스 템플릿), 인스턴스 스토어 볼륨, 영구 스토리지 볼륨 (EBS), 인스턴스와 볼륨들을 다른 물리적 장소에서 액세스 할 수 있는 리전 및 가용 영역, 방화벽 기능, VPC(논리적으로 격리, 하지만 원할때마다 다른 네트워크와 연결 가능)        S3 스토리지는 주로 웹 규모의 컴퓨팅 작업을 위한 저장 고안    1) 스토리지의 추가          AMI에서 인스턴스를 실행할 때마다 해당 인스턴스에 대한 루트 스토리지 디바이스가 생성됨      루트 스토리지 디바이스에는 인스턴스를 부팅하기 위해 필요한 모든 정보가 포함      블록 디바이스 매핑을 사용하면 AMI를 생성하거나 인스턴스를 실행할 때 루트 디바이스 볼륨과 스토리지 볼륨을 지정 가능2. Relational Database Service(RDS)            클라우드에서 관계형 데이터베이스를 더욱 쉽게 설정, 운영 및 확장할 수 있도록 지원하는 웹 서비스이며 이 서비스는 산업 표준을 위해 경제적이고 크기조절 가능한 용량을 제공하며 공통 DB 관리 작업을 관리        CPU, 메모리, 스토리지 및 IOPS(Input/Output Operations per sec)가 따로 분할되므로 독립적으로 확장 가능        백업, 소프트웨어 패치, 자동 장애 감지 및 복구를 관리하며, 필요할 때 자동 백업을 수행하거나 고유한 백업 스냅샷을 수동으로 생성 가능하며 이러한 백업을 사용하여 데이터베이스를 복원 가능3. Elastic Load Balancing(ELB)        들어오는 애플리케이션 트래픽을 Amazon EC2 인스턴스, 컨테이너, IP 주소와 같은 여러 대상에 자동으로 분산        단일 AZ 또는 여러 AZ에서 다양한 애플리케이션 부할르 처리하므로 내결함성과 가용성이 향상됨        세가지 로드 밸런서가 존재하며 모두 애플리케이션의 내 결함성에 필요한 고가용성, 자동 확장/축소, 강력한 보안 기능 제공              Application Load Balancer      Network Load Balancer      Classic Load Balancer                  애플리케이션 레이어7(layer7)에서 작동하는 Application Load Balancer는 요청 콘텐츠를 기반으로 VPC 내의 대상으로 트래픽 라우팅, Layer 7 HTTP/HTTPS 애플리케이션을 로드 밸런싱하고 SSL/TLS 암호 및 프로토콜이 사용되도록 하여 애플리케이션의 보안을 개선      Layer 4에서 작동하는 Network Load Balancer는 IP 프로토콜 데이터를 기반으로 VPC 내의 대상으로 연결 라우팅, TCP 트래픽의 로드 밸런싱에 적합하며 AZ 당 하나의 정적 IP 주소를 사용하면서 갑작스럽고 변동이 심한 트래픽 패턴 처리      여러 Amazon EC2 인스턴스에서 기본적인 로드 밸런싱을 제공하며, Layer4 및 Layer 7에서 작동, Classic Load Balancer는 Application Load Balancer나 Network Load Balancer보다 이전에 나온 서비스이며 EC2-Classic 네트워크 내에 구축된 애플리케이션용이지만, VPC에서도 사용 가능하고 비교적 설정이 간단하므로 해당 메뉴얼에서는 Classic Load Balancer를 사용      05 AWS 스토리지 활용하기스토리지(Storage)  데이터를 저장하는 안정적이고 확장 가능하며, 안전한 장소 클라우드 컴퓨팅의 매우 중요한 구성 요소로서, 애플리케이션에서 사용하는 정보를 보관  기존 온프레미스 스토리지 시스템보다 안정성, 확장성, 보안성 뛰어남  모든 범위의 클라우드 스토리지 서비스를 제공하여 애플리케이션과 아카이브 규정 준수 요구사항을 모두  지원함  클라우드 데이터 마이그레이션 옵션으로 클라우드 IT 환경의 기초 설계 시작 가능1. 스토리지 제품1) EBS(Elastic Block Store): EC2, DB, 데이터 웨어하우징, 기업의 앱, 빅데이터 처리 또는 백업 및 복구를 위한 영구 로컬 스토리지2) EFS(Elastic File System): Linux 기반의 워크로드를 AWS 클라우드 서비스와 온프레미스 리소스에서 사용할 수 있도록 지원하는 간단하고 확장가능하며 탄력적인 파일 시스템, 앱 중단 없이 페타바이트 규모까지 자동 확장 축소 가능, 3) FSx for Lustre : 고성능 컴퓨팅, 기계 학습 및 미디어 데이터 처리 워크플로우 같은 집약적 워크로드에 최적화된 완전 관리형 파일 시스템, S3에 완벽하게 통합4) FSx for Windows File Server : Windows Server를 기반으로 구축도니 완전관리형 네이티브 파일 ms Window 시스템, NTFS, AD(Active Directory) 통합, DFS에 대한 지원5) S3(simple Storage Service) 6) Glacier : 아카이브 및 규제 준수를 위한 장기 스토리지7) Storage Gateway : 버스팅, 계층화, 마이그레이션을 위한 하이브리드 스토리지 클라우드8) 클라우드 데이터 마이그레이션 서비스9) Backup 등이 있다 2. AWS의 클라우드 스토리지 제품      EC2 인스턴스의 영구 볼륨에서 블록 데이터를 저장 처리 , EBS는 EC2에 비해 가용성이 뛰어나고 일관되며 지연 시간이 짧은 블록 스토리지 제공, 용량, 성능 ,비용이 뛰어남        단일 EC2 인스턴스에서 영구 스토리지에 액세스 해야하는 워크로드에 적합함        DB, 빅데이터 분석엔진, 스트림 및 로그 처리 애플리케이션 및 데이터 웨어하우징 등에 사용3. Amazon Elastic File System(EFS)        Linux를 위한 확장 가능하고 탄력적이며 클라우드 네이티브 파일 시스템, AWS 클라우드 서비스와 온프레미스 리소스에서 사용할 수 있는 간단하고 확장 가능하며 탄력적인 Linux 기반 워크로드용 파일 시스템        앱 중단 없이 페타바이트 규모까지 자동으로 확장 축소 가능,        수천 개의 EC2 인스턴스에 대한 병렬 공유 액세스를 대량으로 제공, 일관적으로 낮은 지연 시간을 유지하면서 높은 수준의 집계 처리량과 IOPS를 달성함**4. Amazon S3(simple Storage Service) **        모든 유형의 데이터를 저장하고 액세스할 수 있도록 설계된 객체 스토리지        안전하고 99.99999%의 내구성제공, 수십조 객체로 확장 가능        빅데이터 분석 웨어하우스 플랫폼, 데이터 레이크, 서버리스 컴퓨팅 설계의 기반        S3 Select는 데이터 읽기 및 검색 응답 시간이 최대 400%까지 단축됨, 강력한 보안과 규정 준수5. FSx for Lustre        컴퓨팅 집약적 워크로드를 위한 완전관리형 Lustre 파일 시스템        고성능 컴퓨팅, 기계학습 등같은 집약적 워크로드에 최적화된 완전 관리형 파일 시스템        초당 최대 수백 기가바이트 처리량, 수백만 IOPS 및 1 밀리초 미만의 지연시간,        S3와 완벽하게 통합되어 있음6. FSx for Windows File Server        완전관리형 네이티브 Windows 파일 시스템        파일 스토리지가 필요한 Windows 기반 애플리케이션을 손쉽게 AWS로 이전 가능        SMB 프로토콜 및 NTFS, AD(Active Directory) 통합, DFS(분산 파일 시스템)에 대한 전체 지원을 비롯하여, Windows 기반 애플리케이션이 사용할 수 있는 기능과 호환성을 갖춘 공유 파일 스토리지 제공        SSD 스토리 사용으로 빠른 성능 및 처리량, 낮은 지연시간7. S3 Glacier        저렴한 스토리지에 데이터 아카이브        데이터 유형과 관계없이 장기 백업 및 자주 액세스 하지 않는 데이터에 사용8. AWS Backup        AWS 서비스 전체에 걸쳐 중앙에서 백업 관리 및 자동화        AWS 리소스들에 대한 백업 활동을 모니터링, 백업 정책 구성 가능9. AWS Storage Gateway        완벽한 통합과 최적화된 데이터 전송을 지원하는 하이브리드 클라우드 스토리지        연결 기능이 고도로 최적화된 로컬 스토리지 제공, 온프레미스 환경을 AWS에 완벽 연결10. AWS 데이터 전송 서비스        AWS 클라우드에서 데이터 송신 및 수신        이를 통해 안전하고 빠르게 데이터 마이그레이션 가능    데이터 백업(Data Backup)    1. AWS Backup1) AWS Backup        AMI, EBS, Snapshot 등의 기능을 이용해 백업이 용이하며 인스턴스를 종료해도 데이터를 쉽게 보존 가능        EC2 인스턴스를 사용하지 않더라도 켜놓은 과금되므로 중지하거나 제거해야함        언제 사용할지 모르면 백업을 해야함2) AMI, EBS        AMI(Amazone Machine Image) : 인스턴스를 시작할 때 필요한 정보를 포함함, ex) OS 정보, AMI 접근 권한, EBS 볼륨 매핑 정보 등을 포함        EBS(Elastic Block Store) : 컴퓨터의 디스크 드라이브, 즉 데이터를 담고 있음, 하나의 인스턴스를 정상적으로 백업 및 복원을 하귀 위해서는 1개의 AMI와 그에 딸린 n개의 EBS 볼륨이 필요함3) Create Image    EC2 콘솔창에서 스냅샷을 뜨고 싶은 인스턴스를 우클릭해 Image-&gt;Create Image를 클릭하여 인스턴스와 EBS 전체를 한번에 스냅샷을 떠서 저장 가능2. AWS Backup 개요1) 백업을 중앙에서 관리  중앙 백업 콘솔에서 백업 정책을 구성하여 백업 관리를 간소화하고 AWS 서비스 전체에서 애플리케이션 데이터를 백업하고 보호 가능  Backup의 중앙 콘솔, API 또는 명령줄 인터페이스를 사용하여 AWS Storage Gateway를 통해 온프레미스와 클라우드의 AWS 서비스 전체에 걸쳐 백업하고 복원하고 백업 보존 정책을 설정 가능2) 백업 프로세스 자동화  완전관리형, 정책 기반솔루션을 이용해 시간과 비용 절약  자동화된 백업 일정, 보존 관리 및 수명 주기 관리 제공함  리소스에 태그를 지정해 간단하게 백업 정책 적용 가능3) 백업 규정 준수 개선  중앙집중식콘솔에서 백업 정책을 적용하고 백업을 암호화하고 백업 활동을 감사하여 규정 준수 요구사항 충족 가능  전송 데이터 및 저장 데이터를 암호화하여 백업함3. AWS Backup 작동 방식      AWS 서비스 전체를 아우르는 클라우드 네이티브 백업, 클라우드와 온프레미스 모두에서 애플리케이션 데이터를 모두 백업하는 하이브리드 백업, 온프레미스 백업 등이 가능함    스냅샷(Snapshot)    지정 시간 스냅샷을 만들어 S3에 EBS 볼륨의 데이터를 백업 가능  스냅샷은 증분식 백업이어서 마지막 스냅샷 이후 변경된 디바이스의 블록만이 저장됨  스냅샷을 만드는데 필요한 시간이 최소화되며 데이터를 복제하지 않으므로 스토리지 비용이 절약됨  스냅샷 삭제시 해당 스냅샷에 고유한 데이터만 제거됨  데이터를 새 EBS 볼륨에 복원하는 데 필욯나 모든 정보가 있음      다중 볼륨 스냅샷 : 여러 EBS 볼륨에 걸쳐있는 파일 시스템 또는 대규모 DB 등의 중요한 워크로드 백업을 생성 가능    S3 Glacier    데이터 아카이빙을 위한 안전하고 안정적인 장기 객체 스토리지  저렴하고 안전하며 안정적인 클라우드 스토리지 서비스, 엄격한 규제 요구 사항도 총족하는 종합적인 보안 및 규정 준수 기능 제공  현 위치에서 쿼리하는 기능 제공, 직접 강력한 분석 실행 가능, 온프레미스 솔루션과 비교해 상당힌 비용절감  아카이브에 액세스하는 3가지 옵션 (몇분 ~몇시간) 제공, 주로 자주 액세스하지 않는 정보에 사용      동영상, 뉴스 같은 미디어 자산 워크플로, 의료 정보 아카이빙, 과학 데이터 등에 사용    AMI와 Market Place    1. AMI(Amazon Machine Image)    인스턴스 시작시 필요한 정보를 제공함  동일한 구성의 인스턴스가 여러개 필요시 한 AMI에서 여러 인스턴스를 시작 가능  서로 다른 구성일 경우 여러 AMI로 여러 인스턴스 가능  주로 1개 이상의 EBS 스냅샷 또는, 인스턴스 저장 지원 AMI의 경우 인스턴스의 루트 볼륨에 대한 템플릿 (ex: 운영 체제, 애플리케이션 서버, 애플리케이션), AMI를 사용하여 인스턴스를 시작할 수 있는 AWS 계정을 제어하는 시작 권한, 시작될 때 인스턴스에 연결할 볼륨을 지정하는 블록 디바이스 매핑을 포함함2. AWS Marketplace  고객이 구매한 소프트웨어 및 서비스를 쉽게 찾아 구매, 배포 및 관리 할 수 있또록 큐레이션 프로세스를 거친 디지털 카탈로그  수많은 소프트웨어 제품이 포함되어있으며, 유연한 요금 옵션과 다양한 배포 방법을 통해 소프트웨어 라이센스 취득 및 구매의 간소화 가능  클릭 몇번 만으로 사전 구성된 소프트웨어 시작, AMI 및 Saas 형식으로 선택 가능  소비자와 공급자 어느쪽으로 여러 요금체계로 등록 가능, AWS 청구서에 요금이 표시됨      Saas, AMI 제품 등을 공급 가능    06 AWS 데이터베이스 서버 만들기    데이터베이스(DB)    1. AWS 데이터베이스    모든 어플리케이션 요구에 맞춰 특별히 구축된 데이터베이스  클라우드 덕분에 스토리지 및 컴퓨팅 비용이 하락하여 새로운 세대의 애플리케이션이 등장함  DB에 새로운 요구사항이 제기되며 이를 위해 특별히 구축된 관계형, 비관계형 DB가 필요함  AWS 완전관리형 DB 서비스  트랜잭션 어플리케이션을 위한 관계형 데이터베이스  인터넷 규모의 애플리케이션을 위한 비관계형 데이터베이스  분석을 위한 데이터 웨어 하우스  캐싱 및 실시간 워크로드를 위한 인 메모리 데이터 스토어  상호 연결성이 높은 데이터가 있는 애플리케이션 구축을 위한 그래프 데이터베이스등이 있다.  기존 DB를 AWS로 마이그레이션 하려면 AWS DB Migration Service로 쉽게 가능하다.  관계형 DB는 미리 정의된 스키마와 데이터 간의 관계로 데이터를 저장하며, ACID 트랜잭션 지원, 참조 무결성 유지, 데이터 일관성을 위해 설계 됨, 전자 상거래, ERP, CRM, 기존 앱등에 사용됨  키-값 DB는 키-값 페어를 대량으로 밀리초 단위로 저장 및 검색하는 데 최적화 되어있어 규모 제한 및 성능 오버헤드가 없음, 실시간 입찰, 장바구니 , 고객 선호도, 큰 규모의 애플리케이션 등에 사용됨  문서 DB는 반구조화된 데이터를 개발자가 직관적으로 사용할 수 있는 문서로 저장하도록 설계됨, 가독 가능한 문서로 표시됨, 콘텐츠 관리, 개인화, 모바일 어플리케이션 등에 사용됨  인메모리 DB는 데이터에 실시간으로 액세스해야하는 애플리케이션에 사용됨, 이러한 DB는 메모리에 직접 저장하여 밀리초 지연시간으로 충분하지 않은 경우 마이크로초 까지 제공, 캐싱, 게임 순위표, 실시간 분석 등에 사용 됨  그래프 DB는 수백만명의 사용자가 밀리초 지연시간으로 상호 연결성이 높은 그래프 데이터 세트 간의 관계를 쿼리 및 탐색할 수 있어야 하는 애플리케이션에 사용됨, 부정 탐지, 소셜 네트워킹 및 추천 엔진 등에 사용됨  시계열 DB는 시간에 따라 변화하는 엄청난 양의 데이터(시계열 데이터)를 수집 및 종합하고 통찰력을 도출하는데 사용, IOT, Devops 등에 사용됨      원장 DB는 확장 가능하고 완전하며 암호로 검증 가능한 트랙잭션 레코드를 유지 관리하기 위해 신뢰할 수 있는 중앙기관이 필요한 경우에 사용됩니다. 레코드 시스템, 공급망, 등록, 은행 거래 등에 사용됨    RDS(Relational Database Services)    1. Amazon RDS    클릭 몇 번으로 클라우드에서 관계형 데이터베이스를 설정, 운영 및 확장  하드웨어 프로비저닝, 데이터베이스 설정, 패치 및 백업과 같은 시간 소모적인 관리 작업을 자동화하면서 비용 효율적이고 크기 조정 가능한 용량을 제공함  빠른 성능, 고가용성, 보안 및 호환성을 제공할 수 있도록 지원함  메모리, 성능 또는 I/O 최적화에 여러 데이터베이스 인스턴스 유형으로 제공됨,  AWS Database Migration Service를 이용해 AWS RDS로 기존의 DB를 쉽게 옮길수 있음2. Amazon RDS 데이터베이스 엔진1) Amazon Aurora  고성능 상용 데이터베이스의 속도와 가용성에 오픈 소스 데이터베이스의 간편성과 비용 효율성을 결합한 MySQL 및 PostgreSQL 호환 관계형 데이터베이스 엔진  MySQL보다 5배 뛰어난 성능과 상용 데이터베이스의 보안성, 가용성 및 안정성을 1/10의 비용으로 제공함2) ORACLE  가성비좋고 크기조정 가능한 여러 버전의 Oracle DB를 몇 분 만에 배포할 수 있음  기존의 Oracle 라이선스를 가져오거나 1시간 단위로 라이선스 사용료 지불 가능  RDS를 사용하면 프로비저닝, 백업, 패치 적용, 모니터링, 하드웨어 확장 등 복잡한 DB 관리 작업을 관리함으로써 앱 개발에 집중 가능3) MS SQL Server  SQL Server용 RDS를 활용해 쉽게 설치, 운영, 확장 여러 버전으로 가능  기본 기능에 직접 액세스 권할을 제공하므로 아무런 변경없이 작동 함4) MySQL  매우 많은 웹 기반 애플리케이션에서 사용하는 오픈소스 관계형 DB (RDBMS)  MySQL용 Amazon RDS는 익숙한 MySQL 데이터베이스 엔진 기능에 액세스를 지원  기존 코드, 도구를 아무런 변경없이 사용 가능5) PostgreSQL  확장성 및 표준 준수에 중점을 둔 강력한 엔터프라이즈급 오픈소스 객체 관계형 DB  여러 정교한 기능과 여러 언어로 저장된 프로시저로 실행함6) MariaDB      MySQL의 한갈래로 MySQL을 호환하며(거의 유사), 클라우드에서 손쉽게 설정, 운영, 확장 가능    RDS 사례        airbnb(15분만에 DB 마이그레이션 끝), BandaiNamco(서버 리소수 추가, 변경, 제거 자유로움),    Amazon RDS for MySQL    1. Amazon RDS for MySQL    클릭 몇 번으로 클라우드에서 관계형 데이터베이스를 설정, 운영 및 확장  MySQL은 전 세계에서 가장 널리 사용되는 오픈 소스 관계형 데이터베이스이며 Amazon RDS를 사용하면 클라우드에서 MySQL 배포를 손쉽게 설정, 운영 및 확장할 수 있음  비용 효율적이고 크기 조정 가능한 하드웨어 용량을 갖춘 확장가능한 MySQL 서버를 몇분만에 배포 가능  백업, 소프트웨어 패치, 모니터링, 확장 및 축소, 복제 같은 시간 소모적 DB 관리 업무를 관리하므로 고객은 앱개발에 집중 가능  다수의 산업 표준 준수 (HIPAA 준수, 비즈니스 제휴 계약(BAA) 보호 대상 건강 정보(PHI)를 비롯한 의로 관련 정보 저장 가능), FedRAMP 보안 요건 충족, HIGH Baseline 수준의  P-ATO 취득(리전별 조사 필요)      여러 버전의 DB 인스턴스 지원, DB 인스턴스 크기 조정, 연결 인증, 백업, 스냅샷 생성, 복원, 다중 AZ 보조 생성, 읽기 전용 복제본 생성 및 DB 인스턴스의 성능 모니터링 가능    07 AWS 고가용성 로드 밸런싱 구축하기    Amazon Elastic Load Balancing    1. Amazon Elastic Load Balancing 개요    확장성, 성능 및 보안을 보장하여 애플리케이션의 내결함성 확보  들어오는 애플리케이션 트래픽을 EC2 인스턴스, 컨테이너 IP 주소, Lambda 함수와 같은 여러대상에 자동으로 분산 시킴  단일 가용영역 또는, 여러 가용영역에서 다양한 애플리케이션 부하를 처리 가능  높은 내결함성, 자동 증가축소, 보안성을 가지고 있음2. 사례  애플리케이션 내결함성 향상  컨테이너식 애플리케이션의 자동 로드 밸런싱  고객의 지연시간에 맞추어 auto scaling 하는 애플리케이션 자동 조정  Amazon VPC에서의 ELB 사용(연결 진입점 생성, 요청 트래픽 티어간 라우팅, 보안 그룹 할당)  AWS와 온프레미스 리소스 전체에서 로드 밸런싱 가능한 하이브리드 로드 밸런싱  HTTP(S)를 통해 Lambda 함수 호출 (사용자는 웹 브라우저를 포함해 HTTP 클라이언트에서 서버리스 애플리케이션에 액세스 가능)3. ELB의 기능1) 고가용성  수신 트래픽을 단일 가용 영역 또는 여러 가용 영역에 있는 여러 대상(EC2 인스턴스, 컨테이너, IP주소)에 자동 분산2) 상태 확인  비정상 대상을 감지하고, 해당 대상으로 트래픽 전송을 중단한 다음, 나머지 정상 대상으로 로드 분산3) 보안 기능  VPC 사용하면 로드 밸런서와 연결된 보안 그룹을 생성 및 관리하여 추가 네트워킹 및 보안 옵션을 제공할 수 있으면, 또한, 내부(인터넷 사용 안 함) 로드 밸런서를 생성할 수도 있음4) TLS 종료      통합 인증서 관리 및 SSL/TLS 복호화를 지원하므로 유연하게 로드 밸런서의 SSL 설정을 중앙에서 관리하고 애플리케이션의 CPU 집약적인 작업을 줄일 수 있음5) 계층 4 또는 계층 7 로드 밸런싱        계층 7 전용 기능에 대해 HTTP/HTTPS 애플리케이션을 로드밸런싱하거나 TCP 및 UDP 프로토콜을 활용하는 애플리케이션에 대해 엄격한 계층 4 로드 밸런싱을 사용 가능6) 운영 모니터링        성을 실시간 모니터링하도록 CloudWatch 지표 및 요청 추적과 통합을 제공4. ELB 제품        어플리케이션 요구 사항에 따라 적절한 로드 밸런서 선택 가능        유연한 어플리케이션 관리가 필요한 경우 Application Load Balancer를 사용하는 것이 좋음        애플리케이션에 탁월한 성능 및 고정 IP가 필요한 경우 Network Load Balancer를 사용하는 것이 좋음        기존 애플리케이션이 EC2-Classic 네트워크 내에 구축되어 있는 경우 Classic Load Balancer를 사용해야 함1) Application Load Balancer        요청 수준(계층 7)에서 작동하는 ALB는 요청 콘텐츠에 따라 EC2 인스턴스, 컨테이너, IP 주소, Lambda 함수 등의 대상으로 트래픽을 라우팅함        HTTP 및 HTTPS 트래픽의 고급 로드 밸런싱에 적합한 Application Load Balancer는 마이크로서비스와 컨티에너 기반 애플리케이션을 비롯한 최신 애플리케이션 아키텍처 전달을 위한 고급 요청 라우팅 기능을 제공함        항상 최신 SSL/TLS 암호 및 프로토콜이 사용되도록 하여 보안을 간소화하고 개선함가. 계층 7 로드 밸런싱          HTTP/HTTPS 애플리케이션을 로드 밸런싱하고 X-Forwarded-For 헤더와 같은 계층 7 전용 기능을 사용 가능나. HTTPS 지원      클라이언트와 로드 밸런서 간 HTTPS 종료를 지원함      AWS IAM 및 AWS Certificate Manager의 사전 정의된 보안 정책을 통해 SSL 인증서 관리도 제공함다. SNI(서버 이름 표시)      TLS 핸드쉐이크 시작 시 클라이언트가 연결할 호스트 이름을 표시하는 TLS 프로토콜에 대한 확장 프로그램      같은 보안 리스너를 통해 여러 개의 인증서를 제시할 수 있으므로 단일 보안 리스너를 사용하여 여러 개의 보안 웹 사이트를 지원하도록 할 수 있음      SNI를 살 사용한 스마트 인증서 선택 알고리즘 지원      클라이언트가 표시하는 호스트 이름이 여러 인증서와 일치할 경우 로드 밸런서가 클라이언트의 기능을 포함한 여러 요소를 기반으로 사용할 최적의 인증서를 결정함라. IP 주소를 대상으로 사용            애플리케이션 백엔드의 IP 주소를 대상으로 사용하여 AWS 또는 온프레미스에 호스팅된 애플리케이션을 로드 밸런싱할 수 있음 (인스턴스의 모든 IP 주소 및 인터페이스에 호스팅된 애플리케이션 백엔드로 로드 밸런싱할 수 있음)        같은 인스턴스에 호스팅된 각 애플리케이션은 연결된 보안 그룹을 가질 수 있고 같은 포트를 사용할 수 있음        IP 주소를 대상으로 사용하여 온프레미스 위치(직접 연결 또는 VPN 연결로), 피어링된 VPC 및 EC2-Classic(ClassicLink 사용)에 호스팅된 애플리케이션을 로드 밸런싱할 수도 있음        AWS와 온프레미스 리소스에 걸쳐 로드 밸런싱할 수 있는 기능은 클라우드로 마이그레이션, 클라우드로 버스팅 또는 클라우드로 자애 조치하는데 도움이 됨마. Lambda 함수를 대상으로 사용        HTTP 요청을 처리하는 Lambda 함수를 호출해 웹브라우저 포함 모든 HTTP 클라이언트에서 서버리스 애플리케이션에 대한 사용자 액세스를 제공할 수 있음        Lambda 함수를 로드 밸런스의 대상으로 등록하고 콘텐츠 기반 라우팅 규칙에 대한 지원을 활용하여 요청을 다른 Lambda 함수로 라우팅할 수 있음        서버 및 서버리스 컴퓨팅을 사용하는 애플리케이션에 대해 ALB를 공통 HTTP 엔드포인트로 사용할 수 있습니다. Lambda 함수를 사용하여 전체 웹사이트를 만들거나, EC@ 인스턴스, 컨테이너, 온프레미스 서버 및 Lambda 함수를 결합하여 앱을 개발 가능사. 콘텐츠 기반 라우팅        애플리케이션이 몇 개의 개별 서비스로 구성된 경우 ALB는 요청의 콘텐츠를 기반으로 요청을 서비스로 라우팅 가능2) Network Load Balancer        연결 수준(계층 4)에서 작동하는 Network Load Balancer는 IP 프로토콜 데이터를 기반으로 Amazon Virtual Private Cloud(VPC) 내의 대상(EC2 인슨턴스, 마이크로 서비스 및 컨테이너)으로 연결을 라우팅함        TCP 및 UDP 트래픽 모두의 로드 밸런싱에 적합한 NLB는 매우 짧은 지연 시간을 유지하며 초당 수백만개 요청 처리        NLB는 가용 영역당 하나의 정적 IP 주소를 사용하면서 갑작스럽고 변동이 심한 트래픽 패턴을 처리하는데 최적화        Auto Scaling, EC2 Container Service, CloudFormation 및 Certificate Manager와 같은 서비스와 통합가 . 연결 기반 로드 밸런싱        TCP 및 UDP 트래픽을 모두 로드 밸런싱하여 연결을 대상(EC2 인스턴스, 마이크로 서비스 및 컨테이너)로 라우팅할 수 있음나. 고가용성        가용성이 매우 뛰어남        클라이언트로부터 수신되는 트래픽을 수락하고 동일한 가용 영역 내 대상 전체에 이트래픽을 분산함        등록된 대상의 상태를 모니터링하고 정상 대상으로만 트래픽을 라우팅함        비정상 대상을 감지하면, 해당 대상으로 트래픽 라우팅을 중단하고 나머지 정상 대상으로 트래픽을 다시 라우팅함        한 가용 영역의 모든 대상이 비정상이고 다른 가용 영역에 대상을 설정해둔 경우 자동으로 장애 조치하여 다른 가용 영역에 있는 정상 대상으로 트래픽을 라우팅함다. 높은 처리량라. 짧은 지연시간마. 소스 IP 주소 유지        클라이언트 측 소스 IP를 유지하므로 백엔드가 클라이언트 IP 주소를 확인 가능        그런 다음 추가 처리를 위해 앱에서 사용 가능바. 정적 IP 지원        애플리케이션이 로드 밸런서의 프런트엔드 IP로 사용할 수 있는 가용 영역(서브넷)별로 정적 IP를 자동으로 제공함사. 탄력적 IP 지원        가용 영역(서브넷)별로 탄력적 IP를 지정하여 자체 고정 IP를 제공할 수 있는 옵션이 제공됨아. TLS 오프로드        클라이언트와 로드 밸런서간 TLS 종료를 지원, IAM 및 AWS Certificate Manager를 통해 SSL 인증서 관리를 지원, 클라우드와 로드 밸런서 간의 TLS 핸드셰이크를 완료시 기본적으로 사용할 암호와 프로토콜을 유연하게 설정할 수 있는 사전 정의된 보안 정책도 제공함        TLS가 종료되더라도 원본 IP는 계속 백엔드에 보존됨자. 상태 확인        대상의 전반적인 트래픽에 대한 응답에 기반하여 조사함차. DNS 장애 조치        정상이 대상이 없거나영역의 노드가 비정상인 경우, Route 53이 다른 가용 영역의 로드 밸런서 노드로 트래픽을 보냄카. Amazon Route 53과의 통합        응답하지 않는 경우 서비스에서 사용할 수 없는 로브 밸런서 IP 주소를 제거하고 트래픽을 다른 리전에 있는 대체 Network Load Balancer로 보냄파. AWS 서비스와의 통합하. 수명이 긴 TCP 연결-WebSocket 유형의 애플리케이션에 적합한 수명이 긴 TCP 연결을 지원함3) Classic Load Balancer        기본적인 로드 밸런싱 제공, 요청 수준 및 연결 수준에서 작동하여 트래픽 조정        EC2-Classic 네트워크 내에 구축된 애플리케이션용, 인스턴스의 상태를 감시 가능        VPC를 사용할 때는 계층 7에는 ALB 그리고 계층 4에는 NLB를 사용하는 것이 좋음가 . 고가용성나. 상태확인다. 보안 기능        퍼블릭 주소 없이 CLB를 생성하여 인터넷에 연결되지 않은 내부 로드 밸런서 사용라. SSL 오프로드        app 인스턴스의 SSL 복호화 오프로드, 중앙 집중식 SSL 인증서 관리, 퍼블릭 키 인증을 사용한 백엔드 인스턴스 암호화 등 SSL 종료를 지원, 암호, 프로토콜 제어도 가능마. 고정 세션        쿠키를 사용하여 사용자 세션을 특정 EC 인스턴스에 고정시키는 기능 지원, 사용자가 액세스하는 동안 트래픽은 동일한 인스턴스로 라우팅됨바 . IPv6 지원        EC2-Classic 네트워크에서 인터넷 프로토콜 버전 4 및 6(IPv4 및 IPv6)을 모두 사용할 수 있도록 지원함사. 계층 4 또는 계층 7 로드 밸런싱        HTTP/HTTPS 애플리케이션을 로드 밸런싱하고 X-Forwarded-For 헤더와 같은 계층 7 전용 기능을 사용 가능        TCP 프로토콜만 사용하는 애플리케이션에 대해 엄격한 계층 4 로드 밸런싱을 사용할 수 있음 아. 운영 모니터링자. 로깅        액세스 로그 기능을 사용하여 로드 밸런서로 전송된 요청을 모두 기록, S3에 로그 저장, 이후 장애 진단 및 웹 트래픽 분서에 사용 가능        CloudTrail을 사용해 계정의 API 호출을 기록, 전달 가능, 보안 분석, 리소스 변경 사항 추적, 규정 준수 감사를 수행5. ELB 서비스        여러 관련 서비스를 통해 가용성 및 확장성을 개선함        EC2 : 애플리케이션 실행 가상 서버, 로브랜서 구성으로 인스턴스에 트래픽 라우팅 가능        EC2 Auto Scaling : 인스턴스 수요에 따라 수를 인스턴스 수를 늘리거나 줄일 수 있음        Certificate Manager: HTTPS 리스너를 생성할 때 ACM에서 제공한 인증서를 지정 가능, 인증서를 사용하여 연결을 종료하고 클라이언트의 요청을 암호화 해제함        이외에도 CloudWatch, ECS, Route 53, AWS WAF 등을 사용할 수 있다.    Amazon Auto Scaling    1. 개요        애플리케이션의 로드를 처리할 수 있는 정확한 수의 EC2 인스턴스를 보유하도록 보장        EC2 인스턴스 모음을 생성함, 최소, 최대 인스턴스 수를 지정 가능, 조정 정책을 지정 가능2. 핵심 구성 요소가. 그룹        EC2 인스턴스는 그룹에 정리되어 조정 및 관리 목적의 논리적 단위로 처리 가능        그룹 생성 시 EC2 최소, 최대 인스턴스 수와 원하는 수를 지정 가능나. 구성 템플릿        시작 템플릿 또는 시작 구성을 EC2 인스턴스에 대한 구성 템플릿으로 사용함        AMI ID, 인스턴스 유형, 키 페어, 보안 그룹, 블록 디바이스 매핑 등의 정보를 지정 할 수 있음다. 조정 옵션        EC2 Auto Scaling은 Auto Scaling 그룹을 조정하는 다양한 방법을 제공함  ELB 및 Auto Scaling 구축하기08 AWS 분석 이해하기AWS 분석 서비스 개요1. AWS 기반 데이터 레이크 및 분석      데이터 레이크 및 분석 솔루션 구축을 위한 가장 포괄적이고 안전하고 확장 가능하며 비용 효율적이 서비스 포트폴리오        보안 또는 거버넌스를 저하지 않으면서 모든 관련 데이터에 쉽게 액세스할 수 있는 가장 광범위한 분석 및 기계 학습 서비스를 고객에게 제공        AWS는 데이터 레이크와 분석을 갖춘 조직을 많이 보유 중  2. 데이터 레이크  데이터 레이크는 기존의 데이터 사일로 및 데이터 웨어하우스가 수행할 수 없는 방식으로 다양한 유형의 데이터와 분석 기법을 결합하여 보다 심층적인 통찰력을 얻기 위해 필요한 규모, 민첩성 및 유연성을 처리가능(주로 정제되지 않은 로우 데이터를 의미하며 분석 후, 놓치기 쉬운 가치를 다시 분석하여 좀더 확장 가능함)  Amazon S3, Glacier를 사용해 안전하게 그리고 방대한 규모로 쉽게 저장 가능,  AWS Glue는 사용자가 검색하고 쿼리할 수 있는 단일 카탈로그를 자동으로 생성          AWS Glue : 데이터 레이크에서의 검색, 분석, 추출 변환, 로드(ETL)을 수행할 수 있게 카탈로그 생성하는 완전관리형      간편하고 유연하며 비용 효율적인 ETL tjqltm      모든 데이터 자산에 대한 영구 메타데이터 스토리지로 자동 생성되므로 검색 및 쿼리 가능      데이터 원본과 데이터 대상을 선택하고 ETL 코드를 생성하여 원본으로부터 데이터룰 추출하고 스키마에 맞춰 변환, 이를 대상으로 로드함, 코드를 편집 디버깅 및 테스트 가능        S3, Glacier에 대한 설명의 위 참조3. Amazon Athena(SQL을 이용해 S3의 데이터 쿼리)  표준 SQL 쿼리를 이용해 S3와 Glacier의 데이터를 직접 간편하게 분석하게 해줌  서버리스이며, 데이터를 즉시 쿼리, 몇초 내에 결과를 얻고  실행한 쿼리에 대해서만 비용 지불  S3에 스키마 정의 후 쿼리 시작하기만 하면 됨4. Amazon CloudSearch(관리형 검색 서비스)  웹사이트, 애플리케이션을 위한 검색 솔루션을 간단하게 설정, 관리, 조정 가능      강조 표시, 자동 완성, 지형 정보 검색 등 인기있는 검색 기능과 34개 언어 지원    AWS 분석 서비스    1. Amazon EMR(호스팅된 하둡 프레임워크)1) 빅데이터 처리    Spark 및 Hadoop 프레임워크를 사용한 빅 데이터 처리의 경우, Amazon EMR은 관리형 서비스로서 대량의 데이터를 쉽고 빠르며 비용 효율적으로 처리 가능  EMR은 데이터 엔지니어링, 데이터 과학 개발 및 협업을 위한 관리형 EMR Notebooks로 Hadoop, Spark, HBase 및 Presto를 비롯하여 19가지 오픈소스 프로젝트를 지원2. Amazon Elasticsearch Service(Elasticsearch 클러스터를 실행 및 확장)1) 운영분석  애플리케이션 모니터링, 로그 분석 및 클릭스트림 분석과 같은 운영 분석의 경우, Amazon Elasticsearch Service를 사용하면 실시간으로 데이터를 검색, 탐색, 필터링, 집계 및 시각화 가능  Amazon Elasticsearch Service는 Elasticsearch의 간편한 API 및 실시간 분석 기능과 더불어 프로덕션 워크로드에 필요한 가용성, 확장성, 보안성을 제공함2) 실시간 분석  Amazon Kinesis를 이용해 IoT 텔레메트리데이터, 애플리케이션 로그 웹 사이트 클릭스트림과 같은 스트리밍 데이터를 간편하게 수집 처리 분석 가능  모든 데이터가 수집된 후에야 처리를 시작할 수 있는 것이 아니라 데이터 레이크에 데이터가 수신되는 대로 처리 및 분석하여 실시간으로 대응 가능3. Amazon Managed Streaming for Apache Kafka(MSK, 완전관리형 Apache Kafka 서비스)  Apache Kafka를 사용하여 스트리밍 데이터를 처리하는 애플리케이션의 구축 및 실행을 간소화하는 완전관리형 서비스임  Apache Kafka는 실시간 스트리밍 데이터 파이프라인 및 애플리케이션 빌드를 위한 오픈 소스 플랫폼임  Amazon MSK를 통해 Apache Kafka API를 사용해 데이터 레이크를 채우고, 데이터베이스와 변경 사항을 스트리밍 방식으로 주고 받으며, 기계 학습 및 분석 애플리케이션을 강화 가능  프로덕션에서 설정, 크기 조정 및 관리는 힘든 작업임,  서버 프로비저닝, Kafka 수동 구성, 서버 교체, 서버 패치 및 업그레이드 오케스트레이션, 설계, 데이터 확인, 모니터링, 경보설정, 부하 변경을 위한 크기 조정 이벤트 등을 직접 해야함  MSK를 이용하면 전문적인 지식이 없어도 구축 가능, 인프라 관리 시간을 줄이고 더 많은 시간을 애플리케이션 개발에 활용 가능  데이터 스토어에 스트리밍 데이터를 생성하는 생상자(앱)을 데이터 스토어에 스트리밍 데이터를 소비하는 소비자(앱)에서 분리하는 스트리밍 데이터 스토어  이 스트리밍 데이터를 지속적으로 분석해 관련 대응 조치를 쥐하는 데이터 소스로 Kafka 사용4. Amazon Redshift(빠르고 단단하며 비용 효율적인 데이터 웨어하우징)1) 데이터 웨어하우징  페타바이트의 정형 데이터에 대해 복잡한 분석 커리를 실행할 수 있는 기능을 제공하며 불필요한 데이터 이동 없이 S3의 정형 또는 비정형 데이터에 대해 직접 SQL 쿼리를 실행하는 Redshift Spectrum을 포함함, 매우 경제적임5. Amazon QuickSight(빠른 비즈니스 분석 서비스)  대시보드 및 시각화의 경우, 빠르고 강력한 클라우드 기반 비즈니스 분석 서비스를 제공하므로 모든 브라우저 또는 모바일 장치에서 액세스할 수 있는 멋진 시각화 및 풍부한 대시보드를 간편하게 작성할 수 있음6. AWS Data Pipeline(데이터 중심의 주기적인 워크플로를 위한 오케스트레이션 서비스)  온프레미스 데이터 소스뿐 아니라 여러 AWS 컴퓨팅 및 스토리지 서비스 간에 데이터를 안정적으로 처리하고 지정된 간격으로 이동할 수 있게 해주는 웹 서비스  저장되어 있는 데이터에 정기적으로 액세스하고, 대규모로 데이터를 변환 및 처리하며, 다른 AWS 서비스에 효율적으로 전송 가능  내결함성과 반복 가능, 가용성, 간편함, 안정성 증가, 작업 재시도, 알림, 종속성 관리 기능 포함  이전에는 온프레미스 데이터 사일로에서 묶여 있던 데이터를 이동하고 처리 가능7. AWS Lake Formation(며칠 만에 안전한 데이터 레이크 구축)  데이터 레이크는 큐레이션된 안전한 중앙 집중식 리포지토리로 모든 데이터를 원래 형식 및 분석에 필요한 형식으로 저장  데이터 사일로를 대체하고 다양한 유형의 분석을 조합하여 좀더 나은 통찰력과 비즈니스 결정 가능  다양한 소스로부터 데이터 로딩, 데이터 흐름 모니터링, 파티션 설정, 암호화, 키관리, 변환 작업 정의 및 운영 모니터링, 열 기반 형식으로 데이터 재구성, 액세스 제어 설정 구성, 중복 데이터 제거, 링크도니 레코드 매칭, 데이터 세트에 대한 액세스 권한 부여, 추후 액세스 감사 등이 포함됨  데이터 레이크 생성 과정09 AWS 블록체인 활용하기블록체인 서비스1. AWS에서의 블록체인      조정 가능한 블록체인 및 원장 솔루션을 손쉽게 구축        2가지 유형의 고객 요구사항을 해결 가능    1) 여러 당사자가 중앙의 신뢰할 수 있는 기관과 협력하여 완전하고 검증 가능한 트랜잭션 레코드를 유지          제품의 공급망 이동에 관한 투명하고 검증 가능한 정보의 레코드를 유지하는 중앙 집중식 원장에 공급업체를 연결하고자 하는 소매고객이 예시2) 중앙의 신뢰할 수 있는 기관 필요 없이 여러 당사자가 분산된 방식으로 거래      중앙 집중식 기관을 연락 담당자로 사용하지 않고 자산의 교차 경계 이전(신용장)을 수행하려는 은행 및 수출관 연합            모든 애플리케이션 데이터의 변경 내용을 기록하고 이러한 변경 내용의 변경 불가능한 레코드를 유지하는 중앙 집중식 원장이 필요시 AWS 원장 DB를 사용 가능        변경 불가능하며, 암호화로 검증 가능하여 복잡한 감사 테이블, 블록체인 네트워크 설정 필요 없음        만약 원장의 변경 불가능하며 검증 가능한 기능이 필요한 동시에 여러 당사자가 신뢰할 수 있는 중앙 기관 없이 거래할 수 있도록 하려면 AWS가 제공하는 완전 관리형의 조정 가능한 블록쳉니 서비스 사용 하면됨2. AWS에서의 블록체인 사례        중앙 집중식 소유권으로 트랜잭션 추적 및 확인해야 문제를 예방하고 추적할 수 있음        급여 시스템, 제조업체 생산 유통 계통 등을 중앙 집중식 원장으로 구현하곤 함        보험, 분산된 소유권으로 트랜잭션 및 계약 실행              소유권이 중앙 집중된 원장 DB      소유권이 분산된 블록체인 네트워크                  중앙 집중식, 중앙의 신뢰할 수 있는 기관이 원장을 소유 및 관리하고 함께 일하는 모든 수의 당사자와 공유합니다.      분산화, 여러 당사자가 서로을 알거나 신뢰할 필요 없이 서로 거래할 수 있습니다. 각 당사자는 멤버라고 하며 네트워크의 피어 노드를 소유합니다.              변경 불가능, 모든 트랜잭션을 블록 으로 저장하는 추가 전용 저널을 사용합니다. 블록은 암호화 방식의 시퀀스로 연결되며 중앙의 소유자 또는 다른 엔티티에 의해 삭제되거나 수정 불가      변경 불가능, 커밋된 트랜잭션은 블록에 저장되고, 암호화 방식으로 함께 연결되며, 수정할 수 없습니다. 트랜잭션이 커밋되면 모든 멤버에 복제되어 변경 또는 삭제가 불가능해집니다.              검증 가능, 암호화를 사용하여 변경 기록의 간결한 요약을 생성합니다. 이 안전한 요약을 다이제스트라고 하며 원장 데이터의 계통을 암호화 방식으로 확인하는데 사용 가능      검증 가능, 각 멤버에는 원장의 로컬 복사본이 저장되며 각 멤버는 독립적으로 원장의 콘텐츠를 검증하고 원장의 콘텐츠가 정확한지 확인할 수 있습니다. 변경을 수행하려면 네트워크의 멤버가 새 트랜잭션을 검증해야 하므로 모든 피어 조직의 데이터가 일관적으로 유지됩니다.              투명성, 전체 데이터 기록을 쉽게 쿼리할 수 있으므로 완전하고 투명한 정보로그가 유지됩니다.      투명성 커밋된 모든 트랜잭션은 하나 이상의 엔티티에 귀속될 수 있으며 모든 멤버에게 투명하게 공개됩니다. Hyperledger Fabric과 같은 승인된 블록체인 프레임워크에서는 선별된 피어 그룹만 정보에 액세스할 수 있도록 투명성을 구성할 수 있습니다.              신속함, 중앙 집중식 신뢰가 구축된 원장의 경우 분산된 합의를 도출할 필요 없으므로 일반적인 블록체인 프레임워크의 원장보다 빠른속도로 손쉽게 트랜잭션을 확장하고 실행 가능      중개자 불필요, 각 피어 조직은 인코딩된 애플리케이션 논리를 사용해 새로운 트랜잭션을 시작할 수 있습니다. 트랜잭션이 시작되면 네트워크의 모든 피어에 복제되므로 여러 당사자가 정보에 액세스하여 검증할 수 있습니다. 멤버 간의 연락을 담당할 중개자가 필요하지 않기 때문에 복잡한 트랜잭션의 효율성이 개선되고 비용이 절감됩니다.      Amazon Managed Blockchain**1. 확장 가능한 블록체인 네트워크를 생성 및 관리 **  널리 사용되는 오픈 소스 프레임워크인 Hyperledger Fabric 및 Ethereum을 사용하여 조정 가능한 블록체인 네트워크를 생성하고 관리할 수 있도록 지원하는 완전관리형 서비스  네트워크 생성에 필요한 오버헤드를 제거하며 수백만 건의 트랜잭션을 실행하는 수천 개 애플리케이션의 수요를 충족하도록 자동 조정됨  네트워크를 설정하고 실행하기만 하면 Managed Blockchain을 통해 블록체인 네트워크를 손쉽게 관리하고 유지할 수 있음  인증서를 관리하고 네트워크에 조인할 새 멤버를 초대하고, 컴퓨팅, 메모리 및 스토리지 리소스 사용량 같은 운영 지표를 추적하는 기능 제공  신뢰할 수 있는 중앙기관이 없어도 여러 당사자가 트랜잭션을 실행할 수 있는 애플리케이션 구축 가능  블록체인 네트워크에 필요한 노력 (수동 하드웨어 프로비저닝, 소프트웨어 설치, 액세스 제어용 인증서 관리, 지속적 인프라 모니터링 및 변경사항에 맞게 조정)을 손쉽게 해줌      블록체인의 네트워크 활동의 변경 불가능한 사본을 완전관리형 원장 데이터베이스인 QLDB로 복제할 수 있어 이를 통해 외부에서 손쉽게 네트워크를 활동하고 추세에 대한 통찰력 얻을 수 있음    Amazon Quantum Ledger Database(QLDB)    1. 완전관리형 원장 데이터베이스    투명하고 변경 불가능하며 암호화 방식으로 검증 가능한 트랜잭션 로그를 제공하는 완전관리형 원장 데이터베이스로, 신뢰할 수 있는 중앙기관에서 이러한 로그를 소유함  모든 애프리케이션 데이터 변경내용을 추적하며 시간이 지나도 완전하고 검증 가능한 변경 내역을 유지 관리함  원장은 일반적으로 조직의 경제 및 금융 활동 내역을 기록하는데 사용됨  종종 관계형 데이터베이스에서 만든 사용자 지정 감사 테이블 또는 감사 추적을 사용하여 구현되지만, 시간이 많이 소요되며, 실수, 데이터 변조가 발생할 수 있음  이를 방지하기 위해 블록체인 프레임워크를 원장으로 쓸 수 있지만 이러면 복잡성과 관리가 힘들어짐  이를 위해 QLDB를 이용해 복잡한 개발 노력을 줄이고 변경 불가능하고 암호화를 사용하여 의도치 않은 수정을 막고 추적 가능  저널이라고 하는 변경 불가능한 트랜잭션 로그를 사용하여 각 애플리케이션 데이터 변경 사항을 추적하고 시간이 지나도 검증 가능한 모든 변경내역을 유지 가능  개발자가 익숙한 SQL형 API 유연한 문서 데이터 모델, 서버리스 등을 지원,  읽기 쓰기 제한을 구성할 필요 없고, 사용한 만큼만 비용 지불2. QLDB 작동 방식AWS Blockchain Templates      자체 블록체인 내트워크를 직접 관리하면서 설정 및 시작할 수 있음        인기 오픈 소스 프레임워크를 사용해 안전한 블록체인 네트워크를 ECS 클러스터에 컨테이너로 배포하거나 Docker를 실행하는 EC2 인스턴스에 직접 배포 가능    AWS Blockchain Partners        완벽한 블록체인 플랫폼을 경제적인 방식으로 대규모로 구축 가능한 포괄적이고 심층적인 기능과 최대 규모의 글로벌 인프라를 제공, 다양한 프로토콜과 분산 원장 솔루션 제공    10 AWS 사물인터넷 서비스    AWS IoT    1. 산업, 소비자 및 상업 솔루션용 IoT 서비스        디바이스를 서로 연결 후 데이터를 수집하고 저장하고 분석할 수 있는 솔루션의 필요성이 증가        AWS IoT는 엣지 영역부터 클라우드에 이르기까지 광범위하고 심층적인 기능을 제공하므로 다양한 디바이스에서 거의 모든 사라예 적합한 IoT 솔루션 개발 가능        인터넷이 연결되지 않을때도 디바이스가 스마트해짐, 포괄적인 보안 기능을 제공2. AWS IoT 솔루션1) 산업용        AWS IoT 고객은 예방적 품질 관리 및 유지 관리를 지원하고 운영 상태를 원격으로 모니터링하기 위해 산업용 IoT 애플리케이션을 구축 중2) 커넥티드 홈        AWS IoT 고객은 홈 자동화, 홈 보안 및 모니터링, 홈 네트워킹을 위한 커넥티드 홈 애플리케이션을 구축 하고 있음3) 상업용        교통 모니터링, 공공 치안 및 건강 모니터링을 위한 상업용 애플리케이션을 구축하고 있음3. AWS IoT 제공 서비스        AWS IoT는 디바이스 소프트웨어, 제어 서비스 및 데이터 서비스를 제공함        디바이스 소프트웨어는 디바이스를 안전하게 연결하고, 데이터를 수집할 뿐만 아니라 인터넷 연결이 불가능할 때에도 로컬에서 지능적인 조치를 취할 수 있도록 지원함        제어 서비스를 사용하면 크고 다양한 디바이스 플릿을 제어하고 관리하며 보호 가능        데이터 서비스는 IoT 데이터를 통한 가치 창출을 지원함    AWS IoT 서비스    1. Amazon FreeRTOS(마이크로 컨트롤러용 IoT 운영체제)        소형 저출력 엣지 디바이스를 쉽게 프로그래밍, 배포, 보호, 연결 및 관리할 수 있는 오픈 소스 마이크로 컨트롤러용 운영 체제        소형 저출력 디바이스를 AWS IoT Core와 같은 AWS 클라우드 서비스 또는 AWS IoT Greengrass를 실행하는 좀더 강력한 엣지 디바이스로 안전하게 연결할 수 있게 해주는 소프트웨어 라이브러리를 통해 인기 있는 마이크로컨트롤러용 오픈 소스 운영 체제인 FreeRTOS 커널을 확장함        마이크로컨트롤러(MCU)는** 가전제품, 센서, 피트니스 트래커, 산업 자동화 및 자동화를 비롯한 많은 디바이스에서 발견되는 간단한 프로세스가 탑재된 단일 칩**임        소형 디바이스 대부분이 클라우드에 연결하거나 로컬로 다른 디바이스에 연결시 이점 활용        MCU의 컴퓨팅 파워와 메모리 용량이 제한적이며 간단하고 기능적 작업 수행        마이크로 컨트롤러는 로컬 네트워크 또는 클라우드에 연결하는 기능이 내장되지 않은 운영 체제에서 주로 실행, IoT 어플리케이션은 어려운 과제임1) 작동방식가. Amazon FreeRTOS는 손쉽게 커넥티드 마이크로컨트롤러 기반 디바이스를 프로그래밍하고 해당 디바이스로부터 IoT 애플리케이션을 위한 데이터를 수집하는 데 필요한 모든 것을 제공함나. 시작하려면 AWS Partner Device Catalog 에서 Amazon FreeRTOS 지원 마이크로컨트롤러를 선택할 수 있음다. Amazon FreeRTOS 콘솔 또는 GitHub에서 디바이스에 사용할 관련 소프트웨어 라이브러리를 선택하고, 디바이스 및 애플리케이션별 라이브러리를 비롯하여 전체 Amazon FreeRTOS 운영체제 다운로드2. AWS IoT 1-Click(클릭 한 번으로 AWS Lambda 트리거를 생성)        간단한 디바이스에서 AWS Lambda 함수를 트리거하여 작업을 실행할 수 있도록 지원하는 서비스        관련 디바이스 사용시 기술 지원에 알림, 자산 추적, 상품 또는 서비스 보충과 같은 작업을 손쉽게 가능        별도의 설정 없이 바로 사용할 수 있으며, 자체 펌웨어를 작성하거나 안전한 연결을 위해 구성할 필요가 없고 손쉽게 관리 가능        손쉽게 디바이스 그룹을 생성하고 이를 Lambda 함수와 연결해 함수가 트리거 되면 원하는 작업이 실행되게 가능, 디바이스 상태, 활동 추적 가능3. AWS IoT Analytics(IoT 디바이스에 대한 분석)        자체 IoT 분석 플랫폼을 구축하는데 일반적으로 필요한 비용과 복잡성에 대해 전혀 걱정할 필요 없이 대규모 IoT 데이터에 대한 정교한 분석을 손쉽게 실행 및 운용할 수 있게 해주는 완전관리형 서비스        IoT 데이터에 대한 분석을 실행하고 IoT 애플리케이션 및 기계 학습 사용 사례에 대해 더욱 유용하고 정확한 결정을 내리는 데 도움이 되는 통찰력을 얻는 가장 쉬운 방법        IoT 데이터는 대부분 비정형 데이터이므로 정형 데이터를 처리하도록 설계된 기존 분석 및 비즈니스 인텔리전스 도구로 분석하기 어려움        IoT 데이터는 잡음 처리를 기록하는 디바이스로부터 오는 경우가 많습니다.          이러한 디바이스의 데이터는 큰폭의 차이, 손상된 메시지, 틀린 판독값을 함유하므로 주기적으로 정리해야함            IoT 데이터는 타사 데이터 입력의 추가 컨텍스트에서만 의미가 있는 경우가 많음        IoT 디바이스의 데이터를 분석하는 데 필요한 까다로운 각 단계를 자동화함        필터링, 변환 및 보강 한 뒤 분석을 위해 시계열 데이터 스토어에 저장        디바이스에서 필요한 데이터만 수집하도록 서비스를 설정하고, 데이터를 처리하는 데 수학적 변환을 적용하고, 처리된 데이터를 저장하기 전에 디바이스 유형 및 위치와 같은 디바이스별 메타데이터로 데이터를 보강 가능        내장된 SQL 쿼리 엔진으로 임시 또는 예정된 쿼리를 실행하여 데이터를 분석하거나, 좀더 복잡한 분석 및 기계 학습 추론을 수행 가능        일반 IoT 사용 사례를 위한 사전 빌드 모델이 포함 되므로 쉽게 시작 가능        컨테이너 패키징 된 사용자 지정 분석을 사용해 실행 가능4. AWS IoT Button(클라우드 프로그래밍 가능한 대시 버튼)        Amazon Dash Button 하드웨어를 기반으로 한 프로그램이 가능한 버튼        wifi로 작동하며 손쉽게 사용할 수 있고, AWS service와 연동 가능        버튼의 로직을 코딩하여 클릭수 세기, 호출, 경고, 중지, 서비스 주문 등을 할 수 있다5. AWS IoT Core(디바이스를 클라우드에 연결)        AWS IoT Core는 커넥티드 디바이스가 쉽고 안전하게 클라우드 애플리케이션 및 다른 디바이스와 상호 작용할 수 있게 해주는 관리형 클라우드 서비스임        수십억개의 디바이스, 수조건의 메시지 지원, AWS 엔드포인트 및 다른 디바이스로 라우팅 가능        디바이스가 연결되어 있지 않더라도 언제나 애플리케이션에서 모든 디바이스를 추적하고 디바이스와 통신 가능        인프라 관리 필요없이 AWS 서비스를 사용하여 커넥티드 디바이스에서 생성한 데이터를 수집, 처리 및 분석하고 이를 기반으로 운영하는 IoT 애플리케이션을 손쉽게 구축가능6. IoT Device Defender(IoT 디바이스를 위한 보안 관리)        지속적으로 IoT 구성을 모니터링하고 감사하여 보안 모범 사례 준수 확인, 플릿을 보호 할 수 있는 완전 관리형 서비스        디바이스가 서로 통신하고 클라우드와 통신할 때 정보를 안전하게 유지하도록 설정하는 일련의 기술 제어 항목임        디바이스 자격 증명 보장, 디바이스 인증 및 권한 부여, 디바이스 데이터 암호화 같은 IoT 구성을 손쉽게 유지 관리하고 적용 가능        만약 작격 증명 인증서를 여러 디바이스가 공유하거나 자격 증명 인증서가 취소된 디바이스가 연결하려 시도하는 등, IoT 구성에 보안 위험을 초래할 것 같으면 알림을 전송함        다른 서비스와 연계해 보안을 강화거나 알림을 다른 곳에서 받을 수 있음7. AWS IoT Device Management(IoT 디바이스를 온보딩, 조직화 및 원격으로 관리)        AWS IoT Device Management를 사용하면 손쉽게 대규모의 IoT 디바이스를 안전하게 온보딩, 구성 및 모니터링하고 원격으로 관리할 수 있음        많은 IoT 배포가 수십만 개에서 수백만 개의 디바이스로 구성되므로 연결된 디바이스 플릿을 추적, 모니터링 및 관리하는 것은 필수        배포 이후 디바이스가 올바르고 안전하게 작동하록 보장해야함,        디바이스에 대한 액세스를 보호, 상태를 모니터링, 문제를 탐지 및 원격으로 해결, 소프트웨어, 펌웨어 업데이트 관리        손쉽게 대규모의 IoT 디바이스를 온보딩, 체계화 및 모니터링, 원격 관리 가능        대량 등록, 디바이스 분류, 펌웨어 업데이트가 쉬워짐8. AWS IoT Events(IoT 이벤트 감지 및 대응)        수많은 IoT 센서와 애플리케이션에서 이벤트를 손쉽게 감지하여 대응        IoT 센서와 애플리케이션에서 이벤트를 쉽게 탐지하고 대응할 수 있는 완전관리형 서비스        이벤트란 예상 보다 더 복잡한 상황을 식별하는 데이터의 패턴을 의미한다.        냉동실 온도, 호흡 장치의 습도 또는 모터의 벨트 속도 같은 원경데이터를 전송하는 수천 개의 IoT 센서에서 간단하게 이벤트 탐지 가능9. AWS IoT Greengrass(디바이스를 위한 로컬 컴퓨팅, 메시징 및 동기화)        커넥티드 디바이스에서 로컬 컴퓨팅, 메시징, 데이터 캐싱, 동기화 및 기계 학습추론 기능을 안전한 방식으로 실행할 수 있는 소프트웨어        인터넷에 연결되어 있지 않더라도 커넥티드 디바이스에서 AWS Lambda 함수를 실행, 기계 학습 모델 기반으로 예측을 실행, 데이터를 동기화 상태로 유지, 안전하게 디바이스간 통신        디바이스 데이터를 필터링 ,필요 정보만 클라우드로 전송, 다른 서비스와 연결, 인증 간소화**10. AWS IoT SiteWise(IoT 데이터 수집기 및 인터프리터) **        산업 장비에서 손쉽게 데이터 수집 및 구성, 검색, 장비 및 프로세스 성능 분석        데이터가 사유 온프레미스 데이터 스토어에 잠겨있는 경우와 특수한 전문지식이 있어야 하는 경우를 쉽게 해결        시설 내의 게이트웨이에서 실행되는 소프트웨어를 제공하여 프로세스 간소화, 자동화 이후 온프레미스 데이터 서버에 연결하여 데이터를 수집 후 AWS 클라우드로 전송11. AWS IoT Things Graph(디바이스 및 웹 서비스를 손쉽게 연결)        IoT 애플리케이션을 시각적으로 개발, 다양한 디바이스와 클라우드 서비스를 연결해 IoT 애플리케이션을 쉽게 구축        디바이스와 웹 서비스 간 상호 작용을 연결하고 조율할 수 있는 시각적인 끌어 놓기 인터페이스 제공하여 IoT 애플리케이션을 빠르게 구축 가능,    11 AWS 기계학습    AWS 기계학습 개요    1. AWS에서의 기계 학습        가장 폭넓고 깊이 있는 비즈니스용 기계 학습 및 AI 서비스 세트를 갖추고 있음        모든 개발자가 기계 학습을 손쉽게 활용하는 데 방해가 되는 가장 까다로운 몇 가지 문제를 고객을 대신해 해결함        컴퓨터 비전, 언어, 추천 및 예측을 위해 사전 학습된 AI 서비스를 선택 가능하며, Amazon Sage Maker를 통해 기계 학습 모델을 대규모로 구축, 학습 및 배포하거나 모든 주요 오픈 소스 프레임워크에 대한 지원을 바탕으로 사용자 지정 모델을 구축할 수 있음        AWS의 기능은 가장 포괄적인 클라우드 플랫폼을 기반으로 구축되며 고성능 컴퓨팅을 통해 기계 학습에 최적화됨        최고 수준의 보안 및 분석 기능도 제공함2. 애플리케이션에 손쉽게 인텔리전스 추가        기계학습 기술과 경험이 없어도 AI 서비스 사용 가능        AWS의 사전 학습된 AI 서비스는 애플리케이션 및 워크플로에 바로 사용 가능한 인텔리전스를 제공        AI 서비스는 애플리케이션에 쉽게 통합되므로 개인화된 추천, 콜 센터 현대화, 안전 및 보안 개선과 고객 참여 증진 같은 일반적인 사용 사례 해결 가능        아마존 상용 수준과 동일한 딥러닝 기술이 사용되므로 지속적으로 학습하는 API의 품질 및 정확성을 얻을 수 있음        추천 : Personalize,        예측 : Forecast        이미지 및 비디오 분석 : Rekognition        고급 텍스트 분석 : Comprehend        문서 분석 : Textract        음성 : polly        대화 에이전트 : lex        번역 : translate        전사(음성 텍스트 변환) : transcribe    AWS 기계학습 서비스    1.Amazon Sage Maker(기계 학습 모델을 대규모로 구축, 훈련 및 배포)        개발자 및 데이터 과학자가 모든 규모의 기계 학습 모델을 쉽고 빠르게 구축, 교육 및 배포할 수 있도록 지원함                            구축          학습          배포                                      학습 데이터 수집 및 준비 데이터 레이블 지정 및 일반적인 문제에 대한 기본 제공 노트북          학습 데이터 수집 및 준비 최고 성능의 인프라에서 원클릭 학습          프로덕션 환경에 모델 배포 클릭 한 번으로 배포                          ML 알고리즘 선택 및 최적화 AWS Marketplace의 기본 제공 고성능 알고리즘 및 즉시 사용 가능한 수백가지 알고리즘          모델 학습 및 튜닝 한 번의 학습, 모든 위치에서 실행 및 모델 최적화          프로덕션 환경 조정 및 관리 Auto Scailing 기능이 포함된 완전관리형 서비스로 75%의 비용 절감                            구축 : 10배 더 높은 성능을 제공하는 알고리즘, 사전 구축된 Jupyter Notebook(완전 관리형 인스턴스), 알고리즘 마켓플레이스(알고리즘, 모델 등), 70% 비용 절감(Ground Truth를 통한 학습 데이터 세트 구축으로)      학습 : AutoML 기능을 통한 원클릭 학습 및 자동 모델 튜닝으로 모델의 예측의 정확도 최대화, 한 번 학습 후 모든 위치에서 2배 높은 성능으로 실행(Neo), 완전관리형      배포: 원클릭 배포(Auto-scaling 클러스터), 추론 비용 최대 75% 절감(Elastic Inference), 자동 A/B 테스트(최대 5개 모델), Auto Scaling을 통한 완전관리형 호스팅(관리형 프로덕션 인프라에서 상태확인, 보안패치, 유지관리)      1) 학습 데이터 신속 분류(라벨링: Amazon Sage Maker Ground Truth)  Amazon Sage Maker Ground Truth를 통해 학습 데이터 세트를 매우 정확하고도 신속하게 구축, 관리  Ground Truth는 공공 및 개인 분류자에게 쉽게 이용할 수 있도록 지원하며, 일반적인 라벨링 작업을 위해 미리 작성된 워크플로와 인터페이스를 제공함  Ground Truth는 작업자가 지정한 레이블을 통해 학습하여 고품질의 자동 주석을 생성하므로 라벨링 비용을 상당히 낮출 수 있음 2) 머신러닝 알고리즘 선택 및 최적화  여러 알고리즘 및 오픈소스 프레임워크에서 자동으로 구성하고 최적화함  일반적으로 사용되는 기계학습 알고리즘은 AWS Marketplace에서 제공하는 200개 이상의 사전 학습 모델과 알고리즘을 포함하여 규모, 속도 및 정확도에 따라 내장되고 조정됨  Docker 컨테이너에 구축함으로써 다른 알고리즘이나 프레임워크를 가졍로 수도 있음3) 한 번의 학습을 통해 어디서나 실행  Neo를 사용하면 모델을 한 번 학습하여 어디에나 배포 가능  머신러닝을 통해 Neo는 지정된 하드웨어 플랫폼에 일반적으로 사용되는 프레임워크로 구축되어 학습된 모델을 자동으로 최적화  EC2 인스턴스 및 SageMaker 인스턴스 또는 Neo 런타임을 포함하는 엣지의 디바이스(AWS Greengrass 디바이스 등)에 이 모델을 배포할 수 있음4) 모델 자동 튜닝  머신러닝을 사용하여 모델을 신속하고도 최대한 정확하게 튜닝함  이 기능을 사용하면 모델 파라미터를 수동으로 튜닝하는 지루한 시행착오 과정을 건너뛸 수 있음  모델 자동 튜닝은 여러 번의 학습 실행에서 데이터로부터 흥미로운 특징을 발견하고 그 특징들이 어떻게 상호 작용하여 정확도에 영향을 주는 지 학습함으로써 하이퍼파라미터 최적화를 수행함  학습된 모델의 품질을 극대화하기 위해 며칠 또는 몇 주까지 시간 절약 가능2. Amazon Comprehend(텍스트에서 통찰력 확보 및 관계 파악)  기계 학습을 사용하여 텍스트 안에 있는 통찰력과 관계를 찾아내는 자연어 처리(NLP) 서비스  기계 학습 경험은 필요 없음, 비정형 데이터에서 높은 가치를 찾아냄3. Amazon Elastic Inference(딥 러닝 추론 가속화)  Amazon EC2 및 Amazon SageMaker 인스턴스에 낮은 비용의 GPU 지원 가속을 연결해 딥 러닝 추론 비용을 최대 75% 절감 가능  TensorFlow, Apache MXNet 및 ONNX 모델을 지원하며 더 많은 프레임워크가 곧 추가 예정  코드 변경 없이 매우 적절한 양의 GPU 지원 추론 가속을 EC2 또는 SageMaker 인스턴스 유형에 연결하여 이러한 문제를 해결함  애플리 케이션의 전체 CPU 및 메모리 요구 사항에 가장 적합한 이스턴스 유형을 선택한 후 리소스를 효율적으로 사용하고 추론 실행 비용을 절감하는 데 필요한 양의 추론 가속을 별도로 구성 가능4.Amazon Forecast(기계 학습을 사용하여 예측 정확도 개선)  매우 정확한 예측을 위해 기계 학습을 사용하는 완전관리형 서비스  Amazon.com에서 사용하는 것과 동일한 기술 기반으로 기계 학습을 통해 시계열 데이터를 추가 변수와 결합해 예측을 만들어냄  기계 학습 경험이 없어도 시작 가능  기록 데이터와 예측에 영향을 줄 수 있는 추가 데이터만 제공하면 됨  완전관리형 서비스이며, 서버를 프로비저닝, 기계 학습 모델 구축, 교육, 배포할 필요가 없으며 사용한 만큼만 비용 지불, 최소 요금, 약정 없음5. Amazon Lex(음성 및 텍스트 챗봇 구축)  음성과 텍스트를 사용하는 애플리케이션에 대화형 인터페이스를 구축하는 서비스  음성을 텍스트로 변환하는 자동 음성 인식(ASR)과 텍스트의 의도를 이해하는 자연어 처리(NLU)라는 첨단 딥 러닝 기능을 제공, 상당히 매력적인 사용자 경험과 생생한 대화형 인터페이스를 구축 가능6. Amazon Personalize(애플리케이션 실시간 추천 기능)  개발자가 애플리케이션을 사용하는 고객에게 개별화된 추천을 손쉽게 생성할 수 있도록 하는 기계 학습 서비스  맞춤형 제품, 콘텐츠 추천, 맞춤형 검색, 마케팅 프로모션에 사용됨7. Amazon Rekognition(이미지 및 비디오 분석)  이미지, 비디오를 Rekognition API에 제공하기만 하면, 서비스에서 객체, 사람, 텍스트, 장면 및 동작을 식별하고 부적절한 콘텐츠 탐지 가능  정확한 얼굴 분석 및 얼굴 인식 제공, 아마존 상용 급, 기계 학습 전문 지식 필요 없음8. Amazon SageMaker Ground Truth(정확한 ML 교육 데이터 세트 구축)  기계 학습을 위해 매우 정확한 교육 데이터 세트를 신속하게 구축할 수 있도록 지원함  일반 및 사내 레이블링 작업자에게 간편한 액세스를 제공하며, 일반적인 레이블링 작업에 대한 워크플로와 인터페이스를 기본적으로 제공함  자동레이블링 기능을 사용해 독립적으로 데이터에 레이블을 지정하는 것을 배우도록 사람이 레이블을 지정한 데이터로 GRound Truth를 교육 시키면 최대 비용이 70%까지 낮아짐  이를 통해 품질이 뛰어나고 방대한 교육 데이터로 더욱 성공적인 기계학습 모델 생성 가능9. Amazon Textract(문서에서 텍스트와 데이터 추출)  스캔한 문서에서 텍스트 및 데이터를 자동으로 추출하는 서비스  단순한 OCR을 넘어 양식의 필드 콘텐츠와 테이블에 저장된 정보를 식별함  기계 학습을 사용하여 사실상 모든 유형의 문서를 즉시 읽고 수동 작업 또는 사용자 지정 코드 없이 텍스트와 데이터를 정확하게 추출함으로써 이러한 문제를 해결함  추가로 스마트 검색 인덱스 생성, 자동화된 승인 워크플로 구축, 교정이 필요할 수 있는 데이터를 플래그 지정해 문서 아카이브 규칙에 관한 규정 준수를 더욱 원할하게 유지할 수 있음10. Amazon Translate(자연스럽고 유창한 언어번역)  합리적인 가격으로 고품질의 언어 번역을 빠르게 제공하는 신경망 기계 번역 서비스  인공신경망 기계 번역은 언어 번역 자동화의 한 형태로, 딥 러닝 모델을 사용하여 기존 통계 및 규칙 기반 번역 알고리즘보다 더 정확하고 자연스러움  어플리케이션 현지화에 사용됨11. Amazon Transcribe(자동 음성 인식)  개발자가 음성을 텍스트로 변환하는 기능을 애플리케이션에 쉽게 추가할 수 있게 지원하는 자동 음성 인식(ASR) 서비스  API를 통해 S3에 저장된 오디오 파일 분석, 텍스트 파일 변호나 가능, 라이브 자막 등에도 사용  단어마다 타임스탬프를 추가하므로 텍스트 검생으로 원래 소스에서 오디오의 정확한 위치 손쉽게 찾을 수 있음12. AWS Deep Learning AMI(Amazon EC2 기반 딥 러닝)  딥 러닝 애플리케이션을 빠르게 구축할 수 있는 사전 구성된 환경  규모 관계없이 딥 러닝 가속화 인프라, 도구 제공, 여러 인기 있는 딥러닝 프레임워크와 인터페이스가 준비됨13. AWS Deep Learning Containers(딥 러닝을 위한 Docker 이미지)  최적화되고 사전에 패키징된 컨테이너 이미지로 딥 러닝 환경을 신속하게 설정  AWS DL Containers는 딥 러닝 프레임워크가 사전 설치된 Docker 이미지로, 처음부터 환경을 구축하고 최적화하는 복잡한 프로세스를 건너 뛰어 커스텀 ML 환경을 신속하게 배포하도록 지원  여러 인기 딥러닝 프레임워크를 지원하며 다른 AWS 서비스와 연동됨  ECR, Marketplace를 통해 무료로 제공되며, 사용된 리소스에 대해서만 요금 지불14. AWS DeepLens(딥 러닝이 지원되는 비디오 카메라)  딥 러닝 기술을 확장하도록 설계된 완전히 프로그래밍 가능한 비디오 카메라, 자습서, 코드 및 사전 교육된 모델을 통해 말그대로 개발자의 손에 기계 학습을 쥐어줌  딥러닝 기본사항 교육 가능  카메라에서 로컬로 딥러닝 모델을 실행하여 카메라가 봅는 것을 분석하고 조치를 취할 수 있음15. AWS DeepRacer(ML로 움직이는 1/18 크기의 자율줗애 경주용 자동차)  기계 학습을 시작하는 가장 빠른 방법**16. AWS Inferential(기계 학습 추론 칩) **  저렴한 비용으로 높은 성능을 제공하도록 설계된 기계 학습 추론칩  여러 인기 딥러닝 프레임워크, ONNX 형식을 사용하는 모델 지원  훈련된 기계학습 모델을 사용해 예측하면 컴퓨팅 비용 90% 증가할 수 있음  일부 추론 워크로드는 전체 GPU가 필요하거나 짧은 지연 시간 요구사항이 매우 낮으므로 저렴한 비용으로 해결하기 위해서는 전용 추론 칩이 필요함17. AWS Apache MXnet(확장 가능한 오픈소스 딥 러닝 프레임워크)  빠르고 확장 가능한 교육 및 추론 프레임워크, 기계 학습을 위해 사용이 쉽고 간단한 API가 제공됨  MXNet에는 Gluon 인터페이스가 포함되어 모든 기술 수준의 개발자가 딥러닝 시작 가능18. AWS 기반 TensorFlow(오픈소스 인공지능 라이브러리)  다양한 산업에서 사용되고 있는 딥러닝 라이브러리를 AWS 서비스와 연동하여 사용 가능12 AWS 자습서AWS 자습서      컴퓨팅, 스토리지, 데이터베이스, 개발자 도구, 보안, 자격 증명 및 규정 준수, 암호화 및 PKI, 기계 학습, 관리 및 거버넌스, 로봇 공학, 블록체인, 게임개발 등등 내용이 있음, 한글도 지원        자습서 예시 : ML        1단계 데이터 준비, 2단계 교육 데이터 원본 생성, 3단계 ML 모델 생성, 4단계 ML 모델의 예측 성능 검토 및 점수 임계값 설정, 5단계 ML 모델을 사용하여 에측생성, 6단계 정리    AWS 기계학습 자습서    1. Amazon Machine Learning        설명은 위 ML 부분 참조, 복잡한 ML 알고리즘 및 기술을 배우지 않고도 기계 학습 모델을 만드는 과정을 안내하는 시각화 도구 및 마법사 제공        모델이 준비되면 간단한 API를 사용하여 애플리케이션에 대한 예측 정보를 쉽게 얻도록 지원하므로 사용자 지정 예측 생성 코드를 실행하거나 인프라를 관리할 필요가 없음  2. Amazon Machine Learning의 주요 개념            단어      개념                  데이터 원본      Amazon ML에 대한 데이터 입력과 관련된 메타데이터를 포함함              ML 모델      입력 데이터에서 추출된 패턴을 사용하여 예측을 생성함              평가      ML 모델의 품질을 측정함              배치 예측      비동기식으로 여러 입력 데이터 관측에 대한 예측을 생성함              실시간 예측      동기적으로 개별 데이터 관측에 대한 예측을 생성함      3. Amazon Machine Learning에 액세스  Amazon ML 콘술  AWS CLI  Amazon ML API  AWS SDK4. 리전 및 Endpint  AWS ML은 미국 동부와 eu 에서만 실시간 예측 엔드포인트 지원  데이터 집합을 호스팅하고 모델을 교육 및 평가하고 모든 리전에서 예측을 트리거 가능 모든 리소스를 동일한 리전에 유지5. AWS ML 설정  AWS 가입  AWS에 가입하면 AWS ML을 포함해 AWS의 모든 서비스에 AWS 계정이 자동 등록6. 자습서 : AWS ML을 사용한 마케팅 반응 예측  한번 해보자  UCI ML 리포지토리의 은행 및 마케팅 데이터 집합 사용해서 고객 식별  "
  }
  , 
  
  "/articles/web/CI,CD/Docker/Docker%20%EA%B8%B0%EB%B3%B8.html": {
    title: "Docker 기본",
    date: " Mar 23, 2021 ",
    url: "/articles/web/CI,CD/Docker/Docker%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["DOCKER","CICD"],
    content: "Docker로 서버 가상화해보기컨테이너를 통해 여러 환경이나 커널을 호스트 OS나 다른 컨테이너와 공유함으로, 기존의 하이퍼 바이저 기반 가상화에 비해 높은 성능을 달성한 가상화 플랫폼.Docker 설치Linux더욱 자세한 것은 도커 공식 문서 참조설치 확인# 보통 안해도 된다systemctl enable dockerservice docker startservice docker status # 안될 경우 /etc/init.d/docker start# 주로 WSL이나 Docker container의 linux에서 일어나는 현상docker versionAWS EC2 linux images1. apt-get 업데이트 및 도커 설치sudo apt-get update # ubuntu 계열sudo apt-get install docker sudo yum -y upgrade # red hat 계열sudo yum install docker 2. 도커 실행sudo systemctl start docker # docekr 서비스의 시작이 필요할 수도 있다.sudo service docker startdocker -v Docker version 20.10.13, build a224086이후 docker를 위와 같이 설치한다.sudo usermod -aG docker $USERNAME이후 위와 같이 특정 사용자에게 도커의 실행 권한을 준다.Dockerfile 설정 및 이미지 빌드FROM python:3.10.5ENV PYTHONUNBUFFERED 1ENV ON_PRODUCTION FALSEENV BACKEND_SECRET_KEY TEMP // 임시 키설정 나중에 꼭 제대로된 키관리 툴로 바꾸기RUN apt-get -y updateRUN apt-get -y install vimRUN mkdir /srv/docker-serverADD . /srv/docker-serverWORKDIR /srv/docker-serverRUN pip install --upgrade pipRUN pip install -r requirements.txtEXPOSE 8000 CMD [&amp;#34;python&amp;#34;, &amp;#34;manage.py&amp;#34;, &amp;#34;runserver&amp;#34;, &amp;#34;0.0.0.0:8000&amp;#34;]위와 같이 Dockerfile을 프로젝트 폴더 내에 작성한 후, sudo docker build -t [repository_name]/[tag_name]:[version] $project_directory을 이용해 도커 이미지를 빌드한다.  -t: 태그 사용sudo docker imagesREPOSITORY          TAG       IMAGE ID       CREATED          SIZEserver_dev/django   latest    68e495e3bc5b   17 seconds ago   1.04GB오류 없이 종료되면 위와 같이 sudo docker images를 통해 생성된 이미지를 확인 할 수 있다.기타 유용한 커맨드sudo docker image rm [--force] ImageID : 특정 이미지 지우기  --force, -f : 가끔 서로 참조 관계로 지워지지 않을 때 강제로 지움, 다만 이 경우 캐쉬가 지워지므로 이미지 빌드가 느려질 수 있음  sudo docker image prune : 사용하지 않고 있는 이미지들 일괄 삭제 sudo docker rm CONTAINERID : 특정 컨테이너 삭제  sudo docker stop CONTAINERID : 특정 컨테이너 중단 sudo docker container prune : 사용하지 않고 있는 컨테이너 일괄 삭제도커 컨테이너 실행sudo docker run -p 80:8000 -d server_dev/django위의 커맨드를 통해 이미지의 레포지토리를 입력해 실행된 컨테이너를 통해 접속할 수 있다.      -p 서버측 열린 포트: 컨테이너측 열린 포트 : 서버로 들어오는 연결의 포트 번호와 컨테이너 측의 포트를 서로 연결해준다.        -d : detached  모드로, 서버의 백그라운드에 컨테이너를 실행하게 한다.  sudo docker ps -aCONTAINER ID   IMAGE               COMMAND                  CREATED         STATUS                     PORTS                                   NAMESc93671164afa   server_dev/django   &amp;#34;python manage.py ru…&amp;#34;   7 seconds ago   Up 5 seconds               0.0.0.0:80-&amp;#38;#62;8000/tcp, :::80-&amp;#38;#62;8000/tcp   amazing_pike상기 커맨드를 통해 돌아가고 있는 컨테이너의 상태를 확인할 수 있다.도커 허브도커 허브를 이용하면 다른 서버와 이미지를 공유할 수 있다.도커 허브의 경우 Repository는 [docker_hub_id]/[허브 내 repo 이름]으로 저장한다.  docker login -u [id]로 로그인  docker push [dockerhub_id]/[dockerhub repository]:[push 태그명]으로 push  docker pull [dockerhub_id]/[dockerhub repository]:[pull 태그명]으로 pull.title: 로그인이 안된다면gnupg2 : 디지털 서명과 인증서 암호화 툴pass: 스탠더드 유닉스 패스워드 매니저sudo apt install gnupg2 pass# 혹은{: #혹은}sudo yum install gnupg2 passdocker-composedocker-compose를 이용하면 여러 컨테이너의 연계와 설정을 쉽게 할 수 있다.docker-compose 개념 참조"
  }
  , 
  
  "/articles/web/CI,CD/Docker/docker-compose%20%EA%B0%9C%EB%85%90.html": {
    title: "docker-compose 개념",
    date: " Mar 23, 2021 ",
    url: "/articles/web/CI,CD/Docker/docker-compose%20%EA%B0%9C%EB%85%90.html",
    tags: ["DOCKER","CICD"],
    content: "docker-composedocker-compose를 이용한다면 여러 도커 컨테이너의 연계와 설정을 손쉽게 할 수 있다.docker-compose 설치먼저 docker-compose를 깔기 위해 앞선 Docker가 설치되어야 있어야 한다.                              [Install Docker Compose CLI plugin          Docker Documentation](https://docs.docker.com/compose/install/) 참고(최신 버전으로 업데이트 되는 코드)                    DOCKER_CONFIG=$&amp;#123;DOCKER_CONFIG:-$HOME/.docker&amp;#125;mkdir -p $DOCKER_CONFIG/cli-pluginscurl -SL https://github.com/docker/compose/releases/download/v2.14.0/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose만약, 무조건 최신의 docker-compose를 다운받고 싶다면sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose만약 위 코드로 안된다면 그 위에 적어 놓은 링크를 참조해 보자.chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose위 코드를 통해 docker-compose의 권한을 바꿔준다.version: &amp;#39;3&amp;#39;services:  nginx:    container_name: nginx # 컨테이너명    build: ./nginx # dockerfile 경로    image: server_dev/nginx # 빌드 후 이미지명    restart: always # 죽으면 재시작    ports:      - &amp;#34;80:80&amp;#34;    volumes: # 컨테이너가 사용할 저장 공간, 이를 통해 컨테이너가 없어져도 저장 공간의 값은 남음      - ./backend:/srv/docker-server/backend      - ./log:/srv/docker-server/log/nginx    depends_on: # 먼저 시작해야 하는 순서의 컨테이너    - django  django:    container_name: django     build: ./backend    image: server_dev/django    restart: always     command: uwsgi --ini uwsgi.ini # Dockerfile의 CMD 명령문을 무시하고 실행할 명령어를 설정하기 위해서 사용됩니다.    volumes:       - ./backend:/srv/docker-server/backend      - ./log:/srv/docker-server/log/uwsgi 위와 같이 docker-compose.yml을 설정할 수있다. 예제는 백엔드 서버와 웹서버를 연결하는 예제이다.sudo docker-compose up -d --build      -d : 백그라운드 실행        --build : 새로 이미지를 빌드  위와 같은 커맨드로 Docker 컨테이너들을 한꺼번에 실행할 수 있고, 오류가 없다면 아래와 이 컨테이너가 실행된다. (예제는 꺼져있는 상태)sudo docker-compose ps // 도커 컨테이너들의 목록 보기NAME                COMMAND                  SERVICE             STATUS              PORTSdjango              &amp;#34;uwsgi --ini uwsgi.i…&amp;#34;   django              exited (137)        nginx               &amp;#34;/docker-entrypoint.…&amp;#34;   nginx               exited (0)  🔵 로깅 설정 추천  참고로, 한꺼번에 컨테이너를 실행하면서 서버의 메시지가 콘솔에 겹쳐서 보이며, 데몬 상태로 실행중 일 때는 아예 보이지 않으므로, 디버깅을 위해 사용하는 기술 스택의 로깅을 제대로 설정하는 것이 좋다.  &amp;#38;#62; django  | [uWSGI] getting INI configuration from uwsgi.ini&amp;#38;#62; nginx   | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration&amp;#38;#62; nginx   | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/&amp;#38;#62; nginx   | /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh&amp;#38;#62;     일반적으로 위와 같이 표시된다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-Counting.html": {
    title: "알고리즘 수학 기본-Counting",
    date: " Jul 6, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-Counting.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 셈(Counting)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 조합론의 표기법, 정의, 속성 같은 기본적인 것만 배운다.들어가기에 앞서 집합에 대한 기본적인 이해가 필요하므로 알고리즘을 위한 수학 -집합 편을 보고 오는 것을 추천한다.셈(Counting)셈 이론은 예를 들어, “n 개의 원소를 다른 순서대로 나열하는 방법” 같은 “수가 얼마나 되는가?”에 대한 물음을 전부 세보지 않고 알기 위한 이론이다.합과 곱 규칙(Rules of sum and product)조합론(combinatorics)에서 셈하고 싶은 원소들의 집합을 서로소 집합의 합이나 집합들의 곱집합으로 나타낼 수 있다는 규칙이 합 규칙과 곱 규칙이다.합 규칙(Rules of sum)합 규칙은 두 서로소 집합에서 원소를 하나 고르는 경우의 수는 각 집합의 카디널리티의 합이라는 규칙이다.쉽게 말해 여러 원소를 가지고 있는 집합들에서 원소를 집합 전체에서 하나 고르는 경우의 수는, 각 집합에서 원소를 하나 고르는 경우의 수를 모두 더한 값, 즉 각 집합의 원소의 수를 모두 더한 값과 같다는 규칙이다.            수학적으로 집합 A, B를 공통되는 원소가 없는 두 유한 집합이라고 가정하면 $      A \\cup B      =      A      +      B      $가 되며, 예를 들자면 듣고 싶은 과목의 인강이 A 사이트에 7개, B 사이트에 5개 있다면, 내가 고를 수 있는 인강은 총 8+5 = 12개가 될 것이다.      곱 규칙(Rules of product)곱 규칙은 순서쌍을 고르는 경우의 수가 첫번째 원소의 경우의 수와 두번째 원소의 경우의 수의 곱이라는 규칙이다.즉, 여러 원소를 가지고 있는 집합들에서 원소를 집합 마다 하나씩 고르는 경우의 수는, 각 집합에서 원소를 하나 고르는 경우의 수를 모두 곱한 값, 즉 각 집합의 원소의 수를 모두 곱한 값과 같다는 규칙이다.            수학적으로 $      A\\times B      =      A      \\cdot      B      $ 로 나타내며, 예를 들자면, 아이스크림 종류가 4개, 토핑 종류가 3개라면, 총 12개 종류의 색다른 아이스크림을 즐 길 수 있다.      문자열(Strings)유한 집합 S에 대한 문자열은 S의 원소들의 수열을 의미한다. 예를 들어, 길이가 3인 이진 문자열(binary string)은 000,001,010,011,100,110,111 총 8개가 존재 가능하다. 이때 n 만큼의 길이를 가진 문자열을 n-문자열이라고도 말한다.문자열 s의 부분문자열(substring) s’은 s의 연속된 원소로 이루어진 순차 수열이다. 예를 들어, 010은 01011110의 부분 문자열일 수 있다.            집합 S에 대한 k-문자열을 k-튜플의 곱집합 $S^k$의 원소로 볼 수 있다. 즉, $      S      ^k$개 만큼의 k 길이 문자열이 존재한다. 에를 들어 이진 k-문자열의 수는 $2^k$이며, 직관적으로, 생각하자면, 선택지 n개를 총 k번 선택하는 가지수를 의미하므로 $n\\cdot n\\cdots n = n^k$를 통해 경우의 수를 구한다.      순열(Permutations)유한 집합 S의 순열(Permutations)은 중복이 존재하지 않는 S의 원소들의 순서있는 수열을 의미한다.예를 들어 $S={a,b,c}$의 순열은 abc, acb, bac, bca ,cab, cba 총 6개의 순열을 가지고 있다.            순열의 경우의 수는 총 $      S      !$만큼 가지고 있는데, 각각 순서의 경우의 수를 알아보자면, 첫번째 원소의 경우의 수가 $      S      $개, 두번째는 중복을 허용하지 않으므로 첫번째에 고른 원소를 제외한 $      S      -1$개, 세번째의 경우의 수는 $      S      -2$개로, 최종적으로 마지막 순서의 원소가 1개 남을때 까지 점점 줄어드는 방식이기 때문이다.      집합 S에 대한 k-순열은 S의 k개의 원소들의 중복이 존재하지 않는 순서있는 수열인데, 예를 들어 $S={a,b,c}$의 2-순열은 ab, ac, ba, bc, ca, cb 총 6개이다.n개의 원소를 가진 집합에 대한 k-순열은 다음과 같은 방법으로 구한다.\\(n(n-1)(n-2)\\cdots(n-k+1)=\\frac{n!}{(n-k)!}\\label{eq:kPermuNSet}\\tag{1}\\)기본적인 순열과 달리, 1부터 k번째 까지 경우의 수를 구하기 때문이다.조합(Combinations)n개의 조합 S에 대한 k-조합은 집합 S의 k-부분 집합을 의미한다. 예를 들어 ${a,b,c,d}$의 2-조합은 $ab, ac, ad, bc, bd, cd$, 총 6개이다.k-조합은 집합 내에서 k개의 구분되는 원소를 고름으로 생성할 수 있으며, n-집합의 k-조합의 경우의 수는 k-수열을 구하는 식으로 표현할 수 있다.모든 k-조합은 k-순열에, 순서가 다르지만 원소가 동일한 순열은 제외하는 것과 같으므로, 이를 이용해 아래와 같은 식으로 조합의 경우의 수를 나타낼 수 있다.\\(\\frac{n!}{k!(n-k)!}\\label{eq:kCombNSet}\\tag{2}\\)k=0일 때는, 조합의 경우의 수가 1인데, 이를 통해 0!=1임을 알 수 있다.이항 계수(Binomial coefficients)$\\begin{pmatrix} n\\k\\end{pmatrix}$ 표기는  $x^ay^b$ 같은 이항식을 전개했을 때 각 항의 계수이며, 조합론에서는 n-집합에 대한 k-조합의 경우의 수를 의미한다.우리가 앞서 배웠던 조합의 식을 이용해 아래와 같은 식이 성립한다.\\(\\begin{pmatrix} n\\\\k\\end{pmatrix} = \\frac{n!}{k!(n-k)!}\\)또한, n-k와 k값에 대하여 아래처럼 서로 동일한 값을 가진다.\\(\\begin{pmatrix} n\\\\k\\end{pmatrix} = \\begin{pmatrix} n\\\\n-k\\end{pmatrix}\\label{eq:symmetericK}\\tag{3}\\)이를 이항 계수(Binomial coefficients)라고도 부르는데, 이는 이항식을 전개했을 때의 전체 식을 알 수있는 이항 정리(binomial expansion)에서 유래되었다.  이항 정리는 아래와 같이 두 개의 항으로 되어있는 식이 전개되었을 때 나오는 항들의 계수를 알 수 있는 정리이다. 즉 이항 계수는 여러 항 중에 하나만, 이항 정리는 전체 전개식을 표현한다.\\[(x+y)^n=\\sum^n_{k=0}\\begin{pmatrix} n\\\\k\\end{pmatrix}x^ky^{n-k}\\label{eq:binomialExpansion}\\tag{4}\\]이항 정리는 x=y=1일때 특별한 성질을 가지는데, 다음과 같은 꼴로 변한다.\\(2^n=\\sum^n_{k=0}\\begin{pmatrix} n\\\\k\\end{pmatrix}\\)이 공식은 이진 n-문자열의 경우의 수를 구하는 방법과 같다.예시로, n=1, 즉 길이 1의 이진 문자열은 0 또는 1 두개이며, n=2일때는 00, 01, 10, 11 총 4개 이다. 이 경우의 수는 위 이항정리에 넣어 구할 수 있음을 알 수 있다.$\\begin{pmatrix} n\\k\\end{pmatrix}$ 이진 n-문자열은 정확히 k개의 1을 포함한다. 왜냐하면 $\\begin{pmatrix} n\\k\\end{pmatrix}$가 n개의 위치가 존재할 때 k개를 선택하는 방법(=1의 자리)의 경우의 수이기 때문이다.예시로, n=3, k=1일때는 1을 한개만 사용하여 만들 수 있는 이진 3-문자열의 경우의 수이다. (001, 010, 100, 총 3개)이런식으로 이항 계수는 여러 곳에서 사용될 수 있다.이항 한계(Binomial bounds)유한합의 범위 제한처럼(알고리즘을 위한 수학 유한합편 참조) 이항 계수의 범위를 제한할 때, 하한값의 경우 아래와 같이 구할 수 있다.\\(\\begin{pmatrix} n\\\\k\\end{pmatrix}=\\frac{n(n-1)\\cdots(n-k+1)}{k(k-1)\\cdots 1}\\\\=(\\frac{n}{k})(\\frac{n-1}{k-1})\\cdots(\\frac{n-k+1}{1})\\\\\\geq (\\frac{n}{k})^k\\)스털링 근사를 이용해 얻은 $k! \\geq (k/e)^k$을 통해 아래와 같은 상한값을 얻을 수 있다.  스털링 근사는 수학에서 팩토리얼 값을 추정하는 방법이다. $n! \\sim \\sqrt{2\\pi n}(n/e)^n$ 자세한 내용은 추가 예정\\[\\begin{pmatrix} n\\\\k\\end{pmatrix}=\\frac{n(n-1)\\cdots(n-k+1)}{k(k-1)\\cdots 1} \\leq \\frac{n^k}{k!} \\leq (\\frac{en}{k})^k\\label{eq:upperBound}\\tag{5}\\]모든 n보다 작은 양의 정수 k에 대해 귀납적 방법으로 다음과 같은 상한 값도 얻을 수 있다.\\[\\begin{pmatrix} n\\\\k\\end{pmatrix} \\leq (\\frac{n^n}{k^k(n-k)^{n-k}})\\label{eq:BinomialBoundByInduction}\\tag{6}\\]$0^0=1$로 가정하고, $k=\\lambda n,\\ 0\\leq \\lambda \\leq 1$일때, 다음과 같은 상한값을 얻을 수 있다.\\(\\begin{pmatrix} n\\\\k\\end{pmatrix} \\leq (\\frac{n^n}{(\\lambda n)^{\\lambda n}((1-\\lambda)n)^{(1-\\lambda)n}})\\\\=((\\frac{1}{\\lambda})^\\lambda(\\frac{1}{1-\\lambda})^{1-\\lambda})^n=2^{nH(\\lambda)}\\)이를 통해 아래와 같은 식을 얻을 수 있는데\\(H(\\lambda)=-\\lambda \\lg \\lambda -(1-\\lambda)\\lg(1-\\lambda)\\label{eq:binaryEntropyFunction}\\tag{7}\\)이는 (이진) 엔트로피 함수(bianary entropy function)이라고 하며, $0\\log0 = 0$으로 가정하면 $H(0)=H(1)=0$이 된다.  이진 엔트로피 함수에 대한 내용 추가 예정"
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%95%A8%EC%88%98.html": {
    title: "알고리즘 수학 기본-함수",
    date: " Jul 6, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%95%A8%EC%88%98.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 함수(functions)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 함수의 표기법, 정의, 속성 같은 기본적인 것만 배운다.앞선 알고리즘을 위한 수학 - 집합(Sets)편을 먼저 보고 오는 것을 추천한다.함수(functions)함수 $f$는 두 집합 A, B의 원소 a, b에 대하여, a에 대한 b의 값이 정확히 하나씩만 대응되는 이항관계이다.이때 집합 A를 함수 $f$의 정의역(domain, 도메인), 집합 B를 함수 $f$의 공역(codomain, 부도메인)이라 하며, $f:A\\rightarrow B$로 표현한다.이때 정의역의 원소 a와 공역의 원소 b의 관계가 성립된다면$(a,b)\\in f$, $b=f(a)$로 표기하며, 이때 b의 값은 오직 원소 a의 값에 의해서만 결정된다.      $b=f(a)$에서 정의역의 원소 a는 함수 f의 인자(argument)이며, 정의역이 원소 b는 함수 f의 값(value)이다.    만약, 정의역이 곱집합으로 되어있는 경우에는 괄호를 활용해 인자를 표현한다. 예를 들어 $f:A_1\\times A_2 \\times \\cdots \\times A_n \\rightarrow B$는 $f(a_1,a_2,\\cdots,a_n)$으로 표현된다.    사실, 엄밀히 말하면 $(a_1,a_2,\\cdots,a_n)$ 형태의 n-튜플이 통째로 하나의 인자이며, $f((a_1,a_2,\\cdots,a_3))$가 옳은 표현이지만, 각 $a_i$를 하나의 인자로 보고, 여러개의 인자가 들어가 있는 것으로 표현한다.  앞서 언급했듯이,인자 a는 오직 한 값 b에 대응되지만, b은 여러 인자 a값에 대응될 수 있다.예시로, 함수 $f={(a,b):a,b\\in \\N\\ and\\ b=a\\mod2}$의 경우, $f:\\N \\rightarrow {0,1}$로 표현하며, a 값은 b값으로 0 또는 1만 가질 수 있고, b는 많은 수의 a를 가진다.반대로 이항관계 $g={(a,b):a,b\\in \\N\\ and\\ a+b는\\ 짝수}$에서는 예를 들어 a가 홀수이면, 나머지 모든 홀수들이 전부 b가 될 수 있으며, 함수의 정의 중, 정의역의 원소가 여러 공역의 원소에 대응되서는 안되므로, 함수가 될 수 없다.그러므로 함수는 각 값에 대해 하나의 인자, 즉, 하나의 정의역의 원소를 대응시켜 주어 정의할 수 있으며, 만약 두 함수 $f$와 $g$의 정의역과 공역이 같다면, 두 함수는 같은 걸로 정의한다.전사함수(Surjection), 단사함수(Injection) 그리고 전단사함수(Bijection)수학에서 상(image)은 어떤 함수에 대한 정의역의 원소들에 대응하는 공역의 원소들이며, $b=f(a)$에서 b를 의미한다. 이를 이용해 부분집합 $A’$를 다음과 같이 표현할 수 있다.$f(A’)={b\\in B:b=f(a)\\ for\\ some\\ a \\in A’}$또한, 함수의 치역(range)은 정의역의 상(image), 즉 함수의 출력값들의 집합을 의미하며, 정의역 집합 A에 대해 $f(A)$로 표현된다.예를 들면 $f(n)=2n$에서 치역은 $f(\\N)={m:m=2n\\ for\\ some\\ n \\in \\N}$, 즉, 양수의 짝수 정수로 이루어진다.전사함수(surjection, surjective funtion) 또는 A 위로의 B 함수( A onto B)은 이러한 치역(range)와 공역(codomain)이 같은 함수를 의미한다.즉, 공역의 모든 원소들이 어떤 정의역 원소들에 의해 하나도 빠짐없이 대응되고 있어야 한다. 이때, 정의역 원소의 중복을 허용한다.함수 $f(n)=\\left \\lfloor n/2  \\right \\rfloor$의 경우, n의 값이 바뀌면서 모든 정수를 $\\N$의 모든 값이 함수의 결과값으로 나오므로 전사함수이다.하지만, 함수 $f(n)=2n$의 경우, 함수의 결과값은 언제나 짝수로 나오므로, 홀수 정수는 대응되는 정의역 원소가 없으므로 전사함수가 아니다.단사함수(injection, injective function), 또는 일대일(one-to-one) 함수는 각기 다른 정의역의 원소가 각기 다른 공역의 원소에 대응 되는 함수를 의미한다.즉, $a\\neq a’$은 곧 $f(a)\\neq f’(a)$를 의미한다.전사함수의 예시와 반대로 함수 $f(n)=\\left \\lfloor n/2  \\right \\rfloor$의 경우, n의 값이 바뀌어도 같은 결과값이 나올 수 있으므로, 단사함수가 아니다.(n이 2,3 이 전부 같은 1이 나온다.)하지만, 함수 $f(n)=2n$의 경우, 함수의 결과값은 n의 2배인 짝수가 나오므로, 중복되는 결과값이 없어 단사 함수이다.전단사함수(bijection, bijective function) 또는 일대일 대응(one-to-one correspondence) 함수(일대일 함수와 다르다)는 전사함수와 단사함수가 합쳐진 것으로, 즉 정의역과 공역의 원소들이 하나도 빠짐없고 중복없이 모두 일대일 대응되는 함수이다.예를 들어 함수 $f(n)=(-1)^n \\left \\lceil n/2 \\right \\rceil$의 경우 정의역 $(0, 1, 2, 3, 4)$에 대해 순서대로 $(0,-1,1,-2,2)$가 생성되므로 전단사함수이다.집합 A가 정의역이자 공역이 같은 전단사 함수, 즉 자기자신과 일대일 대응되는 전단사함수는 순열(permutation)이라고도 부른다.  집합 A의 원소가 함수에 대입되 집합 A의 다른 원소가 나오는 꼴이 마치 자리가 바뀌는 것 처럼 보이므로. 물론, 자리가 바뀌지 않고 같은 값이 나와도 전단사 함수이며, 순열의 경우의 수중 하나이다.또한 전단사 함수의 성질로, 정의역과 공역이 반전되어 있는 형태의 역함수 $f^{-1}(b)=a$로 표현될 수 있으며, 위에서 언급한  함수$ f(n)=(-1)^n \\left \\lceil n/2 \\right \\rceil$ 의 역함수를 예시로 들자면\\(f^{-1}(m)=\\left\\{\\begin{matrix}2m\\ &amp; if \\ m\\geq0\\\\ -2m-1 &amp; if\\ m &lt; 0\\end{matrix}\\right.\\)"
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EA%B8%B0%ED%95%98,%20%EC%9D%B4%ED%95%AD%20%EB%B6%84%ED%8F%AC.html": {
    title: "알고리즘 수학 기본-기하, 이항 분포",
    date: " Jul 8, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EA%B8%B0%ED%95%98,%20%EC%9D%B4%ED%95%AD%20%EB%B6%84%ED%8F%AC.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 기하, 이항 분포(geometric and binomial distributions)_Introduction to Algorithm, 3rd, Cormen_을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.추가로 존이(mykepzzang)님의 블로그에서 내용을 발췌하였다.이 장에서는 분포의 표기법, 정의, 속성 같은 기본적인 것만 배운다. 들어가기에 앞서 확률에 대한 기본적인 이해가 필요하므로 알고리즘을 위한 수학 확률편, 확률 변수편을 보고 오는 것을 추천한다.베르누이 시행의 분포 (Bernoulli trial Distribution)베르누이 시행 (Bernoulli trial)은 동전 던지기처럼 상호 독립적인 2가지 결과(성공, 실패)만 나올 수 있는 경험을 의미하며, 보통은 두 결과 모두 동등하게 절반의 확률이다.베르누이 시행에는 중요한 분포로 기하 분포와 이항 분포가 존재한다.기하 분포 (The geometric distribution)아래는 성공 확률과 실패 확률이 각각 $p,\\ q=1-p$인 베르누이 시행의 시도에 대해, 확률 변수 X를 성공할 때까지 경험을 시도한 횟수라고 가정하면, $k \\geq 1$일 때, 다음과 같은 확률을 구할 수 있다.$\\Pr{X=k}=q^{k-1}p$이때 k-1의 실패 뒤에 k번재에 성공하게 되며, 이러한 식이 만족하는 확률 분포를 기하 분포라고 하며, Figure 1과 같은 모습이다.또한, q &lt; 1을 가정하고, 기하 분포의 기대값을 다음과 같이 구할 수 있다.\\[\\begin{align}E[X]&amp;=\\sum_{k=1}^\\infty kq^{k-1}p\\\\&amp;=\\frac{p}{q}\\sum^\\infty_{k=0}kq^k\\\\&amp;=\\frac{p}{q}\\cdot\\frac{q}{(1-q)^2}\\\\&amp;=\\frac{p}{q}\\cdot\\frac{q}{p^2}\\\\&amp;=1/p\\end{align}\\]따라서, 성공을 얻는데 평균적으로 1/p 번의 시도가 필요하며, 다음과 같이 분산을 구할 수 있다.$Var[X]=q/p^2$예시로, 우리가 반복적으로 두개의 주사위를 던져 합이 7이나 11이 나오기 위해선, 36가지의 가능한 결과에서 6개의 합이 7인 경우와 2개의 합이 11인 경우가 존재한다. 따라서 성공 확률 $p=8/36=2/9$이며, 평균적으로 4.5회 던져야 성공할 수 있다.이항 분포(The binomial distribution)n번 시도에 대한 성공 횟수를 확률 변수 X로 놓는다면, $0 \\leq k \\leq n$일 때, 각기 일어날 확률이 $p^kq^{n-k}$이며, 총 $\\binom{n}{k}$개의 방법이 존재하므로, 다음 같은 확률이 나온다.\\(\\Pr\\{X=k\\}=\\binom{n}{k}p^kq^{n-k}\\)위와 같은 식을 만족하는 확률 분포를 이항 분포(binomial distribution)라고 하며, Figure 2와 같이 표현된다.“이항”은 위의 식이 전개시, $(p+q)^n$의 전개식 처럼 보이기 때문에 붙여진 이름이며, $p=1-q$일 때, 위 식을 아래와 같이 표기하기도 한다.\\(b(k;n,p)=\\binom{n}{k} p^k(1-p)^{n-k}\\)또한, 확률 공리 “모든 사건들의 확률의 합은 1이다.”에 의해 다음과 같이 된다.\\(\\sum^n_{k=0}b(k;n,p)=1 \\tag{1} \\label{eq:binomial sums}\\)또한, 이를 이용해 다음과 같은 이항 분포의 기댓값을 얻을 수 있다.\\(\\begin{align}E[X]&amp;=\\sum^n_{k=0}k\\cdot \\Pr\\{X=k\\}\\\\&amp;=\\sum^n_{k=0}k\\cdot b(k;n,p)\\\\&amp;=\\sum^n_{k=1}k\\binom{n}{k}p^kq^{n-k} \\\\&amp;= np\\sum^n_{k=1}\\binom{n-1}{k-1}p^{k-1}q^{n-k} \\\\&amp;=np\\sum^{n-1}_{k=0}\\binom{n-1}{k}p^k q^{(n-1)-k}\\\\&amp;=np\\sum^{n-1}_{k=0}b(k;n-1,p)\\\\&amp;=np\\ &amp;(식\\ \\eqref{eq:binomial sums}에\\ 의해)\\label{eq:expectation of binomial distribution}\\tag{expectation}\\end{align}\\)또 다른 방법으로 구하려면, 확률 변수 $X_i$를 i 번째 시도에서의 성공 횟수로 가정하면,(즉, 한번 시도했을 때의 성공횟수 =&gt; 최대 1 또는 0) $E[X_i]=p\\cdot 1+ q\\cdot 0=p$가 나오게 된다. 기댓값의 선형성에 의해 아래와 같이 기댓값을 구할 수 있다.\\[\\begin{align}E[X]&amp;= E\\left[\\sum^n_{i=1}X_i\\right]\\\\&amp;=\\sum^n_{i=1}E[X_i]\\\\&amp;=\\sum^n_{i=1}p\\\\&amp;=np\\end{align}\\]이러한 방법으로 분포의 분산 또한 쉽게 구할 수 있다. 앞선 확률 변수편에서 구했던 분산 공식 $Var[X_i]=E[X_i^2]-E^2[X_i]$에서, 확률 변수 $X_i$는 위에서 언급했듯이 1아니면 0이므로 $X_i^2=X_i$가 되며, 이는 곧 $E[X^2_i]=E[X_i]=p$를 의미한다.따라서 $Var[X_i]=p-p^2=p(1-p)=pq$를 만족하며, 이를 이용해 아래와 같이 확률 변수 X에 대한 분산을 구할 수 있다.\\[\\begin{align}Var[X]&amp;= Var\\left[\\sum^n_{i=1}X_i\\right]\\\\&amp;=\\sum^n_{i=1}Var[X_i]\\\\&amp;=\\sum^n_{i=1}pq\\\\&amp;=npq\\end{align}\\]Figure 2에서 보면, 이항 분포는 k가 증가할 수록 증가하는 추세에서, np에 다다르면 감소하게되는데, 이러한 추세는 다음과 같은 연속식의 비율로 증명 가능하다.\\[\\begin{align}\\frac{b(nk;n,p)}{b(k-1;n,p)}&amp;=\\frac{\\binom{n}{k}p^kq^{n-k}}{\\binom{n}{k-1}p^{k-1}q^{n-k+1}}\\\\&amp;=\\frac{n!(k-1)!(n-k+1)!p}{k!(n-k)!n!q}\\\\&amp;=\\frac{(n-k+1)p}{kq}\\\\&amp;=1+\\frac{(n+1)p-k}{kq}\\label{eq:ratio of binomial distribution}\\tag{2}\\end{align}\\]이 비율은 $(n+1)p-k$가 양수일때 1보다 크며, 즉 $b(k;n,p)&gt;b(k-1;n,p)$일때, $k&lt;(n+1)p$, 즉 분포가 증가하는 추세이고, $b(k;n,p)&lt;b(k-1;n,p)$일때, $k&gt;(n+1)p$, 즉 분포가 감소하는 추세이다.만약 $k=(n+1)p$가 정수라면, $b(k;n,p)=b(k-1;n,p)$이며, k의 범위가 $np-q &lt;k&lt;(n+1)p$일때 최대값을 얻을 수 있다.이항 분포의 상한을 얻을 수 있는 부명제 증명부명제 1. $n \\geq 0$이고, $0&lt;p&lt;1$ $q=1-p$이며, $0\\leq k \\leq n$일때, 다음 식이 성립한다.\\(b(k;n,p)\\leq (\\frac{np}{k})^k(\\frac{nq}{n-k})^{n-k}\\)증명:앞서 Counting 편의 이항 한계의 식에서 배웠던 $\\binom{n}{k}\\leq \\frac{n^n}{k^k(n-k)^{n-k}}$을 통하여 다음이 성립한다.\\(\\begin{align}b(k;n,p)&amp;=\\binom{n}{k}p^kq^{n-k}\\\\&amp;\\leq \\left(\\frac{n}{k}\\right)^k\\left(\\frac{n}{n-k}\\right)^{n-k}p^kq^{n-k}\\\\&amp;=\\left(\\frac{np}{k}\\right)^k \\left(\\frac{np}{n-k}\\right)^{n-k}\\end{align}\\)이항 분포의 꼬리들 (The tails of the binomial distribution)이항 분포의 가장 확률이 높은 지역인 평균($np$)에서 가장 먼 양 끝단을 이항 분포의 꼬리(tail) 부분이라고 하며, 좌측 꼬리는 최소의 성공, 우측 꼬리는 최대의 성공확률를 의미한다.먼저 분포에서 우측 꼬리의 한계(bound)에 대해 알아보자.정리(Theorem) 1n번의 베르누이 시행에서 성공 확률이 p이며, 확률 변수 X가 성공 횟수를 의미할 때, $0\\leq k \\leq n$일 때, 최소 성공 횟수 k의 확률의 상한은 다음과 같다.\\(\\Pr\\{X\\geq k\\}=\\sum^n_{i=k}b(i;n,p)\\leq \\binom{n}{k}p^k\\)증명$S \\subseteq {1,2,\\dots,n}$일 때,  $A_S$를 $i\\in S$인 i번째 시도 하나가 성공인 사건으로 놓으면 $|S|=k$일 때, $\\Pr{A_S}=p^k$이게 된다.(모든 시도 중에 S에 속한 i번째 시도가 전부 성공할 확률)그럴 경우, 아래가 성립한다.\\(\\begin{align}\\Pr\\{X\\geq k\\}&amp;=\\Pr\\{there\\ exists\\ S\\subseteq \\{1,2,\\dots,n\\}:|S|=k\\ and\\ A_S\\}\\\\&amp;=\\Pr\\left\\{\\bigcup_{S\\subseteq\\{1,2,\\dots,n\\}:|S|=k}A_S\\right\\} &amp; (S에\\ 속한\\ 모든\\ i\\ 번째\\ 시도가\\ 성공할\\ 확률)\\\\&amp;\\le \\sum_{S\\subseteq\\{1,2,\\dots,n\\}:|S|=k}\\Pr\\{A_S\\} &amp;(Boole 부등식에\\ 의해)\\\\&amp;=\\binom{n}{k}p^k\\end{align}\\)NOTE부울의 부등식 (Boole’s inequality)유한 또는 가산 무한 이벤트들의 수열 $A_1, A_2, \\dots$에 대하여 확률의 공리 $\\Pr{A\\cup B}=\\Pr{A}+\\Pr{B}-\\Pr{A\\cap B}\\leq \\Pr{A}+\\Pr{B}$$가 만족하므로, \\Pr{A_1\\cup A_2\\cup\\cdots}\\le \\Pr{A_1}+\\Pr{A_2}+\\cdots$를 만족한다.따름 정리(Corollary) 2성공확률이 p이며, n번 시도하는 베르누이 시행에서 확률 변수 X를 성공 횟수로 놓고, $0\\leq k \\leq n$일 경우, 최대 성공횟수 k에 대한 확률의 상한은 다음과 같다.이는 또한, 반대편 꼬리에도 적용 가능하다.\\(\\begin{align}\\Pr\\{X\\leq k\\}&amp;=\\sum^k_{k=0}b(i;n,p)\\\\&amp;\\leq \\binom{n}{n-k}(1-p)^{n-k}\\\\&amp;=\\binom{n}{k}(1-p)^{n-k}\\end{align}\\)정리(Theorem) 3성공확률이 p이며, 실패 확률이 $q=1-p$인 n번 시도하는 베르누이 시행에서 확률 변수 X를 성공 횟수로 놓고, $0 &lt; k&lt; np$에서 k보다 적게 성공할 확률의 상한은 다음과 같다. \\(\\begin{align}\\Pr\\{X&lt;k\\}&amp;=\\sum^{k-1}_{i=0}b(i;n,p)\\\\&amp;&lt; \\frac{kq}{np-k}b(k;n,p)\\end{align}\\)이는 이항 분포의 좌측 꼬리에 대한 한계이며, 좌측 꼬리부터 기하급수적으로 줄어드는 이유이다.증명먼저 수열 $\\sum^{k-1}_{i=0}b(i;n,p)$을 연속합편에서 배웠던 등비수열을 통해 한계를 구한 뒤, $i=1,2,\\dots,k,$일 때, 식 $\\eqref{eq:ratio of binomial distribution}$를 통해 다음이 성립한다. \\(\\begin{align}\\frac{b(i-1;n,p)}{b(i;n,p)}&amp;=\\frac{iq}{(n-i+1)p}\\\\&amp;&lt;\\frac{iq}{(n-i)p}\\\\&amp;\\leq\\frac{kq}{(n-k)p}\\end{align}\\)이때 x를 다음과 같이 정하면, x의 상한을 얻을 수 있다.\\(\\begin{align}x&amp;=\\frac{kq}{(n-k)p}\\\\&amp;&lt;\\frac{kq}{(n-np)p}\\\\&amp;=\\frac{kq}{nqp}\\\\&amp;=\\frac{k}{np}\\\\&amp;&lt;1\\end{align}\\)위 식을 통해 식 $b(i-1;n,p)&lt;xb(i;n,p)$이 성립하게 된다.위 부등식을 $0&lt;i\\leq k$일 때, k-i번 연속적으로 적용하면 $b(i;n,p)&lt;x^{k-i}b(k;n,p)$를 구할 수 있다.$0\\leq i &lt;k$일 때, 다음이 성립한다.\\(\\begin{align}\\sum^{k-1}_{i=0}b(i;n,p)&amp;&lt;\\sum^{k-1}_{i=0}x^{k-i}b(k;n,p)\\\\&amp;&lt;b(k;n,p)\\sum^\\infty_{i=0}x^i\\\\&amp;=\\frac{x}{1-x}b(k;n,p)\\\\&amp;=\\frac{kq}{np-k}b(k;n,p)\\end{align}\\)따름 정리(Corollary) 4성공확률이 p이며, 실패 확률이 $q=1-p$인 n번 시도하는 베르누이 시행에서 확률 변수 X를 성공 횟수로 놓고, $0&lt;k\\leq np/2$에서 k번 보다 적게 성공할 확률은 k+1번 보다 적게 성공할 확률보다 절반 이하이다.증명$k\\leq np/2$이기 때문에 다음이 성립한다.\\(\\frac{kq}{np-k}\\leq \\frac{(np/2)q}{np-(np/2)}=\\frac{np/2q}{np/2}\\leq 1\\)확률 q는 1보다 작으므로, 정리(Theorem) 3과 위의 식에 의해 k보다 적게 성공할 확률은 다음과 같이 된다.\\(\\Pr\\{X&lt;k\\}=\\sum^{k-1}_{i=0}b(i;n,p)&lt;b(k;n,p)\\)따라서, $\\sum^{k-1}_{i=0}b(i;n,p)&lt;b(k;n,p)$에 의해 다음과 같은 식이 성립한다.\\(\\begin{align}\\frac{\\Pr\\{X&lt;k\\}}{\\Pr\\{X&lt;k+1\\}}&amp;=\\frac{\\sum^{k-1}_{i=0}b(i;n,p)}{\\sum^k_{i=0}b(i;n,p)}\\\\&amp;=\\frac{\\sum^{k-1}_{i=0}b(i;n,p)}{\\sum^{k-1}_{i=0}b(i;n,p)+b(k;n,p)}\\\\&amp;&lt;1/2\\end{align}\\)이는, 다른 반대편 꼬리에도 똑같이 적용된다,즉 k번보다 많게 성공할 확률은 k-1번 보다 많게 성공할 확률의 절반 이하이다. (따름 정리 6 참조)따름 정리(Corollary) 5성공확률이 p이며, 실패 확률이 $q=1-p$인 n번 시도하는 베르누이 시행에서 확률 변수 X를 성공 횟수로 놓고, $np&lt;k&lt;n$일 때, k번보다 높게 성공할 확률은 다음과 같은 상한을 가지고 있다.$\\Pr{X&gt;k}=\\sum^n_{i=k+1}b(i;n,p)&lt;\\frac{(n-k)p}{k-np}b(k;n,p)$따름 정리(Corollary) 6성공확률이 p이며, 실패 확률이 $q=1-p$인 n번 시도하는 베르누이 시행에서 확률 변수 X를 성공 횟수로 놓고 $(np+n)/2&lt;k&lt;n$ 를 만족할 때, k보다 많이 성공할 확률은 k-1보다 많이 성공할 확률의 절반 이하이다.정리 (Theorem) 7n번의 베르누이 시행에서,$i=1,2,\\dots,n$에서 각 i번째 시도의 성공확률을 $p_i$로 놓고, 실패 확률을 $q_i=1-p_i$로 놓는다.확률 변수 X를 총 성공 횟수로 놓고, $\\mu=E[X]$로 놓고, $r&gt;\\mu$일 경우, $\\Pr{X-\\mu \\geq r} \\leq (\\frac{\\mu e}{r})^r$가 성립한다.이 정리를 통해 이항 분포의 우측 꼬리의 한계를 알 수 있다.증명$\\alpha &gt;0$일 때, 함수 $e^{\\alpha x}$는 x에 대해 크기가 비례하게 되므로, 다음이 성립한다.\\[\\Pr\\{X-\\mu \\geq r\\}=\\Pr\\{e^{\\alpha(X-\\mu)}\\geq e^{\\alpha r}\\}\\label{eq:possibility bound by markov inequality}\\tag{3}\\]위 식을 이용해 음이 아닌 확률 변수 X의 확률의 상한을 알 수 있는 마르코프 부등식(Markov’s inequality)에 의하여 $t&gt;0$일 경우, $\\Pr{X \\geq t}\\leq E[X]/t$이 성립하므로 다음 식 $\\eqref{eq:possibility bound by markov inequality with exponential}$이 성립한다.\\(\\Pr\\{e^{\\alpha(X-\\mu)}\\geq e^{\\alpha r}\\}\\leq E[e^{\\alpha (X-\\mu)}]e^{-\\alpha r}\\label{eq:possibility bound by markov inequality with exponential}\\tag{4}\\)NOTE마르코프 부등식(Markov’s inequality)의 증명\\(\\begin{align}E(X)&amp;=\\int^\\infty_{-\\infty}xf(x)dx\\ &amp;(기댓값의\\ 정의)\\\\\\\\&amp;=\\int^\\infty_{0}xf(x)dx&amp;(X는\\ 음이\\ 아닌\\ 확률\\ 변수이므로)\\\\&amp;=\\int^a_{0}xf(x)dx+\\int^\\infty_{a}xf(x)dx\\\\&amp;\\geq\\int^\\infty_{a}xf(x)dx\\\\&amp;\\geq \\int^\\infty_{a}af(x)dx\\\\&amp;=a\\int^\\infty_{a}f(x)dx\\\\&amp;=a\\Pr(X\\geq a)\\\\&amp; \\therefore \\Pr\\{X \\geq a\\}\\leq E[X]/a\\end{align}\\)증명 과정은 $E[e^{\\alpha (X-\\mu)}]$의 한계 구하기와 식 $\\eqref{eq:possibility bound by markov inequality with exponential}$의 $\\alpha$값의 적절한 대체가 주를 이룬다.먼저 지시 확률 변수(indicator random variables)를 이용해 확률 변수 $X_i$를 i번째 베르누이 시행이 성공이면 1, 실패이면 0으로 설정한다. 따라서$X=\\sum^n_{i=1}X_i$가 성립하며,기댓값의 선형성에 의해 $\\mu=E[X]=E\\left[\\sum^n_{i=1}X_i\\right]=\\sum^n_{i=1}E[X_i]=\\sum^n_{i=1}p_i$이 성립하며 이는 $X-\\mu=\\sum^n_{i=1}(X_i-p_i)$을 성립하게 한다.이제 $X-\\mu$를 위에 구한 식으로 대체하면, 다음이 성립한다.\\(\\begin{align}E[e^{\\alpha(X-\\mu)}] &amp;=E[e^{\\alpha \\sum^n_{i=1}(X_i-p_i)}]\\\\&amp;=E\\left[\\prod^n_{i=1} e^{\\alpha(X_i-p_i)}\\right]\\\\&amp;=\\prod^n_{i=1}E[e^{\\alpha(X_i-p_i)}]\\end{align}\\)이때 상호 독립적인 확률 변수 $X_i$는 확률변수 $e^{\\alpha (X_i-p_i)}$ 또한 상호 독립적으로 만드므로, 기댓값의 정의와 $\\alpha &gt;0, q_i\\leq 1, e^{\\alpha q_i}\\leq e^\\alpha,e^{-\\alpha p_i}\\leq 1, e^x \\ge 1+x$등의 부등식들에 의해 다음이 성립한다.\\(\\begin{align}E[e^{\\alpha(X_i-p_i)}] &amp;= e^{\\alpha(1-p_i)}p_i+e^{\\alpha(0-p_i)}q_i\\\\&amp;= p_ie^{\\alpha q_i}+q_ie^{-\\alpha p_i}\\\\&amp;\\leq p_ie^\\alpha+1\\\\&amp;\\leq \\exp(p_ie^\\alpha)\\end{align}\\)여기서 $\\exp(x)=e^x$는 지수함수를 의미한다.따라서 $\\mu =\\sum^n_{i=1}p_i$이므로, 다음이 성립하며,\\(\\begin{align}E[e^{\\alpha(X-\\mu)}] &amp;= \\prod^n_{i=1}E[e^{\\alpha(X_i-p_i)}]\\\\&amp;\\leq \\prod^n_{i=1}\\exp(p_ie^\\alpha)\\\\&amp;= \\exp\\left(\\sum_{i=1}^n p_ie^\\alpha\\right)\\\\&amp;= \\exp(\\mu e^\\alpha)\\label{eq:Expectation to exp func}\\tag{5}\\end{align}\\)식 $\\refeq{eq:possibility bound by markov inequality}$, 식 $\\refeq{eq:possibility bound by markov inequality with exponential}$, 식 $\\refeq{eq:Expectation to exp func}$에 의해 다음이 성립한다.\\[\\Pr\\{X-\\mu \\geq r\\}\\leq \\exp(\\mu e^\\alpha -\\alpha r)\\label{eq:upper bound of possibility}\\tag{6}\\]이때, $\\alpha =\\ln(r/\\mu)$로 대입하면 다음과 같은 식이 나온다.\\[\\begin{align}\\Pr\\{X-\\mu \\geq r\\} &amp;\\leq \\exp(\\mu e^{\\ln(r/\\mu)}-r\\ln(r/\\mu))\\\\&amp;=\\exp(r-r\\ln(r/\\mu))\\\\&amp;= \\frac{e^r}{(r/\\mu)^r}\\\\&amp;= (\\frac{\\mu e}{r})^r\\end{align}\\]따름 정리(Corollary) 8언제나 같은 확률인 베르누이 시행에서는 이항 분포의 우측 꼬리에서 다음이 성립한다.성공확률이 p이며, 실패 확률이 $q=1-p$인 n번 시도하는 베르누이 시행에서 $r &gt; np$일 때, 다음이 성립한다.\\(\\Pr\\{X-np\\geq r\\}=\\sum^n_{k=\\lceil np+r \\rceil}b(k;n,p)=(\\frac{npe}{r})^r\\)증명이항 분포의 기댓값을 유도하는 식 $\\refeq{eq:expectation of binomial distribution}$에 의해 $\\mu= E[X]=np$이 성립한다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EA%B7%B8%EB%9E%98%ED%94%84.html": {
    title: "알고리즘 수학 기본-그래프",
    date: " Jul 9, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EA%B7%B8%EB%9E%98%ED%94%84.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 그래프(graphs)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 그래프(graphs)의 표기법, 정의, 속성 같은 기본적인 것만 배운다.앞서 올린 포스트 집합(sets)와 관계(relations)에 대한 글을 읽고 오는 것을 추천한다.유향 그래프와 무향 그래프유향 그래프와 무향 그래프의 정의유향 그래프(directed graph, digraph) G는 유한한 집합 V와 V의 원소 간의 이항관계 E의 쌍, (V, E)이다.집합 V는 G의 정점 집합(vertex set)이라고도 불리우며, 이들의 원소들을 정점들(vetices)이라고 부른다.집합 E는 G의 간선 집합(edge set)이라고 불리우며, 이들의 원소들을 간선들(edges)라고 부르며, 순서가 존재하는 쌍으로 표현한다. (예시: (1,2), (2,2))Figure 1의 (a) 그래프가 유향 그래프의 예시로, 간선은 화살표, 정점은 숫자가 적힌 원으로 표현하였다. 자기 자신으로 회귀하는 2번 정점과 같은 간선 또한 그래프에 나타날 수 있다.무향 그래프(undirected graph) G는 간선 집합 E가 유향 그래프와 달리 순서가 없는 쌍, 즉 원소가 둘인 집합을 간선으로 가지고 있지만, 표기는 동일하게 (u, v) 처럼 한다. 이는 즉, 두 그래프간의 선분의 방향성이 존재하지 않다는 것이며, 양방향이기도 하다는 의미이다.이때, 무향 그래프는 유향 그래프와 달리 자기 회귀(self-loop) 간선이 존재하면 안되며, 따라서 간선의 두 원소는 서로 다른 정점이어야 한다.유향 그래프에서 정점 A의 이웃(neighbor) 정점은 정점 A와 해당 이웃 정점으로 오는 간선과 가는 간선이 둘다 존재하는 정점이다.무향 그래프에서 정점 A의 이웃(neighbor) 정점은 정점 A와 간선이 존재하는 정점이다.무향 그래프와 유향 그래프의 전환무향 그래프 $G=(V,E)$를 유향 그래프로 바꾸려면, G의 간선 집합 E의 간선들 (u,v)를 모두 유향 간선 (u, v)와 (v, u)로 바꾸면 된다.반대로 유향 그래프 $G=(V, E)$를 무향 그래프로 바꾸려면, 자기 회귀(self-loop) 간선 (u, u)는 모두 삭제하고, 간선들 (u, v)을 중복이 존재하지 않도록 무향 간선 (u, v)으로 바꾸면 된다. 즉 (u, v) 간선과 (v, u) 간선이 둘다 존재한다면 무향 간선 (u, v)하나로만 바꾼다.그래프 용어무향 그래프와 유향 그래프의 정의들은 거의 유사하지만 일부 정의에서 다르다.부속과 인접(incident, adjacent)유향 그래프에서는 선분 (u, v)을 정점 u에서 부속 출발(incident from) 또는 떠나서(leaves) 정점 v에 부속 도착(incident to) 또는 도착하는 선분이라고 부른다.유향, 무향 그래프에서 선분 (u, v)를 정점 u, v에 부속(incident)한다고 표현하며, 정점 u와 v가 서로 인접(adjacent)하다고 표현한다.무향 그래프와 달리 유향 그래프에서는 정점 간의 인접 관계가 비대칭인데, 예를 들어 Figure 1 (a)에서 2번 정점은 1번 정점과 인접하지만 1번 정점은 2번 정점과 인접하지 않다. 즉, 유향 그래프에서 어떤 정점 A 입장에서 B와의 인접은 A 정점으로 도착할 수 있는 선분을  B 정점이 가지고 있는가를 의미한다.차수와 고립(degree, isolated)무향 그래프에서의 정점의 차수(degree)는 해당 정점에 부속된 간선들의 수이다. 예를 들어 Figure 1 (b)의 정점 2는 차수가 2이며, 정점 4처럼 차수가 0인 정점을 고립된(isolated) 정점이라고 한다.유향 그래프에서 진출 차수(outdegree)는 해당 정점에서 나가는 간선의 수를 의미하며, 진입 차수(indegree)는 반대로 들어오는 간선의 수를 의미한다.유향 그래프에서 정점의 차수는 진출 차수와 진입 차수의 합을 의미한다. Figure 1 (a)의 정점 2의 진입 차수는 2, 진출 차수는 3이므로 정점 2의 차수가 총 5이며, 이때 자기회귀 선분은 2번 더해진다는 것을 알 수 있다.경로와 간단 경로(path, simple path)시작 정점 $u$에서 도착 정점 $u’$로 가는 경로(path)는 정점들로 이뤄진 수열 $&lt;v_0,v_1,v_2,\\cdots,v_k&gt;$이며, 이때 $u=v_0,u’=v_k$이고, i가 $1,2,\\cdots, k$일 때, 선분 $(v_{i-1},v_i)$는 그래프의 선분 집합 E에 포함된다. 자기자신으로 가는 경로는 길이가 0인 경로이다.경로의 길이(length) k는 경로 내의 선분들의 수이며, 경로가 존재한다는 의미는, 해당 경로를 통해 시작 정점에서 도착 정점으로 도착 가능하다는 의미이다.이때, 경로의 정점들이 중복이 없다면, 간단 경로(simple path)라고 부른며, 예를 들면 &lt;1,2,5,4&gt; 경로는 간단 경로, &lt;2,5,4,5&gt;는 간단하지 않은 경로이다.부분 경로(subpath)는 경로 수열의 일부분의 연속된 수열이며, 경로와 마찬가지로 다음 정점과 간선으로 이어져있어야 한다.사이클(cycle)시작 정점과 도착 정점이 같은 길이가 1이상의 경로, 즉 $v_0=v_k$이면서, 최소 하나의 간선을 포함한 경로는 사이클(cycle)이라고 하며, 시작지점 $v_0$를 제외하고 나머지 정점들이 전부 중복이 없다면 간단 사이클이라고 부른다. 자기회귀는 길이가 1인 사이클이다.서로 다른 두 경로는 같은 사이클을 형성할 수 있는데, 예를 들어 Figure 1 (a)의 경로 &lt;1,2,4,1&gt;은 경로 &lt;2,4,1,2&gt;와 다른 경로이되 같은 사이클이다.수학적으로 표현하면 경로 $&lt;v_0,v_1,v_2,\\cdots,v_{k-1},v_0&gt;$와 경로 $&lt;v’0,v’_1,v’_2,\\cdots,v’{k-1},v’0&gt;$에서 $v’_i=v{(i+j)\\mod k}$를 만족하는 i가 존재한다면 같은 사이클이다.자기회귀가 없는 유향 그래프를 간단 그래프라고 한다.무향 그래프에서는 경로의 길이가 3 이상인 경로, 즉 중간 경유지가 존재하는 경로만 사이클로 인정된다.사이클이 존재하지 않는 그래프를 acyclic 그래프라고 한다.연결 그래프(connected graph)와 연결 요소 (connected component)무향 그래프에서 모든 정점들이 고립되지않았다면, 연결(connected) 그래프라고 하며, 그래프의 연결 요소(connected component)는 정점 A에서 B로 도달 가능한 가? 관계의 동치류들을 의미한다. 예를 들어 Figure 1 (b)에는 {1,2,5}, {3,6}, {4} 총 3개의 연결 요소가 존재한다. 연결 그래프에는 연결 요소가 1개 밖에 없다.연결 요소의 선분들은 연결 요소 내 정점만 연결되있는 선분이다, 즉 선분 (u, v)에서 u와 v는 언제나 같은 연결 요소 내에 존재해야 한다.만일 방향 그래프 내의 모든 쌍의 정점들이 서로 도달 가능하다면 강연결 그래프(strongly connected)가 된다.방향 그래프의 강연결 요소(strongly connected components)는 상호 도달 가능한 관계에 대한 정점으로 이루어진 동치류들이다. 강연결 요소가 한 그래프에 하나만 존재한다면 강연결 그래프가 된다.Figure 1 (a)에는 {1,2,4,5}, {3}, {6}, 총 3개의 강연결 요소가 존재한다.동형 그래프 (isomorphic graph)두 그래프 $G = (V, E)$와 $G’=(V’,E’)$에서, 간선 $(u,v) \\in E$일 때, 간선 $(f(u), f(v)) \\in E’$ 인 전단사 함수 $f:V \\rightarrow V’$ 가 존재할 시, 서로 동형(isomorphic) 그래프라고 한다. 즉, 두 그래프가 정점의 번호만 다르고 모양이 같을 때 동형 그래프라고 한다.Figure 2 (a)의 위 그래프와 아래 그래프는 정점의 번호만 1,2,…,6와 u,v,…,z로 서로 다를 뿐, 그림 상의 정점의 위치를 조정하고 그에 따라 선분도 조정하면 동일한 모형의 그래프가 된다. 예를 들어 $f(1)=u, f(2)=v … f(6)=z$ 같은 식으로 대입해보면 된다.하지만 Figure 2 (b) 오른쪽의 상하 그래프는 그리하여도 모양이 다르게 되므로 동형 그래프가 아니다.부분 그래프 (subgraph)만약,두 그래프 $G = (V, E)$와 $G’=(V’,E’)$에서, $V’ \\subseteq V, E’ \\subseteq E$일 경우, $G’$를 $G$의 부분 그래프(subgraph)라고 표현한다.부분 그래프에서는 $V’$를 이용해 식  $E’= {(u,v) \\in E:u,v \\in V’}$를 이용해 $E’$ 또한 도출할 수 있다.예를 들어 Figure 1 (c)의 그래프는 Figure 1 (a)의 부분 그래프이며, $V’={1,2,3, 6}$이므로 $E’={(1,2), (2,2), (6,3)}$임을 알 수 있다.기타 여러 종류의 그래프완전 그래프 (complete graph)소속한 모든 정점이 다른 모든 정점과 연결된 무향 그래프를 완전그래프라 한다.이분 그래프 (bipartite graph)이분그래프는 정점을 두 집합으로 나눈 뒤, 같은 집합의 정점 간에는 간선이 존재하지 않는 그래프를 의미한다.포레스트(forest)와 트리((free) tree)사이클과 고립 정점이 없는 무향 그래프, 즉 acyclic 무향 연결 그래프(acyclic undirected connected graph)를 트리(tree) 또는 자유 트리(free tree)라고 한다.하나 이상의 트리가 모인 무향 그래프, acyclic 무향 그래프를 포레스트(forest)라고 한다.포레스트 그래프는 시작 정점과 도착 정점이 같은 경로가 2개 이상 존재하지 않는다.방향 acyclic 그래프(directed acyclic graph)는 줄여서 DAG 라고도 부른다.다중 그래프(multigraph)어떤 두 정점 간의 간선이 중복되어 여러개 있거나, 자기 회귀(self-loop)가 존재하는 무향 그래프이다.하이퍼그래프 (hypergraph)그래프 내의 정점 간의 집합을 형성한 뒤, 해당 집합 간의 간선이 존재하는 그래프, 기존의 그래프 알고리즘을 활용 가능하다.수축 (contraction)어떤 무향 그래프의 G에 대해 새로운 정점들의 집합 $V’=V-{u,v}\\cup {x}$을 만들고, 정점 u와 v를 이어주는 모든 간선을 삭제한 뒤, u와 v 정점에 인접한 정점들을 새로 만든 정점 x과 연결하는 행위.쉽게 말해 두 정점과 그 사이의 간선을 하나의 새로운 정점으로 합치는 것이다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%96%89%EB%A0%AC.html": {
    title: "알고리즘 수학 기본-행렬",
    date: " Jul 10, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%96%89%EB%A0%AC.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "알고리즘을 위한 수학 - 행렬 (Matrices)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다. 일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 행렬의 표기법, 정의, 속성 같은 기본적인 것만 배운다.행렬과 행렬 연산 (Matrices and matrix operations)행렬과 벡터(Matrices and vectors)행렬(matrix)는 숫자들의 정방형 배열이다.\\(A=\\begin{pmatrix} a_{11}&amp;a_{12}  &amp;a_{13}  \\\\ a_{21}&amp;a_{22}  &amp;a_{23}  \\\\\\end{pmatrix}=\\begin{pmatrix} 1&amp;2  &amp;3  \\\\ 4&amp;5  &amp;6  \\\\\\end{pmatrix}\\)위의 예시는 $i=1,2, j=1,2,3$인 $2 \\times 3$행렬 $A=(a_{ij})$로, $a_{ij}$는 i열 j행의 원소를 의미한다. 주로 행렬은 대문자, 행렬 내의 원소는 소문자로 표기하면 예를 들어 $\\R^{m\\times n}$은 $m\\times n$ 크기의 실수 행렬을 의미한다.A의 전치 행렬(transpose matix)는 $A^T$로 표기하며, 행렬 A의 열과 행을 서로 맞바꾼 형태의 행렬이며 예를 들면 다음과 같다.\\(A^T=\\begin{pmatrix} 1&amp;4  \\\\ 2&amp;5  \\\\ 3&amp;6  \\\\\\end{pmatrix}\\)벡터(vector)는 숫자의 1차원 행렬로 예를 들면 다음과 같은 형태는 크기가 3인 벡터이며, 크기가 n일때의 벡터를 n-벡터라고도 부른다.\\(x=\\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\\\\\end{pmatrix}\\)벡터는 주로 소문자로 표기하며, i번째 원소는 $x_i$같은 형태로 표기한다. 많은 벡터의 표준적인 형태는 열 벡터(column vector)로, 위와 같은 형태이며 $n \\times 1$ 행렬 이라고 표기하며, 그에 따른 전치행렬인 행 벡터(row vector)는 $x^T=\\begin{pmatrix} 2&amp;3  &amp;5  \\end{pmatrix}$같은 형태이며, $1\\times n$ 형태이다.단위 벡터(unit vector) $e_i$는 i번째 원소가 1이고 나머지 모든 원소가 0인 벡터이다. 단위 벡터의 크기는 제각기가 될 수 있으며, 보통 문맥에 사용하는 다른 행렬과 같은 크기이다.영 행렬(zero matrix)은 모든 원소가 0인 행렬이다. 그러한 행렬은 보통 0으로 표기하며, 실제 숫자 0과 동일하지만 문맥에 따라 구분하며, 크기 또한 그러하다.정방 행렬(Square matrices)정방 행렬은 $n \\times n$ 꼴의 행렬로, 몇몇 특별한 형태의 행렬이 존재한다.대각 행렬 (diagonal matrix)\\[diag(a_{11},a_{22},\\dots,a_{nn})=\\begin{pmatrix} a_{11}&amp;0&amp;\\dots&amp;0  \\\\ 0&amp;a_{22}&amp;\\dots&amp;0  \\\\ \\vdots &amp; \\vdots&amp;\\ddots&amp;\\vdots \\\\ 0&amp;0&amp;\\dots&amp;a_{nn}  \\\\\\end{pmatrix}\\]대각 행렬은 $i\\neq j$인 곳에서 $a_{ij}=0$인 행렬로 위와 같은 형태를 하고 있다.항등 행렬 (identity matrix)\\[I_n=diag(1,1,\\dots,1)=\\begin{pmatrix} 1&amp;0&amp;\\dots&amp;0  \\\\ 0&amp;1&amp;\\dots&amp;0  \\\\ \\vdots &amp; \\vdots&amp;\\ddots&amp;\\vdots \\\\ 0&amp;0&amp;\\dots&amp;1  \\\\\\end{pmatrix}\\]$n\\times n$ 항등 행렬 $I_n$은 대각선 원소가 모두 1인 대각 행렬이며, n의 크기를 가지고 있다.항등행렬의 i번째 열은 단위 벡터 $e_i$이다.삼중 대각 행렬 (tridiagonal matrix)\\[T=\\begin{pmatrix} t_{11}&amp;t_{12}&amp;0&amp;0&amp;\\dots&amp;0&amp;0&amp;0  \\\\ t_{21}&amp;t_{22}&amp;t_{23}&amp;0&amp;\\dots&amp;0&amp;0&amp;0  \\\\ 0 &amp; t_{32}&amp;t_{33}&amp;t_{34}&amp;\\dots&amp;0&amp;0&amp;0 \\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\  0&amp;0&amp;0&amp;0&amp;\\dots&amp;t_{n-2,n-2}&amp;t_{n-2,n-1}&amp;0\\\\ 0&amp;0&amp;0&amp;0&amp;\\dots&amp;t_{n-1,n-2}&amp;t_{n-1,n-1}&amp;t_{n-1,n}\\\\ 0&amp;0&amp;0&amp;0&amp;\\dots&amp;0&amp;t_{n,n-1}&amp;t_{nn}\\\\\\end{pmatrix}\\]            삼중 대각 행렬 T는 $      i-j      &gt;1$일 때, $t_{ij}=0$인 행렬이다. 0이 아닌 원소는 대각선 원소와 대각선 원소의 바로 위 또는 아래 원소들 뿐이다.      상삼각 행렬 (upper-triangular matrix)\\[U=\\begin{pmatrix} u_{11}&amp;u_{12}&amp;\\dots&amp;t_{1n}  \\\\ 0&amp;u_{22}&amp;\\dots&amp;u_{2n}\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\0&amp;0&amp;\\dots&amp;u_{nn}\\end{pmatrix}\\]상삼각 행렬 U는 $i&gt;j$일 때, $u_{ij}=0$인 행렬로, 대각선 원소보다 아래의 원소들은 모두 0인 행렬이다.상삼각 행렬의 원소가 모두 0 아니면 1일 경우, 단위 상삼각 행렬(unit upper-triangular matrix)이라고 불린다.하삼각 행렬 (lower-triangular matrix)\\[L=\\begin{pmatrix} l_{11}&amp;0&amp;\\dots&amp;0\\\\ l_{21}&amp;l_{22}&amp;\\dots&amp;0\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\l_{n1}&amp;l_{n2}&amp;\\dots&amp;l_{nn}\\end{pmatrix}\\]하삼각 행렬 L은 $i&lt;j$일 때, $l_{ij}=0$인 행렬로, 대각선 원소보다 위의 원소들은 모두 0인 행렬이다.하삼각 행렬의 원소가 모두 0 아니면 1일 경우, 단위 하삼각 행렬(unit lower-triangular matrix)이라고 불린다.순열 행렬 (permutation matrix)\\[P=\\begin{pmatrix}0&amp;1&amp;0&amp;0&amp;0\\\\0&amp;0&amp;0&amp;1&amp;0\\\\1&amp;0&amp;0&amp;0&amp;0\\\\0&amp;0&amp;0&amp;0&amp;1\\\\0&amp;0&amp;1&amp;0&amp;0\\end{pmatrix}\\]순열 행렬 P는 각 열이나 행에 단 하나의 1만 존재하고, 나머지는 0인 행렬이다.이 순열 행렬에 벡터를 곱하면 원소들의 재배치가 일어나기 때문에 이를 순열 행렬이라고 부른다.대칭 행렬 (symmetirc matrix)\\[P=\\begin{pmatrix}1&amp;2&amp;3\\\\2&amp;6&amp;4\\\\3&amp;4&amp;5\\\\\\end{pmatrix}\\]대칭 행렬은 $A=A^T$를 만족하는 행렬이다.기본 행렬 연산 (Basic matrix operations)행렬과 벡터의 원소들은 실수, 복소수, 소수의 모듈로 결과 등 같은 수계(number system)에서 가져온 숫자로 이루어져 있다. 수계는 숫자들이 어떻게 더해지고 곱해지는지 정의하며, 이를 더욱 확장하여 행렬의 연산을 정의할 수 있다.행렬의 합과 차(matrix addtion and subtraction)행렬의 합(matrix addtion)은 다음과 같은 방법으로 이루어진다.만약, 행렬 $A=(a_{ij}), B=(b_{ij})$ 둘 모두 $m\\times n$행렬이라면, 그 둘의 합행렬 $C=(c_{ij})=A+B$은 마찬가지로 $m\\times n$ 행렬이다.또한, 행렬 C의 원소는 $c_{ij}=a_{ij}+b_{ij}$이며, 즉, 행렬 간의 합은 원소 수준에서 이루어짐을 알 수 있다.영행렬의 덧셈의 경우 식 $A+0=A=0+A$이 성립한다.$\\lambda$가 숫자이고, $A=(a_{ij})$가 행렬일 경우, $\\lambda A=(\\lambda a_{ij})$은 스칼라 곱(scalar multiple)이라고 하여, 행렬 A의 각 원소들에 $\\lambda$만큼 곱하면 된다.만약 행렬에 -1을 곱한 $-$A는 마찬가지로 각 원소에 -1을 곱한것과 같으며 $A+(-A)=0=(-A)+A$를 만족한다.또한, $A-B=A+(-B)$를 통해서 행렬의 차(matrix subtraction) 또한 정의할 수 있다.행렬의 곱(matrix multiplication)행렬의 곱(matrix multiplication)은 다음과 같이 정의할 수 있다. 두 행렬을 곱하려면, 두 행렬 A와 B가 서로 호환되어야(compatible)하며, 이는 A의 열의 갯수와 B의 행의 갯수가 동일함을 의미한다.만약, 행렬 $A=(a_{ik})$가 $m\\times n$이고, 행렬 $B=(b_{kj})$가 $n\\times p$이면, 곱의 결과 $C=AB$는 $m\\times p$가 된다.이때, 행렬 C의 각 원소는 다음과 같이 계산된다.\\(c_{ij}=\\sum^n_{k=1}a_{ik}b_{kj}\\)행과 열의 크기가 동일한 정방 행렬 곱 또한 마찬가지로 진행되며, 정방 행렬의 크기를 $n\\times n$이라고 할때, $n^3$번의 곱셈과 $n^2(n-1)$번의 덧셈을 시행하므로, $\\Theta(n^3)$만큼의 시간복잡도를 가지게 된다.항등 행렬 $I_m$은 $I_mA=AI_n=A$과 같이 곱의 결과가 동일한 행렬이다.영행렬을 행렬에 곱하면 $A0=0$와 같이 영행렬이 나온다.행렬 곱은 $A(BC)=(AB)C$처럼 교환법칙이 성립하며, 덧셈도 마찬가지이다.\\[A(B+C)=AB+AC\\\\(B+C)D=BD+CD\\]하지만 크기가 1 이상인 정방행렬은 교환법칙이 성립하지 않는다. 예를 들어 아래와 같은 정방 행렬이 둘 있다면,$A=\\begin{pmatrix}0&amp;1\\0&amp;0\\end{pmatrix},\\ B=\\begin{pmatrix}0&amp;0\\1&amp;0\\end{pmatrix}$곱 행렬의 두 결과는 다음과 같다.$AB=\\begin{pmatrix}1&amp;0\\0&amp;0\\end{pmatrix}$, $BA=\\begin{pmatrix}0&amp;0\\0&amp;1\\end{pmatrix}$행렬-벡터 간 곱(product)이나 벡터-벡터 간 곱은 벡터를 $n\\times 1$ 또는 $1\\times n$의 행렬로 간주하며, $m \\times n$ 행렬과 n-벡터의 곱은 m-벡터가 된다.만약 두 n-벡터 x, y 간의 곱은 아래와 같이 진행하며, $xy$의 결과는 하나의 숫자, 정확히는 $1\\times 1$ 크기의 행렬이 나오며, 이를 두 벡터간의 내적(inner product)이라고 한다.\\[x^Ty=\\sum^n_{i=1}x_iy_i\\]행렬 $xy^T$는 $n\\times n$ 크기의 행렬이며, 이를 x와 y 간의 외적(outer product)이라고 하며, 외적의 원소의 값 $z_{ij}=x_iy_j$이다.n-벡터의 (유클리드) 노름 ((euclidean) norm) $|x|$은 아래와 같이 정의되며, n-차원 유클리드 공간에서의 x의 길이를 의미한다.\\(\\|x\\|=(x_1^2+x_2^2+\\cdots+x_n^2)^{1/2}=(x^Tx)^{1/2}\\)기본 행렬 성질(Basic matrix properties)역행렬(inverse), 일차 독립(linear independence), 일차 종속(linear dependence), 행렬 계수(rank)와 행렬 식(determinants), 정부호 행렬(positive-definite matrix) 등에 대해 알아본다.역행렬(Matrix inverses)역행렬은 $A^{-1}$으로 표현되며, $AA^{-1}=I_n=A^{-1}A$을 만족하는 성질을 가지고 있다. 예를 들면 아래와 같다.\\[\\begin{pmatrix}1&amp;1\\\\1&amp;0\\end{pmatrix}^{-1}=\\begin{pmatrix}0&amp;1\\\\1&amp;-1\\end{pmatrix}\\]영행렬이 아닌 정방행렬들은 역행렬을 가지고 있지 않은 경우가 존재하며, 이를 비가역(noninvertible) 행렬 또는 특이(singualr) 행렬이라고 하며, 예시로 $\\begin{pmatrix}1&amp;0\\1&amp;0\\end{pmatrix}$가 존재한다. 반대로 역행렬이 존재하면 가역(invertible) 행렬, 비특이(nonsingular) 행렬, 정칙(regular) 행렬이라고 한다.한 행렬의 역행렬이 존재하면, 역행렬은 유일하며, 비특이 행렬 A와 B가 같은 $n \\times n$크기라면, $(BA)^{-1}=A^{-1}B^{-1}$이 성립한다.또한 역행렬 또한 $(A^{-1})^T=(A^T)^{-1}$와 같이 전치행렬이 존재한다.만약, 벡터들의 모임 $x_1, x_2,\\dots,x_n$이 존재할 때, $c_1x_1+c_2x_2+\\cdots+c_nx_n=0$을 만족하는 하나라도 0이 아닌 계수들 $c_1,c_2,\\dots, c_n$가 존재하면, 일차 종속(linear dependence)이라고 한다.예를 들어 $x_1=\\begin{pmatrix}1&amp;2&amp;3\\end{pmatrix},x_2=\\begin{pmatrix}2&amp;6&amp;4\\end{pmatrix},x_3=\\begin{pmatrix}4&amp;11&amp;9\\end{pmatrix}$의 경우, $2x_1+3x_2-2x_3=0$이 성립하므로, 일차 종속이다.반대의 경우에는 일차 독립(linear independence)이라고 하며, 예를 들어 항등 행렬의 각 열들은 일차 독립적이다.행렬 랭크영 벡터가 아닌 $m\\times n$ 행렬 A의 열 랭크(column rank)는 A 행렬의 일차 독립인 열 중 최대 크기이다. 비슷하게, 행 랭크(row rank)는 A 행렬의 일차 독립인 행 중 최대 크기이다. 놀랍게도 행 랭크와 열 랭크는 언제나 동일하므로, 간단하게 랭크(rank)라고도 부른다.$m\\times n$ 행렬의 랭크는 0와 $\\min(m,n)$ 사이이며, 예를 들어 크기가 0인 행렬의 랭크는 0이며, $n\\times n$의 항등 행렬의 랭크는 n이다. 랭크의 또 다른 정의는 영 행렬이 아닌 $m\\times n$ 크기 행렬 A의 랭크는 $A=BC$를 만족하는  $m\\times r$ 크기 행렬 B와 $r \\times n$크기 행렬 C가 존재할 때, 최대한 작은 r이다.정방 $n\\times n$ 행렬의 랭크가 n이라면, 풀 랭크(full rank)라고 하며, $m\\times n$ 행렬의 랭크가 n일 경우, 풀 열 랭크라고 한다.랭크의 성질다음은 랭크의 정리에 대해 알아보자.정리(Theorem) 1정방 행렬이 비특이 행렬일 경우 풀 랭크이다.정리(Theorem) 2행렬 A에 대한 널 벡터(null vector)는 $Ax=0$을 만족하는 영이 아닌 벡터 x이다.행렬 A가 널 벡터를 가지고 있지 않으면, 풀 열 랭크(full column rank)를 가지고 있다.따름 정리(Corollary) 3정방 행렬 A가 널 벡터를 가지고 있으면 특이 행렬이다.크기가 1보다 큰 $n\\times n$크기의 행렬 A의 ij번째 소행렬식(minor)은 i번째 행과 j번째 열이 삭제된 $(n-1)\\times(n-1)$ 행렬 $A_{[ij]}$이며, 행렬식(determinant, $\\det(A)$)은 아래와 같이 소행렬식을 이용해 재귀적으로 정의 된다.\\[\\det(A)=\\left{\\begin{matrix} a_{11}&amp;if\\ n=1\\ \\sum^n_{j=1}(-1)^{1+j}a_{1j}\\det(A_{[1j]})&amp;if\\ n&gt;1\\end{matrix}\\right.\\]식 $(-1)^{i+j}\\det(A_{[ij]})$은 원소 $a_{ij}$의 여인수(cofactor)라고 부른다.정리(Theorem) 4 (행렬식 성질)정방 행렬 A의 행렬식은 다음과 같은 성질을 가지고 있다.  만약 행렬 A의 어느 행이나 열이 0이라면 $\\det(A)=0$이다.  행렬 A의 어떤 열이나 행의 원소가 $\\lambda$만큼 곱해지면, A의 행렬식 또한, $\\lambda$ 로 곱해진다.  한 행의 원소가 다른 행의 원소로 더해지거나 한 열의 원소가 다른 열의 원소로 더해져도, A의 행렬식은 변하지 않는다.  A의 행렬식은 A의 전치행렬 $A^T$의 행렬식과 동일하다. ($\\det(A)=\\det(A^T)$)  어떤 두 행이 서로 바뀌거나 어떤 두 열이 서로 바뀌면 A의 행렬식에 -1을 곱한 것($-\\det(A)$)과 같다.  두 정방 행렬 A와 B에 대해 $\\det(AB)=\\det(A)\\det(B)$정리(Theorem) 5만일 $\\det(A)=0$이면, $n\\times n$ 행렬 A는 특이 행렬이다.정부호 행렬(positive-definite matrix)$x\\neq0$인 모든 n-벡터에 대해서 $x^TAx&gt;0$를 성립하면 행렬 A는 정부호행렬(positive-definite matrix)이다.예를 들어, 항등 행렬은 영벡터가 아닌 벡터 $x=\\begin{pmatrix}x_1&amp;x_2&amp;\\cdots x_n\\end{pmatrix}^T$에 대해 다음이 성립하므로, 정부호 행렬이다.\\(x^TI_nx=x^Tx=\\sum^n_{i=1}x^2_i&gt;0\\)Theorem 6풀 열 랭크를 가진 모든 행렬 A에서, $A^TA$ 행렬은 언제나 정부호 행렬이다.증명$x$가 영벡터가 아닌 모든 벡터에서 $x^T(A^TA)x&gt;0$를 충족해야하며, 모든 x에서 다음이 성립한다.$x^T(A^TA)x=(Ax)^T(Ax)=|Ax|^2$이떄, $|Ax|^2$은 벡터 $Ax$의 요소들의 제곱의 합이며, 언제나 $|Ax|^2\\ge0$를 만족한다.만약, $|Ax|^2=0$이라면, Ax의 모든 원소는 0이며, 곧 $Ax=0$이 된다.해열 A가 풀 열 랭크를 가지므로, $Ax=0$은 정리 2번에 의해 $x=0$을 의미하며, 따라서 $A^TA$는 정부호 행렬이 된다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%99%95%EB%A5%A0.html": {
    title: "알고리즘 수학 기본-확률",
    date: " Jul 10, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%99%95%EB%A5%A0.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 확률(Probability)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 확률의 표기법, 정의, 속성 같은 기본적인 것만 배운다.들어가기에 앞서 집합에 대한 기본적인 이해가 필요하므로 알고리즘을 위한 수학 -집합 편을 보고 오는 것을 추천한다.확률(Probability)확률은 확률적, 무작위성을 띄고 있는 알고리즘을 설계하고, 분석하는 중요한 장치이다.우리는 확률을 확률적으로 더 이상 쪼개질 수 없는 최소 단위 사건인 근원 사건(elementary events)을 기본 원소로 하는 집합인 표본 공간(sample space) S에 대하여 정의한다.예를 들어 2개의 동전 튕기기에서 각 튕김의 결과인 앞면, 뒷면을 각각 H, T 라고 정의할 때, 이를 근원 사건은 ${H, T}$이 존재하며, 표본 공간 S는 아래 같이 된다.$S={HH, HT, TH, TT}$사건(event)는 표본 공간 S에 대한 부분집합으로, 사건의 모음이라 할 수 있다. 예를 들어 앞면 하나, 뒷면 하나만 나올 사건은 ${HT, TH}$이다.표본 공간과 같은 전체 근원 사건들의 집합 사건 S를 전사건(cetrain event)라고 하며, 사건 $\\O$를 공사건(null event)이라 한다.두 사건 A와 B가 만약 $A\\cap B=\\O$라면 상호배타적인 관계라고 하며, 모든 근원사건들은 서로에게 상호배타적이다.가끔 표본 공간 S에 속한 근원사건 s를  사건 ${s}$로 생각하기도 한다.확률에 대한 공리(Axioms of probability)표본 공간 S에서의 확률 분포 $\\Pr{}$는 S의 사건들을 실수로 투영한 것으로, 예를 들어 $\\Pr{A}$는 사건 A가 일어날 확률이다. 또한, 다음 같은 확률 공리를 만족한다.  모든 사건 A에 대해 $\\Pr{A}\\geq0$를 만족  $\\Pr{S}=1$, 즉 모든 사건들의 합은 1, 이는 정규화(normalization)를 가능케 한다.  모든 상호배타적 사건 A, B에 대하여 $\\Pr{A\\cup B}=\\Pr{A}+\\Pr{B}$를 성립,          정확히는, 유한 또는 계수 가능 무한 이벤트 수열 $A_1,A_2,\\dots$에 대해 모든 쌍은 아래와 같이 상호 배타적이다. (아래 이산 확률 분포 참고)      즉, 각 사건이 일어날 확률들의 합은 모든 확률의 합이다.      \\[\\Pr\\begin{Bmatrix}\\bigcup_iA_i\\end{Bmatrix}=\\sum_i\\Pr\\{A_i\\}\\]앞서 배웠던 집합에 대한 공리와 이론이 많은 연관을 가지고 있다.먼저,      $\\Pr{\\O}=0$, 즉 공사건의 확률은 0이다.        만약 $A\\subseteq B$라면,  $\\Pr{A}\\leq \\Pr{B}$이며,        여사건 $\\bar{A}$은 $S-A$로, 확률은 $\\Pr{\\bar{A}}=1-\\Pr{A}$이 된다.        모든 경우의 두 사건 A, B에 대하여 다음이 성립한다.  \\[\\begin{align}\\Pr\\{A\\cup B\\}&amp;=\\Pr\\{A\\}+\\Pr\\{B\\}-\\Pr\\{A\\cap B\\}\\tag{1}\\label{eq:equal}\\\\&amp;\\leq \\Pr\\{A\\}+\\Pr\\{B\\}\\tag{2}\\label{eq:bound}\\end{align}\\]이산 확률 분포 (Discrete probability distributions)  주사위 던지기에 대한 이산 확률 분포, 각 사건의 확률들이 동일한 값을 가진 다른 사건이 존재하거나 다른 확률의 배수인 것 등을 확인할 수 있는데, 이산 확률 분포는 확률 변수가 가질 수 있는 값의 갯수가 가산개라는 특징이 있다.만약 유한하거나 계수 가능 무한한 표본 공간 S에 대해서 확률 분포가 정의된다면, 이를 이산 확률 분포 (Discrete probability distributions)라고 한다.상호배타적인 근원사건으로 구성된 사건 A에 대해 다음이 성립한다.$\\Pr{A}=\\sum_{s\\in A}\\Pr{s}$만약 표본 공간 S가 유한하다면, S에 속한 모든 근원사건 s의 확률은 다음과 같이 된다.            $\\Pr{s}=1/      S      $      이는 균등 확률 분포(uniform probability distribution)를 나타나게 한다.  균등 분포의 예, 두 사건 a, b의 확률이 같음을 알 수 있다.            예를 들어 앞뒤가 나올 확률이 1/2로 동일한 동전이 있을 때, 코인을 n번 튕기면, 표본 공간 $S ={H,T}^n,      S      =2^n $에 대한 균등 확률 분포를 얻을 수 있다.      우리는 S의 근원 사건을 H와 T로 이루어진 길이 n의 문자열로 나타낼 수 있으며, 각 문자열의 확률은 $(1/2)^n$이다.  예를 들어, 동전을 3번 던진다면, HHH, HHT, HTH, THH, HTT, THT, TTH, TTT 총 8개의 문자열이 나올 것이며, 각 확률은 1/8이다.            사건 A는 정확히 k번의 앞면과 n-k번의 뒷면이 나올 방법이며, 크기가 $      A      =\\binom{n}{k}$인 S의 부분집합이다.      사건 A의 확률은 $\\Pr{A}=\\binom{n}{k}/2^n$이다.                              예를 들어 3번 동전을 던져 2번의 앞면과 3-2번의 뒷면이 나올 사건 A가 포함하는 근원 사건은 HHT, HTH, THH이며,  사건 A의 크기는 $          A          =\\binom{3}{2}=\\frac{3!}{2!1!}=3$이며, 사건 A의 확률은 $\\Pr{A}=\\binom{3}{2}/2^3=3/8$이다.                    연속 균등 확률 분포 (Continuous uniform probability distribution)연속 균등 확률 분포 (Continuous uniform probability distribution)는 표본 공간의 일부 부분집합이 사건으로 여겨지지 않는 확률 분포의 예이다.연속 균등 확률 분포는 a&lt; b인 닫힌 실수 구간 [a,b] 사이로 정의되며, [a, b] 사이의 실수 점들이 무수히 많기 때문에 모든 점들이 같은 확률을 가진다면, 확률의 총합이 1이 넘어가버린다. 따라서, S의 일부 부분집합에만 확률을 적용한다.  보통 특정 구간 [a, b]나 (a, b)의 유한 또는 가산 가능한 부분집합이나, 좀 더 복잡한 집합들을 포함한다.일정 구간 [c,d]에 대해, $a\\leq c \\leq d \\leq b$를 만족할 때, 사건 [c, d]의 확률은 $\\Pr{[c,d]}=\\frac{d-c}{b-a}$로 정의 된다.만약, 구간이 [x, x]처럼 시작과 끝이 동일한 경우에 확률은 0이 된다.닫힌 구간 [c,d]의 양 끝단 지점, [c, c]와 [d, d]를 지운 구간을 열린 구간 (c, d)라고 표기한다고 하면, $[c,d]=[c,c]\\cup(c,d)\\cup[d,d]$를 만족하며, 양 끝단의 확률은 0이므로, $\\Pr{[c,d]}=\\Pr{(c,d)}$를 만족한다.조건부 확률과 독립성(Conditional probability and independece)조건부 확률은 부분적인 사전 지식이 있을 때의 확률을 의미하며, 예를 들자면, 동전 1개 이상의 앞면이 확정되었을 때, 동전 2개가 전부 앞면일 확률이다.원래는 둘다 앞면일 확률은 1/4이겠지만, 동전 1개 이상이 앞면으로 확정되었으므로, 뒷면이 2개가 나올 확률이 없어져, 1/3으로 바뀐다.사건 B가 확정된 상황에서 A가 일어날 조건부 확률($\\Pr{A|B}$)은 아래와 같이 정의 된다. \\(\\Pr\\{A|B\\}=\\frac{\\Pr\\{A \\cap B\\}}{\\Pr\\{B\\}} \\tag{3} \\label{eq:conditional probability}, where\\ \\Pr\\{B\\}\\neq 0\\)이때, $A \\cap B$는 사건 A와 사건 B가 같이 일어난 결과(outcome)(=둘 다 만족하는 결과)의 집합이며, 이때 결과(outcome)은 표본 공간의 원소를 의미한다.$A\\cap B$에 속한 결과들은 사건 B의 근원 사건이기도 하므로, $\\Pr{B}$로 나누어 정규화되며, 따라서 사건 B의 근원사건의 확률을 모두 더하면 1이 나온다.            따라서 B가 확정된 A의 조건부 확률은 사건 $A \\cap B$의 확률과 사건 B의 확률 간의 비율이며, 예를 들면 사건 A는 두 동전이 모두 앞면이 나올 확률이며, B는 적어도 하나의 동전이 앞면일 확률이다. $\\Pr{A      B}=(1/4)/(3/4)=1/3$이 나옴을 알 수 있다.      만약 두 사건이 $\\Pr{A\\cap B}=\\Pr{A}\\Pr{B} \\tag{4} \\label{eq:independent condition}$가 성립된다면, 두 사건은 독립적(independent)이라고 한다.  예를 들어 서로 다른 두 동전의 앞면이 나올 확률은 각각 1/2, 1/2이며, 두 동전다 앞면이 나올 확률은 (1/2)(1/2) =1/4이므로, 위 식을 만족하여 이 두 사건은 서로 독립적이다.            위의 식 $\\eqref{eq:independent condition}$은 $\\Pr{B}\\neq 0$일 때, $\\Pr{A      B}=\\Pr{A}$과 동치이다.      만약 1개 이상의 사건들 $A_1,A_2,\\dots,A_n$의 집합이 쌍방향 독립적(pairwise independent)이라면, $1\\leq i&lt;j\\leq n$에 대해 $\\Pr{A_i\\cap A_j}=\\Pr{A_i}\\Pr{A_j}$을 만족한다.만약 1개 이상의 사건들 $A_1,A_2,\\dots,A_n$의 집합이 상호 독립적(mutually independent)이라면, $1\\leq i_n&lt;i_{n+1}\\leq n$에 대해 $\\Pr{A_{i_1}\\cap A_{i_2}\\cap\\cdots\\cap A_{i_k} }=\\Pr{A_{i_1}}\\Pr{A_{i_2}}\\cdots\\Pr{A_{i_k}}$을 만족한다.만약 $A_1$을 첫번째 동전이 앞면이 나오는 사건, $A_2$를 두번째 동전이 앞면이 나오는 사건, $A_3$를 두 동전이 서로 다른 면이 나오는 사건이라고 하면, 확률은 다음과 같다.\\({align}\\Pr\\{A_1\\}=1/2,\\\\\\Pr\\{A_2\\}=1/2,\\\\\\Pr\\{A_3\\}=1/2,\\\\\\Pr\\{A_1 \\cap A_2\\}=1/4,\\\\\\Pr\\{A_1 \\cap A_3\\}=1/4,\\\\\\Pr\\{A_2\\cap A_3\\}=1/4,\\\\\\Pr\\{A_1\\cap A_2\\cap A_3 \\}=0\\) {algin}$1\\leq i &lt; j \\leq 3$에서 $\\Pr{A_i \\cap A_j}=\\Pr{A_i}\\Pr{A_j}=1/4$를 만족하므로, 쌍방향 독립적이지만,$\\Pr{A_1\\cap A_2\\cap A_3}=0$이며, $\\Pr{A_1}\\Pr{A_2}\\Pr{A_3}=1/8\\neq0$이므로 상호 독립적이지 않다.베이즈 정리(Bayes’s theorem)조건부 확률의 정의 $\\eqref{eq:conditional probability}$와 교환 법칙 $A\\cap B =B\\cap A$에 의해 두 사건 A와 B가 확률이 0이 아닌 사건일 때, 다음과 같은 식이 성립한다.\\(\\Pr\\{A \\cap B\\}&amp;= \\Pr\\{B\\}\\Pr\\{A|B\\}\\\\\\label{eq:eq by commutative law}\\tag{5}&amp;=\\Pr\\{A\\}\\Pr\\{B|A\\}\\)이를 통해 다음과 같은 식, 베이즈 정리(Bayes’s theorem)를 얻을 수 있다.\\(\\Pr\\{A|B\\}=\\frac{\\Pr\\{A\\}\\Pr\\{B|A\\}}{\\Pr\\{B\\}} \\label{eq:Bayes's theorem} \\tag{6}\\)분모 $\\Pr{B}$는 정규화 상수(normalizing constant)로, 다음과 같은 식으로 변경할 수 있다.$B=(B\\cap A)\\cup(B\\cap\\bar{A})$이고, $B \\cap A$와 $B \\cap \\bar{A}$가 상호배타적 사건일 때, 다음이 성립한다.\\(\\begin{align}\\Pr\\{B\\}&amp;=\\Pr\\{B\\cap A\\}+\\Pr\\{B\\cap \\bar{A}\\}\\\\&amp;=\\Pr\\{A\\}\\Pr\\{B|A\\}+\\Pr\\{\\bar{A}\\}\\Pr\\{B|\\bar{A}\\}\\end{align}\\)이 값을 식 $\\eqref{eq:Bayes’s theorem}$의 분모와 바꾸면, 다음과 같은 베이즈 정리의 동치 형태 식이 나온다.\\(\\Pr\\{A|B\\}=\\frac{\\Pr\\{A\\}\\Pr\\{B|A\\}}{\\Pr\\{A\\}\\Pr\\{B|A\\}+\\Pr\\{\\bar{A}\\}\\Pr\\{B|\\bar{A}\\}} \\tag{7}\\label{eq: equivalent form of Bayes's theorem}\\)베이즈의 정리는 조건부 확률을 구하는데 편리하다.예를 들어, 무조건 앞면만 나오게 되는 편향된(biased) 동전과 확률이 동일한 공평한 동전이 존재한다고 하자.이 두 동전 중 하나를 무작위로 골랐고, 해당 동전을 2번 튕겼을 때 두번 다 앞면이 나왔다고 가정하자, 이때 이 동전이 편향된 동전이였을 확률은 얼마나 되는가?            이를 베이즈 정리를 이용해 풀 수 있는데, 사건 A를 편향된 동전을 골랐을 확률, 사건 B를 두 번의 기회 전부 앞면이 나올 확률이라고 할 때, $\\Pr{A      B}$를 구해보자.      $\\Pr{A}=1/2$, $\\Pr{B|A}=1$, $\\Pr{\\bar{A}}=1/2$, $\\Pr{B|\\bar{A}}=1/4$(공평한 동전의 경우 1/4 확률로 두번 다 앞면이 나올 확률)이며, 다음과 같이 구하게 된다.\\(\\Pr\\{A|B\\}=\\frac{(1/2)\\cdot 1}{(1/2)\\cdot 1+ (1/2)\\cdot(1/4)}=4/5\\)"
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%99%95%EB%A5%A0%20%EB%B3%80%EC%88%98.html": {
    title: "알고리즘 수학 기본-확률 변수",
    date: " Jul 10, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%99%95%EB%A5%A0%20%EB%B3%80%EC%88%98.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 확률 변수(random variables)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.추가로 _존이(mykepzzang)님의 블로그(https://blog.naver.com/PostList.naver?blogId=mykepzzang)_에서 내용을 발췌하였다.이 장에서는 확률의 표기법, 정의, 속성 같은 기본적인 것만 배운다.들어가기에 앞서 확률에 대한 기본적인 이해가 필요하므로 알고리즘을 위한 수학 -확률편을 보고 오는 것을 추천한다.확률 변수(random variables)이산 확률 변수와 연속 확률 변수(discrete random variables and continuous random variables)확률 변수는 표본 공간의 각 원소사건을 특정 실수에 대응시켜 확률이 나오게 하는 함수이다.예를 들어, 동전 두개를 던져, 앞면이 2개 나올 확률은 1/4, 앞면 1개, 뒷면 1개가 나올 확률은 1/2, 뒷면 2개가 나올 확률은 1/4가 된다.이때, 앞면의 갯수는 앞에서 부터 2개,1개,0개가 되는데, 이를 확률 변수로 이용할 수 있다.즉 앞면의 갯수 확률 변수 X 0, 1, 2에 따라 다음과 같다.\\(P(X=0)=1/4\\\\P(X=1)=1/2\\\\P(X=2)=1/4\\)확률변수는 가능한 결과에 대한 실수값을 할당하여 이를 통해 확률 분포를 유도할 수 있게 해준다.이산 확률 변수(Discrete random variables) X는 유한 또는 가산 무한 표본 공간 S를 입력으로 받아, 실수를 출력하는 함수이다.연속 확률 변수(Continuous random variables) X는 적분을 이용하여, 불가산 무한 표본 공간에도 확률 변수를 이용할 수 있게 한 함수이다.확률 질량 함수와 확률 밀도 함수(Probability Mass Function and Probability Density Function)확률 변수 X와 실수 x 에 대해 사건 X=x를 ${s \\in S : X(s)=x}$로 정의할 때, 다음과 같이 표현할 수 있다.\\(\\begin{align}&amp;이산\\ 확률\\ 변수의\\ 확률\\ 질량\\ 함수 :\\\\&amp;\\Pr\\{X=x\\}=\\sum_{s\\in S:X(s)=x}\\Pr\\{s\\}\\\\&amp;연속\\ 확률\\ 변수의\\ 확률\\ 밀도\\ 함수 :\\\\&amp;\\Pr\\{X=x\\}=\\int_{-\\infty}^\\infty \\Pr\\{s\\}\\end{align}\\)이때, 이산 확률 변수 X에 관한 확률의 함수 $f(x)=\\Pr{X=x}$를 확률 질량 함수(probability Mass Function) Pr(X)이라고 한다.위의 예시에서는 $P(X)={}_2 C_x\\cdot(1/2)^x(1/2)^{2-x}={}_2 C_x \\cdot (1/4)$이 될 것이다.연속 확률 변수 처럼 연속적인 공간에서는 확률 질량 함수가 아닌 확률 변수 X에 대한 확률 밀도 함수(probability density function)라고 부르며,확률 질량 함수와 확률 밀도 함수 둘다 확률 정리에 의해 $\\Pr{X=x}\\geq 0$이며,  $\\sum_x\\Pr{X=x}=1$이다.예를 들어, 두개의 6면 주사위의 결과 실험은 표본 공간 내에서 6 * 6, 총 36개의  근원 사건으로 이루어져 있다. 확률 분포가 균등하다고 가정하면, 각 근원 사건 $s\\in S$의 확률은 정확히 $\\Pr{s}=1/36$이 된다.확률 변수 X를 “주사위 두 값 중의 최댓값”이라고 정의하면, $\\Pr{X=3}=5/36$이 되는데, 최대값이 3이 되는 방법은 (1,3), (2,3), (3,3), (3,2), (3,1) 총 5가지이기 때문이다.여러개의 확률 변수를 이용할 수도 있다. 확률 변수 X, Y에 대해 함수 $f(x,y)=\\Pr{X=x\\ and\\ Y=y}$를 X와 Y의 결합 확률 밀도 함수(joint probability density function)라고 한다.            고정된 값 x, y에 대하여 각각 식 $\\Pr{X=x}=\\sum_y\\Pr{X=x\\ and\\ Y=y}$과 식 $\\Pr{Y=y}=\\sum_x\\Pr{X=x\\ and\\ Y=y}$이 성립하며, 확률 편에서 배웠던 조건부 확률의 정의를 이용해 식 $\\Pr{X=x      Y=y}=\\frac{\\Pr{X=x\\ and\\ Y= y}}{\\Pr{Y=y}}$을 얻을 수 있다.      모든 x와 y에 대해 사건 X=x와 Y=y은 독립적이라 가정하면 두 확률 변수 X와 Y가 서로 독립적이며,만약 모든 x와 y에 대해 식 $\\Pr{X=x\\ and\\ Y=y}=\\Pr{X=x}\\Pr{ Y=y}$이 성립한다면 두 확률 변수 X와 Y는 서로 동일하다는 의미이다.동일한 표본 공간에 정의된 여러 확률 변수의 집합으로 곱, 합, 또는 다른 함수를 통해 새로운 확률 변수들을 정의할 수 있다.확률 변수의 기대값 (Expected value of a random variable)확률 변수의 분산을 가장 간단하고 유용하게 요약하는 방법은 바로 값들의 평균을 취하는 것이다.이산 확률 변수 X에 대해 기대값(expected value), 기대치(expactation), 혹은 평균(mean)은 다음 식과 같다.\\(이산 확률 변수의\\ 기대값 :\\\\E[X]=\\sum_x x\\cdot\\Pr\\{X=x\\}\\tag{1}\\label{eq:expected value of discrete random variable X}\\)이 식은 예상치가 수렴 또는 유한할 때, 적합하며 $\\mu_x$ 또는 $\\mu$라고 표기한다.확률 변수 X의 확률분포함수가 $f(x)=\\Pr{X=x}$일 때, 연속 확률 변수의 기대값은 아래와 같다.\\(\\begin{align}  \\\\&amp;연속 확률 변수의\\ 기대값:   \\\\&amp;E(X)=\\int^\\infty_{-\\infty}x\\cdot \\Pr\\{X=x\\}dx  \\end{align}\\)앞면(H) 하나당 3달러를 얻고 뒷면(T) 하나당 2달러를 잃는 동전을 두개 튕기는 게임에서 확률 변수 X의 기대값은 다음과 같은 방법으로 구한다.\\(\\begin{align}E[X]&amp;=6\\cdot\\Pr\\{2\\small{H}'s\\}+1\\cdot\\Pr\\{1\\small{H},1\\small{T}\\}-4\\cdot \\Pr\\{2\\small{T}'s\\}\\\\&amp;=6(1/4)+1(1/2)-4(1/4)\\\\&amp;=1\\\\\\end{align}\\)기대값의 선형성(linearity of expectation)두 확률 변수의 합의 기대값은 두 예상치의 합과 같으며, 이는 다음과 같은 식으로 표현된다.$1.\\ E[X+Y]=E[X]+E[Y]$이를 연속 확률 변수의 기대값의 정의를 이용해 증명하자면 다음과 같다.\\(\\begin{align}E(X+Y)&amp;=\\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}(x+y)\\cdot f(x,y)\\ dxdy\\\\&amp;=\\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}x\\cdot f(x,y)+y\\cdot f(x,y)\\ dxdy\\\\&amp;=\\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}x\\cdot f(x,y)\\ dxdy+\\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}y\\cdot f(x,y)\\ dxdy\\\\&amp;=E(X)+E(Y)\\end{align}\\)이외에도 확률 변수를 이용하여 아래와 같은 다른 성질들을 증명 할 수 있다.\\(\\begin{align}&amp;2.\\ E(aX+b)=aE(X)+b\\\\&amp;3.\\ E(XY)=E(X)E(Y)\\\\&amp;4.\\ E(aX^2+bX+c)=aE(X^2)+bE(X)+c\\\\&amp;5.\\ E(aX+bY)=aE(X)+bE(Y)\\end{align}\\)모든 확률 변수 X에 대해, 함수 g(x)가 새로운 확률 변수 g(X)를 정의할 수 있고, g(X)의 기대값이 정의될 수 있다면, 아래와 같은 식이 성립된다.\\(E[g(X)]=\\sum_xg(x)\\cdot \\Pr\\{X=x\\}\\)이때 $g(x)=ax$로 놓고, a를 상수로 본다면, $E[aX]=aE[X]$가 성립된다.결과적으로 기대값은 선형이며, 식 $E[aX+Y]=aE[X]+E[Y]$ 또한 성립된다.만약 두 확률 변수 X와 Y가 서로 독립적이고 정의된 기대값이 존재한다면, 아래와 같은 결과가 나온다.\\(\\begin{align}E[XY]&amp;=\\sum_x\\sum_yxy\\cdot\\Pr\\{X=x\\ and\\ Y=y\\}\\\\&amp;=\\sum_x\\sum_yxy\\cdot \\Pr\\{X=x\\}\\Pr\\{Y=y\\}\\\\&amp;=\\left(\\sum_xx\\cdot\\Pr\\{X=x\\}\\right)\\left(\\sum_yy\\cdot\\Pr\\{Y=y\\}\\right)\\\\&amp;=E[X]E[Y]\\end{align}\\)보통, n개의 확률 변수 $X_1,X_2,\\dots,X_n$가 상호 독립적이라면, 선형성에 의해 아래 같은 식이 성립한다.$E[X_1X_2\\cdots X_n]=E[X_1]E[X_2]\\cdots E[X_n]$이러한 성질을 기대값의 선형성(linearity of expectation)이라고 하며, X와 Y가 서로 독립적이지 않아도 성립하며, 유한 또는 완전 수렴하는 기대값의 유한합에도 성립한다.기대값의 선형성은 지시 확률 변수(indicator random variables)를 이용한 확률적 분석을 하는데 중요한 역할을 한다.  지시 확률 변수 : 어떤 사건이 일어나면 1 아니면 0이 되는 함수, 자세한 이야기는 이곳에서 하지 않는다.확률 변수 X가 값이 자연수 집합 $\\N={0,1,2,\\dots}$에 속한다면, 다음과 같은 기대값에 대한 식을 사용할 수 있다. \\(\\begin{align}E[X]&amp;=\\sum_{i=0}^\\infty i\\cdot\\Pr\\{X=i\\}\\\\&amp;=\\sum_{i=0}^\\infty i(\\Pr\\{X\\geq i\\}-\\Pr\\{X\\geq i+1\\})\\\\&amp;=\\sum_{i=1}^\\infty \\Pr\\{X\\geq i\\}\\end{align}\\tag{2}\\label{eq: expectation on natural numbers}\\)볼록 함수(convex function) $f(x)$를 확률 변수 X에 적용하며, 옌센 부등식(Jensen’s inequality)을 통해 다음과 같은 식을 얻을 수 있다.$E[f(X)]\\geq f(E[X])$  옌센 부등식(Jensen’s inequality) : 기댓값의 볼록함수와 볼록 함수의 기댓값 사이에 성립하는 부등식이 식을 통해 기대값의 존재와 유한함을 예상할 수 있다.      볼록 함수 $f(x)$    모든 x와 y, 그리고 $0\\leq \\lambda \\leq 1$에서 식 $f(\\lambda x + (1-\\lambda )y)\\leq \\lambda f(x)+(1-\\lambda)f(y)$이 성립하는 함수  확산과 표준 편차 (Variance and standard deviation)확률 변수의 기댓값만으로는 얼마나 값들이 확산되어있는지 알 수 없다.확산(Variance)는 확률 변수의 값들이 평균에서 얼마나 동떨어져 있는 가에 대한 표현이다.평균 기대값 E[X]에 대한 확률 변수 X의 확산은 다음과 같이 구할 수 있다.\\(\\begin{align}Var[X] &amp;= E[(X-E[X])^2]\\\\&amp;= E[X^2-2XE[X]+E^2[X]]\\\\&amp;= E[X^2]-2E[XE[X]]+E^2[X]\\\\&amp;= E[X^2]-2E^2[X] +E^2[X]\\\\&amp;= E[X^2]-E^2[X]\\\\\\end{align}\\tag{3}\\label{eq:variance}\\)위 식의 $E[E^2[X]]=E^2[X]$의 경우, 앞서 배운 기댓값의 선형성 2번 ()$E(aX+b)=aE(X)+b$)에 의해, $E[X]$의 값이 실수일 것이므로, $b$와 같은 상수로 취급 받아($E[X^2]=b$)$E[E^2[X]]=E[b]=b=E^2[X]$가 되어 성립한다.$E[XE[X]]=E^2[X]$은 기댓값의 선형성 2번과, 3번 ($E(XY)=E(X)E(Y)$)을 이용해 증명할 수 있다.  $E[XE[X]]=E[X]E[E[X]]=E[X]E[X]=E^2[X]$이렇게 증명한 확산의 식을 이용해 아래와 같이 확률 변수 제곱에 대한 기대값을 구할 수 있다.$E[X^2]=Var[X]+E^2[X]$확률 변수 X에 대한 확산과 aX에 대한 확산은 다음과 같은 관계를 가진다.$Var[aX]=a^2Var[X]$확률 변수 X와 Y가 상호 독립적이라면, 다음이 성립한다.$Var[X+Y]=Var[X]+Var[Y]$일반적적으로, n개의 확률 변수 $X_1,X_2,\\dots,X_n$가 쌍방향 독립적이라면, 다음이 성립한다.\\(Var\\left[\\sum^n_{i=1}X_i\\right]=\\sum^n_{i=1}Var[X_i]\\)확률 변수 X의 표준 편차(standard deviation)는 확률 변수 X의 분산의 음이 아닌 제곱근이며, $\\sigma_x$ 또는 $\\sigma$로 표현한다. 즉, 확률변수 X의  분산은 $\\sigma^2$이다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EA%B4%80%EA%B3%84.html": {
    title: "알고리즘 수학 기본-관계",
    date: " Jul 10, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EA%B4%80%EA%B3%84.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 관계(Relations)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 관계의 표기법, 정의, 속성 같은 기본적인 것만 배운다.관계(Relations)이항 관계(binary relation)두 집합 A와 B에 대한 이항 관계 R은 곱집합(Cartesian product) $A\\times B$의 부분집합이며, 만약 순서쌍 $(a, b)\\in R$일 경우 이러한 관계를 $a\\ R\\ b$로 표현한다.즉 $a\\ R\\ b$는 “a와 b의 관계가 성립 가능”을 의미한다.만약 R이 집합 A의 이항 관계라면 R은 $A \\times A$의 부분집합, 즉 집합 A의 원소들끼리의 관계가 성립된다라는 의미이다.이외의 예시로, 자연수에서의 미만 관계(“less than” relations)는 집합 ${(a,b):a,b \\in \\N\\ and\\ a&lt;b}$를 의미하며, 집합족 $A_1, A_2,\\cdots,A_n$에 대한 n항 관계(n-ary relations)는 $A_1\\times A_2\\times \\cdots \\times A_n$의 부분집합을 의미한다.이항관계의 성질반사관계(Reflexive relations)이항관계 $R\\subseteq A\\times A$는 모든 원소 $a\\in A$에 대하여 $a\\ R\\ a$가 성립될 경우, R은 반사(reflexive) 관계라 한다.예를 들어 관계 “=”와 “$\\leq$”은 자연수 집합 $\\N$에 대하여 반사관계이지만 “$&lt;$”은 아니다.(0~무한까지 자기자신보다 같거나 작거나 같은 수들 뿐이지만, 자기자신보다 작은 수는 없으니까.)대칭관계(Symmetric relations)어떤 집합 A의 원소 $a,b\\in A$인 순서쌍 (a, b)와 (b, a)가 관계 R에 속할때, 즉 $a\\ R\\ b$성립되면서 동시에 $b\\ R\\ a$ 성립될 때, R을 대칭관계(Symmetric relations)라고 부른다.예를 들면 “=”는 대칭관계 이지만 “&lt;”나, “$\\leq$”는 아니다.추이관계(transitive relations)만약, $a,b,c\\in A$일 때, $a\\ R\\ b$, $b\\ R\\ c$ 그리고 $a\\ R\\ c$를 만족할 때, 추이관계(transitive relations)라고 한다.예를 들면, “&lt;”, “$\\leq$”, “=”는 추이 관계이지만, 관계 $R={(a,b):a,b\\in \\N\\ and\\ a=b-1}$은 예를 들어 $3\\ R\\ 4$이며, $4\\ R\\ 5$이지만 $3\\ R\\ 5$는 성립되지 않으므로 아니다.위에 설명한 세 관계를 모두 만족하는 관계를 동치관계(equivalence relation)이라고 부르는데, 대표적으로 “=”는 동치 관계이다.집합 A에 포함된 원소 a의 동치류(equivalence class)는 $[a]={b\\in A: a\\ R\\ b}$로 표현하며, 동치류 내의 원소들은 모두 a에 대하여 동치관계이다.해당 동치류는 앞서 언급했듯이 대칭관계를 만족하므로, 동치류는 [a] 뿐만 아니라 같은 동치류 내의 다른 원소들 x들의 동치류 [x] 이기도 하다.예를 들어, $R={(a,b):a,b \\in \\N\\ and\\ a+b는\\ 짝수}$에서a+a는 짝수이면 반사관계a+b가 짝수이고, b+a가 짝수이면 대칭 관계a+b가 짝수이고, b+c도 짝수인 뒤, a+c도 짝수이면 추이관계가 성립되고,관계 R에 대한 4의 동치류는 $[4]={0,2,4,6,\\cdots}$, 3의 동치류는 $[3]={1,3,5,7,\\cdots}$이다.동치 관계에 관한 기본적인 정리 하나 (A basic theorem of equivalence classes)동치 관계에 관한 정리 하나는 다음과 같다.Theorem B1. “동치관계는 우리가 Set에서 배웠던 분할(partition)과 동일하다.”즉, 모든 동치 관계 R에 대한 집합 A의 원소들의 동치류들의 집합들은 A의 분할(partition)이며, A의 모든 분할들은 어떤 동치 관계에 대한 동치류들의 집합이다.사실 직관적으로 예시를 들어보자면, 어떤 회사에서 사원들 집합 A를 부서별로 나눈 표를 분할로 볼 수 있고, 동치관계 R은 “사원 a는 사원 b와 같은 부서 소속이다.”로 집합 A를 나눈 표로 볼 수 있으므로, 동일하다는 것을 알 수 있지만 아래에 수학적 증명이 나와있다.증명R의 동치류 집합은 공집합이 아니며 합집합은 집합 A이며, 서로소 집합이다.먼저 첫번째로, R에 대한 동치류 집합은 비어있지 않고, 모두의 합집합은 집합 A가 되며, 서로간에 서로소인 집합들이다. 왜냐하면      R은 반사관계의 특성을 가지고 있으므로, $a \\in [a]$이며, 따라서 최소한 하나의 원소(자기 자신)을 가지고 있으며 (동치류는 비어있지 않음)        따라서 A의 모든 원소들은 최소한 자기 자신이 포함된 동치류를 가지므로 이 동치류들을 모두 합하면 집합 A가 되며, (합집합이 집합 A)        만약 서로 다른 두 원소 a, b의 동치류 [a], [b]가 공통된 원소 c를 가진다면, 아래에 설명할 증명에 의해 [a] = [b]라는 것이 되는데, 이는 앞선 가정인 동치류 집합이라는 것에 위배된다.(집합은 중복된 원소를 가지지 않는다.)    즉, a와 b는 공통된 원소 c를 가지지 않는다.(동치류들은 모두 서로 서로소)  3번을 좀더 자세하게 알아보자면, 만약 동치류 [a]와 [b]가 공통된 원소 c를 가진다면, $a\\ R\\ c$이면서, $b\\ R\\ c$라는 의미인데, 대칭관계에 의해 $b\\ R\\ c$는 곧 $c\\ R\\ b$이며, 추이관계에 의해 $a\\ R\\ b$가 성립되며, 반대로 $b\\ R\\ a$가 성립된다.여기서 c를 동치류 [a]의 모든 원소들 중 하나 x를 대입을 하여도 $a\\ R\\ x$는 곧 $x\\ R\\ a$이고, 위에서 설명했던 것 처럼 $a\\ R\\ b$이므로 추이관계에 의해 $x\\ R\\ b$가 성립된다.이는, 반대로 동치류 [b]의 원소 중 하나 y로 하여도 마찬가지이다. 즉 [a]와 [b]는 모든 원소를 서로간에 공유하게 되고 $[a] \\subseteq [b]$이자 $[b] \\subseteq [a]$가 되며 이는 곧 $[a]=[b]$이므로, a와 b는 공통된 원소 c를 가지지 않는 서로소라는 점이 증명된다.동치관계 정의그다음 두번째로, 집합 A의 분할을 $P_A$로 놓고, 분할들의 원소, 즉 서로소인 A의 부분집합들을 $A_i$라 놓자.집합 A에 대한 관계 R을 $R={(a,b):a와\\ b가\\ 전부\\ 포함된\\ 집합인\\ A_i가\\ 존재함}$로 정의하자.  앞서 설명한 “a와 b는 같은 부서 소속이다.”의 수학적인 표현과 다를 바 없다.우리가 정의한 R은 A에 대해 동치관계인데,먼저, a와 b가 전부 포함된 집합 $A_i$에는 당연히 a가 들어가 있으므로 $a\\ R\\ a$가 성립되어 반사관계가 성립되며a와 b가 전부 포함된 집합 $A_i$($a\\ R\\ b$)는 b와 a가 전부 포함된 집합과 마찬가지이므로 $b\\ R\\ a$ 또한 성립하므로 대칭관계이며,마지막으로, 집합 A의 분할의 원소들은 각각 집합 A의 원소들을 중복해서 포함하지 않으므로, $a\\ R\\ b$를 증명하는 $A_i$는 $b\\ R\\ c$인 $A_i$와 같은 원소가 같지 않으면 원소 b가 중복됬다는 의미이므로, a, b, c는 같은 집합 내에 존재한다는 의미이며, 따라서 추이관계 또한 만족한다.  $A_i$에 속한 원소 a의 동치류 [a]에 $x \\in [a]$를 만족하는 원소 x들은 모두 $A_i$에 속한다.동치류는 위의 관계 R에 a와 동치관계를 형성해야하는데, 동치류 [a]의 어떤 원소 x가 a와 같은 분할내 집합원소 $A_i$에 속하지 않다면, $a\\ R\\ x$가 성립하지 않으므로, 동치류에 속할 수 없기 때문이다.  반대로 $A_i$의 모든 원소 y가 [a]에 속한다.$A_i$의 모든 원소는 관계 R을 만족시므로 $a\\ R\\ y$가 성립되며, 동치관계이므로 [a]에 속할 수 있다.$A_i$의 모든 원소와 [a]의 모든 원소가 서로 속하므로 ($A_i \\subseteq [a], [a] \\subseteq A_i$) A의 분할은 곧 R의 동치류들의 집합이다.반대칭 관계이항 관계 R에서 $a\\ R\\ b$와 $b\\ R\\ a$일 때, a=b인 관계를 반대칭(antisymmetric) 관계라고 하는데,이는 $a\\ R\\ b$일 때, $b\\ R\\ a$는 성립하지 않는 비대칭 관계(asymmetric) 관계와는 다른 의미이며, 비대칭 관계는 대칭관계의 정반대의 의미이지만, 반대칭 관계는 서로 양립 가능하다.예를 들어 R이 두 원소는 같다 (“=”)일 경우, 반대칭 관계이면서, 대칭관계일 수 있다.하지만 R이 두 원소는 작거나 같다 (“$\\leq$”)일 경우, 반대칭 관계이지만 대칭관계는 아닌데,  $a=b$인 경우에만 $ a\\ R\\ b$, $b\\ R\\ a$이기 때문이다.이때, 반사관계, 반대칭관계, 추이관계를 만족하는 관계를 부분순서 (Partial Order) 관계라고 하며, 부분순서 관계가 정의된 집합을 부분 순서 집합(partially ordered set)이라고 한다. 대표적으로 위에서 예시를 들었던 “$\\leq$”가 있다.부분 순서 집합에서는 집합 A의 모든 원소 b에 대해 $b\\ R\\ a$를 모두 만족하는 최대값 원소 a가 존재하지 않지만, 집합 A의 모든 원소 b에 대해 $a\\ R\\ b$를 만족하는 경우가 존재하지 않는 원소 a는 여럿 존재할 수 있다.예시를 들자면, 박스들의 집합에서 다른 모든 박스들을 수용할 수 있는 가장 큰 박스는 존재하지 않지만, 다른 모든 박스에 들어가지 않는 박스는 존재할 수 있다.집합 A의 모든 원소 쌍들이 관계 R을 만족할 때, 이를 완전관계(total)라고하며, 완전관계인 부분 순서 집합을 완전 순서(total order) 또는 선형 순서(linear order)라고 한다.예를 들어 자연수 집합의 $\\leq$ 관계는 완전 순서이지만, 모든 인간에 대한 가계도의 경우는 완전 순서가 아니다. 생판 모르는 남끼리는 후손도, 자식으로도 인정받지 못하기 때문에 아무런 관계가 아니기 때문이다.(물론 엄밀히 이상적인 유전검사와 고고학, 인류학적 이상을 고려하면 모든 인간은 한 핏줄이겠지만, 현실적인 이야기를 가정한다.)이중에, 완전 관계이면서, 추이관계를 만족하지만, 반사관계와 반대칭 관계는 만족하지 않는 완전관계를 완전 전위순서(total preorder)라고 한다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EC%A7%91%ED%95%A9.html": {
    title: "알고리즘 수학 기본-집합",
    date: " Jul 10, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EC%A7%91%ED%95%A9.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 집합(Sets)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 집합의 표기법, 정의, 속성 같은 기본적인 것만 배운다.집합 (Sets)집합의 기본 표현과 원소와의 관계 표현집합은 원소라 불리우는 서로 구별할 수 있는 객체들의 모임이다. 만약 객체 x가 집합 S에 속한다면 $x \\in S$로 표현하며, 반대로 소속되지 않는다면, $x \\notin S$로 표현한다.집합은 보통 “{”, “}” 사이에 소속된 원소들을 나열하여 표현하며, 이때 같은 원소가 중복되어 소속 될 수 없으며, 만약 중복이 존재할 경우, 2개 이상부터는 없는걸로 취급해도 된다. 중복을 허용하는 집합은 중복집합(multiset)이라고 부른다.집합 내의 원소들은 정렬되어 있을 필요없다.만약 두 집합에 소속된 원소들이 완전 일치한다면, $A=B$로 표현하며, 예를 들어 ${1,2,3,1}={1,2,3}={3,2,1}$로 표현 된다.다음은 자주 사용되는 집합의 표기법들이다.  $\\O$는 아무 원소도 소속되지 않은 빈 집합을 의미하며, 공집합이라고 부른다.  $\\Z$는 모든 정수들로 이루어진 집합을 의미한다. {…-2,-1, 0,1,2,…} 정수집합이라고 부른다.  $\\R$은 모든 실수들로 이루어진 집합을 의미한다. 실수 집합이라고 부른다.  $\\N$은 모든 자연수들로 이루어진 집합을 의미한다. {0,1,2,3 …}, 자연수 집합이라고 부른다.집합 간의 관계 표현집합 A에 속한 원소들이 모두 집합 B에 속할 때, $A \\subseteq B$로 표현하며, A는 B의 부분집합이라고 표현한다.$A\\subset B$는 집합 A에 속한 원소들이 모두 집합 B에 속하면서 동시에 $A \\neq B$인 경우에 사용한다. (즉, $B \\nsubseteq A $인 경우)$A \\subseteq B$이면서 $B \\subseteq A$인 경우는 $A=B$이다.$A \\subseteq B$이며, $B \\subseteq C$인 경우는 $A \\subseteq C$이다.모든 집합 A에 대해서 공집합 $\\O \\subseteq A$이다.집합은 다른 집합을 통해서 표현될 수 있다. 예를 들어 $\\Z$를 이용해 다른 집합을 표현하는 예시로, ${x:x\\in \\Z\\ and\\ x/2\\ is\\ an\\ integer}$            이때 “:”(colon)는 x의 성질을 나타내며, 표기에 따라 “      “로 표현되기도 한다.      집합 연산A와 B의 교집합(intersection)은 $A \\cap B$로 표현하며, 집합 ${x:x\\in A\\ and\\ x \\in B}$, 즉 A와 B에 둘 다 존재하는 원소들의 집합을 의미한다.A와 B의 합집합(union)은 $A \\cup B$로 표현하며, 집합 ${x:x \\in A\\ or\\ x\\in B}$, 즉 A 또는 B에 존재하는 원소들의 집합을 의미한다.A에 대한 B의 차집합(difference 또는 relative compliment)는 $A-B$로 표현하며, 집합 ${x: x \\in A\\ and\\ x\\notin B}$, 즉 A에는 존재하지만, B에는 존재하지 않는 원소들의 집합을 의미한다.집합 연산에 의한 법칙또한 집합 연산에 따른 다음과 같은 법칙들이 존재한다.      공집합 법칙 (Empty set laws) \\(A \\cap \\O=\\O\\\\A \\cup \\O=A\\)        멱등 법칙 (Idempotency laws)\\(A \\cap A=A\\\\A \\cup A = A\\)        교환 법칙 (Commutative laws)\\(A \\cap B = B\\cap A\\\\A \\cup B = B \\cup A\\)        결합 법칙 (Associative laws)\\(A\\cap(B\\cap C)=(A \\cap B)\\cap C\\\\A \\cup (B\\cup C)=(A \\cup B) \\cup C\\)        분배 법칙 (Distributive laws)\\(A \\cap(B\\cup C)= (A \\cap B)\\cup (A \\cap C)\\\\A \\cup (B \\cap C)=(A\\cup B)\\cap(A\\cup C)\\label{law:Distributive}\\tag{B.1}\\)        흡수 법칙 (Absorption laws)\\(A \\cap (A \\cup B)=A\\\\A \\cup (A \\cap B)=A\\)        드 모르간의 법칙 (DeMorgan’s laws) (Figure B.1)\\(A-(B\\cap C)=(A-B)\\cup(A-C)\\\\A-(B\\cup C)=(A-B)\\cap(A-C)\\label{law:Demorgan}\\tag{B.2}\\)  집합 연산 시, 많은 경우 아래와 같이 벤 다이어그램(Venn diagram)으로 표현하곤 한다.모든 집합들은 전체 집합 U의 부분집합이며, 이 전체 집합 U는 해당 집합의 특징에 따라 다르다. 예를 들어 정수로만 이루어진 집합들은 모두 정수 집합 $\\Z$의 부분집합이며, 이때 $\\Z$는 전체집합 U가 된다.전체집합 U에 대한 A의 차집합, 즉 $U-A={x:x\\in U\\ and\\ x \\notin A}$는 $\\overline{A}$로 표현되며, 여집합(complement)라고 한다.집합 A가 전체집합 U의 부분집합 일 때, 다음과 같은 법칙이 성립한다.\\(\\overline{\\overline{A}}=A\\\\A\\cap\\overline{A}=\\O\\\\A\\cup\\overline{A}=U\\)또한, $B,C\\subseteq U$일 때, 드모르간의 법칙을 활용해 다음과 같은 법칙이 성립한다.\\(\\overline{B \\cap C}=\\overline{B}\\cup\\overline{C}\\\\\\overline{B \\cup C}=\\overline{B}\\cap\\overline{C}\\)서로소 집합(disjoint set)두 집합 A와 B의 교집합이 공집합일 때, 즉, $A\\cap B = \\O$일 때 A와 B를 서로소 집합(disjoint set)이라고 한다.특정 집합 S를 i개의 집합으로 나눈 것의 공집합이 아닌 i번째 집합을 $S_i$라 하며 분할(partion)이라 부르자, 이 집합들의 집합을 집합족이라고 하며, 집합족 P라고 하자.  집합족 P의 원소집합들은 모두 서로 간에 서로소(pairwise disjoint)이다. 즉 집합족 P는 서로소 집합족이다. $S_i \\cap S_j = \\O$  집합족 P의 원소집합들 모두의 합집합은 S이다. 즉 $S=\\bigcup_{S_i\\in P}S_i$카디널리티(cardinality)            집합의 원소의 갯수를 크기(size) 또는 카디널리티(cardinality, 농도)라고 부르며 $      S      $로 표현된다.      유한한 집합의 크기 n에 따라 n-set 집합이라고 불리우며, n이 1일 경우 싱글톤(singleton)이라고 부른다. 해당 집합이 어떤 집합의 부분집합(subset)이고, 크기가 n일 겨우 n-subset 집합이라고 부른다.            두개의 집합의 원소가 서로 1대1 대응 관계라면 같은 크기를 가진다는 의미이며, 공집합 $\\O$의 크기, $      \\O      $는 0이다.      집합의 크기는 자연수로 표현되면, 유한한 크기이며, 그렇지 않다면 무한한 크기를 가진다.자연수 집합 $\\Z$과 1대1 대응되는 무한 크기 집합을 가산 무한(countably infinite) 크기 집합이라고 하며, 실수 집합 $\\R$같 은경우 불가산(uncountable) 무한 크기 집합니다.유한한 크기의 두 집합 A, B에 대해\\(|A \\cup B|=|A|+|B|-|A \\cap B| \\label{eq:setUnionSize} \\tag{B.3}\\)이 성립되므로, $|A\\cup B|\\leq |A|+|B|$이며, A와 B가 서로소라면 $|A\\cap B|=0$이고,$|A \\cup B|=|A|+|B|$이다. 또한 이때, $A \\subseteq B$ 일 때, $|A| \\leq |B|$이다.n개의 크기를 가진 집합 S의 모든 부분집합의 경우의 수는 $2^n$이며, 이들을 집합 S의 power set(멱집합)이라고 한다.            파워 셋에는 공집합과 집합 S를 포함하며, 예시를 들자면, $S = {a,b}$의 파워셋은 ${\\O, {a}, {b}, {a,b} }$ 으로 $2^{      S      }$인 4개이다.      순서쌍(ordered pair)순서쌍(ordered pair)는 집합과 비슷하지만, 원소들이 정렬된 형태이며, 두개의 원소 a,b 순으로 포함된 순서쌍은 (a, b)로 표현된다.이는 집합과 달리 (b, a)와 다른 순서쌍이며, a, b를 각각 n번째 성분(entry), 또는 n번째 좌표(coordinate)라고 부른다.순서쌍 (a, b)를 집합론적으로 표현할 시, ${{a}, {a,b}}$로 표현하며 이는 두 순서쌍의 n번째 성분이 모두 다를 경우, 둘은 다른 순서쌍이라는 성질에 의해서이다.예를 들어, (a, b)와 (b,a)를 각각 집합 ${{a}, {b}}, {{b},{a}}$로 표현하면, 이 둘은 집합 관점에서는 서로 같다는 의미이나, 순서쌍에서는 그렇지 않으므로 적절치 않다.위의 집합 표현 시 (a, b)와 (b, a)는 각각 집합 ${{a}, {a,b}}, {{b}, {b,a}}$로 표현되므로 집합 표현에서도 서로 다르고, 순서쌍 표현에서도 서로 다르게 된다.  이를 쿠라토프스키의 순서쌍 정의라고 한다. (https://ko.wikipedia.org/wiki/%EC%88%9C%EC%84%9C%EC%8C%8D)곱집합(Cartesian product)두 집합 A와 B의 곱집합(Cartesian product)은 $A \\times B$로 표현되며, 첫번째 성분은 A의 원소이며, 두번째 성분은 B의 원소인 순서쌍 들의 모든 경우의 수들의 집합이다. 수식으로 표현하자면 $A\\times B={(a,b):a\\in A\\ and\\ b \\in B}$이며, 예시로 ${a,b} \\times {a,b,c}={(a,a),(a,b),(a,c),(b,a),(b,b), (b,c)}$이다.유한한 두 집합 A와 B의 곱집합의 카디널리티(cardinality)는 다음과 같이 표현된다.\\(|A\\times B|=|A|\\cdot |B|\\label{eq:setABCartProd}\\tag{B.4}\\)n개의 집합 $A_1,A_2,\\dots,A_n$의 곱집합은 n-튜플들의 집합이며, 이때 n-튜플 하나는 n 길이의 유한 수열(sequence)로도 볼 수 있으며, 다음과 같이 표현된다.  튜플(tuple): 유한한 크기의 요소들의 순서있는 열거\\[A_1\\times A_2\\times\\cdots \\times A_n=\\{(a_1,a_2,\\cdots,a_n):a_i \\in A_i\\ for\\ i=1,2,\\cdots,n\\}\\]            이때의 카디널리티는 $      A_1\\times A_2\\times \\cdots \\times A_n      =      A_1      \\cdot      A_2      \\cdots      A_n      $이다.      하나의 유한한 크기의 집합 A의 n 반복(n-fold) 곱집합은 $A^n$의 카디널리티를 가진다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EC%9C%A0%ED%95%9C%ED%95%A9.html": {
    title: "알고리즘 수학 기본-유한합",
    date: " Jul 11, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%EC%9C%A0%ED%95%9C%ED%95%A9.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "알고리즘을 위한 수학 - 유한합(Summation)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.유한합 (Summations)유한합은 while이나 for 루프가 존재하는 알고리즘의 시간 복잡도를 계산하는데 사용할 수 있다.때문에, 시그마($\\sum$)으로 표현되는 유한합에 대해서 알 필요가 있다. 이 책에서는 대다수의 공식의 증명은 생략되어 있다.유한합  공식과 성질(Summation formulas and properties)n이 유한한 양의 정수이고, n개의 숫자들 $a_1,a_2,\\cdots, a_n$이 존재할 때, 이들 모두의 합 $a_1+a_2+\\cdots +a_n$은 다음과 같이 표현될 수 있다.\\(\\begin{equation}\\sum^n_{k=1}a_k\\label{eq:sigmaBasic}\\tag{A.1}\\end{equation}\\)이를 유한급수(finite series)라고 하며, 만약 n이 0이면, 식 $\\eqref{eq:sigmaBasic}$의 값은 0이다.만약, n이 무한하고, 무한한 숫자들 $a_1,a_2,\\cdots$를 모두 더한다면 $\\sum^\\infty_{k=1}a_k$로 표현되며, 이는 다음과 같이 극한값으로 해석될 수 있다.\\(\\lim_{n\\rightarrow\\infty}\\sum^n_{k=1}a_k\\)이를 무한급수(infinite series)라고 하며, 어떠한 값에 한없이 가까워지는 수렴 급수(convergent series)와 그렇지 않은 발산 급수(divergent series)로 나뉜다.            이때, 각 항 $a_k$에 절대값을 취한 항 $\\left      a_k\\right      $들의 합, 또는 절대수렴(absolute convergence) 급수가 수렴할 경우, 원래의 수렴급수들의 합 $\\eqref{eq:sigmaBasic}$ 또한 수렴한다.        절대수렴 급수들은 기존의 수렴 급수와 달리 합의 순서가 바뀌어도 결과값이 변하지 않는다.선형성(Linearity)어떠한 실수 c와 어떠한 유한한 양의 정수 n으로 이루어진 두 급수 $a_1,a_2,\\cdots,a_n$과 $b_1,b_2,\\cdots,b_n$이 주어졌을때 다음과 같은 식이 성립한다.\\(\\sum^n_{k=1}(ca_k+b_k)=c\\sum^n_{k=1}a_k+\\sum^n_{k=1}b_k\\)이러한 선형성 설질은 무한수렴급수에도 적용되며, 이를 통해 점근적 표기법을 사용하는 유한합을 조작할 수 있는데 예를 들자면, 다음 식 $\\eqref{eq:sigmaTheta}$이 성립한다.\\(\\begin{equation}\\sum^n_{k=1}\\Theta(f(k))=\\Theta\\left( \\sum^n_{k=1}f(k)\\right)\\label{eq:sigmaTheta}\\tag{A.2}\\end{equation}\\) 다음 식 $\\eqref{eq:sigmaTheta}$의 좌측 항의 $\\Theta$ 표기는 변수 k에 대해서이며, 우측항은 변수 n에 대한 표기이다. 이 또한 무한 수렴 급수에 적용된다.등차 수열(Arithmetic series)유한합 $\\sum^n_{k=1}k=1+2+\\cdots+n$를 등차 수열(arithmetic series)이라고 하며, 다음과 같은 값을 가진다.\\[\\begin{align}\\sum^n_{k=1}k&amp;=\\frac{1}{2}n(n+1)\\label{eq:sigmaArtithSum}\\tag{A.3}\\\\&amp;=\\Theta(n^2)\\label{eq:sigmaArithSumTheta}\\tag{A.4}\\end{align}\\]제곱과 세제곱의 합(Sums of squares and cubes)\\[\\begin{align}&amp;\\sum^n_{k=0}k^2=\\frac{n(n+1)(2n+1)}{6}\\label{eq:sigmaSquareSum}\\tag{A.5}\\\\&amp;\\sum^n_{k=0}k^3=\\frac{n^2(n+1)^2}{4}\\label{eq:sigmaCubeSum}\\tag{A.6}\\end{align}\\]제곱과 세제곱 급수들의 합에 대한 유한합의 공식등비 수열 또는 기하수열(Geometric Series or Exponential Series)실수 $x$가 1이 아닌 급수들을 등비수열 또는 기하수열이라고 표현하며, 등비수열의 합은 다음과 같이 표현할 수 있다.\\(\\sum^n_{k=0}x^k=1+x+x^2+\\cdots+x^n\\)등비수열의 합과 무한급수 등비수열의 합은 다음과 같은 공식으로 표현될 수 있다.\\(\\begin{align}&amp;\\sum^n_{k=0}x^k=\\frac{x^{n+1}-1}{x-1} \\label{eq:sigmaGemetSum} \\tag{A.7}\\\\&amp;\\sum^\\infty_{k=0}x^k=\\frac{1}{1-x},\\ when\\ \\left|x\\right|&lt;1 \\label{eq:sigmaGemetSumInf} \\tag{A.8}\\end{align}\\)조화수열 (Harmonic Series)양의 정수 n에 대하여 n개의 조화수열은 아래와 같은 식으로 표현된다.\\[\\begin{align}H_n&amp;=1+\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}+\\cdots+\\frac{1}{n}\\\\&amp;=\\sum^n_{k=1}\\frac{1}{k}\\\\&amp;=\\ln n+O(1)\\label{eq:sigmaHarmonSum}\\tag{A.9}\\end{align}\\]자세한 증명은 아래 유한합 나누기(Splitting summations) 파트에서 보여준다.수열 합의 적분과 미분(Integrating and differentiating series)기존의 공식에 양 항을 적분 또는 미분하는 것으로 새로운 공식을 얻을 수 있다. 예를 들면 식 $\\eqref{eq:sigmaGemetSumInf}$의 양 항을 미분한 뒤, $x$를 곱하면 다음과 같은 식이 나온다.\\(\\sum^\\infty_{k=0}kx^k=\\frac{x}{(1-x)^2}\\ when\\ \\left | x \\right | &lt; 1.\\label{eq:sigmaGeometSumDiff}\\tag{A.10}\\)망원수열(Telescoping Series)모든 수열에 대하여 다음 식이 성립하며, 이러한 부분적 항들의 합이 소거 후, 일부 고정된 값만 남는 수열을 망원 수열이라 한다.\\(\\sum^n_{k=1}(a_k-a_{k-1})=a_n-a_0 \\label{eq:sumTeleScope} \\tag {A.11}\\)위를 응용하여 두 가지 변형 공식을 얻을 수 있는데, 첫번째는\\(\\sum^{n-1}_{k=0}(a_k-a_{k+1})=a_0-a_n\\)또한, 다음과 같은 식이 성립하는데,\\(\\frac{1}{k(k+1)}=\\frac{1}{k}-\\frac{1}{k+1}\\)이를 통해 아래와 같은 두번째 공식을 얻을 수 있다.\\(\\sum^{n-1}_{k=1}\\frac{1}{k(k+1)}=\\sum^{n-1}_{k=1}\\left(\\frac{1}{k}-\\frac{1}{k+1}\\right)=1-\\frac{1}{n}\\)곱 (Products)우리는 유한한 양의 정수 n에 대하여 수열 $a_1, a_2, \\cdots, a_n$의 곱 $a_1a_2\\cdots a_n$를 아래와 같이 표현한다.\\[\\prod^n_{k=1}a_k\\]이때 n이 0일때의 곱의 값은 1이다. 이러한 공식은 다음과 같은 유한합 공식과 연결될 수 있다.\\(\\lg\\left(\\prod^n_{k=1}a_k\\right)=\\sum^n_{k=1}\\lg a_k \\label{eq:prodToLogSum} \\tag{A.12}\\)유한합의 범위 제한(Bounding summations)수열의 합의 범위를 앎으로써, 알고리즘의 비용에 대해 알 수 있으므로, 유한합의 범위를 제한하는 여러가지 방법에 대해 알아보자.수학적 귀납법(Mathematical induction)수학적 귀납법은 가장 쉽고 빠른 방법이다.수학적 귀납법은 어떠한 자연수가 특정 조건을 만족하고, 다음 자연수 또한 만족한다는 것을 증명하면, 모든 자연수가 해당 조건을 만족한다는 증명이다.예를 들어, $\\sum^n_{k=1}k = \\frac{1}{2}n(n+1)$가 참임을 증명하려면, 먼저 n = 1일 경우, 성립된다는 것은 $\\frac{1}{2}\\cdot 1(1+1)=1$임으로 자명하며, 이제  $\\sum^{n+1}_{k=1}k$의 값을 구한 뒤, $m = n+1$으로 놓는다면, 다음과 같은 식이 성립한다.\\(\\begin{align}\\sum^{n+1}_{k=1}k=&amp;\\sum^n_{k=1}k+(n+1)\\\\&amp;=\\frac{1}{2}n(n+1)+(n+1)\\\\&amp;=\\frac{1}{2}(n+1)(n+2)\\\\&amp;=\\frac{1}{2}m(m+1)\\end{align}\\)이는 기존의  $\\frac{1}{2}n(n+1)$와 같은 꼴이므로, 가정이 n에 대해 성립하고, n+1에 대해 성립함을 보였으니 $\\sum^n_{k=1}k = \\frac{1}{2}n(n+1)$은 참이다.또한, 수학적 귀납법을 통해 유한합의 범위(bound)를 증명할 수도 있다.예를 들어, $\\sum^n_{k=0}3^k$가 $O(3^n)$, 즉 상수 c에 대해 $\\sum^n_{k=0}3^k \\leq c3^n$임을 증명해볼 수 있다.먼저 n이 0일때, $\\sum^0_{k=0}3^k = 1 \\leq c\\cdot 1$ 이므로 가정이 참임을 알 수 있고, n+1의 경우 다음과 같이 증명된다.\\(\\begin{align}\\sum^{n+1}_{k=0}3^k=&amp;\\sum^n_{k=0}3^k+3^{(n+1)}\\\\&amp;\\leq c3^n+3^{n+1}\\ (귀납적\\ 가정에의해)\\\\&amp;=\\left(\\frac{1}{3}+\\frac{1}{c}\\right)c3^{n+1}\\\\&amp;\\leq c3^{n+1}\\end{align}\\)즉 $(1/3+1/c)\\leq 1$, $c \\geq 3/2$인 경우에 성립하므로, $\\sum^n_{k=0}3^k = O(3^n)$이다.단, 아래 예시처럼 귀납적 방법으로 증명하며 점근 표기법을 사용할 때 주의해야 한다.예를 들어 $\\sum^{n}{k=1}k = O(n)$임을 증명할 때, $\\sum^{1}{k=1}k = O(1)$이므로, 아래와 같이 $\\sum^{n}_{k=1}k = O(n)$으로 놓는 것은 옳지 않다.\\(\\begin{align}\\sum^{n+1}_{k=1}k&amp;=\\sum^n_{k=1}k+(n+1)\\\\&amp;=O(n)+(n+1) \\Leftarrow 틀림. \\\\&amp;=O(n+1)\\end{align}\\)이때는 k가 1일 때 뿐만 아니라 모든 n에 대해 성립됨을 보여야 $\\sum^{n}_{k=1}k = O(n)$이 된다.항들의 한계값 (Bounding the terms)수열의 각 항들의 상한(upper bound)들을 통하여 수열합의 상한을 구할 수도 있다.예를 들어 식 $\\eqref{eq:sigmaArtithSum}$을 통해 상한(upper bound)을 통해 아래와 같은 식이 성립된다.\\(\\sum^n_{k=1}k\\leq \\sum^n_{k=1}n=n^2\\)또한, 각 항 중에 최대값인 항을 이용하는 방법은 다음과 같다.$a_1,a_2,\\cdots, a_n$ 까지의 수열 중 최대 값을 $a_{max}$라고 할 때, 다음과 같은 식이 성립된다.  수열에서 가장 큰 값의 n배가 모든 n개의 수열의 합보다 크다는 의미.\\[\\sum^n_{k=1}a_k\\leq n\\cdot a_{max}\\]위처럼 최대값 항을 이용하는 방법은 등비 수열에서는 적합하지 않은 경우가 많으며, 대신 다음과 같이 무한 감소 등비 수열(infinite decreasing geometric series)과 최대값 항을 같이 이용할 수 있다.등비수열 $\\sum^n_{k=0}a_k$에 대하여 $a_{k+1}/a_k \\leq r$이며, $0&lt;r&lt;1$일 때(즉, 점점 일정 비율로 감소하는 등비 수열), 다음이 성립한다.\\(a_k\\leq a_0r^k\\)이를 이용하면 다음과 같은 식이 성립한다.\\(\\begin{align}\\sum^n_{k=0}a_k&amp;\\leq \\sum^{\\infty}_{k=0}a_0r^k\\\\&amp;=a_0\\sum^{\\infty}_{k=0}r^k\\\\&amp;=a_0\\frac{1}{1-r}\\tag{A.13}\\label{eq:boundGeometSum}\\end{align}\\)위의 식 $\\eqref{eq:boundGeometSum}$을 이용해 $\\sum^{\\infty}_{k=1}\\frac{k}{3^k}$의 상한을 알아보자.$\\sum^{\\infty}{k=1}k/3^k$를 k가 0부터 시작하는 수열합으로 바꾸면 $\\sum^{\\infty}{k=0}(k+1)/3^{k+1}$가 되며, 이때 첫번째 항($a_0$)는 1/3이며, 각 항 사이의 비율($a_{k+1}/a_{k}$)은 다음과 같다. \\(\\frac{(k+2)/3^{k+2}}{(k+1)/3^{k+1}}=\\frac{1}{3}\\cdot\\frac{k+2}{k+1}\\leq \\frac{2}{3}\\)즉, 식 $\\eqref{eq:boundGeometSum}$의 항에서 $r=2/3,\\ a_0=1/3$인 경우이므로, k가 0 이상일 경우 다음이 성립한다.\\(\\begin{align}\\sum^{\\infty}_{k=1}\\frac{k}{3^k}&amp;=\\sum^{\\infty}_{k=0}\\frac{k+1}{3^{k+1}}\\\\&amp;\\leq \\frac{1}{3}\\cdot\\frac{1}{1-2/3}\\\\&amp;= 1\\end{align}\\)이때 주의할 점은, 등비수열이 아니며, 무한급수가 수렴하지 않는 경우에서의 증명이다.예를 들어 아래와 같은 무한 발산 조화 수열의 경우, $k/k+1 &lt; 1$이지만, 무한 등비 수열 처럼 수렴하는 상한이 존재하지 않는다.\\[\\begin{align}\\sum^{\\infty}_{k=1}\\frac{1}{k}&amp;=\\lim_{n\\rightarrow \\infty}\\sum^n_{k=1}\\frac{1}{n}\\\\&amp;=\\lim_{n\\rightarrow \\infty}\\Theta(\\lg n)\\\\&amp;=\\infty\\end{align}\\]위의 방법으로 경계를 구하기 위해서는 언제나 $r&lt;1$이며, 일정한 상수임을 보여야 한다. 하지만 위의 조화수열의 경우는 k값이 증가하면서 점점 r 값이 1에 가까워지며 변한다.유한합 나누기 (Splitting summations)범위를 구하기 힘든 유한합의 경우 2개 이상의 수열로 나누어 볼 수 있다. 예를 들어, $\\sum^n_{k=1}k$의 하한(lower bound)를 알아보기 위해, 본능적으로, $\\sum^n_{k=1}k$의 수열의 최소값 경계는 n 임을 직감하겠지만 (n이 1인 경우, 수열의 합이 1이므로), 수열을 나누어 구하는 경우를 알아보자.일단 편의를 위해 아래의 식은 n이 짝수로 가정하면, 다음과 같이 중간값의 수열 두 개로 나눌 수 있다.  홀수인 경우에는 n/2 대신 n을 2로 나누는 값의 몫을 기준으로 나누면 같은 결과가 나올 것이다.\\[\\begin{align}\\sum^n_{k=1}k&amp;=\\sum^{n/2}_{k=1}k+\\sum^n_{k=n/2+1}k\\\\&amp;\\geq\\sum^{n/2}_{k=1}0+\\sum^n_{k=n/2+1}(n/2)\\\\&amp;=(n/2)^2=\\Omega(n^2)\\\\\\end{align}\\]$\\sum^n_{k=1}k$의 점근적 상한(upper bound)이 $O(n^2)$이므로, 최솟값 경계와 최대값 경계가 거의 동일할 정도로 비슷하다는 것을 알 수 있다.또한, 수열의 일부분을 상수로 취급하여 지운 뒤, 수열 합 범위를 구할 수 있다.예를 들어 $k_0 &gt; 0$일 때, $\\sum^{k_0-1}{k=0}a_k$를 $\\Theta (1)$로 처리하고, $\\sum^n{k=k_0}a_k$를 구하여 하한을 구할 수 있을 것이다.\\[\\begin{align}\\sum^n_{k=0}a_k&amp;=\\sum^{k_0-1}_{k=0}a_k+\\sum^n_{k=k_0}a_k\\\\&amp;=\\Theta(1)+\\sum^n_{k=k_0}a_k\\end{align}\\]이러한 방법은 무한 급수의 범위를 찾는데도 사용할 수 있다.예를 들어 $\\sum^\\infty_{k=0}\\frac{k^2}{2^k}$에서는 한 항목과 다음 항목의 비율 $r =\\frac{(k+1)^2/2^{k+1}}{k^2/2^k}$은 값이 0에 가까울 수록 큰데, (정수로 제한할 시, k = 1에서 r = 2),$\\sum^\\infty_{k=3}\\frac{k^2}{2^k}$에서의 r은 8/9보다 언제나 작게 된다.\\[k \\geq 3,\\ \\frac{(k+1)^2/2^{k+1}}{k^2/2^k}=\\frac{(k+1)^2}{2k^2}\\leq \\frac{8}{9}\\]이를 이용해 다음과 같이 첫번째 수열은 상수 개의 항, 두번째 수열은 감소하는 등비 수열로 만들어 보면, 무한급수 $\\sum^\\infty_{k=0}\\frac{k^2}{2^k}$의 상한의 경우에, 상수에 점근한다는 것을 알 수 있다. (실제로 6에 수렴)\\[\\begin{align}\\sum^\\infty_{k=0}\\frac{k^2}{2^k}&amp;=\\sum^2_{k=0}\\frac{k^2}{2^k}+\\sum^{\\infty}_{k=3}\\frac{k^2}{2^k}\\\\&amp;\\leq\\sum^2_{k=0}\\frac{k^2}{2^k}+\\sum^{\\infty}_{k=0}\\frac{3^2}{2^3}\\left(\\frac{8}{9}\\right)^k\\\\&amp;=\\sum^2_{k=0}\\frac{k^2}{2^k}+\\frac{9}{8}\\sum^{\\infty}_{k=0}\\left(\\frac{8}{9}\\right)^k\\\\&amp;=O(1)\\end{align}\\]다음과 같이 더욱 난해한 조화 수열 또한 점근 한계(asymptotic bounds)를 구할 수 있다.예를 들어 $H_n=\\sum^n_{k=1}\\frac{1}{k}$의 점근 한계 $O(\\lg n)$은 식 $\\eqref{eq:sigmaHarmonSum}$처럼 유도하려면, 먼저 1부터 n까지를 $\\lfloor \\lg n\\rfloor + 1$개의 부분으로 나누고, 각 부분항들의 합의 상한을 1 이하로  맞춘다.식 $\\eqref{eq:sumHarmonProof}$의 첫번재 줄이 성립하는 이유는, $\\lfloor\\lg n \\rfloor$가 2의 제곱수가 아니면 마지막 $\\lfloor\\lg n \\rfloor$번째 부분항들 중, 원본 조화수열에 없는 항들이 포함되기 때문이다.  예를 들어 n이 9일때, 원본 조화수열에는 맨 마지막 항이 1/9이지만, 첫번재 줄의 경우 1/10~1/15까지가 더 더해지므로, 원본 조화수열 합보다 커진다.\\[\\begin{align}\\sum^n_{k=1}\\frac{1}{k} &amp;\\leq \\sum_{i=0}^{\\lfloor\\lg n\\rfloor}\\sum_{j=0}^{2^i-1}\\frac{1}{2^i+j}\\\\&amp;\\leq \\sum_{i=0}^{\\lfloor\\lg n\\rfloor}\\sum_{j=0}^{2^i-1}\\frac{1}{2^i}\\\\&amp;=\\sum^{\\lfloor\\lg n \\rfloor}_{i=0}1\\\\&amp;\\leq \\lg n + 1\\label{eq:sumHarmonProof}\\tag{A.14}\\end{align}\\]적분에 의한 근사 (Approximation by integrals)만약 함수 $f(x)$가 일방적으로 증가하는 경향이라면 적분을 통해 다음과 같은 근사를 할 수 있다.\\[\\begin{align}\\int^n_{m-1}f(x)dx\\leq \\sum^n_{k=m}f(k)\\leq \\int^{n+1}_m f(x)dx\\label{eq:inteApproxIncFunc}\\tag{A.15}\\end{align}\\]아래에 나타나 있는 Figure A.1은 위 식 $\\eqref{eq:inteApproxIncFunc}$을 나타낸다. 수열합은 표의 사각형 지역으로 나타나 있으며, 적분 공간은 곡선 아래의 어두운 부분이다.반대로 함수 $f(x)$가 일방적으로 감소하는 경향이라면 다음과 같이 근사할 수 있다.\\(\\begin{align}\\int^{n+1}_{m}f(x)dx\\leq \\sum^n_{k=m}f(k)\\leq \\int^{n}_{m-1} f(x)dx\\label{eq:inteApproxDecFunc}\\tag{A.16}\\end{align}\\)식 $\\eqref{eq:inteApproxDecFunc}$을 이용해 n번째 조화수열의 항의 근사값을 구할 수 있다.먼저 1부터 n까지의 조화수열 합의 하한을 다음과 같이 구한 뒤,\\[\\sum^n_{k=1}\\frac{1}{k} &amp;\\geq \\int^{n+1}_1 \\frac{dx}{x}\\\\&amp;= \\ln (n+1)\\label{eq:inteApprox3}\\tag{A.17}\\]2부터 n까지의 조화수열 합의 상한을 다음과 같이 구하면,\\[\\sum^n_{k=2} \\frac{1}{k} \\leq \\int^n_1 \\frac{dx}{x}= \\ln n\\]이를 통해 조화수열의 상한 값을 다음과 같이 구할 수 있다.\\[\\sum^n_{k=1} \\frac{1}{k} \\leq \\ln n + 1\\label{eq:inteApprox4}\\tag{A.18}\\]"
  }
  , 
  
  "/articles/computer_science/algorithm/%EB%B3%91%ED%95%A9%20%EC%A0%95%EB%A0%AC.html": {
    title: "병합 정렬",
    date: " Jul 11, 2021 ",
    url: "/articles/computer_science/algorithm/%EB%B3%91%ED%95%A9%20%EC%A0%95%EB%A0%AC.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "Merge Sort(병합 정렬)title: 출처  _Introduction to Algorithm, 3rd, Cormen_을 토대로 정리한 내용입니다병합 정렬은 divide-and-conquer 알고리즘을 이용하여 재귀적으로 정렬하는 알고리즘이다.전반적으로 퀵정렬보다 뒤떨어지고 데이터 크기 만큼 메모리가 더 필요한다.안정된 정렬(stable sort)이므로, 만약 같은 크기의 원소가 둘이 존재하면, 둘의 선후 관계가 바뀌지 않는다.Merge 함수먼저 Merge 함수이다. Merge-Sort 함수 내부에 실행될 함수로, 나눈 두 개의 정렬된 배열을 하나로 합치는 역할을 한다.Merge(A,p,q,r)\tn1=q-p+1\tn2=r-q\t//let L[1..n1+1] and R[1..n2+1] be new arrays\tfor i = 1 to n1\t    L[i] = A[p+i-1]\tfor j = 1 to n2\t    R[j] = A[q+j]\tL[n1+1] = inf\tR[n2+1] = inf\ti = 1\tj = 1\tfor k = p to r // 루프 불변성 대상 루프\t    if L[i]&amp;#38;#60;=R[j]\t        A[k] = L[i]\t        i = i + 1\t    else A[k] = R[j]\t        j = j + 1위 과정의 시간 복잡도는 $O(n)$이다.  존재하는 for loop들이 모두 n에 비례하여 진행됨 즉, n에 대한 선형 함수꼴  이외의 코드는 모두 상수 시간임.Merge 함수의 루프 불변성이때, 17번째 줄부터 마지막 까지의 for 루프의 루프 불변성은 다음과 같이 보존된다.루프 불변성(Loop invariant) 가정 :오름차순 병합 정렬에서 부분배열 $A[p..k-1]$은 $L[1..n1+1]$과 $R[1..n2+1]$의 원소들 중 $k - p$ 개의 가장 작은 원소들로 이루어져 있으며, 정렬되어 있다. 또한, $L, R$ 배열 내에서 현재 아직 $A$ 배열에 추가되지 않은 원소들인 $L[i]$와 $R[j]$는 $L, R$ 배열 내에서 가장 작은 원소이다.초기화에서의 루프 불변성:루프에 진입하면서 $k = p$이며, $A[p…k-1]$은 비어있으므로, $k-p(0)$개의 가장 작은 원소들로 이루어져 있고, 정렬되어 있다. 또한 $L$과 $R$ 배열은 각각 정렬되어 있으므로, $L[1], R[1]$은 각자 배열에서 가장 작은 원소이다. 즉, 루프 불변성이 보존된다.유지에서의 루프 불변성:먼저 $L[i] &lt;= R[j]$인 경우, $L[i]$는 $A[k] = L[i]$ 이전에 $A$에 포함되지 않은 $L$ 내에서 가장 작은 원소이며, $A[k] = L[i]$ 이후에 $A[p…k]$는 $k-p+1$개의 정렬된 가장 작은 원소들을 가지고 있게 된다. 이후 $k$와 $i$가 1씩 증가하면서 루프 불변성을 보존한다.$R[j] &lt; L[i]$인 경우, 16, 17번째 줄의 코드가 똑같은 행동으로 루프 불변성을 보존한다.종료에서의 루프 불변성:$k = r + 1$이 되면서, $A[p..k-1]$ 즉, $A[p..r]$은 $k-p=r-p+1$개의 $L[1..n1+1], R[1..n2+1]$에서 가장 작은 정렬된 원소들을 가지게 된다. 이때 $L,R$에는 각각 무한대의 원소만 하나씩 가지게 되므로, 루프 불변성을 보존한다.Merge-Sort 함수이 다음은 Merge-Sort 함수이다. 전체 배열 A의 부분배열 $A[p..r]$을 또 다른 두개의 부분 배열 $A[p..q]$와 $A[q+1..r]$로 나누어 정렬하는 역할을 하며, 재귀 구조를 포함하고 있다.Merge-Sort(A,p,r)if p &amp;#38;#60; r    q = (p+r)//2    Merge-Sort(A, p, q)    Merge-Sort(A, q+1, r)    Merge(A, p, q, r)$p &gt;= r$이 되는 순간, 배열에는 하나의 원소만 남으므로 굳이 정렬하지 않는다.전체적인 병렬 정렬의 동작은 아래에 그림(Figure 1)으로 묘사되어 있다.divide-and-conquer 알고리즘 분석이러한 재귀 구조를 포함하고 있는 경우에는 점화식(recurrence equation)을 통하여 표현할 수 있다.만약, 입력 크기 n이 어느 수준(c) 이하로 극도로 작아지면, 거의 상수 시간($O(1)$) 안에 풀린다고 볼 수 있다. 예를 들어, 배열의 원소가 하나만 있다면 정렬을 하지 않고,아래에 설명할 A 배열로 Combine 하는 시간만 존재하므로, 상수시간에 풀릴 것이다. 이때, 단 1개짜리 배열을 Combine하는데 걸리는 상수시간을 $c_1$라고 하자.이외에는 먼저 배열을 절반으로 나누는데 $D(n)$ 만큼의 시간이 필요하고, 보통 배열의 크기와 관계없이 상수 시간이 걸린다. $D(n)=O(1)$ (Divide 과정)절반의 배열을 정렬하는데 걸리는 시간을, 두 부분배열로 나누어 처리하므로, $2T(n/2)$만큼의 정렬 시간이 필요하다.(Conquer 과정)마지막으로 이 둘을 위의 Merge 함수처럼 하나로 합하는데, $C(n)$ 만큼의 시간이 필요하다.(Combine 과정) 이때, 위의 Merge 함수 때 설명한 것 처럼 $O(n)$의 시간이 걸리며, 위의 1개짜리 배열을 Combined 하는데 걸리는 시간을 $c_1$이라 하였으므로, 이에 비례해 $nc_1$이라 할 수 있다.위 경우를 식으로 정리하면 아래와 같은 점화식이 나온다.\\[T(n)=\\left\\{\\begin{matrix} O(1),when\\ n\\leq c \\\\ D(n)+2T(n/2)+C(n) \\end{matrix}\\right.=\\left\\{\\begin{matrix} c_1,when\\ n = 1 \\\\ 2T(n/2)+nc_1 \\end{matrix}\\right.\\]이때 $T(n/2)$는 또한 다음과 같이 표현될 수 있다.\\[T(n/2)=\\left\\{\\begin{matrix} O(1),when\\ n\\leq c \\\\ D(n/2)+2T(n/4)+C(n/2) \\end{matrix}\\right.=\\left\\{\\begin{matrix} c_1,when\\ n = 1 \\\\ 2T(n/4)+\\frac{n}{2}\\cdot c_1 \\end{matrix}\\right.\\]이를 대입하고, n은 1보다 크다고 가정하면  $\\eqref{eq:T(n)}$ 즉, $T(n)$은 다음과 같이 정리 된다.\\(T(n)=4T(n/4)+ 2nc_1\\) 이런 식으로 $T(n)$을 계속 더 작은 입력 값의 T 함수로 나누다 보면 아래 그림 Figure 2와 같은 재귀 트리 구조가 나오게 된다.이러한 트리 구조는 상기한 그림과 같이 n의 크기가 1이 되어 상수시간이 나올 때까지 진행되는데, 이러한 층 구조는 총 $\\log n + 1$ 개 만큼 생성된다. (컴퓨터 공학에서의 $\\log$의 밑은 생략될 시 기본 2 임.)그리고 각 층 구조에서 k 갈래로 생성된 비용 $cn/k$은 모두 합하면 언제나 $cn$으로 일정하므로, 각 층에서의 시간 비용은 $cn$이라고 할 수 있다.전체 알고리즘의 시간 복잡도는 $(\\log n+1)cn = cn\\log n + cn$이 되며, 이를 빅오 표기법으로 나타니면 차수와 상수가 떨어지므로 최종적으로 병렬 정렬의 시간 복잡도는 $O(\\log n)$이 된다."
  }
  , 
  
  "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%8A%B8%EB%A6%AC.html": {
    title: "알고리즘 수학 기본-트리",
    date: " Jul 11, 2021 ",
    url: "/articles/computer_science/algorithm/MATH/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8-%ED%8A%B8%EB%A6%AC.html",
    tags: ["알고리즘","CS","MATH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true알고리즘을 위한 수학 - 트리(trees)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.일부 표기나 개념이 기존의 수학과 다를 수도 있으므로, 여기서 배운 내용은 단순 해당 책(Introduction to Algorithm, 3rd, Cormen)의 부록으로 취급해야한다.이 장에서는 트리의 표기법, 정의, 속성 같은 기본적인 것만 배운다.앞선 알고리즘을 위한 수학 - 그래프(graphs)편을 먼저 보고 오는 것을 추천한다.자유 트리(Free trees)앞서 그래프편에서 정의했듯이 자유 트리는 acyclic 무향 연결 그래프이다. 아래 Figure 1 (a)는 자유 트리를 나타낸다.만약, 그래프가 acyclic 무향 비연결 그래프라면, 포레스트 그래프라고 부른다. 아래 Figure 1 (b)는 포레스트를 나타낸다.사이클이 존재하는 Figure 1 (c)는 사이클이 존재하므로 트리도, 포레스트도 아니다.트리에 적용할 수 있는 알고리즘들은 대부분 포레스트에도 적용할 수 있다.트리에 관한 정리 : 자유 트리의 성질 (Thorem for tree: Properties of free trees)자유 트리의 성질무향 그래프 $G=(V,E)$에 대하여 다음과 같은 문장들은 모두 동일한 의미다.  G는 자유 트리이다.  G의 모든 한쌍의 정점은 유일한 단순 경로(simple path)로 연결되어 있다.  G는 연결 그래프이며, 만약 어느 간선이라도 하나 없어지면, G는 비연결 그래프가 된다.                              G는 연결 그래프이며, $          E          =          V          -1$이다.                                                  G는 acyclic이며, $          E          =          V          -1$이다.                      G는 acyclic이며, 간선이 어느 곳이든 하나 추가되면, 사이클이 존재하는 그래프가 된다.증명위의 n번째 문장일 때 n+1 문장이 성립하고 마지막 문장에 의해 첫번째 문장이 성립한다는 것을 증명하면 모두 동일하다는 것이 증명된다.그래프 G가 자유트리이면 모든 정점쌍은 유일한 단순 경로로 연결된다.G는 1번에서 자유 트리로 가정했으므로, 어느 두 정점 간의 경로는 최소 하나의 간단 경로로 이어져 있다.아래의 Figure 2처럼 만약 두 임의의 정점 u와 v가 서로 다른 단순 경로 $p_1$과 $p_2$로 연결되어 있다고 가정하자w는 u에서 v로 향할 때, 두 경로 $p_1,p_2$가 각각 정점 x와 정점 y로 갈라지는 갈림길 정점이다. 또한 정점 z는 이 갈라진 두 경로가 v로 도착하기 위해 통합되는 정점이다.이때 $p’$는 정점 w와 정점 z 간의  $p_1$의 부분 경로이고, $p’‘$는 정점 w와 정점 z간의 $p_2$의 부분 경로이다. 이 두 부분경로 $p’,p’‘$는 시작 정점 w와 끝 정점 z를 제외하고 경로상에 다른 정점을 공유하지 않는다.따라서 $p’$와 $p’‘$를 뒤집은 경로, 또는 $p’‘$와 $p’$를 뒤집은 경로에 의해 정점 w와 정점 z를 포함한 사이클이 생성되며, 이는 G가 자유 트리라는 가정에 모순된다.따라서 자유 트리이면, 모든 정점쌍 간에는 단 하나의 유일한 단순경로를 가지게 된다.그래프 G 내의 모든 정점쌍이 유일한 단순 경로로 연결된다면, G는 연결 그래프이며, 간선이 하나라도 없어지면 비연결 그래프가 된다.어떤 정점쌍 u, v가 간선 (u,v) 하나로 이어진다면, 해당 간선은 정점쌍 u, v에 대한 유일한 단순경로일 것이다.만약 이 간선이 지워진다면, u와 v 사이를 연결해주는 경로가 없어진다.해당 경로는 유일하였으므로, 이제 더 이상 u와 v를 통하는 경로가 없어지게 된다. 따라서 G는 비연결 그래프가 된다.G는 연결 그래프이며, 간선이 하나라도 없어지면 비연결 그래프가 된다면, $|E|=|V|-1$이다.연결 그래프 G 에 대하여 $|E|\\geq |V|-1$ 증명            먼저 연결 그래프 G가 $      E      \\geq      V      -1$임을 수학적 귀납 방법으로 증명해보자면,                  최초, 정점이 1개 일때(=$      V      =1$)일때는 간선이 필요 없으므로(=$      E      =0=      V      -1$) 증명된다.                  이제 정점이 n개일 때의 간선의 수를 구해보자, 정점의 갯수 n-1일 때의 그래프 $G’$는 연결 그래프로, 정점이 $      V’      =n-1$이고 간선의 수는 $      E’      \\geq      V’      -1 =  n-2$일 것이다.                  이 그래프 $G’$에 새로운 정점 w를 추가로 붙이면 연결 그래프인 G가 생성될 것이며, $      V      =n-1+1=n$이다.      연결 그래프여야 하므로, 정점 w는 고립 정점이면 안되므로, 간선 1개 이상을 추가해서 그래프 $G’$에 정점 w를 이어야 한다.            새로 연결한 간선의 수를 $k(k\\geq 1)$라고 하면 새로운 그래프 $G$의 간선의 수는 $      E      \\geq      E’      +k\\geq n-2+k \\geq n - 1$이다.                  따라서 $      V      =n,      E      \\geq      V      -1$이된다.      연결 그래프 G에 대하여 $|E| \\leq |V|-1$ 증명            이제 우리는 연결 그래프 G에 대하여 $      E      \\leq      V      -1$ 를 증명하면 앞서 증명한 $      E      \\geq      V      -1 $과 결합해 $      E      =      V      -1$을 증명할 수 있다.                  먼저 정점이 1개 또는 2개일 때는 $      E      \\leq      V      -1$이 성립하며, 3개 이상 부터는 다음과 같이 증명한다.                  먼저 $      E      \\leq      V      -1$을 만족하는 연결 그래프 G에서 간선 하나를 제거하여 $k\\geq 2$개의 연결요소(conneceted components) $G_i=(V_i,E_i)$로 나눈다.      각 연결요소들은 마찬가지로 연결 그래프이면서, 자유트리이다. 원본 그래프 G가 연결 그래프였기 때문이다.            이때 각 연결요소의 정점 갯수 $V_i$은 원본 그래프 G의 정점 갯수 V보다 작으므로 정점 갯수 1,2일 때도 성립하고 G도 성립한다는 점을 이용한 귀납적 가정을 통해 $      E_i      \\leq      V_i      -1$가 성립한다.                  따라서 전체 연결요소들의 간선의 수는 $\\sum^i_{n=0}E_i=      V      -k\\leq      V      -2$이다.                  나눌 때 제거했던 간선 1개를 추가해준다면 $      E      \\leq      V      -1$가 된다.      G는 연결 그래프이며, $|E|=|V|-1$이면, G는 Acyclic이다.            G에 k개의 정점 $v_1,v_2,\\cdots, v_k$를 포함한 단순 사이클이 존재한다고 가정하고, G의 부분 그래프 $G_k=(V_k,E_k),      V_k      =      E_k      =k$가 이 사이클만으로 이뤄졌다고 하자.                  k가      V      보다 작다면, G는 연결 그래프이므로 사이클에 포함된 정점 중 하나와 연결된 사이클에 속하지 않은 정점 $v_{k+1}$이 존재할 것이다.                  $v_{k+1}$을 포함한 새로운 부분 그래프 $G_{k+1}=(V_{k+1},E_{k+1})$라고 하고, 이때도 마찬가지로 $      V_{k+1}      =      E_{k+1}      =k+1$ 를 만족한다.                  만약 k+1 또한      V      보다 작다면, $G_{k+1}$에 속하지 않으면서, $G_{k+1}$과 연결된 새로운 정점 $v_{k+2}$를 이용해 $G_{k+2}$를 만들어 볼 수 있으며, 이를 $n =      V      $까지 적용해 $      E_n      =      V_n      =      V      $까지 증명할 수 있다.                  이 $G_n$은 G의 부분 그래프로, $E_n$은 0개이상의 간선이 E에서 빠진 것으로, $      E_n      =      V      ,      E_n      \\leq      E      $이므로 $      E      \\geq      V      $인데, 이는 $      E      =      V      -1$을 위반하므로 처음의 가정, “단순 사이클이 존재한다”는 모순된 이야기가 되므로, G는 단순 사이클이 없는 acyclic 이다.      G는 Acyclic이며, $|E|=|V|-1$이면, 간선이 하나라도 추가되면 사이클이 생겨난다.k를 G의 연결 요소 갯수로 놓고, 각 연결 요소는 정의에 의해 자유 트리이다.            G 또한, 앞서서 1번째 문장이 5번째 문장까지 일치한다는 것을 증명했으므로, G는 자유트리이며, 모든 연결요소들의 간선의 합은 $      V      -k$이다.      G 는 트리이므로, k=1이 되며, 2번째 문장에 의해 이미 G에는 모든 정점쌍의 유일한 단순 경로가 존재하므로, 간선을 추가하면 사이클이 생긴다.G는 간선이 하나라도 추가되면 사이클이 생겨나는 Acyclic 그래프이면, G는 자유 트리이다.여기선 G가 연결 그래프란 점만 보이면 되는데, G 내의 모든 정점 중의 어떤 정점 둘 u, v를 가정하자.u와 v는 직접 이웃하지 않는다면, 간선 (u,v)를 추가하면 그래프 내에 사이클이 생겨난다.사이클이 생긴다는 것은, 이미 u와 v를 연결하는 경로가 존재한다는 의미이며, u와 v는 모든 정점 중에서 둘을 고른 것이므로, 모든 u와 v에는 경로가 존재한다는 의미이며, 이는 G가 연결 그래프란 의미이다.G가 연결 그래프이고 Acyclic인 무향 그래프이므로 G는 자유트리이다.루트 존재 트리와 순서 트리(Rooted and ordered trees)루트 존재 트리는 루트(뿌리, root)라고 불리우는 정점이 하나 존재하는 자유트리이다.루트 존재 트리에서의 정점은 노드(node)라고도 부르며, Figure 3은 7번 노드를 루트로 하는 12개의 노드를 포함한 루트 존재 트리이다.루트 노드가 r인 루트 존재 트리 T의 노드 x가 존재할 때, x와 r 간의 경로 사이에 존재하는 어떤 노드 y를 노드 x의 조상(ancestor)라고 하며, 반대로 노드 x는 노드 y의 후손(descendant)이라고 한다. 또한 모든 노드는 자기 자신의 조상이면서 후손이다.이때 자기자신을 제외한 조상과 후손 노드들을 각각 정조상(proper ancestor), 정후손(proper descendant) 노드라고 하며, 루트를 x로 하는 부분 트리는 노드 x의 후손을 가지고 있다.만약 루트 r로부터 노드 x까지로 가는 경로의 마지막에 위치한 간선을 (y,x)라고 할 때, 노드 y는 x의 부모(parent)이고, 노드 x는 y의 자식(child)이다.루트 노드 r은 트리 내에서 유일하게 부모가 없는 노드이며, 같은 부모를 공유하는 노드들은 형제(siblings) 노드이다.자손이 없는 노드를 리프(leaf) 노드 또는 외부(external) 노드 라고 하며, 리프 노드가 아닌 노드를 내부(internal) 노드라고 한다.루트 존재 트리의 노드 x의 자식의 수를 x의 차수(degree)라고 한다. 이는 그래프에서 인접한 모든 노드를 차수로 놓는 것과 달리 부모를 제외하고 차수로 친다.루트 r 부터 노드 x까지의 경로의 길이를 T에서의 x 노드의 깊이(depth)라고 하며, 같은 깊이의 노드들의 집합을 레벨(level)이라고 한다.트리의 높이(height)는 리프 노드로 향하는 자식 방향(아래 방향)의 가장 긴 경로의 간선의 수이며, 트리의 높이는 곧 루트의 높이다. 또한 트리의 높이는 가장 깊이 값이 큰 노드의 깊이 값과 같다.순서 트리(ordered tree)는 각 노드의 자식들이 정렬되어 있는 루트 존재 트리이다.즉, 노드에 k 개의 자식이 있다면, 오름차순 혹은 내림차순으로 자식들이 배치된다. Figure 3 (a)와 (b)는 루트 존재 트리로써 동일하지만, 순서 트리로써는 서로 다른 트리이다.이진 트리와 위치 트리(Binary and positional trees)이진 트리(Binary tree)는 재귀적으로 정의되는데, 노드들은 다음 두 종류를 포함한다.  아무 노드도 포함하지 않거나  세가지 노드들의 서로소 집합, 루트 노드, 좌측 부분 이진트리, 우측 부분 이진트리아무 노드도 포함하지 않은 이진 트리를 빈 트리(empty tree) 또는 널 트리(null tree) 또는 NIL 이라고 부른다.만약 왼쪽 부분 트리가 비어있지않으면, 왼쪽 부분 트리의 루트 노드를 루트의 좌측 자식이라고 하며, 오른쪽일 경우 루트의 우측 자식이라고 한다.만약 부분 트리가 NIL 이라면, 자식이 없거나 비어있다고 표현한다. 아래 그림 Figure 4의 (a)가 이진 트리이다.다만, 순서 트리에서는 자식이 하나만 있다면, 자식의 위치, 즉 왼쪽 자식이든 오른쪽 자식이든 구분하지 않는다.  위에서 설명했던 Figure 3의 두 트리와 비교하자, 만약 순서 트리의 노드의 차수가 2라면, 그때는 왼쪽 자식 오른쪽 자식을 엄격하게 구분한다!Figure 4의 (a)와 (b)는 7번 노드의 자식, 5번 노드가 좌측, 우측 노드 차이로 인해 다른 이진 트리로 분류되지만, 만약 순서 트리라면 (a)와 (b)는 같은 트리로 분류된다.이진 트리 내의 위치 정보를 순서 트리의 내부 노드를 이용하여 나타낼 수 있다.  순서 트리의 자식이 비어있는 노드들의 빈 노드 자리에 Figure 4의 (c)처럼 사각형으로 표기된 특별 리프 노드를 집어 넣어 전 이진 트리(full binary tree)로 만든다.          즉, 모든 노드들이 리프 노드이거나 차수가 2가 되게 만든다.        이제 루트 노드를 제외한 모든 자식 노드들에 순서가 부여되어 위치 정보를 나타낸다.          순서 트리는 원래 자식이 하나일 때는 위치가 구분되지 않았지만, 이제 모두 자식이 둘이므로 자식의 위치가 중요하게 되므로 특별한 정보를 가지게 된다.      이러한 방법으로 순서 트리의 노드들에게 구분되는 양수를 주어 순서트리와 다른 위치 트리(positional tree)를 만들 수 있는데, 이때 이진트리 뿐만 아니라 노드가 최대 k개의 자식을 가질 수 있는 k진 트리를 이용할 수 있다.완전 k진 트리(complete k-ary tree)는 모든 리프 노드가 동일한 레벨을 가지고, 모든 내부 노드의 차수가 k인 트리이다. 아래 Figure 5는 높이가 3인 완전 이진 트리를 보여준다.이때, 완전 k진 트리의 특정 높이 h를 가지는 노드의 갯수는 $k^h$로 구할 수 있다. 예를 들어, 깊이가 0일 때는 루트 노드 1개, 깊이가 1일 때는 2개이다.따라서 완전 k진 트리의 리프 노드의 갯수는 총 $k^h$개 이며, 결과적으로 완전 k진 트리의 높이는 리프 노드의 갯수가 n이라 할때 $\\log_kn$이다.완전 k진 트리의 내부 노드의 숫자는 다음과 같이 구할 수 있다.\\(1+k+k^2+\\cdots +k^{h-1}=\\sum^{h-1}_{i=0}k^i=\\frac{k^h-1}{k-1}\\)따라서 완전 이진 트리의 모든 노드의 갯수는 $2^h -1$이다."
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%B4%EB%9E%80.html": {
    title: "알고리즘이란",
    date: " Sep 19, 2021 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%B4%EB%9E%80.html",
    tags: ["알고리즘","CS","요약"],
    content: "알고리즘이란?알고리즘의 정의      유한한 단계를 통해 문제를 해결하기 위한 절차나 방법        컴퓨터가 계산 문제를 수행하기 위한 단계적 방법이나 도구        입력값을 받아 원하는 출력값을 내보내는 잘 정의된 계산 과정    코드 예시 (1~100까지 더하는 코드의 2가지 방법)def calcSum(n) :  sum = 0  for i in range(1, n+1):      sum = sum + i  return sumdef calcSum2(n):    return n*(n+1)//2print(calcSum(100))print(calcSum2(100))알고리즘 표현법슈도 코드(의사 코드)Merge(A,p,q,r)  n1=q-p+1  n2=r-q  //let L[1..n1+1] and R[1..n2+1] be new arrays  for i = 1 to n1      L[i] = A[p+i-1]  for j = 1 to n2      R[j] = A[q+j]  L[n1+1] = inf  R[n2+1] = inf  i = 1  j = 1  for k = p to r      if L[i]&amp;#38;#60;=R[j]          A[k] = L[i]          i = i + 1      else A[k] = R[j]          j = j + 1  병합 정렬의 의사 코드특정 프로그래밍 언어의 문법을 따라 쓰여진 것이 아니라, 일반적인 언어로 코드를 흉내 내어 알고리즘을 써 놓은 코드      의사 코드로 흉내만 냈으며, 컴퓨터에서 실행 불가능        알고리즘을 대략적으로 모델링하는데 쓰임  순서도(flow diagram)      프로그램, 작업의 진행 흐름을 순서에 따라 여러 가지 기호나 문자로 나타낸 도표        흐름도라고도 함, 프로그램의 논리적인 흐름, 데이터의 처리과정을 표현하는데 사용.        프로그램을 작성하기 전에 프로그램의 전체적인 흐름과 과정 파악을 위해 거쳐야하는 작업  알고리즘의 성능 분석      정확성: 얼마나 정확하게 동작하는가?        작업량: 얼마나 적은 연산으로 원하는 결과를 얻어내는가? (=시간 복잡도)        메모리 사용량 : 얼마나 적은 메모리를 사용하는가? (=공간 복잡도)        단순성: 얼마나 단순한가?        최적성 : 더 이상 개선할 여지 없이 최적화되었는가?  많은 문제에서 알고리즘의 성능 분석 기준으로 알고리즘의 작업량을 비교하여 실제 걸리는 시간을 측정, 또는 실행되는 명령문의 개수(주로 쓰임)를 계산한다.이를 시간 복잡도로 많이 표기한다.  알고리즘 1 비교적 성능이 떨어진다.def calcSum(n) :  sum = 0 # 1번  for i in range(1, n+1): # n 갯수마다 1번      sum = sum + i # n 갯수마다 1번  return sum# 시간 복잡도 : 1 + n * 2= 2n+1print(calcSum(100)) # 총 99번의 덧셈 연산  알고리즘 2 비교적 성능이 낫다.def calcSum(n):    return n*(n+1)//2 #3번# 시간 복잡도 : 3print(calcSum(100)) # 단 3번의 연산"
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%84%B1%EB%8A%A5%20%EB%B6%84%EC%84%9D.html": {
    title: "알고리즘 성능 분석",
    date: " Sep 20, 2021 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%84%B1%EB%8A%A5%20%EB%B6%84%EC%84%9D.html",
    tags: ["알고리즘","CS","요약"],
    content: "알고리즘 성능 분석title: 출처  Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.알고리즘 비용에는 메모리, 통신 대역폭, 필요 하드웨어 등이 있겠지만, 가장 중요시 여기는 자원 기준은 역시 연산 시간이다.사실, 알고리즘의 성능은 입력 값, 컴퓨터의 구조, 사용자의 목적에 따라 다르게 평가될 수 있지만, 일반적인 RAM 구조의 컴퓨터를 대상으로 한다.알고리즘 성능 자세히 분석하기 : 삽입 정렬다음과 같은 삽입 정렬 알고리즘을 예시로 분석해보겠다.// 삽입 정렬 예시 코드for j = 2 to A.length    key = A[j]    i = j - 1    while i &amp;#38;#62; 0 and A[i] &amp;#38;#62; key        A[i+1] = A[i]        i = i - 1    A[i+1] = key각 코드의 연산 비용 합산 구하기입력 크기를 n으로 설정한 뒤, 각 코드의 한번의 기본적인 연산이나 명령 수행의 시간적 비용을 $c_i$로 가정하고, 이를 연산 횟수와 곱하여 구해보자. (주석의 경우, 무시됨.)            순서      코드      비용      연산 횟수                  1      for j = 2 to A.length      $c_1$      $n$              2      key = A[j]      $c_2$      $n-1$              3      i = j - 1      $c_3$      $n-1$              4      while i &gt; 0 and A[i] &gt; key      $c_4$      $\\sum^n_{j=2}t_j$              5      A[i+1] = A[i]      $c_5$      $\\sum^n_{j=2}(t_j-1)$              6      i = i - 1      $c_6$      $\\sum^n_{j=2}(t_j-1)$              7      A[i+1] = key      $c_7$      $n-1$            첫번째 줄 코드 : 두번째 원소부터 연산을 시작하여, 입력 크기를 1 넘어가는 순간에 종료 조건이 만족되어 종료되므로, $n-1$이 아니라 $n$번 연산이다.        2, 3, 7번째 줄 코드: 첫번째 줄 코드가 종료 조건이 만족되는 순간을 제외하고 모두 연산되므로 $n-1$번 연산이다.        4, 5, 6번째 줄 코드: 마찬가지로 j가 2부터 $n$까지 $n-1$번의 연산을 거치겠지만, 각각의 연산은 미리 정렬된 부분배열의 원소들의 크기와 현재 A[j]의 비교 결과에 따라 달라지므로, $t_j$로 둘 것이다.                  연산 결과의 달라짐을 알아보기 위해, 오름차순으로 정렬의 극단적인 예 두가지를 들어보자.                              A[j] 값이 5이고, 부분배열 A[1...j-1]이 [1,2,3,4]일 때 $\\rightarrow$ 첫 번째 연산에서 종료조건이 만족되어 연산이 종료되므로 연산 횟수 $t_j$는 단 1번만 이루어진다.                                A[j] 값이 0이고, 부분배열 A[1...j-1]이 [1,2,3,4]일 때 $\\rightarrow$ A[j]는 맨 앞에 위치해야하므로, 연산은 부분 배열의 크기인 4번에 종료조건 확인을 위한 1번을 추가해  $t_j$는 총 5가 될 것이다.                                    5, 6번째 줄 코드 : 4번의 while문의 종료 조건이 만족될 때는 실행되지 않으므로 $t_j-1$번 수행된다.  이러한 연산들의 비용과 연산 횟수를 곱해서 모두 더하면 알고리즘의 총 비용, $T(n)$을 계산 할 수 있다.\\[T(n)=c_1n+c_2(n-1)+c_3(n-1)+c_4\\sum^n_{j=2}t_j+c_5\\sum^n_{j=2}(t_j-1)+c_6\\sum^n_{j=2}(t_j-1)+c_7(n-1)\\]이러한 방법을 이용해 연산 비용 뿐만 아니라 메모리 사용량 등 또한 계산할 수 있다.이상적인 상황과 최악의 상황에서의 삽입 정렬$t_j$를 설명할 때 사용했던 이상적인 상황과 최악의 상황에 따른 성능을 알아보자.이상적인 상황에서는 첫번째 연산에서 종료조건을 만족하므로 $t_j=1$일 때, 순서 5, 6 코드의 연산 횟수는 $\\sum^n_{j=2}(1-1)$은 0이 되며, 나머지는 다음과 같이 계산 된다.\\[\\displaylines{T(n)=c_1n+c_2(n-1)+c_3(n-1)+c_4(n-1)+c_7(n-1)\\\\=(c_1+c_2+c_3+c_4+c_7)n-(c_2+c_3+c_4+c_7)}\\]이상적인 상황에서 삽입정렬은 다항식 $an+b$ 형태로, n에 대한 선형 함수가 된다.최악의 상황에서의 삽입 정렬의 경우, 정렬된 부분 배열의 모든 원소에 대해 연산이 진행되므로 A[1...j-1]의 길이, 즉 j-1번의 자리 바꿈 연산과, 마지막 종료조건 확인 연산이 합해져, $t_j=j$가 되게 된다.시그마 값들에 대입해 연산해보면,\\[\\sum^n_{j=2}j=(\\sum^n_{j=1}j)-1=\\frac{n(n+1)}{2}-1\\\\\\sum^n_{j=2}(j-1)=\\sum^{n-1}_{j=1}j=\\frac{n(n-1)}{2}\\]알고리즘의 총 비용은 다음과 같이 된다.\\[\\displaylines{T(n)=\\\\c_1n+c_2(n-1)+c_3(n-1)+c_4 \\left(\\frac{n(n+1)}{2}-1\\right)+c_5\\left(\\frac{n(n-1)}{2}\\right)+c_6\\left(\\frac{n(n-1)}{2}\\right)+c_7(n-1)\\\\=\\left(\\frac{c_4}{2}+\\frac{c_5}{2}+\\frac{c_6}{2}\\right)n^2+\\left(c_1+c_2+c_3+\\frac{c_4}{2}-\\frac{c_5}{2}-\\frac{c_6}{2}+c_7\\right)n -(c_2+c_4+c_5+c_8)}\\]최악의 상황에서 삽입정렬은 다항식 $an^2+bn+c$ 형태로, n에 대한 이차 함수(quadratic function)가 된다.이러한 표기는 성능을 표기하는데 복잡하고, 정수의 입력값만 받기 때문에 사용되지 않고, 주로 점근적인 표기법인 시간복잡도를 이용해 표기한다.현실적인 대회에서의 알고리즘 분석대회의 성능 합격 기준은  연산 시간  메모리 사용량즉, 입력값의 양 $N$과 성능 제한을 비교하여 알고리즘의 합격 여부를 구현 이전에 예측 가능시간 복잡도보통의 CPU 구조의 개인 PC나 채점용 서버, C, C++ 언어 기준으로 대략 10억번의 연산에 1초 이상의 시간이 걸림.다만, 컴파일러, CPU, 세부 구현 등에 따라 성능이 수십배 차이 날 수 있으므로, 절대 맹신하지 말자예시만약 입력값의 양 $N$이 최대 2,000이고, 시간 제한이 1초 이내인 문제에서 시간 복잡도 $O(n^3)$인 알고리즘을 구현했다면, 대략적인 연산량과 예상 연산 시간은 다음과 같다.\\[\\displaylines{operations\\ = 2000^3 = 8,000,000,000\\\\estimated\\ operation\\ time =  8,000,000,000 \\times \\frac{1s}{1,000,000,000}\\approx8s}\\]시간 복잡도 $O(n^3)$의 알고리즘은 제한을 통과하지 못할 것이다.따라서, 시간 복잡도가 $O(\\log{n} \\cdot n^2)$ 이하인 알고리즘을 구현해야할 것이다.\\(\\displaylines{operations\\ = \\log{2000} \\cdot 2000^2 \\approx 4.386\\times 10^7 = 43,860,000\\\\estimated\\ operation\\ time =  43,860,000 \\times \\frac{1s}{1,000,000,000}\\approx0.44s}\\)평소에 알고리즘들의 시간복잡도를 미리 파악하고 정리를 통해 구현 시, 이를 기준으로 알고리즘을 바꾸거나 프루닝 등의 최적화를 고려하여 시간 절약 가능시간의 측정파이썬의 경우 다음과 같은 방식으로 수행 시간을 측정할 수 있다.import timestart_time = time.time() # 측정 시# # 당신의 알고리즘 코드# end_time = time.time() # 측정 종료print(&amp;#34;time:&amp;#34;, end_time - start_time) # 수행 시간 출력(초 단위)파이썬은 C 계열 언어보다 느리므로 1초에 2000만 연산 정도, pypy3는 초당 1억번 연산으로 계산해도 된다.공간 복잡도메모리 사용량의 경우 MB 단위로 제시되는 경우가 많음. ex) 메모리 제한 128MB대부분 문제는 정수형 자료형인 int의 배열을 이용하며 하나에 4byte임.자료형 크기 파악단, 컴파일러에 따라 자료형의 크기는 달라질 수 있으므로, 사용하는 기술의 자료형 크기를 미리 파악하자.  Java, C 계열 int : 4 Bytes  범위 : -2,147,483,648 ~ 2,147,483,647 ($-2^{31} \\sim 2^{31}-1$)  Java, C 계열 long long : 8 Byte  범위 : -9223372036854775808 ~ 9223372036854775807 ($-2^{63} \\sim 2^{63}-1$)Java의 표준 혹은 C 계열의 외부 라이브러리인 BigInter 클래스를 이용하면 무제한의 범위를 가질 수 있다.      C 계열 char : 1 Byte  범위 : $-128 \\sim 127$        Java 계열 char : 2 Byte  범위 : 0 ~ 65535        Python 계열 int : 정수형의 제한 범위가 존재하지 않아 엄밀히 말하면 무제한 이지만, python3의 list 기준으로 정수 하나는 8 byte를 차지한다.    즉 일반적으로 2배 정도 사용하지만, $O(n)$으로 비례하는 것은 같으므로 점근적으로 4byte로 보아도 무방하다.    ```pythonimport sysprint(sys.getsizeof([0])) # 64print(sys.getsizeof([0 for i in range(10)])) # 184print(sys.getsizeof([0 for i in range(100)])) # 920print(sys.getsizeof([0 for i in range(1000)])) # 8856lst = []print(sys.getsizeof(lst)) # 56lst.append(1)print(sys.getsizeof(lst)) # 88 # 32만큼 증가lst.append(1)print(sys.getsizeof(lst)) # 88lst.append(2)print(sys.getsizeof(lst)) # 88lst.append(2)print(sys.getsizeof(lst)) # 88, 32만큼 증가lst.append(2)print(sys.getsizeof(lst)) # 120lst.append(2)print(sys.getsizeof(lst)) # 120lst.append(2)print(sys.getsizeof(lst)) # 120lst.append(2)print(sys.getsizeof(lst)) # 120lst.append(2)print(sys.getsizeof(lst)) # 184, 64 만큼 증가    python의 `list`는 C++의 `Vector` 처럼 `list`의 크기를 미리 크게 잡아놓고, 사이즈 한도가 커질 때 마다 사이즈 증가량을 2배로 키우는 방식으로 증가한다.- Python 계열 `str` : 무제한만약, 기억나지 않거나 컴파일러가 생소한 경우 다음과 같이 직접 코드로 알 수 있다.```c#include &amp;#38;#60;stdio.h&amp;#38;#62;int main()&amp;#123;    printf(&amp;#34;char: &amp;#37;d, short: &amp;#37;d, int: &amp;#37;d, long: &amp;#37;d, long long: &amp;#37;d\\n&amp;#34;,         sizeof(char),         sizeof(short),        sizeof(int),        sizeof(long),        sizeof(long long)    );    // char: 1, short: 2, int: 4, long: 4, long long: 8    return 0;&amp;#125;예시  int a[1000];: 4KB  int a[1000000];: 4MB  int a[2000][2000];: 16MB"
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%8B%9C%EA%B0%84%20%EB%B3%B5%EC%9E%A1%EB%8F%84.html": {
    title: "시간 복잡도",
    date: " Sep 22, 2021 ",
    url: "/articles/computer_science/algorithm/%EC%8B%9C%EA%B0%84%20%EB%B3%B5%EC%9E%A1%EB%8F%84.html",
    tags: ["알고리즘","CS","요약"],
    content: "시간 복잡도(Time complexity)Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.앞서 알고리즘 연산 비용 구하기 글을 보고 오는 것을 추천시간 복잡도(Time complexity)란?시간복잡도는 알고리즘이 문제를 해결하는데 걸리는 시간과 입력의 함수 관계를 점근적인 표기법으로 나타내어 정량화한 것이다.title:  시간 복잡도 표기법의 그래프 예시[Fig 1]주로 빅오(big-O) 표기법으로 표현하며, 이외에도 빅오메가(big-$\\Omega$) 표기법, 빅세타(big-$\\Theta$) 표기법이 존재한다.title: 표기법의 오용사실, 여기 설명할 모든 표기법은 엄밀히 말하면 함수들의 집합이고, $f(n)$은 $O(n^2)$의 복잡도를 무한한 수의 함수 중 하나이기 때문 함수 $f(n)$에 대하여 $f(n)\\in O(g(n))$으로 표현하는게 맞지만, $f(n)=O(g(n))$으로 오용되곤 한다.또한, 상수 시간을 의미하는 $O(1)$ 또한 오용이며, 정확히는 $O(n^0)$이 옳은 표기 (무한으로 발산할 수 있는 변수 n을 표기해줘야 한다. )하지만 나는 앞으로 위 두 표기 모두를 정상으로 표기할 것이다.$\\Theta-표기법$빅세타(big-$\\Theta$) 표기법은 알고리즘 실행 시간에 대한 점근적 상한과 하한을 표기하는 표기법이다.주어진 함수 $g(n)$(주로 알고리즘 시간 복잡도의 최고차항)에 대하여 $\\Theta(g(n))$은 다음과 같은 함수의 집합으로 정의된다.$\\Theta(g(n))={f(n):모든\\ n\\geq n_0에\\ 대해\\ 0\\leq c_1g(n)\\leq f(n)\\leq c_2g(n)를\\ 만족하는\\ 양의\\ 상수\\ c_1,c_2,n_0가\\ 존재}$즉, 우리가 구한 알고리즘의 연산비용 다항식 $f(n)$의 충분히 큰 수 $n_0$부터 그래프는 점근적 표기인 $g(n)$의 서로 다른 정수배 $c_1,c_2$의 그래프에 둘러쌓인 형태가 되야 한다. (Fig 1 좌)이를 $g(n)$이 $f(n)$의 asymptotically tight bound(점근적 극한 한계?)라고 한다.즉, 빅세타(big-$\\Theta$) 표기법은 특정 상수 c를 바꾸는 것으로 최대 예상 비용과 최소 예상 비용을 예측할 수 있는 함수를 의미한다.$\\Theta$-표기법 예시\\[c_1n^2\\leq \\frac{1}{2}n^2-3n\\leq c_2n^2 \\rightarrow divide\\ all\\ item\\ into\\  n^2 \\\\=c_1\\leq \\frac{1}{2}-\\frac{3}{n}\\leq c_2\\]위의 경우 $c_2$가 $1/2$이상일 경우, $n$은 0 이상부터 성립하게 되므로 $f(n)\\in \\Theta(n^2)$이다.\\[c_1n^2\\leq 6n^3\\leq c_2n^2 \\rightarrow divide\\ all\\ item\\ into\\ n^2 \\\\=c_1\\leq 6n\\leq c_2\\]위의 경우 $c_2$가 아무리 크더라도, $c_2$를 넘어서는 양수 $n$이 존재하므로, $f(n) \\notin \\Theta(n^2)$이다.O-표기법빅오(big-O) 표기법은 최악의 상황에서의 점근적 상한을 표기하는 표기법이며, 가장 널리 사용된다.$O(g(n))$은 다음과 같은 집합으로 정의된다.$O(g(n))={f(n): 0\\leq f(n) \\leq cg(n)을\\ 만족하는\\ 양의\\ 정수\\ c, n_0가\\ 존재한다.}$즉, 빅오(big-O) 표기법은 점근적으로 함수의 상한을 알려준다.(Fig 1 중앙)앞서 설명한 $\\Theta$-표기법이 상한과 하한을 알려주므로, $\\Theta(g(n)) \\in O(g(n))$이 성립한다.title: 상한과 하한을 모두 알려주는 $\\Theta$-표기법과 비교하여 상한만 알려주는 O-표기법을 더 많이 사용하는 이유?$\\Theta$ 표기법은 모든 입력에 대해 보장하지 않는다.예를 들어, 삽입 정렬은 최악의 상황, 보통 상황에서 $\\Theta(n^2)$이다.하지만 모든 원소가 이미 정렬되어 있는 최적의 상황에서 $\\Theta(n)$이다.따라서  $\\Theta$ 표기법으로 “삽입 정렬은 최악과 보통 상황에서 $\\Theta(n^2)$, 최적의 상황에서는 $\\Theta(n)$이다”라고 두개로 나누어 표기해야 옳은 표현이며 복잡하기 그지없다.반면에 $O$-표기법은 애초에 성능의 하한을 고려하지 않기 때문에 삽입정렬은 $O(n^2)$이다. 최고차항이 1인 다항식 $an+b$ 또한 $O(n^2)$에 속하기 때문이다.$\\Omega$-표기법빅오메가(big-Omega) 표기법은 최적의 상황에서의 점근적 하한을 표기하는 표기법이다.(Fig 1 우측)$\\Omega(g(n))={f(n): n\\geq n_0일\\ 때,\\ 0\\leq cg(n)\\leq f(n)를\\ 만족하는\\ 양의\\ 상수\\ c와\\ n_0가\\ 존재한다.}$이외에도      빅오 표기법과 비슷하나 점근적으로 근접하지 않은 o(리틀-오)-표기법                  즉, 최악의 상황의 연산 비용보다 많은 비용을 표기함. 예를 들어, 식 $f(n)$이 $2n$일 경우, 빅오 표기법으로는 $O(n)$이지만, 리틀오 표기법으로는 $o(n)$이 성립하지 않으며, $o(n^2), o(n^3),\\dots$이 가능하다.                    수학적 정의로는$\\lim_{n\\rightarrow\\infty}=\\infty$이 성립할 때, $o(g(n))=f(n)$이 성립.                  빅오메가 표기법과 비슷하나 점근적으로 근접하지 않은$\\omega$(리틀 오메가)-표기법                  즉, 최적의 상황의 연산 비용보다 적은 비용을 표기함. 예를 들어 식 $f(n)$이 $2n^2$일 경우, 빅오메가 표기법으로는 $\\Omega(n^2)$이지만, 리틀 오메가 표기법으로는 $\\omega(n^2)$이 성립하지 않으며, $w(n), w(n^0),\\dots$이 가능하다.                    수학적 정의로는 $\\lim_{n\\rightarrow\\infty}=0$이 성립할 때, $\\omega(g(n))=f(n)$이 성립.            이 존재한다.최악의 상황(=big-O notation)을 기준으로 분석하는 이유알고리즘 연산 비용 구하기에서 구한 삽입 정렬의 연산비용 다항식은 우리가 익히 들어왔던 시간복잡도 빅오(big-O) 표기법에 의하면 $O(n^2)$으로 표기.빅오 표기법은 이전 최악의 상황에서의 비용값 중 최고차항만 간략표기하여 구함. 입력값이 충분히 커지면, 같은 최고차항을 가진 식 간의 비용 비교시, 계수와 상수의 영향이 거의 의미가 없어지기 때문.      예를 들어 상수, 계수가 큰 $100n+200$의 알고리즘 1과 차수가 큰 $n^2$의 알고리즘 2가 존재할 때                  $n= 100$ 까지는 알고리즘 1이 더욱 효율적.                    $n = 101$ 부터는 알고리즘 2가 더욱 효율적.        입력값이 적은 경우, 어느쪽이나 연산속도가 빠름, 입력값이 크다면 연산속도가 더욱 빠른 알고리즘 2를 선호 $\\rightarrow$ 결국 최고차항이 작은 알고리즘이 더욱 유리하다.            알고리즘을 평가 시, 평균적인 상황, 최적의 상황이 아니라, 최악의 상황을 기준으로 분석하는 이유      어떠한 입력값이든지, 필요 시간의 상한을 예상하기 위해. 즉, 연산 완료를 보장받을 수 있는 가장 빠른 시간이기 때문에.        의외로 최악의 상황에서의 연산은 실제 업무의 상황에서 평균적인 상황 만큼이나 자주 나타난다.          예를 들어, 검색 엔진이나 데이터베이스에서 특정 항목을 색인할 때, 해당 항목이 존재하지 않다면, 모든 항목을 찾아보게 된다.(=최악의 상황에서의 연산)            평균적인 상황에서의 연산 비용은 의외로 최악의 상황에서의 연산 비용과 큰 차이가 나지 않는 경우가 많다.          예시를 위해 평균적인 상황에서의 삽입 정렬의 비용은 키값 A[j]가 미리 정렬된 부분 배열의 중간까지만 비교하며, 연산 횟수 $t_j$는 $j/2$이다.삽입 정렬의 총 비용 다항식을 구하면 계수와 상수가 조금 작을 뿐이지 이차 함수인 $O(n^2)$의 성능임.      주요 알고리즘들의 시간 복잡도title: Fig 2. 입력 크기 $n$과 함수의 값에 대한 그래프, 빅오 복잡도 그래프 또한 이에 비례한다.정렬            정렬 방법      최적      평균      최악      공간 복잡도                  선택(selection)      $O(n^2)$      $O(n^2)$      $O(n^2)$      $O(n^2)$              버블(bubble)      $O(n^2)$      $O(n^2)$      $O(n^2)$      $O(n)$              삽입(insert)      $O(n)$      $O(n^2)$      $O(n^2)$      $O(n^2)$              병합(merge)      $O(n\\log n)$      $O(n\\log n)$      $O(n\\log n)$      $O(n\\log n)$              퀵(quick)      $O(n\\log n)$      $O(n\\log n)$      $O(n^2)$      $O(n\\log n)$              힙(heap)      $O(n\\log n)$      $O(n\\log n)$      $O(n\\log n)$      $O(n)$              쉘(shell)      $O(n^{1.25})$      $O(n^{1.25})$      $O(n^{1.25})$      $O(n)$      그래프                   최적      평균      최악      공간                                                     기타                   최적      평균      최악      공간                                                     "
  }
  , 
  
  "/articles/computer_science/algorithm/%EB%A3%A8%ED%94%84%20%EB%B6%88%EB%B3%80%EC%84%B1.html": {
    title: "루프 불변성",
    date: " Sep 23, 2021 ",
    url: "/articles/computer_science/algorithm/%EB%A3%A8%ED%94%84%20%EB%B6%88%EB%B3%80%EC%84%B1.html",
    tags: ["알고리즘","CS","요약"],
    content: "루프 불변성(Loop Invariant)  Introduction to Algorithm, 3rd, Cormen을 토대로 정리한 내용입니다.루프 불변성의 정의알고리즘이 타당한지 확인하기 위해 사용되는 성질로, 루프 불변성을 정의하고 루프가 제대로 돌아가는 동안 루프 불변성이 변하지 않음을 확인함으로써, 구현 알고리즘이 옳은지 점검하는 데 도움을 줄 수 있다.루프 불변성이 만족하는 지는 총 3가지를 증명해야한다.      초기화(Initialization) 조건 : 첫번째 루프에 들어가기 전에 루프 불변성이 참이다.        유지 (Maintenance) 조건 : 루프에 들어가기 전에 참이였다면, 다음 루프의 이전에도 루프 불변성은 참이다.        종료 (Termination) 조건 : 루프가 종료되었을 때, 루프 불변성이 참이다.  루프 불변성 유용성의 근거: 수학적 귀납법위 세 가지의 경우가 증명을 통해 알고리즘의 옮음을 증명할 수 있다. 이는 마치 수학적 귀납법 증명 과정과 비슷하다.  🔵 수학적 귀납법  어떠한 자연수가 특정 조건을 만족하고, 다음 자연수 또한 만족한다는 것을 증명하면, 모든 자연수가 해당 조건을 만족한다는 증명.예를 들어 초기화와 유지 단계가 참이면, 우리는 언제나 루프에 진입하기 전에 루프 불변성이 참이라는 것을 증명할 수 있다.초기화는 귀납법에서 전제(Base case)에 해당하고, 유지는 귀납적 단계에 해당한다.세번째 종료 단계는 알고리즘 증명에 있어서 가장 중요한데, 루프의 종료 조건에 관련되어 사용되며, 유한한 루프 이후에 루프가 종료되기 때문에, 무한한 자연수에 적용되는 수학적 귀납법과의 차이를 보인다.루프 불변성의 예시 : 삽입 정렬//의사 코드는 숫자가 1부터 시작함for j = 2 to A.length    key = A[j]    // 정렬된 배열 A[1..j-1]에 A[j]를 삽입.    i = j - 1    while i &amp;#38;#62; 0 and A[i] &amp;#38;#62; key        A[i+1] = A[i]        i = i - 1    A[i+1] = key루프 불멸성 : 부분 배열 A[1...j-1]은 입력 배열의 A[1...j-1]까지의 원소들이며, 언제나 정렬되어 있다.초기화 조건 성립 증명  첫 루프 진입 전에, j = 2이며, 부분 배열 A[1..j-1]은 A[1]로 하나만 존재하므로, 정렬되어 있으며, 이는 루프 불멸성을 참으로 만든다.유지 조건 성립 증명  상기 의사 코드의 while부터 끝 부분은 A[j] = key가 올바른 순서에 위치할 때까지 i가 1씩 줄어들다가, 올바른 위치에 A[j]를 집어넣게 된다.  A[1...j]까지의 모든 원소는 입력 배열 A[0...j]의 원소이며, 모두 정렬되게 된다. 따라서 루프 불멸성은 언제나 참이다.종료 조건 성립 증명  j는 for loop에 의해 언제나 1씩 증가하므로 결국엔 A의 길이를 넘게 되고, 종료 조건이 성립되어 루프가 종료되게 된다.  이때의 부분 배열 A[1...A.lenght]는 입력 배열 A[1...A.length]와 같고, 모두 정렬되어 있으며, 이는 모든 배열이 정렬되어 있음을 의미하며, 루프 불멸성이 참이 된다.이를 통해 세 단계 모두 참이므로 해당 알고리즘의 루프의 결과는 옳다."
  }
  , 
  
  "/articles/web/backend/ExpressJs%20%ED%95%99%EC%8A%B5.html": {
    title: "ExpressJs 학습",
    date: " Mar 16, 2022 ",
    url: "/articles/web/backend/ExpressJs%20%ED%95%99%EC%8A%B5.html",
    tags: ["EXPRESSJS","JS","BE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueExpress.js  Fast, unopinionated, minimalist web framework for Node.jsNode.js 기반의 심플한 웹 프레임워크, 주로 벡엔드 서버를 만드는데 사용한다.정말 기본적인 서버이고, MIT 라이센스이므로, 좋게 말하면 가볍고 여러 기능을 제약없이 구현하기 쉬우며, 나쁘게 말하면 많은 부분을 직접 구현해야 한다.더욱 자세한 사항은 Express.js 공식 문서 참조 바람.개발 환경 및 기본 구동 설정 (Configure Dev settings.)설치(Install)기본적으로 최신 버전의 Node.js가 설치가 끝난 상태여야 한다.$ mkdir myapp # 프로젝트 폴더 생성$ cd myapp # 프로젝트 폴더 이동$ npm init # 기본적인 프로젝트 설정, package.json 생성$ npm install express --save # node_modules에 express 설치 및 dependency 설정이 글에서는 package.json 생성 설정의 기본값을 전제로 한다. 추가적인 설정에 따라 일부 용어가 바뀔 수 있다. 예를 들면,  main 항목을 index.js 대신 app.js로 사용할 경우, app.js로 생성해야 한다.  Javascript 대신 Typescript를 사용할 수도 있다.  자세한 사항은 Node.js참조기본 세팅(Default setting)이후, index.js 파일을 형성하여 아래와 같이const express = require(&amp;#39;express&amp;#39;)const app = express()const port = 3000app.get(&amp;#39;/&amp;#39;, (req, res) =&amp;#38;#62; &amp;#123;  res.send(&amp;#39;Hello World!&amp;#39;)&amp;#125;)app.listen(port, () =&amp;#38;#62; &amp;#123;  console.log(`Example app listening at http://localhost:$&amp;#123;port&amp;#125;`)&amp;#125;)이후 node index.js 를 콘솔에 입력하면 아래와 같은 메시지가 표시된다.$ node index.jsExample app listening at http://localhost:3000http://localhost:3000 주소를 입력하여 들어가보면 아래와 같은 창이 표기된다.추가적으로 Nodemon이나 Typescript 설정을 해주는 것도 나쁘지 않다.기능라우팅(routing) 설정라우팅 기본(Basic Routing)Express.js는 다음과 같은 구조로 라우팅한다.app.METHOD(PATH, HANDLER)      app은 express의 인스턴스이다.        METHOD는 get, post 등의 원하는 HTTP 요청 메소드이다.        PATH는 라우팅할 경로이다.        HANDLER는 해당 주소로 라우팅됬을 때 실행되는 함수이다.  이러한 라우팅 설정을 app.listen(path, [callback]) 함수가 실행되기 전에 끝마치면 된다.app.post(&amp;#39;/&amp;#39;, function (req, res) &amp;#123;  res.send(&amp;#39;Got a POST request&amp;#39;)&amp;#125;) // post 요청app.listen(port, () =&amp;#38;#62; &amp;#123; // backend 실행  console.log(`Example app listening at http://localhost:$&amp;#123;port&amp;#125;`)&amp;#125;)라우팅 인자 (Route parameters)다음과 같이 :를 이용해 url 인자를 얻어낼 수 있다.app.get(&amp;#39;/users/:userId/books/:bookId&amp;#39;, function (req, res) &amp;#123; // ex) http://localhst:3000//users/42/books/21  res.send(req.params) //&amp;#34;:userId&amp;#34;(=&amp;#34;42&amp;#34;) 부분과 &amp;#34;:bookId&amp;#34;(=&amp;#34;21&amp;#34;) 부분에 존재하는 값들이 string 타입으로 표시.&amp;#125;)이때 인자의 이름으로 숫자와 영어 대소문자만 가능하다.즉, 특수문자가 들어가면 특수문자 이전까지만 인자의 이름으로 인정된다.app.get('/flights/:to-:from', function (req, res) { // ex) http://localhst:3000/flights/LAX-SFOres.send(req.params) //\":to-:from\" 전체를 변수명으로 인식하는 것이 아닌 :to\"(=\"LAX\") 부분과 \":from\"(=\"SFO\") 부분에 존재하는 값들이 string 타입으로 표시.}) 라우팅 Extras (Routing extras)라우팅 정규 표현식(Routing Regular Expression)라우팅에 정규 표현식이나 문자열 패턴을 이용할 수 있다.문자열 ?,+,*, (),$은 문자열 패턴으로 이용되며, 특히 $을 이용하고 싶으면 [\\$]로 대신 입력해야한다.?은 바로 앞 문자 하나를 optional하게 만든다.app.get(&amp;#39;/ab?cd&amp;#39;, function (req, res) &amp;#123;// acd, abcd로 연결  res.send(&amp;#39;ab?cd&amp;#39;)  &amp;#125;)+는 바로 앞 문자 하나를 반복 가능하게 만든다.app.get(&amp;#39;/ab+cd&amp;#39;, function (req, res) &amp;#123;// abcd, abbcd, abb..bcd 로 연결  res.send(&amp;#39;ab+cd&amp;#39;)&amp;#125;)*은 모든 길이의 모든 문자열이 들어갈 수 있음을 의미한다.app.get(&amp;#39;/ab*cd&amp;#39;, function (req, res) &amp;#123;// ab와 cd 사이에 무슨 문자가 들어가든 연결(ex) ab/이것도연결가능/cd)  res.send(&amp;#39;ab*cd&amp;#39;)&amp;#125;)()은 앞의 문자열 패턴들과 함께 사용되며, 대상을 문자 하나 대신, () 사이에 존재하는 문자열을 대상으로 한다.app.get(&amp;#39;/ab(cd)?e&amp;#39;, function (req, res) &amp;#123;//abcde, abe로 연결  res.send(&amp;#39;ab(cd)?e&amp;#39;)&amp;#125;)평범한 정규 표현식 또한 사용 가능하다.app.get(/.*fly$/, function (req, res) &amp;#123;//fly로 끝나는 주소로 연결  res.send(&amp;#39;/.*fly$/&amp;#39;)&amp;#125;)route 함수(route function)만약에 동일한 주소로 여러 메소드에 따라 동작을 달리하면서, 함수 체이닝을 통해 코드의 중복을 줄이고 싶다면, route(path) 함수나 all(path, callback, [,callback ...])을 이용하면 된다.app.route(&amp;#39;/book&amp;#39;)  .get(function (req, res) &amp;#123; // get 요청 시의 동작    res.send(&amp;#39;Get a random book&amp;#39;)  &amp;#125;)  .post(function (req, res) &amp;#123; // post 요청 시의 동작    res.send(&amp;#39;Add a book&amp;#39;)  &amp;#125;)  .put(function (req, res) &amp;#123; // put 요청 시의 동작    res.send(&amp;#39;Update the book&amp;#39;)  &amp;#125;)app.all(&amp;#39;/user&amp;#39;, function(req, res, next)&amp;#123;    res.send(&amp;#39;Accessing the user section&amp;#39;)    next()&amp;#125;)Router 클래스(Router Class)express.Router 클래스는 라우팅에 활용할 수 있는 미들웨어이다. 추가적인 미들웨어 함수를 적용하여 사용하거나, 라우터를 모듈화, 파일 구조 라우팅 등을 하는데 사용한다.🔵 파일 구조 라우팅(File-system Routing) : Next.js의 기능처럼 폴더와 파일경로를 url 주소로 이용하여 라우팅 하는 방법.var express = require(&amp;#39;express&amp;#39;)var router = express.Router()// 라우터가 사용할 미들웨어 함수 정의, 현재 이 라우터 인스턴스로 전달되는 요청마다 실행됨router.use(function timeLog (req, res, next) &amp;#123;  console.log(&amp;#39;Time: &amp;#39;, Date.now()) // 현재 시간 출력  next() //next 함수 : 다음 미들웨어 기능(=여기선 라우팅)을 불러옮.&amp;#125;)// 홈페이지 경로 설정router.get(&amp;#39;/&amp;#39;, function (req, res) &amp;#123;  res.send(&amp;#39;Birds home page&amp;#39;)&amp;#125;)// /about 경로 설정router.get(&amp;#39;/about&amp;#39;, function (req, res) &amp;#123;  res.send(&amp;#39;About birds&amp;#39;)&amp;#125;)module.exports = router이후, 해당 라우팅 클래스를 미들웨어로 부른 뒤, 경로를 설정해주면, 함수가 적용된 파일 구조 라우팅이 가능하다.var birds = require(&amp;#39;./birds&amp;#39;)app.use(&amp;#39;/birds&amp;#39;, birds)// &amp;#34;birds/&amp;#34; 경로와 &amp;#34;birds/about/&amp;#34;경로가 이용 가능해짐. router.all(path, callback, [,callback ...]) 함수와 콜백 함수들을 이용하면 전역 인증 등을 구현 가능하다, 모듈화가 가능하다는 점을 제외하곤, app.all(path, callback, [,callback ...])과 다른점 없어보인다.router.all(&amp;#39;*&amp;#39;, requireAuthentication, loadUser) // &amp;#39;*&amp;#39;를 이용한 라우터를 모든 라우터보다 먼저 정의하면 모든 url에 적용되게 할 수 있다.// requireAuthentication : 인증에 관련된 함수// loadUser : 유저 정보를 가져오는 함수router.all(path, callback, [,callback ...])을 포함해 router.METHOD(path, [callback, ...] callback) 함수들은 첫번째 인자로 url, 두번째부터는 차례대로 next()를 부를 때마다 실행되는 콜백 함수를 인자로 받는다.// 위의 예시 코드와 동일한 동작을 하는 코드router.all(&amp;#39;*&amp;#39;, requireAuthentication)router.all(&amp;#39;*&amp;#39;, loadUser)응답 방법(Response methods)클라이언트에게 응답을 보낼 때 사용할 수 있는 함수, 이 함수를 부름으로써, 클라이언트는 대기 상태를 끝내고 요청-응답 사이클이 종료된다.  res.send([body]) : HTTP 응답을 보냄. 주로 비스트림 응답에 사용됨app.get(&amp;#39;/&amp;#39;, (req, res) =&amp;#38;#62; &amp;#123;  res.send(&amp;#39;Hello World!&amp;#39;) // Hello World! 라는 응답을 되돌림&amp;#125;)app.get(&amp;#39;/json&amp;#39;, (req, res) =&amp;#38;#62; &amp;#123;  res.send(&amp;#123;messange: &amp;#34;ok&amp;#34;&amp;#125;)// Json 형식으로 응답을 되돌림&amp;#125;)app.get(&amp;#39;/octet&amp;#39;, (req, res)=&amp;#38;#62;&amp;#123;    res.set(&amp;#39;Content-Type&amp;#39;, &amp;#39;text/html&amp;#39;)// Content-Type을 text 형태로 강제    res.status(500).send(&amp;#39;unavailable&amp;#39;) // 500 코드와 함께 응답&amp;#125;)만약, 응답이 JSON 형식에 맞지 않아 굳이 res.set(field[, value])형식을 바꿔줘야 한다면, 차라리 res.json()를 이용하여 코드의 길이를 줄이자.app.get(&amp;#39;/&amp;#39;, (req, res) =&amp;#38;#62; &amp;#123;  res.json(&amp;#39;Hello World!&amp;#39;) // string임에도 Content-Type은 application/json;&amp;#125;)  res.append(field[, value]): HTTP 응답 헤더의 필드와 값을 추가, res.set(field[, value])==res.header(field[, value])을 이용하면 obeject를 주어 여러 값을 동시에 변경이 가능하다.res.append(&amp;#39;Link&amp;#39;, [&amp;#39;&amp;#38;#60;http://localhost/&amp;#38;#62;&amp;#39;, &amp;#39;&amp;#38;#60;http://localhost:3000/&amp;#38;#62;&amp;#39;])res.append(&amp;#39;Set-Cookie&amp;#39;, &amp;#39;foo=bar; Path=/; HttpOnly&amp;#39;)res.attachment([filename])을 통하여 Content-Disposition 헤더를 설정해줄 수 있다.res.attachment(&amp;#39;path/to/logo.png&amp;#39;)// Content-Disposition: attachment; filename=&amp;#34;logo.png&amp;#34;// Content-Type: image/png좀 더 쿠키를 세분화하여 정해주려면 res.cookie(name, value [, options]) 함수를 사용할 수 있고, res.clearCookie(name, value [, options])함수로 지워줄 수 있다.res  .status(201)  .cookie(&amp;#39;access_token&amp;#39;, &amp;#39;Bearer &amp;#39; + token, &amp;#123;    expires: new Date(Date.now() + 8 * 3600000) // cookie will be removed after 8 hours  &amp;#125;)  .cookie(&amp;#39;rememberme&amp;#39;, &amp;#39;1&amp;#39;, &amp;#123; expires: new Date(Date.now() + 900000), httpOnly: true &amp;#125;)  .redirect(301, &amp;#39;/admin&amp;#39;)  res.redirect([status,] path) : 다른 URL로 리다이렉트 해준다. 기본 status 코드는 302 Found이다.res.redirect(301, &amp;#39;http://example.com&amp;#39;)res.redirect(&amp;#39;../login&amp;#39;)res.redirect(&amp;#39;back&amp;#39;)//이전 referer로 돌림  res.render(view [, locals] [, callback]) : HTML view를 보낸다.          view: html 파일이 존재하는 파일 경로      locals: view에서 이용할 로컬 변수들의 object 형태      callback:  에러와 html 파일의 문자열을 인자로 가지고 있는 콜백 함수      // view에게 로컬 변수 전달하기res.render(&amp;#39;html/user&amp;#39;, &amp;#123; name: &amp;#39;Tobi&amp;#39; &amp;#125;, function (err, html) &amp;#123;      if (err) &amp;#123;        res.status(400).send(&amp;#39;error!&amp;#39;)    &amp;#125; else &amp;#123;          res.send(html)                &amp;#125;&amp;#125;)  res.download(path [, filename] [, options] [, fn]) : path에 존재하는 파일을 attachment로 보내며, 브라우저가 다운로드를 진행한다. filename 인자는 다운로드 될 파일 명으로, 주어지지 않았다면, Content-Disposition 필드의 filename= 인자가 기본값이다.res.download(&amp;#39;/report-12345.pdf&amp;#39;, &amp;#39;report.pdf&amp;#39;, function (err) &amp;#123;  if (err) &amp;#123;      // 에러 핸들링  &amp;#125; else &amp;#123;  &amp;#125;&amp;#125;)이외의 추가적인 메소드들은 여기 참조정적 파일(static file) 설정Express.js는 serve-static 모듈을 기반으로 만든 빌트인 미들웨어(built-in middleware) 함수인 express.static이 존재한다.이를 이용해  이미지, CSS 파일, JS 파일 등의 정적 파일을 이용할 수 있다.express.static(root, [options])      root 인자는 정적 에셋들이 위치한 경로를 설정한다.        [options] 인자는 static 함수가 가질 수 있는 옵션이다. 예를 들면          dotfiles : .으로 시작하는 파일과 폴더는 어떻게 다룰 것인가? ex) \"ignore\" : 없는 걸로 취급. (default : \"allow\", 특별한 조치 취하지 않음)      etag: HTTP 응답 헤더에 ETag 헤더를 추가한다. (default: \"true\", weak ETag)      lastModified:  HTTP 응답 헤더에 Last-Modified 헤더를 추가한다. (default: \"true\")        등이 존재한다.  app.use(express.static(&amp;#39;public&amp;#39;))위와 같은 코드일 경우, 아래와 같은 경로의 public 폴더의 파일들을 url을 통해 접근 할 수 있다. |-public/ | |-hello.html | |-css/ | | |-main.css | |-images/ | | |-dog.png | |-js/ | | |-SPA.jshttp://localhost:3000/hello.htmlhttp://localhost:3000/css/main.csshttp://localhost:3000/images/dog.jpghttp://localhost:3000/js/SPA.js추가로, 경로 접두어를 이용하고 싶다면 다음과 같은 코드를 이용하면 된다.app.use(&amp;#39;/static&amp;#39;, express.static(&amp;#39;public&amp;#39;))http://localhost:3000/static/hello.htmlhttp://localhost:3000/static/css/main.csshttp://localhost:3000/static/images/dog.jpghttp://localhost:3000/static/js/SPA.jsDB 연결(DB Connection)DB 연결 방법은 공식 문서에 DB 별로 상세히 설명되어 있다.각기 DB에서 지원하는 모듈을 이용하는 방식으로 진행되며, 여기서는 MySQL과 MongoDB의 예시를 알아보겠다.MySQLnpm install mysqlmysql에서 지원하는 npm 모듈을 설치한다. mysqljs github에서 좀더 자세한 사항을 알 수 있다.var mysql = require(&amp;#39;mysql&amp;#39;)var connection = mysql.createConnection(&amp;#123;  // 실제로는 env 설정할것!  host: &amp;#39;localhost&amp;#39;,  port: &amp;#39;3306&amp;#39;,  user: &amp;#39;dbuser&amp;#39;,  password: &amp;#39;s3kreee7&amp;#39;,  database: &amp;#39;my_db&amp;#39;,  debug: ENV.PRODUCTION, // true 시, 콘솔 창에 SQL 쿼리 진행이 출력됨  supportBigNumbers: true, // db의 BIGINT나 DECIMAL 타입은 데이터 크기상 자바스크립트에서 지원하지 않으므로, 문자열 형식으로 바꿔주는 옵션  ssl: &amp;#123;      // ssl 연결 설정을 위한 옵션  &amp;#125;&amp;#125;)connection.connect() // 연결 시작connection.query(&amp;#39;SELECT 1 + 1 AS solution&amp;#39;, function (err, rows, fields) &amp;#123; // SQL Mapping, 실제로는 ORM으로 진행하는 것을 추천  if (err) throw err  console.log(&amp;#39;The solution is: &amp;#39;, rows[0].solution)&amp;#125;)connection.end() // 연결 종료MongoDBnpm install mongodb마찬가지로, MongoDB NodeJS를 위한 드라이버를 이용하면 된다.var MongoClient = require(&amp;#39;mongodb&amp;#39;).MongoClientMongoClient.connect(&amp;#39;mongodb://localhost:27017/animals&amp;#39;, function (err, client) &amp;#123;  if (err) throw err  var db = client.db(&amp;#39;animals&amp;#39;)  db.collection(&amp;#39;mammals&amp;#39;).find().toArray(function (err, result) &amp;#123;    if (err) throw err    console.log(result)  &amp;#125;)&amp;#125;)Node.js와 MongoDB를 함께 쓸때는, Mongoose와 함께 쓰는 것도 고려해볼만 하다.Mongoose를 이용하면 관계형 데이터베이스와 달리 자유로운 형식을 가지는 콜렉션(Collection)들의 형식을 정의하고, 제약(Constraint)을 설정해줄 수 있다.npm install mongoose validatorMongoose가 대신 MongoDB와 연결을 하므로, MongoDB 연결은 필요없다.let mongoose = require(&amp;#39;mongoose&amp;#39;);const server = &amp;#39;127.0.0.1:27017&amp;#39;; // DB 서버 주소const database = &amp;#39;fcc-Mail&amp;#39;;      // DB 명class Database &amp;#123;  constructor() &amp;#123;    this._connect()  &amp;#125;_connect() &amp;#123;     mongoose.connect(`mongodb://$&amp;#123;server&amp;#125;/$&amp;#123;database&amp;#125;`)       .then(() =&amp;#38;#62; &amp;#123;         console.log(&amp;#39;Database connection successful&amp;#39;)       &amp;#125;)       .catch(err =&amp;#38;#62; &amp;#123;         console.error(&amp;#39;Database connection error&amp;#39;)       &amp;#125;)  &amp;#125;&amp;#125;module.exports = new Database()프록시 설정(Proxy setting)Express.js에서는 리버스 프록시(Reverse Proxy)를 이용할 경우, proxy-addr 패키지 기반인 trust proxy 설정을 해주어야 정상 작동한다.이를 설정해주지 않는다면, 클라이언트의 IP 주소 대신, 리버스 프록시의 IP 주소를 클라이언트로 착각한다는 듯하다.X-Forwarded-For: &amp;#38;#60;client&amp;#38;#62;, &amp;#38;#60;proxy1&amp;#38;#62;, &amp;#38;#60;proxy2&amp;#38;#62;X-Forwarded-For: 203.0.113.195, 70.41.3.18, 150.172.238.178//X-Forwarded-For 헤더의 예시이때, HTTP 헤더 중 X-Forwarded-for 헤더를 이용해 클라이언트 주소를 판명하며, 보통 최좌측이 클라이언트 IP 주소이다.app.set(&amp;#39;trust proxy&amp;#39;, true)true : HTTP 메시지의 X-Forwarded-For 헤더의 최좌측의 IP 주소를 클라이언트 IP 주소로 설정🔵 HTTP 메시지의 X-Forwarded-For, X-Forwarded-Host, X-Forwarded-Proto 헤더를 리버스 프록시가 덮어쓰게 설정 하지 않으면, 클라이언트가 이를 이용해 다른 클라이언트인 척 행세할 수 있다.false: 리버스 프록시가 존재하지 않으며, req.socket.remoteAddress에 존재하는 IP 주소를 클라이언트로 간주, 기본값app.set(&amp;#39;trust proxy&amp;#39;, [&amp;#39;loopback&amp;#39;, &amp;#39;linklocal&amp;#39;, &amp;#39;uniquelocal&amp;#39;, &amp;#39;123.123.123.123&amp;#39;])리버스 프록시의 IP 주소를 명시해줄 수도 있다. 아래는 미리 정의된 서브넷 문자열 들이다.  loopback - 127.0.0.1/8, ::1/128  linklocal - 169.254.0.0/16, fe80::/10  uniquelocal - 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, fc00::/7이때 명시된 IP 주소나 서브넷은 클라이언트 IP 주소가 아닌 것으로 판단하며, req.socket.remoteAddress의 주소가 명시되어 있다면(trusted), 해당 메시지의  X-Forwarded-For 헤더에 명신된 주소에서 최우측부터 좌측 순으로 확인하면서 가장 첫번째로 명시되어 있지 않은(untrusted) 주소를 클라이언트 주소로 결정한다.app.set(&amp;#39;trust proxy&amp;#39;, 2)숫자를 명시해줄 경우, 몇 홉의 리버스 프록시 이후에 클라이언트 주소가 나오는지로 결정한다는 의미이다.예를 들어, 0을 명시하면, 리버스 프록시가 존재하지않으며, req.socket,remoteAddress가 클라이언트 주소로 판명되며, 1을 명시하면, X-Forwarded-For 헤더의 최우측에서 두번째를 클라이언트 IP 주소로, 나머지만 리버스 프록시 주소로 결정한다.app.set(&amp;#39;trust proxy&amp;#39;, function (ip) &amp;#123;  if (ip === &amp;#39;127.0.0.1&amp;#39; || ip === &amp;#39;123.123.123.123&amp;#39;) return true // trusted IPs  else return false&amp;#125;)또는, 함수를 정의해주어, 리버스 프록시를 판명할 수 있다. true일 경우 리버스 프록시 주소이며, X-Forwarded-For 헤더의 다음 주소를 함수에 넣어보게 되고, false일 경우 해당 IP 주소가 클라이언트의 주소이다.trust proxy 설정 이후, req.hostname의 값은 X-Forwarded-Host 헤더에서 가져오게 되며, X-Forwarded-Proto 헤더가 리버스 프록시에 의해 변경되어 프로토콜 등을 확인할 수 있게 되며, req.ip, req.ips 값이 설정되게 된다.미들웨어(Middleware)미들웨어 함수란?(About middleware function)미들웨어 함수는 응용프로그램의 요청-응답 사이클의 요청 객체(request object)와 응답 객체(response object), 그리고 next() 함수를 이용하는 함수이다.  요청 객체(request object) : 클라이언트가 요청한 HTTP 요청의 요소(쿼리 문자열, 인자, 바디, 헤더 등)를 포함하고 있는 객체  응답 객체(response object) : 클라이언트에게 돌려줄 HTTP 응답을 위한 객체, 함수를 통해 응답 메시지를 조성하고 응답할 수 있다.  next() 함수 : 실행시 다음 순서의 미들웨어를 불러오는 함수, 해당 미들웨어에서 통신을 종료하지 않고 다음 미들웨어로 넘기려면 이용해야 한다.미들웨어 함수를 통해, 추가적인 로직을 실행하거나, 요청과 응답에 변형을 가하거나, 통신을 종료하거나 다음 미들웨어 함수를 불러올 수 있다.예를 들어, 인증, 인가 시스템을 구현하거나, 필터링, 캐쉬 구현 등이 가능하다.var express = require(&amp;#39;express&amp;#39;)var app = express()var router1 = express.Router() // router-레벨 미들웨어 생성을 위한 라우터 선언var router2 = express.Router()app.use(function(req,res, next)&amp;#123; // 응용프로그램 레벨 미들웨어 생성    console.log(&amp;#39;middleware 1&amp;#39;)    next()&amp;#125;)router1.use(function (req, res, next) &amp;#123;    console.log(&amp;#39;middleware 2&amp;#39;)    next()&amp;#125;)router1.use(function (req, res, next) &amp;#123;    console.log(&amp;#39;middleware 3&amp;#39;)    next()&amp;#125;)router2.use(function (req, res, next) &amp;#123;    console.log(&amp;#39;middleware 4&amp;#39;)    next(&amp;#39;router&amp;#39;)  // 현재 라우터 레벨의 모든 미들웨어를 skip&amp;#125;)router2.use(function (req, res, next) &amp;#123;    console.log(&amp;#39;middleware 5&amp;#39;) // 상위 미들웨어에서 라우터 레벨 미들웨어들을 skip했기 때문에 실행되지 않는다.    next()&amp;#125;)app.use(function(req,res, next)&amp;#123;    console.log(&amp;#39;middleware 6&amp;#39;)    next()&amp;#125;)app.use(&amp;#39;/&amp;#39;, router1) // 라우터 미들웨어 선언은 빨랐지만, app에 적용이 느리므로 middleware 6번보다 뒤에 실행된다.app.use(&amp;#39;/&amp;#39;, router2)위 코드의 실행결과는 아래와 같다.middleware 1middleware 6middleware 2middleware 3middleware 4유용한 미들웨어 (Additional Middleware)static            [[#정적 파일(static file) 설정      앞서]] 설명했던 Express.js 빌트인 모듈, 정적 파일을 이용하는데 사용한다.      express.static(root, [options])corsnpm install corsCORS는 Express 팀에서 만든 서드 파티 미들웨어이다.CORS(Cross-Origin Resource Sharing, 교차 출처 리소스 공유) 설정을 통해 접근 권한을 설정할 수 있게 한다.var express = require(&amp;#39;express&amp;#39;)var cors = require(&amp;#39;cors&amp;#39;)var app = express()var corsOptions = &amp;#123;  origin: [&amp;#39;http://example.com&amp;#39;, &amp;#39;http://example2.com&amp;#39;], // &amp;#39;*&amp;#39;을 이용하면 모든 요청 CORS 화이트리스트.  optionsSuccessStatus: 200, // 오래된 브라우저는 코드 204를 쓰므로 200으로 강제  credentials: true  //&amp;#39;Access-Control-Allow-Credentials&amp;#39; CORS 헤더 설정 여부&amp;#125;//일부 route에만 cors 적용 예시app.get(&amp;#39;/products/:id&amp;#39;, cors(corsOptions), function (req, res, next) &amp;#123;  res.json(&amp;#123;msg: &amp;#39;This is CORS-enabled for only example.com.&amp;#39;&amp;#125;)&amp;#125;)// 모든 route에 cors 적용 예시app.use(cors())app.listen(80, function () &amp;#123;  console.log(&amp;#39;CORS-enabled web server listening on port 80&amp;#39;)&amp;#125;)express-sessionnpm install express-sessionSession 미들웨어를 형성할 수 있게 해주는 Express 팀에서 만든 서드 파티 미들웨어.Session을 이용하여, 유저의 쿠키를 저장하고, 요청을 유저별로 구분하여, 보안과 유저 특화 서비스 등을 구현할 수 있다.var app = express()var sess = &amp;#123;  secret: &amp;#39;keyboard cat&amp;#39;,  resave: false,  saveUninitialized: true,  cookie: &amp;#123; secure: false &amp;#125;&amp;#125;if (app.get(&amp;#39;env&amp;#39;) === &amp;#39;production&amp;#39;) &amp;#123;  app.set(&amp;#39;trust proxy&amp;#39;, 1) // trust first proxy  sess.cookie.secure = true // serve secure cookies&amp;#125;app.use(session(sess))helmetnpm install helmetHTTP 헤더 설정을 통해 어플레이케이션 보안 향상을 도와주는 서드 파티 미들웨어.const express = require(&amp;#34;express&amp;#34;);const helmet = require(&amp;#34;helmet&amp;#34;);const app = express();const helmet_setting = &amp;#123;    referrerPolicy: &amp;#123; policy: &amp;#34;no-referrer&amp;#34; &amp;#125;, // 세부 보안 설정   contentSecurityPolicy: false // 사용 안함 설정&amp;#125;app.use(helmet(helmet_setting));helmet은 15개의 보안 미들웨어를 포함하고 있으며, 각자 설정 및 사용 여부를 조정할 수 있다."
  }
  , 
  
  "/articles/etc/etcs/git%20deploy%20key%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.html": {
    title: "git deploy key 사용하기",
    date: " Mar 23, 2022 ",
    url: "/articles/etc/etcs/git%20deploy%20key%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.html",
    tags: ["GIT"],
    content: "GIT Clone by deploy key각 서버마다 GIT의 유저명과 패스워드를 설정하면, 번거로울 뿐만 아니라 보안상으로 문제가 생길 수 있다.ssh-keygen -t ed25519 -C &amp;#34;USERNAME@EMAIL.com&amp;#34;      -t : 사용할 알고리즘, ed25519가 일반적이고 안전하다.        -C: 키 마지막에 자신의 이메일을 삽입하여 커맨팅  나오는 설정은 파일 설정, passphrase이며, 기본 설정도 괜찮다.touch ~/.ssh/config chmod 600 ~/.ssh/config vim ~/.ssh/config .ssh 폴더에 config 파일을 만들고, 권한을 바꾼 뒤  vim을 이용해 아래와 같은 설정을 넣는다.Host github-YOUR-APP     HostName github.com     AddKeysToAgent yes     PreferredAuthentications publickey     IdentityFile ~/.ssh/id_ed25519이는 EC2 글에서 본 적 있을 것이다.cat ~/.ssh/id_ed25519.pubssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILs35pzG5jZakTEHDWeRErgkAmabhQj2yj/onxlIQgli user@example.comcat 명령어를 통해 생성된 키를 복사하고 이를 대상 GIT repository -&gt; Settings -&gt; Deploy keys -&gt; Add deploy key에 키 이름과 키를 저장한다.아래와 같이 등록된다.이후 아래와 같이 SSH를 통해서 Clone을 할 수 있다.[ec2-user@ip-xxx-xxx-xxx-xxx docker-server]$ git clone git@github.com:RoadVirusHN/Movi-Dick.gitCloning into &amp;#39;Movi-Dick&amp;#39;..."
  }
  , 
  
  "/articles/etc/etcs/gitignore%20%EC%9E%91%EC%84%B1%EB%B2%95.html": {
    title: "gitignore 작성법",
    date: " Mar 24, 2022 ",
    url: "/articles/etc/etcs/gitignore%20%EC%9E%91%EC%84%B1%EB%B2%95.html",
    tags: ["GIT"],
    content: ".gitignore 작성법  의도적으로 Commit 되지 않도록 무시되어야할 파일을 정해주는 것 -(https://git-scm.com/docs/gitignore)git 폴더(git init을 통해 .git 숨김 폴더가 생성된 디렉토리) 내부에 “.gitignore”라는 이름의 파일을 생성한 뒤, 보안상, 프로젝트 관련상, 파일 크기상 원격 레포지토리에 올리고 싶지 않은 파일이나 폴더를 add 대상에서 제외하는데 사용할 수 있다.gitignore.io(https://www.toptal.com/developers/gitignore)를 통해 손쉽게 자신의 기술 스택에 관련된 gitignore 파일을 만들 수 있지만, 개인이 생성한 민감한 파일을 제외하기 위해 정확한 사용법를 알아야할 필요가 있다.자세한 내용은📖 https://git-scm.com/docs/gitignore주의할 점은 이미 원격 레포지토리에 올라간 파일은 뒤늦게 gitignore에 패턴을 추가한다해도 영향을 받지 않으며, 이를 위해 따로 처리해줘야한다. “원하지 않은 Git 파일 기록 지우기” 참조 바람gitignoregitignore 파일 예시아래와 같은 방식으로 .gitignore 파일 내부에 파일명에 대한 패턴을 기재하여 파일이 기록되지 않도록 할 수 있다. 이때, 심볼릭 링크를 사용할 수 없다....# Created by https://www.toptal.com/developers/gitignore/api/visualstudiocode,windows,python,react# Edit at https://www.toptal.com/developers/gitignore?templates=visualstudiocode,windows,python,react### react ###.DS_*logs**/*.backup.***/*.back.*node_modulesbower_components*.sublime*psdthumbsketch### VisualStudioCode ###.vscode/*!.vscode/settings.json!.vscode/tasks.json!.vscode/launch.json!.vscode/extensions.json*.code-workspace# Local History for Visual Studio Code.history/...gitignore 파일의 다른 활용 방법들Git은 여럿의 gitignore 파일과 파일 내부 라인을 읽고 무시할 파일을 인지하는데, 다음과 같이 복잡한 우선순위와 가독성을 위해 보통 하나의 .gitignore 파일을 프로젝트 최상단 폴더에 유지한다.  gitignore 파일이 여러 폴더에 존재할 경우, 현재 폴더의 gitignore파일을 최우선으로, 상위 폴더일 수록 낮은 우선순위로 gitignore 파일 설정을 덮어씌운다.  이외에도 .git 폴더 내부에도 git에 의해 생성되는 파일들을 무시하기 위한 기본 생성 gitignore가 존재함이러한 .gitignore 파일은 push 시에 원격 레포지토리에 같이 올라가므로, 만약 많은 사람이 참여하고 있는 프로젝트 중에, 나의 로컬 레포지토리에서만 제외하고 다른 사람들은 제외할 필요 없는 파일의 경우는 $GIT_DIR/info/exclude 파일에 기재하면 된다.프로젝트나, 레포지토리와 관계없이 GIT을 사용하는 어떠한 상황에서든 무조건 제외하고 싶은 나의 파일의 패턴(전역 무시, Global ignore)은 $XDG_CONFIG_HOME/git/ignore 또는 $HOME/.config/git/ignore 파일에 기재하면 된다.  ignore 파일의 경로나 이름을 바꾸고 싶으면 git config –global core.excludesfile ~/{원하는 경로}, 또는 직접 경로 ~/.gitconfig/core.excludesFile에 들어가서 경로를 바꿔줄 수 있다.git check-ignore [파일, 폴더명]을 통해 해당 파일, 폴더가 제외되어있는 지 확인할 수 있다.  만약 대상이 제외되어있다면 해당 폴더명이 나타난다.  대상이 포함되어 있다면 아무것도 결과창에 나타나지 않는다.패턴 포맷과 예시git이 이용하는 패턴은 glob 패턴에서 brace expression을 제외한 것과, 정규표현식의 [a..z] expression을 추가한 것을 합친 문법이며, 여기서는 패턴에 따른 git의 .gitignore 파일 예시를 중심으로 이야기할 것이다.여기서 “제외”는 원격 레포지토리에 올라가지 않음을 의미하며, “포함”은 올라감을 의미한다.  예를 들어 “파일 a는 제외되며, 파일 b도 포함이다” 같이, b가 git에 의해 commit 된다는 의미인지, 아니면 a처럼 제외된다는 의미인지 헷갈리는 표현은 최대한 자제했으며, 만약 존재한다면, 이때 b는 a와 달리 제외되지 않고 올라간다는 의미이다.# using # to comment. if you need to use # using \\# instead.# using blank line as a separator빈 공간은 구분자로 사용되며, #으로 주석을 남길 수 있다.mustinclude.txt# mustinclude.txt 제외 패턴!mustinclude.txt# 상위의 제외 패턴을 무시하고 무조건 포함시킴!/mustIncludeFolder/# mustIncludeFolder를 무조건 포함시킴\\!important!.txt# 파일명 중간에 위치한 !는 \\를 접두사로 넣을 필요없다./specificDir/”\\“가 앞에 있지 않은 공백 문자는 무시된다.”!“가 접두어로 있는 패턴은 이전에 제외 패턴이 존재해도 반드시 포함된다. 하지만, 해당 파일의 상위 경로가 통째로 제외된 경우나, 또 다시 아래에 다시 제외된 경우에는 다시 포함될 수 없다.”!“를 파일명의 최선두 등으로 사용하고 싶을 때는 “\\!“를 이용하자./FolderInSameDir/ChildDir/excludedfile.txt # .gitignore 파일과 같은 폴더에 위치한 FolderInSameDir 폴더 내의 ChildDir 폴더 내의 excludedfile.txt 제외# 다른 폴더 내부에 있는 otherFolder/FolderInSameDir/ChildDir/excludedfile.txt은 그대로 포함됨FolderInSameDir/ChildDir/excludedfile2.txt # 위와 같은 경로의 excludedfile2.txt 제외, 즉 맨 앞의 \"/\"은 없어도 됨/fileInHere.txt# .gitignore 파일과 같은 폴더에 존재하는 fileInHere.txt 제외# 다른 폴더 내부에 있는 /otherFolder/fileInHere.txt은 그대로 포함됨\"/\"를 파일 경로 구분자로 사용하며, 패턴의 처음이나 중간에 존재할 시에는, .gitignore 파일에서의 상대적 파일 경로에 존재하는 파일을 의미하며, 존재하지 않을 시에는 .gitignore 파일과 같은 폴더 내의 파일을 제외시킨다.folderOrFile# folderOrFile이라는 이름의 폴더만 전부 제외onlyTxt.txt# onlyTxt.txt이라는 이름의 파일만 전부 제외, onlyTxt.exe, onlyTxt.md 등, 다른 확장자는 그대로 포함됨.onlyFolder/# onlyFolder라는 이름의 폴더만 제외onlyFile.*# 아래에서 배울 *를 이용한 onlyFile이라는 이름을 가진 파일만 전부 제외 하는 방법# 파일, 폴더명만 덩그러니 있는 경우들이며, git이 관리하는 모든 경로에 존재하는 folderfolderOrFile, onlyFolder 폴더와 onlyFile.txt, folderOrFile 파일들은 전부 제외된다.# 예를 들어 otherFolder/folderfolderOrFile/, otherFolder/anotherFolder/onlyFile.txt 등 또한 제외된다.\"/\"이 맨 마지막에 존재할 경우나 존재하지 않을 경우 모두, 해당 이름의 폴더를 제외하며, 파일을 제외하고 싶다면 아래에 배울 \"*\"(별표, asterisk)를 사용하면 된다.앞에 추가 폴더 경로가 제시되지 않고, 이름만 덩그러니 있는 경우에는 모든 경로의 폴더나 파일이 제외 대상이다.*.html# 모든 html 확장자 파일을 제외시킴noInclude.*# noInclude라는 이름의 파일을 확장자 관계없이 모두 제외시킴exclude-[0-9]/# exclude-0 부터 exclude-9까지의 폴더를 전부 제외시킴credential??# 접두어 credential로 시작하고 뒤에 임의 두 글자가 추가된 파일과 폴더를 전부 제외시킴no*.js# 접두어 no로 시작하는 모든 길이의 파일명을 가진 js 확장자 파일을 제외시킴\"*\"은 \"/\"을 제외한 모든 문자열을, \"?\"은 \"/\"을 제외한 모든 문자 하나를 사용할 수 있으며, \"[a-zA-Z]\"는 해당 범위 내의 문자 하나를 의미한다.애초에 파일 및 폴더명에 포함시키는 건 불가능하긴 하지만 \"\\?\", \"\\*\"를 이용해 패턴이 아닌 문자 그대로 사용할 수 있다.**/allExclude# allExclude란 이름의 모든 파일과 폴더를 경로와 관계없이 제외시킨다.allExclude# 이전에 배웠던 방법, **를 사용한 바로 위와 같은 동작이지만, 가독성적으로 **를 사용하는게 더 좋으며, 아래와 같은 동작이 가능하다.**/otherFolder/*.html# 어떠한 경로에서든 otherFolder라는 폴더 내부의 모든 html 파일을 제외함, **를 사용해야만 쓸 수 있는 표현rootFolder/**/anyFile.*# rootFolder내의 anyFile이라는 이름의 파일을 추가적인 경로와 관계없이 모두 제외한다. 마찬가지로 **를 사용해야만 쓸 수 있는 표현# rootFolder/extra/anyFile.txt나, rootFolder/moreFolder/anyFile.js, rootFolder/long/longFolders/anyFile.md 모두 제외된다.두개의 별표(asterisk) “**“를 이용해 경로와 관계없이 모든 파일 및 폴더를 제외시킬 수 있다.두개 이상의 별표, 예를 들어 “***” 부터는 각자 1개의 별표와 같이 작동한다. 즉 “*“와 별 차이가 없을 것이다."
  }
  , 
  
  "/articles/web/backend/GraphQL%20with%20flask.html": {
    title: "GraphQL with flask",
    date: " Apr 16, 2022 ",
    url: "/articles/web/backend/GraphQL%20with%20flask.html",
    tags: ["FLASK","PYTHON","GRAPHQL","BE"],
    content: "https://dev.to/mesadhan/python-flask-graphql-with-graphene-nla (Md. Sadhan Sarker)의 글을 번역, 정리한 글입니다.style: numbermin_depth: 2max_depth: 3varied_style: trueGraphQL with flaskGraphQL은 API를 위한 query language이다. REST API에 비해 여러 장점이 있으며 특히 data fetching 부분이 효율적이다. 규모가 큰 API일수록 더욱 효과적이고 강력하며, facebook에서 open source로 배포하여 커다란 커뮤니티를 이루었다.요즘은 선언형 프로그래밍이 점점 인기를 얻고 있는데, GraphQL 또한, 여러 API 호출을 불러와야하는 REST API와는 달리 선언형 데이터 fetching이용한다. GraphQL 서버는 오직 하나의 endpoint(root URL 뒤에 붙는 추가 주소)와 클라이언트가 요청한 여러 response로 이루어졌다. 앞으로 예시를 살펴볼 것이다.GraphQL vs REST  GraphQL은 쿼리로 우리가 필요한 데이터만  명시하게 해주며, 한번의 요청으로 정확히 그 데이터만 응답에 포함시켜줄 것이다.  이에 반해 REST API는 여러 응답과 호출을 요구한다.GraphQL과 REST의 차이 예시      API에서 data를 가져올 때, 큰 차이를 보여주는데. 블로그에서 어떤 유저의 작성글들의 제목이 필요할 경우, 또한 같은 페이지에서 그 유저의 최신 팔로워들 3명의 이름을 가져오려 할 때, GraphQL과 REST API는 어떤 차이를 보일까?          REST API의 경우                      REST API에서는 여러 endpoint에서 데이터를 가져온다.                  예를 들어, /user/&lt;id&gt; endpoint에서 먼저 유저 데이터를 가져오낟          그 후, /user/&lt;id&gt;/posts endpoint에서 해당 유저의 모든 게시글들을 가져온다          마지막으로, /user/&lt;id&gt;/followers에서 해당 유저의 모든 팔로워들을 가져온다                                    ​\t- 그림 출처 : source :howtographql.com                  REST API를 사용하면 3번의 각자 다른 주소로 3번의 요청을 보내야 하며, 쓸데없는 추가 데이터들을 추가로 가져온다는 것을 알 수 있다.                          아니면 /users/posts/follwers/&lt;id&gt; 라는 새로운 endpoint를 만들고, 우리가 원하는 데이터만 가져오게 만들어도 된다. 하지만 그렇게하면 끔직한 생산성과 재사용성을 가져올 것이다. 추가로 날짜에 따라 정보를 가져오려면 /users/posts/follwers/&lt;id&gt;/&lt;date&gt;라는 endpoint를 만들어야한다.    GraphQL의 경우  GraphQL은 단 하나의 query를 서버에 보낸다. 서버는 JSON 형식의 respond를 돌려준다.  그림 출처 : source :howtographql.com  GraphQL에서는 클라이언트 서버가 필요한 데이터만 쿼리에 포함할 수 있다. 백엔드 서버의 응답 또한 정확히 쿼리에 정의한 구조를 따른다멋지지 않는가? 이론은 충분하니 이제 Python Graphne을 이용해서 GraphQL 서버를 만들어보자.Python Graphene을 이용한 GraphQL 서버 구현  Graphene은 python으니 GraphQL 클라이언트 라이브러리이다. 이를 이용해 우리만의 GraphQL 서버를 만들 수 있다.Setting up your poject  먼저 프로젝트 경로와 폴더를 생성한다.```bash$ mkdir graphql-flask$ cd graphql-flask- 가상환경설정을 통해 global package들과의 충돌을 피하고, 각 프로젝트 별로 패키지 버전 관리를 쉽게 할 수 있다.```bash$ pip install virtualenv$ virtualenv venv$ source venv/bin/activate참고로 $ deactivate로 가상 환경에서 나갈 수 있다.  필요한 depndency를 맞추기 위해 아래와 같이 터미널에 입력하라$ pip install flask flask-graphql flask-migrate flask-sqlalchemy graphene graphene-sqlalchemy  이후 우리의 데이터베이스를 실행해야 한다. 아래 seed.py를 만들어 보자.from app import db, User, Postdb.create_all()     # create tables from modelsuser1 = User(    name=&amp;#34;Sadhan Sarker&amp;#34;,    email=&amp;#39;cse.sadhan@gmail.com&amp;#39;)post1 = Post()post1.title = &amp;#34;Blog Post Title 1&amp;#34;post1.body = &amp;#34;This is the first blog post 1&amp;#34;post1.author = user1db.session.add(post1)db.session.add(user1)db.session.commit()print(User.query.all())print(Post.query.all())  위 스크립트를 실행하려면 아래와 같이 입력하라```bashpython seed.py- 좋다! 거의 다됬다! 마지막으로 app.py 스크립트를 만들고 프로젝트에 추가하자```pythonimport osimport graphenefrom flask import Flaskfrom flask_graphql import GraphQLViewfrom flask_sqlalchemy import SQLAlchemyfrom graphene_sqlalchemy import SQLAlchemyObjectType, SQLAlchemyConnectionFieldapp = Flask(__name__)basedir = os.path.abspath(os.path.dirname(__file__))# Database Configs [Check it base on other Database Configuration]app.config[&amp;#39;SQLALCHEMY_DATABASE_URI&amp;#39;] = &amp;#39;sqlite:///&amp;#39; + os.path.join(basedir, &amp;#39;database.sqlite&amp;#39;)app.config[&amp;#39;SQLALCHEMY_COMMIT_ON_TEARDOWN&amp;#39;] = Trueapp.config[&amp;#39;SQLALCHEMY_TRACK_MODIFICATIONS&amp;#39;] = True# Initialize Databasedb = SQLAlchemy(app)# ------------------  Database Models ------------------class User(db.Model):    __tablename__ = &amp;#39;users&amp;#39;    id = db.Column(db.Integer, primary_key=True)    name = db.Column(db.String(256))    email = db.Column(db.String(256), index=True, unique=True)  # index =&amp;#38;#62; should not be duplicate    posts = db.relationship(&amp;#39;Post&amp;#39;, backref=&amp;#39;author&amp;#39;)    def __repr__(self):        return &amp;#39;&amp;#38;#60;User &amp;#37;r&amp;#38;#62;&amp;#39; &amp;#37; self.emailclass Post(db.Model):    __tablename__ = &amp;#39;posts&amp;#39;    id = db.Column(db.Integer, primary_key=True)    title = db.Column(db.String(256))    body = db.Column(db.Text)    author_id = db.Column(db.Integer, db.ForeignKey(&amp;#39;users.id&amp;#39;))    def __repr__(self):        return &amp;#39;&amp;#38;#60;Post &amp;#37;r&amp;#38;#62;&amp;#39; &amp;#37; self.title# ------------------ Graphql Schemas ------------------# Objects Schemaclass PostObject(SQLAlchemyObjectType):    class Meta:        model = Post        interfaces = (graphene.relay.Node,)class UserObject(SQLAlchemyObjectType):    class Meta:        model = User        interfaces = (graphene.relay.Node,)class Query(graphene.ObjectType):    node = graphene.relay.Node.Field()    all_posts = SQLAlchemyConnectionField(PostObject)    all_users = SQLAlchemyConnectionField(UserObject)# noinspection PyTypeCheckerschema_query = graphene.Schema(query=Query)# Mutation Objects Schemaclass CreatePost(graphene.Mutation):    class Arguments:        title = graphene.String(required=True)        body = graphene.String(required=True)        email = graphene.String(required=True)    post = graphene.Field(lambda: PostObject)    def mutate(self, info, title, body, email):        user = User.query.filter_by(email=email).first()        post = Post(title=title, body=body)        if user is not None:            post.author = user        db.session.add(post)        db.session.commit()        return CreatePost(post=post)class Mutation(graphene.ObjectType):    save_post = CreatePost.Field()# noinspection PyTypeCheckerschema_mutation = graphene.Schema(query=Query, mutation=Mutation)# Flask Rest &amp;#38; Graphql Routes@app.route(&amp;#39;/&amp;#39;)def hello_world():    return &amp;#39;Hello From Graphql Tutorial!&amp;#39;# /graphql-queryapp.add_url_rule(&amp;#39;/graphql-query&amp;#39;, view_func=GraphQLView.as_view(    &amp;#39;graphql-query&amp;#39;,    schema=schema_query, graphiql=True))# /graphql-mutationapp.add_url_rule(&amp;#39;/graphql-mutation&amp;#39;, view_func=GraphQLView.as_view(    &amp;#39;graphql-mutation&amp;#39;,    schema=schema_mutation, graphiql=True))if __name__ == &amp;#39;__main__&amp;#39;:    app.run()  이제 우리는 graphene으로 graphql을 만들었다. 실행해보자.```bash$ python app.py## GraphQL API 테스트- PostMan이나 cRUL을 이용해 서버와 통신해보자.- GraphQL은 원칙적으로 POST와 GET 요청만 받는다.```postman## 1. Rest API examplesGET http://127.0.0.1:5000/### Graphql query-api examplePOST http://127.0.0.1:5000/graphql-queryContent-Type: application/graphql&amp;#123;  allPosts&amp;#123;    edges&amp;#123;      node&amp;#123;        title        author&amp;#123;          email        &amp;#125;      &amp;#125;    &amp;#125;  &amp;#125;&amp;#125;### 2. Graphql mutation-api examplePOST http://127.0.0.1:5000/graphql-mutationContent-Type: application/graphqlmutation &amp;#123;  savePost(email:&amp;#34;cse.sadhan@gmail.com&amp;#34;, title:&amp;#34;Title 2&amp;#34;, body:&amp;#34;Blog post 2&amp;#34;) &amp;#123;    post&amp;#123;      title      body      author&amp;#123;        email      &amp;#125;    &amp;#125;  &amp;#125;&amp;#125;###"
  }
  , 
  
  "/articles/web/CI,CD/Docker/DinD%EC%99%80%20DooD.html": {
    title: "DinD와 DooD",
    date: " May 12, 2022 ",
    url: "/articles/web/CI,CD/Docker/DinD%EC%99%80%20DooD.html",
    tags: ["DOCKER","CICD"],
    content: "DinD와 DooD                    [DinD(docker in docker)와 DooD(docker out of docker)        아이단은 어디갔을까](https://aidanbae.github.io/code/docker/dinddood/)              도커 컨테이너 안에서 도커 실행하기(Docker in Docker, Docker Out of Docker) : 네이버 블로그  참조DinD(Docker in Docker) 구조도커가 설치된 도커 컨테이너 내부에 도커 데몬을 추가로 돌리는, 즉 컨테이너 안에 컨테이너가 중첩되어 있는 구조🟢 별도의 가상환경을 격리하고, 하위의 여러 컨테이너를 한꺼번에 관리할 수 있다.🔴 겉껍질 컨테이너는 --privileged 플래그를 통해 호스트 권한을 획득하는데, 이 때문에 보안상 좋지 않을 수 있다.따라서 Docker에서는 DinD보다 DooD 구조를 추천한다.DooD(Docker out of Docker) 구조주 컨테이너 내부에 속하는 방식이 아니라, 병렬적으로 컨테이너를 둘 생성한 뒤, socket을 공유하는 방식이다.아래와 같이 소켓을 공유한 컨테이너만 관리할 수 있다.docker run -v /var/run/docker.sock:/var/run/docker.sock ...🟢 DinD와 달리 유저그룹을 공유하지 않으므로, 하나의 컨테이너가 모든 컨테이너를 제어하는 보안 위험이 없다.🔴 물론, volume의 디렉토리를 공유 시, 컨테이너 간의 자원이 공유 가능하므로 위험한것은 여전하다."
  }
  , 
  
  "/articles/etc/etcs/%EC%98%81%EC%96%B4%20%EC%8B%9C%EC%A0%9C%20%EC%A0%95%EB%A6%AC.html": {
    title: "영어 시제 정리",
    date: " Jun 10, 2022 ",
    url: "/articles/etc/etcs/%EC%98%81%EC%96%B4%20%EC%8B%9C%EC%A0%9C%20%EC%A0%95%EB%A6%AC.html",
    tags: ["ENG"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true영어 시제(tense) 정리_arinabi님의 블로그와 _해커스 토익 문법서_를 참고하여 작성하였습니다.            시제(tense)      현재(Present)      과거(Past)      미래(Future)                  단순(Simple)      동사+(s)      과거동사 또는 + ed      will + 동사원형              진행(Continuous)      be동사 + 동사ing      과거 be동사  + 동사ing      will + 동사ing              완료(Perfect)      has/have + 과거완료 동사      had + 과거완료 동사      will + have + 과거완료 동사              완료 진행(Perfect Continuos)      has/have + been + 동사ing      had + been + 동사ing      will + have + been + 동사 ing      단순 시제(Simple tense)현재 특정 시간의 동작이나 상태를 나타낸다.      단순 현재(Present Simple): 보통 과거부터 현재, 미래까지 일정한 주기로 일어나는 일, 현재의 상태, 일반적인 사실에 사용한다.    The store opens every day at 9 o’clock.(그 가게는 매일 9시에 문을 연다.) -&gt; 일정 주기        The love is an open door.(사랑은 열린 문이다.) -&gt; 일반적인 사실        I’m hungry. (배고파)-&gt; 현재의 상태            단순 과거(Past Simple): 과거에 일어났었던 일, 이미 끝난 동작이나 상태를 말한다.    The store opened in various regions in North America. (그 가게는 북미 여러 지역에 문을 열었다.) -&gt; 주기적으로 열거나 앞으로 다시 열거나 하지 않음.            단순 미래(Future Simple) : 미래에 일어날 일을 나타낸다.    The store will open tomorrow after lunch. (그 가게는 내일 점심 이후에 문을 열 것이다.) -&gt; 엄밀히 말하면 미래에 대한 추측이나 의지이다.        be going to가 will의 강조(확실한 의지)로 대신 사용될 수 있으며, Shall 또한, will의 대체로 사용시 예언의 의미를 띈다.    You shall not pass!(넌 지나가지 못할 것이다.)    🔵 NOTE  when, If 등이 들어가는 시간, 조건 종속절(subordinate phrases)에는 종속되는 대상 문장이 과거나 미래여도 현재 시제를 쓴다.      When the writer finishes the first draft. I will review it immediately.(작가가 초안을 끝내면 내가 바로 리뷰할 것이다.)    이외에도 before, after, as soon as, once, unless 등이 종속절의 접속사로 온다.  만약, If 절의 시제가 현재 시제가 아니라면, 종속절이 아닌 If 가정법일 수 있다.진행 시제 (Continuous Tense)한 시점에 동작이 계속 진행되고 있는 것을 나타냄.      현재 진행(Present Continuous): 현재에 진행 중인 일을 설명    Ganghui is dancing (강희는 춤추고 있다.)    we are currently hiring! (지금 모집 중입니다.) -&gt; 동작 이외에도 상태의 진행 또한 포함    I am visiting your home next week. (나는 이번 주에 너의 집을 방문할 것이다.) -&gt; 가끔 미래 시제 대용으로도 사용된다.            과거 진행(Past Continuous): 과거에 진행 중이던 일을 설명    Ganghui was dancing(강희는 춤추고 있었다.) -&gt; 과거의 특정 시점에 진행되던 일            미래 진행(Futre Continuous): 미래에 진행 중일 일을 설명    Ganghui will be dancing(강희는 춤추고 있을 것이다.) -&gt; 특정 미래 시점에 진행되고 있던 일        🔵 NOTE  감정 동사(surprise, shock, hate, want, belive 등), 상태 동사(include, need, know, exist, consist 등) 등은 진행시제로 사용할 수 없다.  I prefer(o)(am prefering(x)) a M4A1. (난 M4A1을 선호한다.(O)),(난 M4A1을 선호 중이다.(X))  I need(o) (am needing(x)) it now.(지금 당장 필요해(O)) (지금 당장 필요 중이다.(X))완료 시제(Perfect Tense)특정 기준 시점에서 그 이전에 일어난 일, 그 이전 부터 특정 기준 시점까지 계속 됬던 것을 나타냄.      현재 완료(Present Perfect): 현재에 과거에서 시작된 일이 끝나고 난 직후, 과거의 경험, 과거에 발생한 일이 현재까지 영향을 미치는 일, 현재까지 해오고 있는 일을 설명    Ellen has left. (엘렌은 떠나고 없다.)-&gt; 사건이 끝난 직후 (완료적 용법)        I have never eaten lamb before.(나는 양고기를 먹어본적 없다.) -&gt; 현재의 경험 여부(경험적 용법)        I have lost the document(내가 서류를 분실했다.)-&gt; 과거의 사건이 지금도 영향을 미침.(결과적 용법)        I have lived here since 20 years ago.(나는 이 곳에서 20년간 살아왔다.)-&gt; 현재까지 진행되고 있음.(계속적 용법)              🔵 NOTE      현재 진행형과 다른 점은, 현재 완료는 행동의 경험이 중점이고, 현재 진행은 진행중인 행동 자체가 중점이라는 점이다. 예를 들어 한국어로 해석하자면 현재 완료는 “지금까지 ~해왔다.”이고, 현재 진행은 “~하는 중이다.”이다.      그리고 추가로, 현재 완료의 계속적 용법은 보통 아주 오래된 기간의 일을 지칭하는 경우가 많으며, 현재 진행형은 정말 눈으로 보이는 짧은 동작을 의미하는 경우가 많다.            과거 완료(Past Perfect) : 과거에 일이 끝나고 난 직후, 대과거 등을 설명    I had already left when you arrived. (니가 도착했던 때는 나는 이미 떠나고 없었었다.) -&gt; 특정 과거보다 더욱 오래된 대 과거를 설명(대과거)        Ellen had solved this problem by that time (엘런은 그 문제를 그때 막 풀었었다.) -&gt; 특정 과거 시점에서 끝난 직후를 의미(완료적 용법)        I had met her before. (당시 난 그녀를 이미 만난 적 있었었다.) -&gt; 특정 과거 당시에 경험 여부(경험적 용법)        I had lost the document(내가 서류를 분실했었었다.)-&gt; 대과거 사건의 영향이 대상 과거에도 영향을 미침.(결과적 용법)        I had lived here since I was born.(나는 태어난 뒤로 이전에 이 곳에 살아왔었다.)-&gt; 해당 과거 시점까지 진행되던 상태나 행동(계속적 용법)            미래 완료(Futre Perfect): 미래에 어떤 일이 끝나고 난 직후를 설명    Ellen will have left by then. (엘렌은 이미 떠나고 없을 것이다.) -&gt; 그 미래보다 과거 시점에 일이 끝났을 때를 설명 (완료적 용법).        If you do this one more time, you will have made mistakes twice.(니가 한번 더 그러면, 넌 두번 실수하는 것이다.)-&gt; 특정 미래 당시의 경험 여부(경험적 용법)        Even if you leave me, I will still have remembered you.(니가 나를 떠난다 해도, 나는 여전히 너를 기억하고 있을 것이다.) -&gt; 대상 미래 시점 이전에 끝난 사건이 미래 시점에도 여전히 영향을 미침.(결과적 용법)        He will have waited here for a month by next week.(그는 다음 주면 여기서 한달동안 기다린게 될 것이다.)-&gt; 해당 미래시점까지 진행되던 상태나 행동 (계속적 용법)        🔵 NOTE  조동사(auxiliary verb, do/does, will, must, may, can, etc…) 뒤의 동사는 원형으로 존재해야 한다.  ex 1) I will be missing you (O), I will am missing you (X)  ex 2) He must like it (O), He must likes it (X)  ex 3) I did love you (O), I did loved you (X)완료 진행 시제(Perfect Continuos Tense)일종의 완료 시제의 계속적 용법의 강조, 완료 시제의 연장선으로 보기도 한다.      현재 완료 진행(Present Perfect Continous): 현재 진행되고 있는 상태나 행동의 기간을 강조하고 싶을 때 사용.    I have been playing this game for 8 hours.(나는 지금 이 게임을 8시간 째 플레이해오고 있다.)            과거 완료 진행(Past Perfect Continous): 과거에 진행되던 상태나 행동의 기간을 강조하고 싶을 때 사용.    It had been snowing for 2 days. (이틀째 눈이 오고 있었었다.)            미래 완료 진행(Future Perfect Continous):미래에 진행될 상태나 행동의 기간을 강조하고 싶을 때 사용.    we will have been staying here tommorrow, Until the door open. (우리는 내일 그 문이 열릴 때까지 여기에 머무르고 있을 것이다.)      "
  }
  , 
  
  "/articles/etc/etcs/English%20Determiner.html": {
    title: "English Determiner",
    date: " Jun 11, 2022 ",
    url: "/articles/etc/etcs/English%20Determiner.html",
    tags: ["ENG"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true한정사(Determiner)명사(noun)의 의미를 한정/특정 해주는 역할을 하는 품사.한정사는 명사 앞에 단 하나만 나오며, 중복하여 붙일 수 없다.      My the(❌) Computer (나의 그 컴퓨터) 👉 관사는 여러개 연속해 붙일 수 없다. The computer of mine이 가까운 표현        Some your(❌) kids  (당신의 몇몇 아이들) 👉 Some of your kids가 가까운 표현        My very little friend (나의 아주 작은 친구) 👉 하지만 관사와 명사 사이에 다른 수식어는 존재할 수 있다.  개인적으로 말하거나 작문시 자주 잊어먹는 경우가 많아 공부해보자!관사(Article)영어의 관사는 크게 두가지로 나뉜다. 정관사(Definite Article)과 부정관사(Indefinite Article)이다.정관사(Definite Article)the는 영어의 정관사로 주로 한국어로 그로 해석되나 생략되는 경우도 많다.주로, 대화 중에 언급한적 있는 단어 앞에 붙히거나, 유일무이하고 모두가 알고 있는 명사 앞에 붙인다.      I saw a dog yesterday, And the dog looked sad. (나 어제 개 한마리를 봤는데, 그 개는 슬퍼 보였어)👉 이 경우, 앞선 개를 언급하는 것이므로 the를 붙이는 경우다. 굳이 앞서 언급하지 않아도, 상대방이 알법한 존재면 the를 붙여도 된다.        The Sun is rising.(해가 뜨는 중이야.) 👉 해는 유일무이하고, 모두가 아는 하나의 존재이므로 the를 붙인다.  또한, 뒤의 명사의 첫글자가 자음(cosonant)일 경우 더(ðə) 로 발음 하며, 모음(vowel)일 경우 디(ðiː)로 바꾸어 발음해야 한다.정확히는 글자보다는 발음이 모음 발음이냐, 자음발음이 냐느냐에 따라 바뀌며, 따라서 예외로 The hour처럼 발음이 모음처럼 나는 경우에는 첫글자가 자음이지만 디(ðiː)로 발음하는 경우도 존재한다.아래에 배울 부정관사와 달리 명사의 수, 가산/불가산(countable/uncountable) 명사와 관계없이 사용할 수 있다.  (The) dogs of war (전쟁의 개들 (영화)) 👉 복수명사 앞이지만 정관사 The 사용 가능부정관사(Indefinite Article)a, an 는 영어의 부정관사로 주로 한국어로 한으로 해석되나 생략되는 경우도 많다. 하나의라는 의미를 가지는 수량 형용사로 해석되기도 한다.중요한 점은, 가산(countable) 명사의 단수형(singular) 앞에서만 필수적으로 사용 해야하며, 복수형(plural) 일 경우, 사용되지 않고, 명사 대신 복수명사나 명사 +s가 대신 사용되며, 이때는 한정사 없이 사용 가능하다..      Eating an apple in the morning is good for your health. (아침에 사과 하나를 먹는 것은 건강에 좋다.)  👉 옳은 표현        Get a(❌) devices! (장비들을 가져와!) 👉 여러 장비를 포함하므로, 부정관사 a가 들어가선 안된다.        A(❌)  Water is important. (물은 중요하다.) 👉 불가산 명사이므로 앞에 부정관사를 사용할 수 없으며, 앞에 관사를 사용하지 않아도 됨.        A(❌) Cats are very cute. (고양이들은 정말 귀엽다.) 👉 복수 명사이므로 앞에 부정관사를 사용할 수 없으며, 어떠한 관사도 사용하지 않아도 됨.  앞서 배운 정관사처럼 뒤에 나온 단어의 첫글자 발음이 자음이면 a, 첫글자 발음이 모음이면 an을 사용하며, 예외 또한 마찬가지이다.      an office (사무실 한곳) 👉 정상적인 사용        a jacket (자켓 하나) 👉 정상적인 사용        an X-ray machine (엑스레이 기계 하나) 👉 자음 앞에 an이 나오는 예외 1        an honor student(우등생 한명) 👉자음 앞에 an이 나오는 예외 2        a user (사용자 한명) 👉 모음 앞에 a가 나오는 예외 1 (발음이 y 발음이라)        a one-eyed pirate (한 외눈 해적) 👉 모음 앞에 a나오는 예외 2 (발음이 w 발음이라)  수량 형용사(Quantifiers)참조 블로그 : 어쩌다 서른수량 형용사(Adjective)는 명사 앞에 위치하면서 명사의 수나 양을 나타내는 형용사이다.크게, 가산 명사 앞, 불가산 명사 앞, 또는 둘 모두 앞에 사용하는 종류로 구분된다.가산(Countable) 명사 앞셀수 있는 명사의 대략적인 수를 나타내기 위한 형용사로, 단수 명사 앞에만 오는 종류와 복수 명사 앞에만 오는 종류로 구분된다.단수(singular) 명사 앞a single, a(하나의), every(모든), another(또 다른), each(각각의) 등이 존재한다.      One litte indian (한 명의 작은 인디언) 👉 여기서하나의라는 뜻으로 쓰인 수량 형용사 one과 달리 little은 작은이란 뜻의 형용사로 쓰였으므로, 한정사가 두개 붙은 오류가 아니다.        Every day has its night (매일 저녁이 있다.= 좋은 일이 있으면 나쁜일이 뒤따른다.) 👉every가 모든 같은 복수 명사의 형용사로 보여도, 뒤는 단수 명사이므로, 단수 비동사(has)가 쓰여야 한다.  every, another의 경우, few/숫자 + 복수 명사 앞이라면, 뒤가 단수 명사가 아니여도 올 수 있다. 이는, 그 뒤의 한 묶음을 한 단위로 해석하기 때문이다.  Another 100 bucks would be reasonable.(100 달러 추가 정도면 적절한 가격일 것이다.)👉 여기서 another는 ~만큼 더라는 의미이다.복수(plural) 명사 앞several(여러 개의), both(둘 다), a number of(많은), a majority of( 대부분의) 등이 존재한다. 뒤의 명사는 꼭 복수형이어야 한다.      (A) few people were there. (몇몇 사람이 그곳에 존재했다.) 👉 (a) few의 a는 생략 가능하며,  몇몇의, 몇 개의라는 의미를 가진다. few의 경우 조금 긍적적인 느낌의 a few보다 좀 더 수가 적은 느낌이며, 더 부정적인 느낌이다.        Be one of us! (우리와 함게 하라!)👉 one of는 ~ 중 하나라는 의미를 가지며, us, you, them 같은 일부 목적격(Accusative) 대명사(pronoun)를 제외하곤 명사 앞에 the를 붙여야 한다.        Many (climbers) can’t reach the top of this mountain.(다수(의 등산가)들이 이 산의 정상에 도달하지 못한다.) 👉Many는 대표적인 가산 명사 수량 형용사로, 많은 (수의)라는 의미이며, 자주 사용된다. 가끔 뒤의 명사가 생략된 채로 사용되는 데, 이때는 다수라는 뜻의 대명사(pronoun)로 사용된다.  불가산(Uncountable) 명사 앞much(많은),(a) little(적은), a great deal of(많은 양의), less(더 적은) 등이 존재하며, 불가산 명사 앞에서만 사용된다.      I just need (a) little assistance by you.(전 당신의 조금의 도움이 필요할 뿐이에요.) 👉(a) little은 가산 명사의 (a) few에 대응되며, 마찬가지로 a를 생략 가능하며, 생략하면 겨우 조금의 같은 부정적이고 더 적은 느낌의 표현이 된다. few는 수를, litte은 양을 표현하다고 생각하면 된다.        Too much water.(물이 너무 많아.) 👉 much는 가산 명사의 many와 대응되며, 많은 (양의)라는 의미를 가진다.  모두 사용 가능(a) lot of(많은), all(모든), most(대부분의), some(어떤, 몇몇의), other(다른) 등이 존재하며, 양측에 모두 사용 가능하지만 일부 규칙이 다르다.      Plenty of dishes mean Plenty of work. (많은 양의 접시는 많은 양의 일을 의미한다.)👉 plenty of, (a) lot of는 많은이라는 뜻이며, 가산, 불가산 어느 쪽에나 사용 가능하다.        All people who visit here should be checked their body temperature. (이곳에 방문한 모든 사람들은 체온을 검사받아야 한다.) 👉 단, 가산 명사 앞에서 사용될 때는 언제나 복수형 앞에만 사용할 수 있다.  추가로, 모든 종류의 수량 형용사는 수량 형용사 + of the + 명사를 통해 ~의 부분, ~의 전체, ~ 중 하나 등, 집합 중 일부를 제한할 수 있는데, 이때 ,of the는 일부 예외를 제외하고 the 가 강제된다.      None of the apples are rotten.(어느 사과 하나도 썩지 않았다.) 👉 of the가 포함되어야 하는 예시        Many foods contain a lot of salt. (많은 음식들이 다량의 염분을 포함하고 있다.) 👉 many는 이 예외를 피해가고 of the 전체를 생략할 수 있다.        Both (of) the dogs are male. (두 개 모두 수컷이다.) 👉 both와 all은 of를 생략하고 사용할 수 있다.        Most of our products are highly customizable. (우리의 제품들의 대부분은 커스텀성이 뛰어나다.) 👉 소유격이나, 목적격(us, them, you) 대명사 앞에서는 the가 생략된다.  소유격 형용사(Possessive Adjectives)영어에서 소유를 나타내는 방법 중 하나로, 소유격 형용사를 사용할 수 있다.my, his, her, your, their, our, 's 등이 존재하며, 명사 앞에 수식되어 명사의 소유를 표시하며, 한국어로 ` ~의`로 해석된다.      My own dream (나 자신의 꿈) 👉own또한 ~의 것이라는 의미이지만, 소유격 뒤, 명사 앞에 붙어 소유를 강조하는 의미로 사용되는 경우가 많다. 동사로는 소유하다, 인정하다라는 의미를 가진다.        The car is Ganghui’s (그 차는 강희의 것이다.) 👉소유격 대명사는 her,mine, ours가 존재하며, ~의 것이라는 의미를 가진다. 's 형태는 대명사와 형용사의 형태가 같다.  글의 처음에 예시를 보였던 것처럼, 다른 한정사와 같이 사용할 수 없으며, 여러 의미를 주기 위해서 부사나 형용사 같은 수식어나 전치사를 이용한다.지시 형용사(Demonstrative Adjectives)this, that, these, those 등이 존재하며, 이들은 형용사 뿐만 아니라 대명사의 역할도 가능하므로, 지시 대명사라고 불린다.한국어로 지시 형용사일 경우, 이, 저로 해석되며, 지시 대명사일 경우 이것, 그것, 이것들, 그것들 등으로 해석된다.      This is pencil. (이것은 연필이다.) 👉 지시 대명사로 사용된 형태.        This love has taken its toll on me. (이 사랑은 나에게 요금을 가져갔다.= 이 사랑은 나에게 상처를 주었다.) 👉 지시 형용사로 사용된 형태.  that의 경우 접속사(Conjunction)로 사용되는 경우가 많으므로, 주의해야한다."
  }
  , 
  
  "/articles/etc/etcs/Git%20%ED%8C%8C%EC%9D%BC%20%EA%B8%B0%EB%A1%9D%20%EC%A7%80%EC%9A%B0%EA%B8%B0.html": {
    title: "Git 파일 기록 지우기",
    date: " Jul 12, 2022 ",
    url: "/articles/etc/etcs/Git%20%ED%8C%8C%EC%9D%BC%20%EA%B8%B0%EB%A1%9D%20%EC%A7%80%EC%9A%B0%EA%B8%B0.html",
    tags: ["GIT","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueGit 파일 기록 지우기Git을 이용할 때 실수로 잘못된 파일이 올라가면 곤란한 경우가 있다.내가 겪은 경험은 Javascript의 node_modules폴더가 대표적인데, 이 안에는 프로젝트에서 사용할 패키지들의 수많은 파일과 폴더들이 존재하므로 보통 Git에 올리지 않고, 팀원들이 각자 pull할때마다 npm install 커맨드를 통해 갱신하는게 일반적인 방법이다. 또 다른 예로, AWS 강의를 들을 때, 개인의 AWS private key이 올라가, 누군가 제 3자가 EC2 인스턴스를 이용해 비트코인 채굴에 악용하여, 금전적 피해를 입은 사례도 있었다.아무리 개인의 사소한 개인의 저장소여도, 공개 저장소의 키워드 등을 탐지하는 봇을 이용해 공격을 시도하는 해커들이 존재하므로, 방심하면 안된다. 이렇게, 생각보다 중요하고 까다로운 과정이므로 이런식으로 글을 남겨 기록하게 되었다.                   확인 명령어      취소 명렁어                  git add 전      git check-ignore &lt;file&gt;      .gitignore 파일 작성              git add 후      git status      git restore --staged &lt;file&gt;, git reset              git commit 후      git log -p      git reset --hard, git reset --soft, git reset HEAD~1              git push 후                            git clone, fork 후      😱      😱      상황별 Git 파일 기록 지우기실무적으로 큰 사고가 났을 때는 대응하기 위해 📄공식적인 정보를 참조하기 바란다.공통적인 필요 조치와 팁들알아보기 앞서, 조치를 취하기 전, 중, 후의 공통적인 조치들을 알아보자.      .gitignore 파일을 제대로 작성하자.    🔵 gitignore 파일 작성법, git check-ignore 명령어, 전역 제외 설정 등의 도움되는 정보는  📝gitignore 작성법 확인        프로젝트 시작부터 env 세팅을 하자.          env 세팅에 대한 이야기는 나중에 포스트로 업로드 예정            코드 리뷰를 꼼꼼히 하자.        공용되는 API 키나 암호화 키들은 언제나 프로젝트 파일 바깥에 보관, 암호는 서비스 별로 다르게 사용하자.        git add ., git add * 그리고 git add -A 사용 금지, 프로젝트에 따라 다르겠지만 보통 파일 별로 코딩 후 커밋하자.    🔵 GUI GIT 프로그램, git add --interactive 같이 겉으로 보이는 정보량이 많은 방법을 사용하는 것도 도움이 될 수 있다.        가장 중요한 마지막으로, ↩git push 이후 조치 이후 부터 유출된 민감한 정보(암호 등) 등을 꼭 변경하고, Github 고객지원 팀에 연락하여 캐쉬된 데이터와 참조 등을 지워야 한다.  잘못된 파일을 git add민감한 정보가 Staging1된 상태이다. git status를 통하여 먼저 확인해보자.$ git statusOn branch mainChanges to be committed:  (use &amp;#34;git restore --staged &amp;#38;#60;file&amp;#38;#62;...&amp;#34; to unstage)        modified:   .gitignore        new file:   can_upload.md        new file:   no_upload.md        deleted:    test.md.gitignore 파일 설정이 잘못되어 올라가선 안될 파일 no_upload.md가 올라가버린 상황이다.이때의 대처는 쉽다. Staging 영역에서 특정 파일을 빼려면, git restore --stage &lt;file&gt; 또는 git reset HEAD &lt;file&gt; 명령어를 통해 가능하다.$ git restore --staged no_upload.md$ git statusOn branch mainChanges to be committed:  (use &amp;#34;git restore --staged &amp;#38;#60;file&amp;#38;#62;...&amp;#34; to unstage)        modified:   .gitignore        new file:   can_upload.md        deleted:    test.mdUntracked files:  (use &amp;#34;git add &amp;#38;#60;file&amp;#38;#62;...&amp;#34; to include in what will be committed)        no_upload.mdno_upload.md 파일이 Staging 영역에서 빠져나와 Untracked files에 추가된 것을 알 수 있다.참고로, git reset 명령어를 사용하면 모든 파일들이 전부 빠지게 된다.이제, .gitignore 파일을 적절히 설정해준 뒤, 다시 작업을 하면 된다.잘못된 파일을 git commit이번에는 Staging됐을 때 조차 눈치채지 못하고, Commit은 했되, 아직 push하지 못한 상태를 알아보자.다행히 당신의 파일들은 컴퓨터에 존재하는 로컬 저장소에만 존재하고, 아직 원격 저장소에 올라가지 않은 상태이다.git log -p를 이용해 자세한 내용을 알아보자.$ git log -pcommit aa4bf546e374e8e4bf************ (HEAD -&amp;#38;#62; main)Author: Junseok YUN &amp;#38;#60;************@gmail.com&amp;#38;#62;Date:   Tue Jan 11 10:40:21 2022 +0900    wrong commit...diff --git a/no_upload.md b/no_upload.mdnew file mode 100644index 0000000..388e2d7--- /dev/null+++ b/no_upload.md@@ -0,0 +1 @@+No! don&amp;#39;t upload it!\\ No newline at end of file잘못된 파일인 no_upload.md가 올라간 것을 알 수 있다.이 과정 중에 조심해야할 점은, 이때 잘못된 파일이 올라갔다면, .gitignore 파일을 수정하여도 제외되지 않는다는 점이다. 즉, 최초로 잘못 올라간 commit부터 조치해야 한다.잘못된 파일을 Commit에서 삭제바로 이전 commit 제거가장 최신의 commit이 잘못되어 있다면, 간단하게 git reset HEAD~1[^HEAD~n]로 가능하다.git reset HEAD~1이제, 가장 최신의 commit은 지워졌으며, 이후에, 잘못을 고친 뒤, 다시 모든 파일을 commit하면 된다.추가로, --hard, --soft 플래그를 이용할 수 있다.git reset --hard HEAD~1--hard 플래그는 해당 commit의 모든 수정과 추가된 파일을 지워버린다. 즉, 방금 당신이 commit 했던 작업한 모든 파일이 날아간다.git reset --soft HEAD~1--soft 플래그를 사용하면 당신이 작업한 파일들은 commit에만 빠지며, 수정한 작업과 파일들은 여전히 존재하며 심지어 staging 되어있는 상태이다.즉, 그대로 다시 git commit 명령어를 입력하면, 방금 했던 실수를 재현할 수 있을 것이다. 앞서 설명했던 ↩git add 이후 조치가 필요하다.여러번 이전 commit 제거여러번 이전 commit을 제거하고 싶다면, 잘못된 commit의 SHA 값을 이용하거나, 해당 commit과 현재 상태의 차이 만큼 뒤의 숫자를 n만큼 늘려 HEAD~n을 reset 시키면 된다.git reset HEAD~5git reset 06b48c9d7880c70f9501611************(돌아가고 싶은 commit의 SHA)  ❔ 잘못된 파일을 추가한 commit 하나만 지우고 싶은가? 모든 commit들은 이전 commit을 참조하는 형식으로 생성되기 때문에 그건 불가능하다.잘못된 파일을 git push당신이 원하지 않던 파일이 원격 저장소의 기록에 올라가게 되고, 이제부터 문제가 복잡해지기 시작한다.저장소가 공개 저장소일 경우, 단순히 저장소 뿐만 아니라 Fork, Clone, 웹 아카이브, 웹 캐쉬 등으로 남기 때문에, 결국 가장 확실한 방법은 민감한 정보를 바꾼 뒤, 저장소를 날리는 것이다.물론, 지금까지의 작업했던 브랜치와 커밋은 날아가게 되며, 이슈, 프로젝트, 마일스톤, PR 등을 옮기는 것도 불가능한 것은 아니지만 많은 시간과 노력이 필요하다.🔵 NOTE이슈 이동, 프로젝트 이동은 📄공식문서 참조잘못된 Commit 삭제잘못된 파일 필터링잘못된 파일을 여러 번의 브랜치 생성, git push 이후누군가가 나의 레포지토리를 Clone, Fork하여 널리 퍼진 이후더이상 막을 수 없다. Clone한 모든 컴퓨터가 본인의 제어 하에 있고, Fork한 모든 저장소가 비공개 + 본인 제어에 존재한 게 아닌 이상, 모두 위에서 말한 조치들을 취하긴 어려우며, 개인용 컴퓨터에 남은 기록은 삭제하여도 복원 가능하다.그저 유출된 민감한 정보를 곧바로 바꾸는 방법 밖에 없다. 다음에 잘하자!            기존의 SVN과 달리 GIT에는 로컬 저장소와 작업 트리(작업 폴더) 사이에 존재하는 인덱스(Index)라는 가상의 공간이 존재한다. Staging은 git commit 명령을 통해 인덱스의 파일들(git add &lt;file&gt;된 파일들)을 로컬 저장소로 파일들을 올리는 과정인데, 이를 통해 여러 파일들을 Staging 한뒤, 한꺼번에 코드를 검사하고, commit할 수 있다. 즉, 이런식으로 잘못된 Commit을 막기위해 만들었다는 의미이다. &#8617;      "
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%201-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98%20%EC%86%8C%EA%B0%9C.html": {
    title: "OS 정리-Chap 1-컴퓨터 시스템의 소개",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%201-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98%20%EC%86%8C%EA%B0%9C.html",
    tags: ["OS","CS","컴퓨터_구조","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true1. 컴퓨터 시스템의 소개title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 컴퓨터 하드웨어의 구성컴퓨터 시스템은 다음 두 가지로 이루어진다.  데이터를 처리하는 물리적인 기계장치 하드웨어(hardware)  어떤 작업을 지시하는 명령어로 작성한 프로그램 소프트웨어(software)운영체제(OS, Operating System)은 컴퓨터 하드웨어를 관리하는 소프트웨어그중 컴퓨터 하드웨어는 크게 프로세서, 메모리(기억장치), 주변 장치로 구성되고, 시스템 버스로 연결된다.1 프로세서(processor)중앙 처리 장치(CPU, Central processing unit)이라고도 하며, 컴퓨터 하드웨어에 부착한 모든 장치의 동작을 제어하고 명령을 실행한다.title: 프로세서의 수에 따른 병렬 처리프로세서의 수가 많을 수록 병렬 처리로 처리속도를 높일 수 있다.프로세서의 수에 따라 1개인 싱글 코어부터 8개인 옥타 코어까지 다양하다.프로세서는 그림 1-2와 같이 연산장치와 제어장치, 레지스터로 구성되고, 내부 버스(시스템 버스)로 연결됨.레지스터(Register)는 다음과 같이 구분한다.  용도에 따라 구분          전용 레지스터      범용 레지스터        사용자의 정보 변경 가능 여부에 따라 다음과 같이 사용자 가시 레지스터와 사용자 불가시 레지스터로 구분한다.title: 사용자 가시 레지스터의 구분사용자 가시 (user-visible) 레지스터: 사용자가 운영체제, 응용 프로그램 등을 통해 정보를 변경 가능\t- 사용자 가시 레지스터는 추가로 저장하는 정보 종류에 따라 표(1-1)와 같이 세분화된다.title: 표 1-1            종류      설명                  데이터 레지스터(DR, Data Register)      함수 연산에 필요한 데이터를 저장, 값, 문자 등을 저장하므로 산술 연산이나 논리 연산에 사용하며 연산 결과로 플래그 값 저장              주소 레지스터(AR, Address Register)      주소나 유효 주소를 계산하는 데 필요한 주소의 일부분을 저장, 주소 레지스터에 저장한 값(값 데이터)를 사용해 산술 연산      또한, 주소 레지스터는 용도 별로 추가로 아래와 같이 구분할 수 있다.title: 주소 레지스터의 용도별 세분화            종류      설명                  기준 주소 레지스터      프로그램을 실행할 때 사용하는 기준 주소 값을 저장, 페이지나 세그먼트처럼 블록화된 정보에 접근하는 데 사용됨              인덱스 레지스터      유효 주소를 계산하는 데 사용하는 주소 정보를 저장              스택 포인터 레지스터      메모리에 프로세서 스택을 구현하는데 사용, 보통 반환 주소, 프로세서 상태 정보, 서브루틴의 임시 변수를 저장        기준 주소 : 하나의 프로그램이나 일부처럼 서로 관련 있는 정보를 저장하며, 연속된 저장 공간을 지정하는 데 참조할 수 있는 주소  많은 프로세서와 주소 레지스터를 통해 큐 포인터로도 사용됨.title: 사용자 불가시 레지스터의 구분(표 1-2)사용자 불가시 (user-invisible) 레지스터: 사용자가 정보를 변경할 수 없는 레지스터, 프로세서의 상태와 제어를 관리            종류      설명                  프로그램 카운터(PC, Program Counter)      다음에 실행할 명령어의 주소를 보관하는 레지스터, 계수기로 되어있어 실행할 명령어를 메모리에서 읽으면 명령어의 길이만큼 증가해 다음 명령어를 가리키며, 분기 명령어는 목적 주소로 갱신              명령어 레지스터(IR, Instruction Register)      현재 실행하는 명령어를 보관하는 레지스터              누산기(ACC, ACCumulator)      데이터를 일시적으로 저장하는 레지스터              메모리 주소 레지스터(MAR, Memory Address Register)      프로세서가 참조하려는 데이터의 주소를 명시하여 메모리에 접근하는 버퍼 레지스터              메모리 버퍼 레지스터(MBR, Memory Buffer Register) 혹은 메모리 데이터 레지스터(MDR, Memory Data Register)      프로세서가 메모리에서 읽거나 메모리에 저장할 데이터 자체를 보관하는 버퍼 레지스터      아래 그림 1-3은 프로세서와 레지스터의 구조를 더욱 자세히 표현한 것이다.2 메모리메모리의 입출력 속도는 컴퓨터에 지대한 영향을 주지만 속도가 빠른 메모리는 상당히 비싸므로, 그림 1-4와 같이 메모리 계층 구조를 구성하여 비용, 속도, 용량, 접근 시간 등을 상호 보완한다.불필요한 프로그램과 데이터는 보조기억 장치에 저장했다가 실행, 참조 시에만 메인 메모리로 옮기는 원리로 비싼 메모리의 필요 용량을 줄일 수 있다.또한 캐시나 레지스터 등을 이용해 프로세서와 메인 메모리와의 속도 차이를 보완하여 시스템 성능을 늘릴 수 있다.2.1 레지스터프로세서 내부에 있으며, 프로세서가 사용할 데이터를 보관하는 가장 빠른 메모리, 앞선 레지스터 분류 참조2.2 메인 메모리프로세서 외부에 존재하며 즉작적으로 수행할 프로그램과 데이터를 저장하거나 프로세서의 처리 결과를 저장함.프로세어와 보조 기억장치 사이에서 입출력 속도 병목 현상을 해결하기도함.입출력 장치들 또한 보조기억장치가 아닌 메인 메모리와 정보를 주고 받는다.주기억장치 또는 1차 기억장치라고 하며, SRAM(Static RAM) 보다는 저장 밀도가 높고 가격이 싼 DRAM(Dynamic RAM)을 이용한다.메인 메모리는 다수의 셀로 구성되며, 각 셀은 비트로 구성됨.셀이  k비트이면 총 $2^k$개의 표현이 가능하며, 셀 여러개로 데이터를 저장한다.셀은 주소로 참조하며, n비트이면 주소 범위는 0~$2^n -1$까지이다.  컴퓨터 메모리 내의 주소를 물리적 주소라고 하며, 프로그래밍 시 이를 직접 사용하지 않음  대신 프로그램은 수식이나 변수에 값을 할당함.  컴파일러가 프로그램을 기계 명령어로 바꾸는 와중에 수식과 변수에 주소를 할당          이 과정을 매핑(mapping) 혹은 사상이라고 한다.        이 주소를 논리적 주소(가상 주소, 프로그램 주소)라고 하며, 이 값들을 별도의 주소 공간에 저장한다.메모리 속도는 메모리 접근 시간과 메모리 사이클 시간으로 표현  메모리 접근 시간: 명령이 발생한 후 목표 주소를 검색하여 데이터 쓰기(읽기)를 시작할 때까지 걸린 시간  메모리 사이클 시간 : 두 번의 연속적인 메모리 동작 사이에 필요한 최소 지연 시간, 보통 접근 시간 보다 김2.3 캐시(cache)프로세서 내부나 외부에 존재하며, 처리 속도가 빠른 프로세서와 상대적으로 느린 메인 의 속도 차이를 보완하는 고속 버퍼캐시는 그림 1-10과 같이 메인 메모리에서 데이터를 블록 단위로 가져와 프로세서에 워드 단위로 전달하여 속도를 높임.또한 대역폭을 확대하여 프로세서와 메모리의 속도 차이를 줄임그림 1-11은 캐시 동작의 구체적 순서이다.캐시는 주소 영역을 한번 읽어 들일 수 있는 크기로 나눈 후 각 블록에 번호를 부여하여 이 번호를 태그로 저장해 놓는다.  프로세스가 필요한 데이터를 메인 메모리에 찾기 전에 캐시에 존재하는 지 확인  접근 하려는 주소 24비트(0001 0110 0011 0011 1001 1100) 중 처음 22 비트로 캐시의 모든 라인에 접근하여 일치를 확인함  일치하는 라인이 존재하면 주소의 나머지 2비트(00)을 이용해 해당 라인의 4개 바이트 중 하나를 가져옴  해당 바이트의 값이 원하는 데이터임캐시의 성능은 캐시 적중율에 달려있다.  캐시 적중(Cache hit)은 프로세서가 참조하려는 정보가 존재할 때  캐시 실패(Cache miss)는 존재하지 않을 때이다.블록의 크기는 캐시의 성능을 좌우하며, 이는 메모리의 지역성이라는 특징 때문이다.[[OS 정리-Chap 8-가상 메모리#2 지역성(구역성, 국부성, locality)]]을 참조지역성은 블록이 크면 캐시의 히트율이 올라가지만, 전송 부담과 캐시 데이터 교체가 늘어나므로 적절한 크기를 찾아야 한다.2.4 보조기억장치주변 장치 중 프로그램과 데이터를 저장하는 하드웨어, 2차 기억 장치 또는 외부 기억 장치라고 하며, 자기 디스크, 광 디스크, 자기 테이프 등이 존재보통 자기 디스크로 된 하드디스크를 많이 사용하지만, 최근에는 메모리 기술이 발달하면서 SSD도 많이 사용한다.  SSD(Solid State Disk): 메인 메모리로 많이 사용하던 플래시 메모리를 컨트롤러와 함께 사용하는 저장 장치, 입출력 속도가 아주 빠르다.  NVRAM(Non-Volatile RAM): 외부 전원이 꺼지거나 상실되더라도 내용이 보존되는 RAM3 시스템 버스(system bus)하드웨어를 물리적으로 연결하여 서로 데이터를 주고받을 수 있게 하는 통로컴퓨터 내부의 데이터 입출력 신호, 프로세서 상태 신호, 인터럽트 요구, 클록 신호 등을 전달하는 역할이며, 기능에 따라 표 1-3 처럼 나뉜다.title: 시스템 버스의 종류 (표 1-3)            종류      설명                  데이터 버스      프로세서와 메인 메모리, 주변 장치 사이에서 데이터를 전송, 데이터 버스 구성 배선 수에 따라 전송하는 비트 수가 결정되며 이를 워드라고 함.              주소 버스      프로세서가 시스템의 구성 요소를 식별하는 주소 정보를 전송, 배선 수가 프로세서와 접속 할 수 있는 메인 메모리의 최대 용량을 결정              제어 버스      프로세서가 시스템의 구성 요소를 제어하는 데 사용, 제어 신호로 연산장치의 연산 종류와 메인 메모리의 읽기나 쓰기 동작을 결정      4 주변 장치프로세서와 메인 메모리를 제외한 나머지 하드웨어 구성 요소입력 장치, 출력 장치, 저장 장치가 존재02 컴퓨터 시스템의 동작컴퓨터 시스템으로 작업 처리 시, 다음 순서에 따라 동작하며, 제어 장치가 이 동작을 제어  입력 장치로 정보를 입력 받아 메모리에 저장  메모리에 저장한 정보를 프로그램 제어에 따라 인출하여 연산 장치에서 처리  처리한 정보를 출력 장치에 표시하거나 보조 기억 장치에 저장1 명령어의 구조명령어는 실행할 산술, 논리 연산의 동작을 명시하는 문장, 이들의 집합이 프로그램이다.명령어의 구성  프로세서가 실행할 연산인 연산 부호 : 왠만하면 한개  명령어가 처리할 데이터  데이터를 저장한 레지스터나 메모리 주소인 피연산자 : 여러 개일 수 있음명령어는 프로세서에 따라 고정 길이나 가변 길이로 구성  연산 부호(OPcode, OPeration code): 프로세서가 실행할 동작인 연산을 지정, 비트 수가 n이면 최대 $2^n$개의 연산 종류가 존재한다.          산술 연산(+, -, *, \\), 논리 연산(AND, OR, NOT, XOR), 시프트, 보수 등이 존재        피연산자(operand) : 연산할 데이터 정보를 저장, 레지스터, 메모리, 가상 기억장치, 입출력 장치 등에서 가져온다. 데이터 값 그 자체 혹은 데이터 주소를 기입함          그림 1-14는 명령어의 구조 예시이며, 피연산자가 2개인 경우 이다.      명령어는 운영체제 실행 이후 (그림 1-15)처럼 메인 메모리에 저장되며, 한 번에 하나씩 프로세서에 전송되어 실행된다.  피연산자 수에 따라 0-주소 명령어, 1- 주소 명령어, 2-주소 명령어 등이 존재그림 1-16의 모드비트(mode bit)는 피연산자의 내용이 직접 주소(0) 혹은 간접 주소(1)임을 나타낸다.그림 1-17은 모드가 1비트, 연산 부호가 3비트, 피연산자가 6비트인 명령어이다.(b)를 보면 알다시피, 직접 주소의 경우 1번의 참조를, 간접 주소의 경우 2번의 참조가 필요하다는 것을 알 수 있다.2 명령어의 실행그림 1-18은 명령어의 실행 과정이다.명령어 인출-해석-실행 사이클, 인출-실행 사이클, 명령어 실행 주기라고 한다.명령어 실행 사이클은 인터럽트 여부, 간접 주소 여부 등에 따라 다양하게 변하며, 이 과정을 반복한다.2.1 인출 사이클(fetch cycle)  명령어 실행 사이클의 첫번째 단계  메모리에서 명령어를 읽어 명령어 레지스터에 저장하고, 다음 명령어를 실행하려고 프로그램 카운터를 증가 시킴  이때 소요되는 시간은 명령어 인출 시간2.2 실행 사이클(execution cycle)  인출한 명령어를 해독하고 그 결과에 따라 제어 신호를 발생 시켜 명령어 실행  이때 소요되는 시간을 실행 시간이라고 한다.2.3 간접 사이클(indirect cycle)가져온 명령어를 즉시 수행하는 직접 주소와 달리, 한번 더 메모리에서 유효 주소를 읽어옴위 그림은 한번만 실행되는 것처럼 보이며, 위 과정을 두 번 반복해야 간접 사이클인듯 하다.2.4 인터럽트 사이클(interrupt cycle)인터럽트란?  프로세서가 프로그램을 수행하는 동안 컴퓨터 시스템의 내부나 외부에서 발생하는 예기치 못한 사건  현재 실행 중인 프로그램을 중단하고 다른 프로그램의 실행을 요구하는 명령어  컴퓨터에 설치된 입출력 장치나 프로그램 등에서 프로세서로 보내는 하드웨어 신호  갑작스런 정전, 잘못된 명령어 수행, 입출력 작업 완료 등에 사용됨  시스템 처리 효율 향상, 실행 순서 변경, 다중 프로그램을 가능케 함.  프로세서는 매 실행 사이클 완료 후 인터럽트 요구를 검사  인터럽트 존재 시, 현재 수행 중인 프로그램 주소(프로그램 카운터의 값)을 스택이나 메모리의 0번지 같은 곳에 저장  인터럽트 처리 루틴의 시작 주소를 프로그램 카운터에 적재하고 인터럽트 처리  다시 스택이나 메모리 0번지의 프로그램 주소를 되돌리고 계속 수행3 인터럽트 명령어인터럽트의 필요성 사용 예시  각 장치는 사용 가능한지 알려주기 위해 1 비트의 상태 비트를 가지고 있음  프로세서는 장치를 사용하기 전에 폴링(polling)으로 각 장치의 상태 비트를 검사  이때 주기적으로 장치를 검사하면 비효율적이므로 장치의 인터럽트를 통해 각 장치의 상태 비트를 업데이트제어 버스 중에 인터럽트 요청 회선(IRQ, Interrupt ReQuest line)은 이러한 인터럽트를 보내는 회선이다.인터럽트는 인터럽트 요청과 인터럽트 서비스 루틴으로 구분할 수 있다.  인터럽트 서비스 루틴(interrupt service routine) : 인터럽트 요청 신호에 따라 수행하는 루틴인터럽트 요청 회선의 연결 방법은 그림 1-23처럼 단일 회선과 다중 회선으로 나뉜다.  단일 회선 : 인터럽트 요청이 가능한 모든 장치를 공통의 단일 회선으로 프로세서에 연결하는 방법, 각 인터럽트를 요청한 장치를 구분할 방법이 필요  다중 회선 : 모든 장치에 고유의 회선이 존재하는 방법, 인터럽트 요청 장치 구분 쉬움그림 1-24는 인터럽트 처리 과정의 예시이다.앞선 [[#2.4 인터럽트 사이클(interrupt cycle)]]처럼, 기존의 명령어를 스택 영역에 저장하고, 인터럽트 처리 후 재 진행한다.인터럽트와 서브루틴의 차이PC 값을 저장하고 재적재하는 방식이 서브루틴과 비슷하나 몇몇 다른 점이 있다.  서브루틴은 자신을 호출한 프로그램이 요구한 기능 수행  인터럽트는 장치가 요구한 기능 수행          현재 실행 중인 프로그램과 관계없을 수 있음      따라서 인터럽트는 프로그램 카운터 뿐만 아니라 영향을 미칠 수 있는 기존 프로그램의 상태 워드와 모든 정보를 저장해두었다가 재적재한다.      "
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%202-%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C%EB%9E%80.html": {
    title: "OS 정리-Chap 2-운영체제란",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%202-%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C%EB%9E%80.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true2. 운영체제란?title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.운영 체제(Operating System)란 사용자를 위해 응용 프로그램 동작의 환경와 편의성을 제공하며, 하드웨어와 컴퓨터 자원을 관리하는 시스템 소프트웨어이다.      시스템 소프트웨어이란? : 응용 프로그램의 실행을 지원하고 컴퓨터를 제어하는 프로그램. 장치드라이버, 운영체제가 대표        커널(Kernel) 이란? : 시스템 호출, 인터럽트 및 예외에 응답하는 운영체제의 핵심 구성 요소. 메모리에 최우선 적재되고 종료될때까지 메모리에 머뭄, 자세한 건 운영체제의 구조 참조  운영체제의 목표      편리성 : 얼마나 사용자가 편리하게 사용할 수 있는가?        효율성 : 4가지로 나누어 진다.                  처리량 : 단위 시간당 처리하는 작업량                    응답시간 : 시스템이 작업을 의뢰한 후 반응을 얻을 때까지 걸린 시간                    신뢰도 : 소프트웨어가 실패없이 주어진 기능을 수행할 수 있는 능력                    가용성 : 사용자가 일정 기간 동안 오류나 보수 중이 아닌 정상 상태로 가동 가능한 비율.                  제어 서비스 향상 : 시스템의 확장가능 정도, 입출력 장치의 동작 관리 및 제어  운영체제의 기능자원 관리 기능      메모리 관리 : 아래의 프로세스 관리와 함께 프로그램의 실행에 필요                  메인 메모리 : 프로세스 별 메모리 공간 관리, 할당 및 회수 방법 결정                    보조 기억 장치 관리 : 메모리 접근 요청 스케줄링, 빈 여유공간 관리                  프로세스 관리  :  프로세스와 스레드의 생성과 제거, 스케줄링, 동기화, 교착상태(deadlock) 방지        입출력 장치 관리 : 장치 드라이버를 이용해 입출력 장치 관리 및 인터페이스 제공        파일 관리 : 파일 생성/삭제, 디렉토리 생성/삭제, 파일 맵핑  시스템 관리 기능      시스템 보호(사용자 권한 부여) : 프로그램, 프로세스, 사용자의 컴퓨터 자원의 접근을 권한 관리, 암호화, 계정 생성 등을 통한 보안        네트워킹(통신) : 다른 시스템의 프로세스 간, 혹은 같은 시스템의 프로세스간의 통신 경로 설정, 접속 정책, 충돌, 보안 관리        명령 해석기(command interpreter) : 사용자나 프로그램이 대화형의 명령어를 운영체제에게 전달하는 인터페이스 역할을 함. 커널에 분리 되어 쉽게 변경되게 끔 되어있음, 사용자 인터페이스 (CLI, GUI 등) 제공  기타 기능      부팅(booting) 서비스 혹은 부스트스래핑(bootstrapping) :        ROM에 존재하는 부스트스트랩 로더를 통해 하드 디스크 같은 보조기억 장치에 저장된 운영체제를 메인 메모리에 적재하고, 초기화를 실시한다.    초기화는 시스템 장치 초기화, 시간 설정, 명령 해석기 적재와 준비 등을 한다. 이때, 시스템 장치 초기화는디렉토리, 파일 점검, 시스템 버퍼, 인터럽트 벡터와 운영체제 루틴 적재 등을 의미        오류 탐지 : 하드웨어, 소프트웨어 수준의 오류 탐지 및 시스템 모니터링, 기억장치 메모리 오류, 정전, 프린터 종이 부족, 오버플로, 프로세서 할당 시간 초과 등의 오류가 존재        서버 가상화(virtualization) : 물리적 시스템 하나에 각기 다른 환경의 논리적 시스템을 여러개 구성하는 방법, 하이퍼 바이저를 이용한 가상 머신을 주로 이용한다.              하이퍼 바이저 (hypervisor) : 호스트 컴퓨터에 가상 머신 환경의  다수의 운영체제를 실행하게 해주는 논리적 플랫폼.        베어메탈 기반은 호스트 기반과 달리 가상화 솔루션을 가지고 있는 시스템을 통해 가상화하며, 향상된 성능과 실시간 운영체제를 지원할 수 있지만, 대신 드라이버 등의 설치와 구성이 어렵다.        시스템 호출(System call): 실행 중인 프로그램과 운영체제 간의 인터페이스, API라고도 한다. 시스템 호출을 통해 운영체제의 기능을 사용할 수 있다.        주로 프로그램에서 명령이나 서브루틴 호출로 부르거나, 명령해석기를 통해 대화 형태로 호출한다.운영체제의 일반적인 시스템 호출 서비스는 아래 표와 같다.              호출 서비스      설명                  프로세스 제어      - 종료와 취소 - 프로세스 속성 획득과 지정- 적재(load)와 실행 - 대기와 대기 이벤트, 신호 이벤트- 프로세스 생성과 종료 - 메모리 할당과 해제              파일 조작      - 파일 생성과 삭제 - 파일 읽기와 쓰기, 파일 재배치- 파일 열기와 닫기 - 파일 속성 획득과 지정              장치 조작      - 장치 요구와 해제 - 장치 속성 획득과 설정- 장치 읽기와 쓰기, 재배치 - 논리적 부착이나 장치 제거              정보 관리      - 시간과 날짜의 설정과 획득- 데이터의 설정과 획득- 프로세스, 파일, 장치 속성의 설정과 획득              통신      - 통신 연결의 생성과 제거 - 정보 상태 전달- 메시지의 송수신 - 원격 장치의 부착 및 제거      운영체제의 유형일괄 처리 시스템(batch processing system)작업을 바로 처리하지 않고 작업 별로 그룹으로 묶어 모아두었다가 한꺼번에 처리하는 방법, 가장 원시적이고 초기적인 형태의 유형이다.            장단      항목                  장점      각 작업 별 준비시간을 줄이고, 컴퓨터 자원을 여러 사용자가 공유할 수 있다.              단점      1. 작업 중에 추가 작업 입력이 불가능하다.2. 작업에 우선순위를 주기 어렵다3. 작업 유형이 다양해질 수록 성능이 떨어진다.4. 입출력 장치 속도에 따라 프로세스가 유휴 상태가 될 수 있다.      단점을 해결하기 위해 다음과 같은 방법이 등장했다.      버퍼링(buffering) : 입출력 장치마다 버퍼를 두어 프로세서 연산 시 동시에 다른 작업을 입출력 하게 하는 방법            스풀링(spooling) : 속도가 빠른 디스크를 버퍼로 사용해 입출력을 입출력 장치에서 읽어온다. 버퍼링과 달리 여러 작업의 입출력과 계산을 할 수 있다.  다중 프로그래밍 시스템(multiprogramming system)여러 프로그램을 메모리에 적재 후, 한 프로그램이 입출력 등을 위해 프로세스 사용을 잠시 멈추면 다른 프로그램이 프로세서를 사용하는 방식.            장단      항목                  장점      1. 프로세서 가동률이 증가2. 여러 프로그램이 프로세서를 나눠 쓸 수 있다.              단점      여러 프로그램들의 메모리 적재, 관리, 프로세스 할당 방법(주로 인터럽트로 구현) 등을 구현하기 힘들다.      시분할 시스템(Time Sharing System)다수의 사용자나 프로세스가 동시에 프로세서 같은 컴퓨터의 자원을 공유할 수 있는 기술로, 각 프로그램에 일정한 프로세서 사용 시간(time slice) 또는 규정 시간량(quantum)을 할당하여 주어진 시간 동안만 컴퓨터를 사용하는 방식으로 구현된다.            장단      항목                  장점      1. 프로세스 유휴시간이 감소2. 소프트웨어의 중복 회피가 가능3. 다중 프로그래밍 시스템에 비해 응답 시간이 빠르다.              단점      1. 컴퓨터 자원을 다른 프로그램에 넘기면서 보안과 신뢰성 문제를 방지해야함.2. 데이터의 무결성과 데이터 통신의 문제도 해결해야 한다.      다중 처리 시스템(multiprocessing)단일 컴퓨터 시스템 내에서 둘 이상의 프로세서를 사용해 동시에 둘 이상의 프로세스를 처리 가능, 여러 프로세서가 시스템 버스, 클록, 메모리와 주변 장치를 공유한다.            장단      항목                  장점      1. 프로세서가 문제가 생기면 다른 프로세서가 처리가 가능하므로 신뢰성이 증가한다.2. 개발자의 구현에 따라, 다중 작업 처리를 통해 빠른 처리가 가능하다.              단점      프로세서 간의 연결, 상호작용, 역할 분담, 자원 공유를 염두에 둬야하므로, 구현이 힘들다.      실시간 처리 시스템(Real time Processing system)언제나 온라인 상태를 유지하며, 응답 시간 간격을 극히 줄인 시스템으로, 적시 응답이 필요하거나 데이터 흐름 또는 프로세서 연산에 엄격한 시간 요구가 있을 때 사용한다.경성 실시간 처리 시스템(hard real time processing system) 은 미사일 자동 조준, 철도 자동 제어 등 시간 제약 조건이 치명적인 시스템을 의미하며,연성 실시간 처리 시스템(soft real time processing system) 은 영상 재생 시스템 처럼 시간 제약 조건이 치명적이지 않은 시스템을 의미한다.분산 처리 시스템(Distributed Processing system)시스템마다 독립적인 운영체제와 메모리를 가진 상태에서 서로 통신하여 작업의 연산을 분산 처리하는 시스템이다.            장단      항목                  장점      1. 시스템 간의 자원 공유, 연산 속도 향상, 신뢰성을 늘릴 수 있다. 2. 여러 시스템을 중앙 집중식 하나의 시스템처럼 사용할 수 있다.              단점      시스템 구현 및 유지보수가 어려울 수 있다.      운영체제의 구조운영 체제의 복잡함을 해결하기 위해 다양한 설계 구조가 생기게 되었다.단일 구조(Monolithic structure)가장 초기에 나타난 방법으로, 운영체제의 모든 기능을 커널과 동일한 메모리 공간에 적재 후, 시스템 호출로 기능을 사용하는 구조이다.            장단      항목                  장점      1. 작고 간단한 구조.2. 직접 통신을 통해 시스템 자원을 효율적으로 관리할 수 있다.              단점      1. 버그의 원인이나 기타 오류 구분이 힘들다.2. 새 기능 추가시 수정과 유지보수가 어렵다.3. 같은 메모리에 기능이 밀집되어 있으므로, 문제가 시스템 전체에 영향을 줄 수 있으며, 악성 코드로 피해를 입기 쉽다.      초기 유닉스는 인터페이스를 커널 인터페이스, 하드웨어 인터페이스 두개로 나누는 구조였다.계층 구조(Layer structure)각 계층 별로 비슷한 기능끼리 그룹화하며, 각 계층은 자신의 상위와 하위 계층과 상호작용하는 구조.            장단      항목                  장점      1. 모듈화가 잘되어 있어, 시스템 검증과 오류 수정이 쉽다.2. 설계나 구현이 쉽다.              단점      1. 언제나 모든 데이터가 모든 계층을 지나며 시스템 호출이 발생하므로 성능이 떨어진다.2. 기능이 천차만별이고, 언제나 상위 하위 계층과만 상호작용하므로, 계층의 그룹화를 정의하기 어렵다. 3. 모든 계층이 시스템에 제한 없이 접근 가능하여 오류나 악성 코드에 취약할 수 있다.      다익스트라가 개발한 THE 운영체제에서 처음 사용한 구조이다.마이크로 커널 구조(Micro-kernel structure)커널에 최소한의 기능만 남기고, 기타 기능은 사용자 영역에서 수행하는 방법. 복잡하고 다재다능한 운영체제들에 적합하며, 모듈화 구성요소 개념은 많은 운영체제들이 채택하고 있다.주로 하드웨어 초기화, 메모리 관리, 프로세스 관리, 프로세스 간 통신 및 협력 기능은 커널이 담당하며, 나머지 기능인 네트워크 시스템, 파일 시스템 상호작용, 장치 관리 등은 커널 외부에서 담당한다.            장단      항목                         장점      1. 커널의 크기와 복잡도가 대폭 줄어든다.2. 사용자 영역의 오류가 커널에 영향을 끼치지 않는다.3. 모듈화 정도가 높아, 이식성, 규모 확장성이 높다.4. 서버 개발이 용이하고, 운영체제 기능을 쉽게 변경 가능5. 발생 지연이 적고 예측 가능하다.                     단점      모듈 간의 통신이 빈번해 성능이 떨어질 수 있다.             프로세스 간 통신 발생을 최소화하는 것이 최우선 과제이다."
  }
  , 
  
  "/articles/etc/etcs/%EA%B0%9C%EB%B0%9C%EC%9E%90%20%EA%B8%80%EC%93%B0%EA%B8%B0%20%EC%A0%95%EB%A6%AC.html": {
    title: "개발자 글쓰기 정리",
    date: " Aug 10, 2022 ",
    url: "/articles/etc/etcs/%EA%B0%9C%EB%B0%9C%EC%9E%90%20%EA%B8%80%EC%93%B0%EA%B8%B0%20%EC%A0%95%EB%A6%AC.html",
    tags: [],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true개발자 글쓰기 정리title: 원문 책개발자의 글쓰기 (김철수 위키북스)를 읽고 정리한 내용입니다.개발자의 글이란?개발자의 글은 주석, 에러 메시지, 릴리스 문서, 개발 가이드, SI 제안서 등이 존재개발자 글에서 중요한 원칙 세가지  정확성  간결성  가독성단, 이 셋은 서로 대치하는 경향이 있다.  정확하게 글을 쓰면 간결하지 않고 가독성이 나쁨  간결하게 쓰면 정확하지 않음∴ 상황에 맞게 이 관계를 잘 조율하자.문장 구조화문장을 구조화하는 법title: 예시 문장색상 RGB 값을 각각 사용하기 때문에 입력 데이터는 3차원 벡터이다.title: 구조화 과정1.중요 주어 앞으로입력 데이터는 색상 RGB 값을 각각 사용하기 때문에 3차원 벡터이다.2.인과 관계에 따라 문장 나누기입력 데이터는 색상 RGB 값을 각각 사용한다. 그래서 입력데이터는 3차원 벡터이다.3.중요한 문장 앞으로입력 데이터는 3차원 벡터이다. 입력 데이터는 색상 RGB 값을 각각 사용하기 때문이다.4.중복되는 주어 삭제입력 데이터는 3차원 벡터이다. 색상 RGB 값을 각각 사용하기 때문이다.서술식, 개조식, 도식 활용법  서술식 : 이 문장처럼 완전한 문장형으로 ‘~다’로 끝나는 문장이다.  주로 설명과 논증, 개발 가이드 등에 사용한다.  줄거리나 인과 관계가 뚜렷한 경우 서술식이 유리하다title: 서술식 예시4가지 푸시 알림으로, 공지 알림과 메시지 알림, 친구 등록 알림이 존재하는데, 공지 알림은 서비스 변경이나 장애, 이벤트 등 메신저 운영사가 직접 보내며, 오전 9시부터 6시 사이에만 발송합니다. 메시지 알림은 등록된 친구가 메시지를 보냈을 때 자동으로 시스템이 전송하며, 친구 등록 알림은 새로운 친구가 등록되었을 때 알려주며, 친구 해제 알림은 친구가 탈퇴 시 알려줍니다.  개조식 : 명사나 명사형으로 끝나는 문장.  주로 릴리스 문서, 장애 보고서 등에 사용  여러 가지 종류의 항목과 내용이 반복되면 개조식 사용title: 개조식 예시      공지 알림: 서비스 변경이나 장애, 이벤트※ 메신저 운영사가 오전 9시부터 오후 6시 사이에 직접 발송        메시지 알림: 등록된 친구가 메시지를 보냈을 때 시스템이 자동으로 전송        친구 등록 알림: 새로운 친구가 등록되었을 때        친구 해제 알림: 친구가 탈퇴했을 때    도식 : 항목의 관계가 명확히 규정되고 중복, 누락이 생긴다면 도식 사용            종류      설명                  도식      표나 그림, 상태도를 이용해 표현하는 것              서술식      이 문장처럼 완전한 문장형으로 ‘~다’로 끝나는 문장이다              개조식      명사나 명사형으로 끝나는 문장.      title: 도식 예시            알림 종류      상황      발송 방식      발송 시간                  공지      서비스 변경, 장애, 이벤트      수동(운영사)      9~18시              메시지      등록된 친구 메시지 발송      자동(시스템)      제한 없음              친구 등록      새로운 친구 등록      자동(시스템)      제한 없음              친구 해제      친구 탈퇴      자동(시스템)      제한 없음      개조식 서술 방식에 따른 글머리 기호기호를 이용해 문장의 내용, 문장 간의 관계, 순서 등을 한눈에 파악할 수 있어 중요하다.  설명: 내용을 구체적으로 설명하고, 하위 요소로 나눌 때 이용, 하위 요소로 갈수록 크가 작아지고 들여쓰기가 되어야 한다.          ex) ■, □, ○, ●, -, *, ※ 등        묘사: 내용을 그림으로 표현 시, 어떤 요소나 영역을 표시하기 위한 문자          ex) ⓐ, ⓑ, ⓒ, ①, ②, ③ 등        논증: 내용이 논리 관계(귀납, 연역, 인과, 유추, 비교, 단계 등)로 구성 시 사용          ex) ∴, ∵, &gt;,&lt;, =, 👉 등        서사: 순서나 단계를 나타낼 시, 사용,          ex) 1, 2, 3, 가, 나, 다 등      단락을 구조화하는 위계문서에는 문단 사이의 위계가 존재한다. 위계는 위치와 계층을 합친 말이다.이를 통해 가독성이 늘어나고 이해하기 쉬워진다.  위치: 내용의 포함 범위와 자세한 정도에 따라 결정됨  주로 들여쓰기와 기호를 이용해 표현한다.  계층: 내용의 중요도나 핵심 주제에 따라 결정됨  주로 글씨 크기, 굵기와 모양, 밑줄 등으로 표현한다.위 문장은 위계를 이용하여 표현되었다.중요한 글씨는 굵게  표시하여 계층을 표현하였고, 들여쓰기와 기호를 통해 위치를 표현하였다.따옴표, 영문 대문자와 표기법큰따옴표와 작은따옴표|             | 한글 따옴표 규정                     | 비즈니스 문서                                      | 컴퓨터 공학                       || ———– | ———————————— | ————————————————– | ——————————— || 큰 따옴표   | 화자간 직접 대화, 말이나 글 인용     | 책, 신문명, 대제목                                | 문자열 표현                       || 작은 따옴표 | 인용구 안의 인용구, 마음속으로 한 말 | 소제목, 예술 작품명, 법률, 규정, 내용 강조 및 비교 | 문자 표현, SQL 쿼리, 자바스크립트 |  SQL 쿼리에서는 다른 프로그래밍 언어에서 SQL 쿼리문을 큰따옴표로 인용하는 경우가 많아 작은 따옴표를 많이 사용한다.  자바스크립트는 HTML이 주로 큰 따옴표를 사용하므로 충돌을 막기 위해 작은 따옴표를 많이 사용하지만, 따옴표 처리를 위해 번갈아가며 사용하기도 한다.대문자를 사용하는 곳      고유 명사 첫 글자 : ex) I went to Tokyo        이름 앞에 오는 직함 : ex) Doctor Mr.Micheal        책, 신문, 잡지, 음악, 영화 이름의 첫 단어와 마지막 단어의 첫 글자, 관사  ex)  Marvel’s The Avengers        요일명, 휴일명, 역사적 사건, 월, 역사적 기간의 첫 글자 : ex) World War 2        천체 이름 : ex) Mars, Jupiter  공통적으로 중요하거나 특정한 것의 첫 글자, 명사나 관사는 소문자이다.표기법PascalCase: 중요하고 고유명사인 경우가 많으므로 주로 클래스명, 인터페이스명에 사용\tex) interface Menu, class CoffeeMenu implements MenucamelCase: 주로 동사로 시작하는 경우가 많기 때문에 변수명, 메소드명에 사용 \tex) int totalCount = 0;, void orderCoffee()snake_case: 일부 프로그래밍 언어에서 메소드명, 변수명에 사용\tex) def add_coffee():, int count_coffeeSCREAMING_SNAKE_CASE: 강조 및 주의를 위해 변하지 않는 값, 상수에 사용됨\tex) static final int COFFEE_MAX = 10;kebab-case: 빼기 기호(-)로 해석되는 경우가 많아 사용 안함nocase: 변수, 클래스 등과 구분하기 위해 주로 패키지나 모듈명에 사용\tex) android.wikibook.developerwriting, import developerwritingBEM(Block, Element, Modifier) case: CSS에서 사용, 대상의 요소와 상태를 구분하여 표시할 때 사용한다.\t예시들\t-  .form__button {} : button이 form 내부에 요소로써 포함되므로 __로 이어줌\t- .form__button--disabled {} : disabled는 button이 가지고 있는 특성 혹은 속성이므로 --로 이어준다.이름 잘 짓기      의미를 알 수 있고 가독성이 높은 이름  ex) int a, float b(X), int userCount, float userAvgRating(O)        앞선 표기법에서 설명한 네이밍 컨벤션 준수    너무 길거나 짧지 않게 보통 16글자 이내, 3단어를 조합          클래스명: 3.18 단어      함수명: 3.36 단어      변수명: 2.57 단어            명사, 동사, 형용사의 조합        복수형 s 대신 Array나 ListOf를 사용  ex) checkUserNames(X), checkUserNameArray(O)        약어는 조심히 접근, 통용되는 약어만 사용하자  ex) AWS, OOP, i, j, k (O), Total Number Of Visitor -&gt; TNOV (X)    중요한 단어를 앞에 두기  ex) int totalVisitor(X), int visitorTotal(O)          검색에도 편하고, 가독성에도 좋음        범주화 이용   ex) TIMEOUT, NO_RESULT(X), ERROR_TIMEOUT, ERROR_NO_RESULT(O)          검색에 편하고, 가독성에 좋음 2        속성이 아닌 개념과 역할을 중점으로 이름 짓기  ex) strong_text, h1, blue_text(X), subtitle, title, emphasis(O)          속성으로 이름을 지으면 속성이 바뀌면 전부 다시 바꿔줘야 함.        오타가 나기 쉬운 변수명 자제  ex) success, lambda, Referrer, getTTTinTTTime, continuous주석 달기title: 더욱 자세한 내용은 주석 참조주석은 프로그래밍 언어마다 다양하게 #, //, /**/, ----, {%%} 등을 이용해 주석을 단다.title: 각 프로그래밍 언어는 보통 여러 줄을 한꺼번에 주석 처리하기 위한 방법을 지원한다.주석의 종류  헤더 주석(Header Comment)          각 파일 상단에 존재하는 주석      작성자, 작성 시간, 해당 코드들의 동작들을 주로 설명한다.      title: 파이썬 헤더 주석 예시  source here#!/usr/bin/env python3 #인터프리터에게 실행 파일임을 명시하고 python3 버전임을 명시# -*- coding: utf-8 -*- {: #coding-utf-8}# 파일의 인코딩을 식별#----------------------------------------------------------------------------# Created By  : name_of_the_creator {: #created-by-name-of-the-creator}# Created Date: date/month/time ..etc{: #created-date-date-month-time-etc}# version ='1.0' {: #version-1-0}# ---------------------------------------------------------------------------{: #}\"\"\" Details about the module and for what purpose it was built for\"\"\" # ---------------------------------------------------------------------------{: #}# Imports {: #imports}# 임포트 모듈들에 대한 설명# ---------------------------------------------------------------------------{: #}from ... import ...  title: C 언어 헤더 주석 예시/*** File: compute_blackjack_odds.C** Author: junseok Yun (m********@naver.com)* Date: 2022 08 09* * Description:*     This file contains code which simulates a blackjack game.         ** Note:*     All Inputs should be Integer.*     DO NOT REMOVE COMMENTS IN HERE.*     */  함수나 클래스 내부 주석          함수 헤더, 혹은 클래스 헤더라고 불리우며, 해당 모듈의 목적을 설명함.      함수나 클래스 상단에 적기도 하나      최신 프로그래밍 언어와 IDE들은 내부의 주석을 인식하여 힌트를 주기도 하므로, 내부에 적는 게 좋다.      title: 파이썬 함수 예시def test(a: str, b: int):\t\"\"\"\ttest function test comment.\t\tArgs:\t\t\ta(str): a value\t\t\tb(int): b value\t\tReturns:\t\t\tint: result\t\"\"\"\treturn a + b  줄 주석(In-Line Comment)          이해하기 힘든 코드 오른쪽이나 위쪽에 설명을 위해 첨가      주석의 내용  코드 메타 데이터          주로 헤더 주석 혹은 내부 주석으로 작성자, 작성일, 파일명, 코드 변경 이력 등을 적음      IDE와 협업 소프트웨어의 발전으로 굳이 적을 필요 없어짐        너무 복잡한 알고리즘          복잡한 알고리즘을 설명하기 위해 각 단계를 코드로 설명      title: 조합 생성 파이썬 코드 중 일부def genCombination(listOfNumbers, indexOfCombination):..    if indexOfCombination == 0: # 마지막 원소의 자리를 바꾸면        result.append(list(combination))  # 완성된 조합을 추가..  코드 원리나 필요성// 다익스트라 알고리즘을 이용한 알고리즘// 응답속도를 낮추기 위해 필요함  새로운 발견// 해당 코드의 순서를 바꾸면 느려짐// 나중에 원인 확인 바람  예상 질문과 답// 이 변수가 필요한 이유는 개발자 가이드 3장 2절 참조 바람  계획된 일, 주의, 개선 아이디어// - [ ] TODO: 함수 리펙토링 // 이 코드는 서버측에서 처리하는 편이 빠르고 사용자 경험에 좋을 것 같다.// WARN: 해당 변수를 절대 직접 참조로 구현하지 말것  다른 사람에게 도움 요청```// 사진 변경 애니메이션 인자의 경우 결정을 위해 UI/UX 담당자와 상의가 필요함- **개발자의 속마음 표현**// 이 코드 어떻게 돌아가는지 모르겠다. 일단 된다.```주석 팁  이름을 잘 짓거나 리팩토링을 잘하면 주석을 상당히 줄일 수 있다.title: 구린 명명, 리팩토링에 주석 사용// 스크린 최대 높이를 480으로 고정int h = 480;// 바뀐 화면에 알맞게 로그인창 위치 변경object loginElement = document.getElementById(\"login\");loginElement.x = getNewLoginPositionX(h);loginElement.y = getNewLoginPositionY(h);title: 좋은 명명, 리팩토링에 주석 삭제int screenHeightMax = 480;refreshLoginPositon();  영어를 잘못하거나 심각한 오해, 오류의 소지가 있다면 주석 반드시 사용title: 유저명의 최소 글자수 제한 체크 함수checkUserNameUnder3Characters() // 3 글자 '이하' 인지 확인  주석의 반복할 때와 생략할 때          주석의 반복이 필요 없을 때 : 같은 혹은 비슷한 기능에 대한 주석이 가까운 곳에 존재      주석의 반복이 필요할 때 : 같은 혹은 비슷한 기능이어도, 해당 주석이 먼 곳에 존재                  많은 개발자들은 필요한 부분만 읽기 때문                      주석의 발췌와 요약 : 주석은 짧을 수록 좋으므로, 비슷한 기능, 주석의 반복 시 인용이나 요약을 이용할 수 있다.// 카카오 로그인 SDK 사용compile group: 'com.kakao.sdk', name: 'usermgmt', version: project.KAKAO_SDK_VERSION// push SDK 사용compile group: 'com.kakao.sdk', name: 'push', version: project.KAKAO_SDK_VERSION// 그외 서비스 SDK 사용  주석 또한 리뷰와 디버깅의 대상으로 생각하자  즉, 잘못 쓰거나 나쁜 주석이 있는지 언제나 살펴보자에러 메시지 체크 리스트  에러 메시지에 포함되어야 할 것들          에러 결과 ex) 사용자 가입을 진행할 수 없습니다.      에러 원인 ex) 자기소개란 내 비속어 사용 불가      에러 해결 방법 ex) 비속어를 제외해 주세요.      에러 이후 행동 선택 ex) 자기소개란 비우기 혹은 수정을 위해 자기소개란 유지하기        에러 내용에 따라 일부 생략하거나 제시 순서를 바꿀 수 있다. 예를 들어 에러 원인인 “비속어 발견” 대신 에러 해결 방법인 “비속어를 제외해 주세요”만 적어도 사용자는 이해할 것이며, 가독성이 늘어날 것이다.    개발자용 에러 메시지와 사용자용 에러 메시지를 분리          사용자 경험, 보안 상 치명적인 결과를 불러옴            확인 버튼과 취소 버튼의 순서는 통일할 것  확인 버튼과 취소 버튼의 순서는 회사나 정책마다 다르다.  하지만 순서가 일정하지 않으면 사용자는 실수로 누를 확률이 높다.  회사의 이익을 위해 원하는 버튼의 크기나 색감을 눈에 띄게 바꿀 수 있다.        사용자 반복 에러를 막기 위해 사용 제한 횟수나 에러 횟수를 보여주기  이를 통해 사용자는 더욱 신중하고 주의 깊게 행동을 하게 됨  ex) 로그인 암호 통과 실패 메시지에서 로그인 기회 횟수 보여주기    에러가 나지 않도록 예방하는 방법을 생각해보자.  ex) 호텔 예약 달력 선택 기능에서 내일 이후 날짜만 고를 수 있게 하기릴리스 문서 체크 리스트릴리스 문서 순서  릴리스 문서에 적을 변경 로그 선정            회사 선호      사용자 선호      개발자 선호      순위      수준      예시                  O      O      O      1      자세히      새롭고 신기하고 경쟁성 있는 새 기능(ex) 인공지능 추천)              O      O      X      2      간단히      자원 추가 등의 쉬운 작업의 영향 (서버 증설로 서비스 쾌적)              O      X      O      3      간단히      어려운 신기술을 이용한 기능 변경              O      X      X      4      맨 아래 주석      면책 조항, 정책 변경              X      O      O      5      자리가 남으면 추가      릴리즈 계획, 작업 과정              X      O      X      6      통합하여 언급      자잘한 오류 수정              X      X      O      삭제      삭제      전문 기술과 노력이 필요했던 피해가 없던 오류        비슷한 변경 로그끼리 분류title: 변경 로그 예시  게임 준비          미리 보기에서 간혹 리부팅 되는 문제 해결      빈 게임방을 자동으로 검색하는 기능 추가      닉네임 만들 시 특수 문자 기능 추가        게임 중          고해상도 폰에서 아이콘 찌그러지는 오류 수정      가로/세로 화면 전환 시 하단 메뉴가 사라지는 오류 수정      애니메이션 스티커가 멈추는 오류 수정        게임 종료          최근 기록이 상위에 올라오도록 개선      게임 종료 후 바로 순위 볼 수 있도록 개선      위는 사용자 기준으로 분류한 것이며, 개발 관점에서 비슷한 관점으로 묶을 수도 있다.ex) 기능 추가, 기능 개선, 오류 수정  요약: 개조식 문장, 형용사, 조사, 어미 정확한 단어로 변경title: 요약  게임 준비          미리 보기 리부팅 문제 해결      빈 게임방 자동 검색 기능 추가      닉네임 만들 때 특수 문자 입력 기능 추가        게임 중          고해상도 안드로이드/IOS 환경에서 아이콘 뭉게짐 오류 수정      가로/세로 화면 전환 시 하단 메뉴 누락 수정      애니메이션 스티커 오류 수정        게임 종료          최근 기록이 상위에 올라오도록 수정      게임 종료 후 바로 순위를 볼 수 있도록 개선      용량이 큰 사진 등록 시 휴대전화 메모리 사용 감소        종합: 여러 로그들의 특성과 결과를 하나로 뭉침, 분석의 반대title: 종합사용자 편리성 개선  게임에 더 빨리 입장  게임 결과 바로 확인 및 개선[세부 내용]  게임 준비          미리 보기 리부팅 문제 해결      빈 게임방 자동 검색 기능 추가      닉네임 만들 때 특수 문자 입력 기능 추가        게임 중          고해상도 안드로이드/IOS 환경에서 아이콘 뭉게짐 오류 수정      가로/세로 화면 전환 시 하단 메뉴 누락 수정      애니메이션 스티커 오류 수정        게임 종료          최근 기록이 상위에 올라오도록 수정      게임 종료 후 바로 순위를 볼 수 있도록 개선      용량이 큰 사진 등록 시 휴대전화 메모리 사용 감소      릴리스 문서 팁      고객에게 유용한 정보 적기        너무 어려운 기술적 내용 적지 않기        추가로 미래 개발 중인 기능이나 주시하고 있는 오류 등을 적어주기        자세한 내용을 적고 싶을 때: 문제, 문제 원인, 해결책, 후속 계획 순으로 적기  ex) 서비스에 사용자가 급증하면 서버가 정지하는 문제는 시스템 재설정으로 해결했습니다. 추후 프로그램 최적화와 DB 재설계도 검토하겠습니다.        유의적 버전(Semantic Versioning) 지키기        법적인 문제 고려하기(면책 조항)  ex) 이 릴리스 노트의 내용은 예고 없이 변경될 수 있으며, 어떠한 내용도 추가 보증할 수 없고, 내용의 결과에 대해 책임을 지지 않습니다.  장애 보고서 체크 리스트  신속하고 빠르게 정리된 보고서          상부에서 빠르게 보고서를 보고 재빨리 대처해야 한다.      장애 내용, 장애 영향, 장애 원인, 조치 상황, 조치 결과, 핵심 원인, 향후 대책 등이 포함되어야 함        원인과 이유가 존재하는 분석적 보고서          단순한 1차적 원인이 아닌 근본적 원인을 보고해야 한다.  ex) 로그인 기능이 작동하지 않은 이유는 프런트 개발자가 13 글자 이상 입력하게 허용해서 (X), 로그인 기능이 작동하지 않은 코드를 내버려둔 이유는 개발자 간의 커뮤니케이션이 문제여서(O)        비 개발자를 고려한 비즈니스 관점 보고서          보고 받을 경영진은 손실로 따지므로 비즈니스 적인 요소를 넣어야 한다.                  장애로 인한 기대 매출 손실에서 지연 매출을 뺀 매출 손실          비슷한 다른 사고 사례          고객들의 이미지 변화                      정치적, 방어적, 공격적 보고서          원하는 것을 얻고, 책임을 피하고, 반복되는 장애를 피하기 위함  ex) 유지 보수 팀의 관리 부주의가 남아있는 한 재발 가능성 높음, 서버 보안팀 구성 안하고 개인정보 유출 시 대표이사 100% 구속      서비스 메뉴얼, 개발 가이드 체크 리스트범주, 용도, 특징  범주, 용도, 특징이 필수적으로 들어가야 하며, 제일 먼저 설명되어야 함.          범주: 서비스를 한마디로 정의하는 문장(누가 무엇을 어떻게)  ex) Amazon S3는 아마존에서 운영하는 인터넷 스토리지 서비스입니다.      용도: 서비스의 용도  ex) Amazon S3를 이용하면 인터넷을 통해 언제 어디서든 원하는 양의 데이터를 저장하고 검색 가능      특징: 경쟁적이고 차별화된 부분  ex) AWS Management Console로 간단하고 직관적인 웹 인터페이스를 통해 여러 작업 수행 가능        범주              범주 구성 시 남들이 이해할 수 있는 보편적인 단어에 추가로 강력한 마케팅을 위한 수식어를 추가          Amazon S3를 ‘웹 하드’ 같은 익숙한 단어를 이용하거나 ‘클라우드 스토리지’ 같은 선도적인 단어를 사용할 수 있다.      경쟁사 보다 발전된 기술 (“클라우드 스토리지 2.0”), 특정 사용자 겨냥(“개발자를 위한”), 회사 자랑(“아마존에서 운영하는”) 등의 수식어를 붙여도 된다.        용도              용도는 범주의 핵심 기능, 즉 사용자 관점 용도로 기술  용도가 많을 경우 길게 늘어뜨린 뒤, 공통점과 특성을 묶어 요약해보자.  ex) Amazon S3를 사용하면 인터넷을 통해 언제 어디서든 원하는 양의 데이터를 저장 및 검색 가능특징  특징은 보편적이지 않고 차별화된 강점과 강력한 장점을 기술          장점은 자기 기준에서 강력한 것 (ex) Amazon S3의 안정적인 서비스)      강점은 경쟁사와 비교해 특별한 것 (ex) Amazon S3의 간단하고 직관적인 웹 인터페이스)  ex) 아마존 S3는 안정적인 서비스와 간단하고 직관적인 웹 인터페이스를 제공합니다.        기타 팁        표현과 논술 팁              주관적, 정성적 표현을 객관적, 정략적으로 표현하자.          디자이너, QA, 고객과의 협의가 쉬워지고 변경할 것이 줄어든다.  ex) ‘눈에 잘 보이는 크기의 창’ -&gt; ‘최대 높이 765px,  너비 600px 이하의 반응형 크기의 창’        의견에 대한 근거를 제시하자.title: ❌ 잘못된 예- 옵션 값이 높을수록 품질이 좋아지지만 인코딩에 더 많은 시간이 소요됩니다.- default 값은 10이며, 30 가량의 값을 사용해도 괜찮습니다.품질과 인코딩 시간의 관계를 잘 모르겠고, 왜 30까지 사용해도 되는지 이유가 없음title: ☑️ 옳은 예- default 값은 10이며, 30까지는 인코딩 시간이 10~20% 정도 소폭 증가합니다.- 인코딩 시간은 35 일때는 100% 증가, 40 일때는 200% 증가합니다.객관적인 근거가 잘나와있어 개발자가 판단하기 쉽다.  너무 거칠게 혹은 공손하게 쓰지 않기, 확실하게 쓰기          가독성을 해치고, 의미를 모호하게 만든다.  ex) ~할 것을 추천합니다. ~를 사용하지 않으면 어려울 것입니다.(X)  ex) ~를 사용하십시오.(O)        근거가 나온 뒤에는 바로 이유가 나오기  앞선 문장 구조화에서 나온 내용이다.  문제를 설명하면 곧바로 답을 보여주기  가독성과 이해하기가 쉬워지며, 개발자와 사용자는 문제를 해결하기 위해 메뉴얼, 가이드를 보므로 바로 답을 보여주는 편이 좋다.  사용자 메뉴얼은 쉽게, 개발자 가이드는 어렵지만 간결하게 써야 된다.  개발자는 어차피 용어를 알아들을 것이며, 빠른 접근이 중요하다.서사와 그림 활용 관련 팁  체계적인 그림와 구성도를 첨부하면 이해가 쉽다          그림과 구성도의 원리, 용어 등을 글과 일치 시키자        설치 메뉴얼, 개발 환경 설정 같이 그림과 서사가 위주인 내용은 글을 많이 요약해서 표현하자.  title: ❌ 틀린 예[다른 폴더 보관함 열기]  좌측 아래의 ‘다른 보관함 열기’ 버튼을 클릭합니다.  ‘열기’ 버튼을 클릭합니다.  원하는 폴더를 클릭하여 엽니다.title: ☑️ 맞은 예[다른 폴더 보관함 열기]다른 보관함을 열려면 ① 다른 보관함 열기 &gt; ② 열기 버튼을 눌러 원하는 폴더를 엽니다.  순서가 있는 서사는 너무 종류가 많을 경우 종합하여 나누자.  릴리스 문서 4번 ‘종합’ 때처럼 나누면 된다.SI 제안서 체크 리스트솔루션 제안서는 대부분 그림과 표, 시스템 구성도 등으로 이루어져 있다.내용 관련 팁  SI 제안서에서 포함되는 내용들          기술 부문 : 시스템 기능, 사양, 구성 장치 내용, 규격도…      전략적 제안들 : 시스템 구축 목적, 시스템 구축 전략 , 소프트웨어 개발 방안, 테스트 방안, 향후 시스템 발전 방안, 기대 효과, 개발의 특징, 개발 방안의 장점…            시스템 구성도 작성법 제안 요청서에 존재하는 시스템 구성도를 분석해 새롭고 알맞은 시스템 구성도 구축 성능, 안정성, 확장성 등을 고려하되, 이를 표현한 글은 자세하고 정량적으로 표현 ex) 10만명 이상의 동시 사용자 처리가 가능하며 한 시간 이내에 100만명까지 추가 확장이 가능하며, 백업을 위한 핫 사이트가 구비되어 있습니다.        제안 요청서 분석  제안 요청서는 고객이 제안을 요청하는 문서로, 고객의 요구와 배경, 목적, 목표 시스템 등이 적혀 있다.    논리적 완결성  내용과 순서가 뒤죽박죽이고, 서로 떨어져 있다면 독자 입장에서 페이지를 옮기면서 읽어야 하며, 가독성과 이해성이 떨어지며, 심사위원은 곤혹스러워 할 것이다.고객 및 경쟁사 관련 팁  고객이 원하는 문제에 대해 기술적으로 우위일 시, 경쟁사와 비교하여 제안  ex) 네비게이션 앱 솔루션에서 인공지능 추천을 구현 가능할 시 비교                   자사      A사      B사                  운전 이력 기록      O      O      O              운전 이력 분석      O      O      X              운전 시간 분석      O      X      X              운전 취향 분석      O      X      X              맞춤형 추천      O      X      X      단, 너무 혁신만 강조하면 실험적이라고 느낄 수 있으므로, 안정성을 기본으로 강조하자.      고객이 원하는 문제에 대해 기술적으로 힘들면 동감하고 다른 방안을 제시  ex) 인공지능 경로 추천 기능은 우리 A사는 다음과 같은 이유로 추천 드리지 않으며, 설문 방식을 추천 드립니다.      - 충분한 학습 시간이 필요하다. 설문 방식은 5분이면 가능      - 인공지능이 추천한 경로가 마음에 들지 않아도 학습으로 인해 변경 불가      - 돌발적이고 이상한 추천 상황이 생길 수 있음        고객이 사소하다고 느끼는 문제에 대해 기술적 우위 시, 문제를 중대하게 보도록 설득    ex) 인공지능 경로 추천 기능은 최근 선지사를 중심으로 도입되고 있는 추세이며, 차별화 기능을 통해 미래를 대비해야 합니다. (추가로, 네비게이션 회사의 인공지능 관련 기사나 도표를 첨부한다)        고객이 사소하다고 느끼고, 기술적으로 힘들면 경쟁사 전략에 따라 대응  ex) 경쟁사가 고객에게 문제를 중대하게 보도록 설득한다면, 심사위원에게 해당 문제에 대한 상담이나 질문이 들어올 수 있으므로 미리 준비  요구 사항 관련 팁      요구 사항을 단순히 분석하지 말고 제시  고객은 자기가 원하는 제품에 대해 정확히 모른다.   따라서 모순되거나, 비현실적이거나, 사용자 경험에 나쁘거나, 심각한 결함이 있는 요구 사항을 요구할 수 있다.  이에 대한 대안을 제시하여 설득할 수 있어야 한다.        변화하는 요구 사항에 대비  고객의 요구 사항은 자주 바뀌므로, 미팅과 테스팅 단계를 자주 잡도록 하자.        상품 기획 이론 카노 모델에 의한 고객의 만족을 높이는 요구 사항 제시          고객이 당연하다고 예상하고 기대하는 기본 기능  ex) 로그인, 회원가입, 게시글 쓰기                  요구 불 충족 시 고객 만족도 대폭 저하 ▼, 요구 충족 시 고객 만족도 소폭 증가 △.                    고객이 잘 인지하지 않으나 보통 존재하는 기능  ex) 로딩 시간 감소, 보안 관련 설정                  요구 충족 시 고객 만족도 보통 상승 ↑, 요구 불충족 시 고객 만족도 보통 감소↓                    고객이 기대하지 않았으나 특별한 기능  ex) 스마트폰 지문, 홍채 인식 로그인                  요구 충족 시 고객 만족도 대폭 상승 ▲, 요구 불충족 시 고객 만족도 소폭 감소 ▽                    기술 블로그기술 저널 글쓰기 팁  주제 의식 버리고 소재 의식 글쓰기          주제 의식 : 추상적 가치에 대한 생각(권선징악, 자존감, 사랑, 자본주의)      소재 의식 : 특정한 대상이나 상황에 대한 자기만의 관점이나 해결 방안  좀 더 담담하고 상황에 대한 해결 방안이나 설명에 대해 쓰자            자기 수준 글쓰기  모든 독자를 위해 글을 쓰지 말자. 기술 저널은 보통 인기를 끌기 위해 쓰는 것이 아니다.  독자의 대부분인 개발자들은 어려운 용어를 잘 이해할 수 있을 것이며, 굳이 설명하려면 위키 문서 등을 링크해두자.        재미있게 글쓰기  글의 기교는 글을 아름답게 만들고 쉽게 읽힌다.  재미있는 글은 인기와 반응이 좋아지고, 그에 따라 당신의 의욕도 늘린다        협업 해서 글쓰기, 페어 글쓰기 해보기        회사의 기술 블로그는 사내 문화를 반영하고, 노하우를 체계적으로 축적할 수 있다.    머릿말과 맺음말은 맨 마지막에 쓰기  주로 서비스 설명, 개발의 필요성, 느낀 점 등을 적음기술 블로그 글의 종류저(Essay)직접 경험하고 실험한 과정이나 결과ex) 개발기, 도입기, 적용기ex) Dagger 적용기, 분산 웹 캐시 개선 과정, TensorFlow를 활용한 네이버 쇼핑의 상품 카테고리 자동 분류      목차를 잘 잡으면 쓰기 쉽지만 목차를 잡기 어렵다   ∵ 개발은 다차원의 병렬적으로 진행되지만 글은 1차원 단방향으로 써야 하기 때문에        따라서 성공한 최종 루트를 기준으로 다른 루트를 조금씩 언급하며 문제 해결이나 팁으로 정리하며 쓰면 된다.  술(Dictionary)어떤 것을 분석하여 의미를 풀이하고 해석한 것ex) 기술 소개, 용어 분석, 에러 해결 방법ex) GET과 POST의 차이, MySQL 1175 에러 해결 방안, MySQL Ascending index vs Descending index  원전의 내용을 쉽게 풀어쓰고 비교하는 것으로도 쓸 수 있다.  추가적으로 직접 실험한 결과나 적용 결과를 보여주어도 좋다.편(Tutorial)산만하고 복잡한 자료를 편집해 질서를 부여한 것ex) 프로그램 설치/설정 방법, 튜토리얼, 세미나 후기, 책 리뷰ex) ES2015 단위 테스트 환경 구축하기, Ubuntu의 apt-get을 이용한 패키지 설치, 예제 코드로 알아보는 React  시간 순서로 하나씩 나열해 내용을 쓴 후, 단계로 묶어서 요약하면 된다.  추가로 태그와 HTML, 구조화된 글쓰기로 가독성을 늘릴 수 있다.집(Collection)여러 사람의 견해나 흩어진 자료를 한데 모아 정리한 것ex) 명령어 모음, 팁, 00가지 규칙ex) 자바스크립트 정규표현식 코딩 팁, 프로그래머를 위한 12가지 규칙, 기술 인터뷰 질문 모음  타인이나 본인의 견해와 자료를 모아 일정 순서와 단계로 엮으면 된다."
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%203-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80%20%EC%8A%A4%EB%A0%88%EB%93%9C.html": {
    title: "OS 정리-Chap 3-프로세스와 스레드",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%203-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80%20%EC%8A%A4%EB%A0%88%EB%93%9C.html",
    tags: ["OS","CS","요약"],
    content: "3. 프로세스와 스레드title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.프로세스의 개념과 상태 변화프로세스(process)란?프로세스(process) 또는 작업(task)는 다음과 같이 여러 의미로 정의 된다.      주소 공간을 가지고 실행 중인 프로그램        비동기적 행위        실행중인 프로시저 또는 이의 제어 추적        운영체제에 들어 있는 프로세스 제어 블록(PCB)        프로세서에 할당하여 실행할 수 있는 개체 디스패치가 가능한 대상  프로그램은 컴파일한 코드, 초기화 전역변수, 문자열, 상수 등의 정적 데이터를 포함한 정적인 개체이다.프로그램이 동적인 개체인 프로세스가 되려면, 자신만의 메모리 영역을 할당받아야 하며, 프로그램 인스턴스 갯수에 따라 프로세스의 갯수도 달라진다. 즉, 같은 프로그램이라도 2개를 실행하면 각기 다른 프로세스이다.프로세스는 프로세서 점유 시간, 메모리, 파일 ,입출력 장치 같은 자원을 스케줄러로부터 할당받아야 하며, 자신의 메모리 영역에는 나중에 배울 프로세스 제어 블록 (PCB)에 자신의 정보를 저장한다.스케줄러는 추가로, 프로세스의 교착 상태, 보호, 동기화 지원 및 실행 순서를 결정하는 역할도 한다.프로세스를 통해서 병행 실행 프로그램들의 자원 공유를 효율적으로 구현할 수 있게 되었다.프로세스 메모리 구조이때 프로세스는 아래와 같은 그림 3-2와 같은 모습으로 메모리에 적재된다.메모리 구조는 다음과 같이 4개로 구분된다.      스택(stack)    스택은 데이터를 일시적으로 저장하는 영역으로, 지역변수, 호출한 함수의 반환 주소, 반환 값, 매개 변수등에 사용된다. 서브루틴(함수)을 호출할수록 스택 포인터로 지시되고 있는 스택 영역의 크기가 커지고, 함수가 반환되면 줄어든다.        힙(heap)    힙은 코드 영역과 별도로 유지되는 자유 영역으로, 시스템 호출을 통한 동적 메모리 할당 시에 사용되며, 스택 영역과 마찬가지로 힙 포인터로 힙 영역의 크기를 알 수 있다.    스택 영역과는 정 반대 방향으로 메모리가 증가하는데, 이때 빈 공간이던 스택  포인터와 힙 포인터 사이가 좁혀져 서로 만나면, 메모리가 소진되었다고 판단한다.        데이터(data)    데이터 영역은 프로그램의 가상 주소 공간으로, 전역, 정적 변수를 저장 및 할당하고, 실행전 초기화한다. 실제로는 읽기 전용 뿐만 아니라 읽기 및 쓰기 영역으로 사용할 수 있으며, 정적변수는 0으로 초기화할 수 있으며, 초기화 하지 않으면 데이터 영역의 끝에서 시작한다.    DB, 문서 편집기처럼 같은 프로그램을 실행하는 여러 프로세스들이 이 부분을 공유하도록 프로그래밍할 수 있는데, 이러한 프로그램을 재진입 프로그램이라고 한다.        코드(code)    코드 영역은 실행 명령, 목적 파일이 존재하는 메모리 영역으로, 즉 디스크에서 읽어 실행을 위해 컴파일된 프로그램 코드가 적재된다. 변경이 불가능하므로, 이 곳에 쓰기를 시도하면 오류가 난다. 여러 프로세스가 공유할 수 있으므로, 같은 프로그램을 여러 창 띄울 경우 모두 공유한다.  프로세스 상태 변호와 상태 정보운영체제는 작업 스케줄러, 프로세서 스케줄러를 통해 프로세스의 상태를 점검 및 제어하여 자원을 효율적으로 사용하게 만든다.프로세스 상태 변화프로세스 스케줄러는 그림 3-5와 같이 프로세스의 상태를 변화시킨다. 각 상태의 설명과 상태변화를 주시하자.추가적으로 위와 같이 생성 상태는, 잡 스케줄러가 디스크에 저장한작업 중 실행할 작업을 선정(Spooling)하여 준비리스트에 삽입한 상태를 의미한다. 잡 스케줄러는 이러한 접수된 프로세스를 준비큐에 넣어 준비상태가 되며, 디스패처에 의해 실행 상태로 바꾼다.            상태 변화      표기 방법      상세                  준비 $\\rightarrow$실행      dispatch(프로세스명)      준비 큐 맨 앞의 프로세스가 프로세서를 점유하는 것을 dispatch라고 하며, 할당 시간이 주어져 프로세서 독점을 방지한다.              실행 $\\rightarrow$준비      timeout(프로세스명)      자발적으로, 혹은 인터럽트 클록을 통해 할당된 시간이 넘어가면 프로세서가 반환되며, 준비 상태로 돌아간다.              준비 $\\rightarrow$대기(보류)      block(프로세스명)      할당 시간 종료 이전에 입출력 연산 등이 필요해지면 새로운 자원 요청 등의 문제로 프로세서를 스스로 양도하고 대기 상태가 된다.              대기(보류) $\\rightarrow$준비      wakeup(프로세스명)      입출력 작업이 끝나면 wakeup으로 대기에서 준비상태가 된다.      프로세스 제어 블록(PCB, Process Control Block)프로세스 제어 블록(PCB, Process Control Block) 혹은 작업 제어 블록(TCB, Task Control Block)은 특정 프로세스 정보를 저장하는 데이터 블록이나 레코드를 의미하며, 프로세스의 생성과 삭제를 함께한다. 아래와 같은 항목의 정보를 가지고 있으며, 운영체제의 모든 모듈이 읽고 수정 가능하다.            블록명      상세                  프로세스 식별자      각 프로세스의 고유 식별자(숫자, 색인 항목)              프로세스 상태      생성, 준비, 실행, 대기, 중단 등 상태 표시              프로그램 카운터      프로세스를 실행하는 다음 명령의 주소 표시              레지스터 저장 영역      누산기, 인덱스 레지스터, 스택 포인터, 범용 레지스터, 조건 코드 등 정보로, 컴퓨터 구조에 따라 수나 형태가 다르다. 인터럽트가 발생하면 프로그램 카운터와 함께 저장하여 재실행할 때 원래대로 복귀할 수 있게 한다.              프로세서 스케줄링 정보      프로세스의 우선순위, 스케줄링 큐의 포인터, 기타 스케줄 매개변수              계정 정보      프로세서 사용 시간, 실제 사용시간, 사용 상한 시간, 계정 번호, 작업이나 프로세스 번호 등              입출력 상태 정보      특별한 입출력 요구 프로세스에 할당된 입출력장치, 열린 파일 리스트 등              메모리 관리 정보      운영체제가 사용하는 메모리 시스템에 따른 경계 레지스터, 페이지 테이블이나 세그먼트 테이블 값 등              …      기타 등등      프로세스의 관리프로세스의 계층 구조프로세스는 실행 중 프로세스 생성 시스템을 호출하여 계층 구조의 새로운 프로세스를 만들 수 있는데, 생성 순서에 따라 부모-자식 관계를 유지하여 그림 3-9와 같이 계층적으로 생성할 수 있다.이때, 프로세스를 생성하는 프로세스가 부모 프로세스이고, 생성되는 프로세스가 자식 프로세스이다.이러한 계층 구조는 다음과 같은 기능을 위해 사용된다.자식 프로세스로의 자원 분배자식 프로세스는 프로세서, 메모리 같은 필요한 자원을 직접 운영체제로부터 얻어내거나, 부모 프로세스로 부터 일부 얻어낼 수 있다.또한 부모 프로세스는 자식 프로세스가 가져갈 수 있는 자원량을 제한할 수도 있다.자식 프로세스로의 데이터 분배초기화 시, 일부 데이터를 자식프로세스에게 줄 수 있다. 예를 들어 자식 프로세스가 파일 이름을 화면에 출력하는 일을 한다면, 생성 시, 부모 프로세스로 부터 파일 이름을 받아올 수 있다.자식 프로세스의 종료부모 프로세스만이 자식 프로세스를 종료시킬 수 있으며, 다음과 같은 경우 자식 프로세스가 종료된다.      부모 프로세스가 종료되면 자식 프로세스도 연속 종료 된다.        자식 프로세스가 자신에게 할당된 모든 작업을 끝낸다.        부모 프로세스가 자식 프로세스를 명시적으로 종료한다.        자식 프로세스가 할당된 자원을 초과하여 자원을 사용했다.  자식프로세스와의 작업 순서 선택생성된 자식 프로세스와 부모 프로세스의 작업 순서는      동시에 실행되거나,        자식 프로세스 작업을 끝낸 뒤, 부모 프로세스를 실행하는  선택지가 있다. 만약 부모 프로세스를 먼저 실행하고 싶다면, 자식 프로세스를 생성하기 전에 작업을 끝내고 생성하면 된다.프로세스의 생애주기프로세스의 생성프로세스는 PCB 생성 뒤, 주소 공간이 할당되면 생성된다. 구체적으로 다음과 같다.      새로운 프로세스에 프로세스 식별자 할당        프로세스의 모든 구성 요소를 포함할 수 있는 주소 공간과 프로세스 제어 블록 공간을 할당        프로세스 제어 블록 정보(프로세스 상태, PC, 우선순위, 자원 요청 등) 초기화        링크 걸기(준비 큐에 삽입)  UNIX 계열 운영체제에서는 fork 명령어를 이용해 프로세스를 생성할 수 있으며, 기본적으로 부모 프로세스와 똑같이 복제되지만, exec 명령어로 자식 프로세스 공간을 덮어쓰는 것으로 새로운 자식 프로세스 또한 생성 가능하다.추가로, 일괄 처리 환경에서는 준비 큐에 작업이 도착하면 프로세스를 생성하고, 대화형 환경에서는 새로운 사용자가 로그온할 때 프로세스를 생성한다.프로세스의 종료프로세스가 마지막 명령을 실행하면 종료하여 운영체제에 프로세스 삭제를 요청한다. 이외에도 프로세스가 종료되는 이유는 여러가지가 있으며 몇가지 예를 들자면, 앞선 정상적인 종료 요청, 할당 시간 초과, 오류 및 실패, 메모리 부족, 엑세스 위반 등이 있다.UNIX 계열 운영체제에서는 exit 명령어로 프로세스를 종료한다.부모 프로세스는 wait 명령어를 자식 프로세스의 종료를 기다릴 수 있다. 자식 프로세스가 종료되면 부모 프로세스에게 종료된 자식 프로세스의 식별자가 돌려지게 된다.추가로 abort 명령어로도 프로세스를 종료할 수 있다. 부모 프로세스만이 자식 프로세스를 종료시킬 수 있으므로, abort 를 자식 프로세스에 사용하면 부모 프로세스가 호출된다.자식 프로세스는 종료될 때 자신의 식별자를 부모 프로세스에게 전달한다.일괄 처리 환경에서는 작업 종료를 의미하는 신호 인터럽트가 발생되거나 시스템 호출로 중단 명령을 받으면 프로세스가 종료된다.대화형 환경에서는 사용자가 로그오프하거나 터미널을 닫으면 프로세스를 종료한다. 오류는 어떠한 환경에서도 종료된다.프로세스의 제거프로세서의 제거는 상태 변화와 관련 없이 프로세스를 파괴하는 것이다. 사용하던 자원과 PCB가 차지하는 메모리는 반환되며, 시스템 리스트나 테이블에서 삭제된다.물론, 디스크에서는 프로그램이 삭제되지 않으며, 부모프로세스가 제거되면 자동으로 자식 프로세스도 제거된다.프로세스 중단과 재시작다중 프로그래밍 환경에서는 입출력 동작이 연산 속도보다 느리기 때문에, 대부분의 시간을 유휴시간으로 기다리게 된다.이 문제를 해결하기 위해 다중 프로그래밍 환경에서는 림 3-10처럼 프로세스 중단(일시정지) 상태를 추가하여 해결할 수 있다.기존의 비슷한 상태인 프로세스 대기 상태는 자원이 할당받기를 기다리는 상태라면, 프로세스 중단 상태는 할당받은 자원을 기다리는 상태이며, 재시작 이벤트를 통해 이전 상태로 돌아가게 된다.사용례자원을 할당받길 기다리지 않아도 되므로, 재시작 이벤트를 감지하는 즉시 실행 상태로 되돌아 갈 수 있어서 시스템 부하 조절에 용이하다.뿐만 아니라, 장애 발생에 따른 기능 회복 대기, 프로세스 검사, 너무 많은 프로세스 적재 등에 사용된다.예를 들어 프로세스가 더 많은 저장장치를 요구하는데 현재 사용가능한 저장장치가 부족하면 대기 상태가 되며, 시간이 지나 자원들이 사용가능하게 되면 대기상태로 바뀐다. 하지만 만약 영원히 자원을 얻을 수 없게 되면, 교착 상태가 되게 된다.중단 상태다중 처리 시스템에서는 다른 프로세서가 실행 중인 프로세스를 중단할 수 있고, 단일 처리 시스템에서는 해당 프로세스 스스로 중단해야 한다.중단된 대기 상태는 프로세스가 보조 메모리에 있고 이벤트를 대기 중인 상태이며, 중단된 준비 상태는 프로세스가 보조 메모리에 있지만 즉시 메인 메모리로 적재하여 실행할 수 있는 상태이다.중단한 프로세스는 원래 상태로 돌아가면 중단된 시점부터 다시 시작한다.프로세스 우선순위 변경프로세스 스케줄러는 PCB의 우선순위를 이용해 준비 리스트의 프로세스를 처리한다.프로세서는 프로세서 중심 프로세스와 입출력 중심 프로세스로 구분할 수 있다.같은 입출력 중심 프로세스 또한, 추가로 속도가 느리면서 빠른 응답을 요구하는 단말기 입출력 프로세스에 높은 우선순위를 부여하고, 속도가 빠른 디스크 입출력 프로세스에는 낮은 우선순위를 부여한다.보통 우선순위가 낮은면 대신 시간을 많이 할당받고 우선순위가 높으면 시간을 짧게 할당 받는다. 따라서 프로세서 중심 프로세스는 우선 순위가 낮은 대신 긴 시간을 할당받고, 입출력 중심 프로세스는 우선순위가 높아 자주 프로세서를 받는 대신 짧은 시간을 할당받는다.프로세스의 문맥교환(context switching)문맥 교환의 원인      트랩 : 부적절한 파일 접근이나, 현재 실행중인 프로세스 오류, 예외 상황에서 치명적인 오류인지 판단하고 치명적일 경우 프로세스 종료 뒤 문맥 교환        인터럽트    만약 외부에서 입출력 동작의 종료와 같은 특정 이벤트가 일어난다면 인터럽트가 발생하게 되고, 인터럽트 유형에 따라 각기 다른 처리 루틴으로 제어가 넘어가게 된다. 예를 들면                  입출력 인터럽트 : 이벤트를 기다리는 프로세스를 준비 상태로 바꾼 후 실행할 프로세스를 결정                    클록 인터럽트 : 현재 실행 중인 프로세스의 할당 시간을 조사해 실행 중인 프로세스를 준비 상태로 바꾸고 다른 프로세스를 실행 상태로 바꾼다.              이때, 같은 프로세스가 아닌 다른 프로세스를 실행 상태로 바꾸면서 아래 그림 3-11과 같은 문맥 교환이 일어나게 된다.  문맥교환(context switching)이전 프로세스의 상태 레지스터 내용을 보관하고 다른 프로세스의 레지스터를 적재하여 프로세스 교환하는 과정을 문맥교환(context switching) 이라고 한다.정확히는 준비 $\\rightarrow$ 실행 단계, 실행 $\\rightarrow$ 준비 단계, 실행 $\\rightarrow$대기 단계에서 발생한다.이 때 기존의 작업이 끝나지 않은 프로세스은 다음 할당 때 이어가야 하므로, 프로세서의 레지스터 내용을 저장해두고 그림 3-8 처럼 저장한다.따라서 문맥교환에는 오버헤드가 발생하는데, 메모리 속도, 레지스터 수, 특수 명령어 유무에 따라 오버헤드 크기가 다르며, 최대한 문맥교환을 줄이게 프로그래밍하는 것이 좋다.문맥교환이 발생되면 종료되는 프로세스는 사용자가 아닌 커널에게 제어가 넘어가며 아래와 같은 문맥 교환이 일어나게 된다.스레드의 개념과 상태 변화스레드(thread)란?프로세스의 제어 부분의 실행단위를 스레드(thread)이라고 하며, 하나의 프로세스는 여러 스레드로 나눌 수 있다.같은 프로세스의 스레드들은 프로세스의 직접 실행정보를 제외한 나머지 프로세스 관리 정보를 모든 스레드가 공유한다.그림 3-12는 각 스레드가 프로그램 카운터(PC), 스택 포인터(SP), 지역 데이터, 스택 등을 독립적으로 가지지만, 코드 , 전역 데이터, 힙을 다른 스레드와 공유하는 것을 보여준다.스레드들은 보통 다른 프로시저를 호출하므로 별도의 스택이 필요하고, PC를 나눔으로써, 동시에 코드의 여러 부분을 실행할 수 있게 된다. 하지만 이 때문에 공유 데이터의 손상이나 이상동작을 조심해야한다.스레드들이 프로세스 속성의 일부를 공유하는 것을 경량 프로세스(LWP, Light Weight Process), 전통적인 프로세스 하나당 스레드 하나인 경우를 중량 프로세스(HWP, Heavy weight Process)라고 한다.스레드의 이점한 프로세스에는 하나 이상의 스레드가 존재할 수 있으며, 이들은 공동의 목적을 위해 병렬로 수행하며, 프로그램의 서로 다른 부분을 동시 실행할 수 있다.이점은 다음과 같다.      사용자 응답성 증가: 응용 프로그램이 작업을 수행중이여도, 다른 스레드를 통해 사용자의 응답을 처리할 수 있다.        프로세스의 자원과 메모리 공유 가능: 스레드들이 프로세스 자원과 메모리를 공유하므로 시스템 성능이 향상된다.        경제성이 좋음: 프로세스 생성보다 스레드 생성이 문맥 교환 오버헤드가 적다.        다중처리(multiprocessing)으로 성능과 효율 향상: 각 스레드를 여러 프로세서에서 병렬로 실행하여 성능과 효율성을 높일 수 있다.  스레드 제어블록(TCB, Thread Control Block)같은 프로세스의 스레드들은 그림 3-13과 같이 주소 공간을 공유한다. 그림 3-19는 더욱 자세한 스레드 제어블록에 관한 그림이다.한 프로세스는 여러 스레드를 가지고 있으므로, PCB 내에는 여러 TCB의 리스트를 가진다.스레드 제어블록(TCB, Thread Control Block)은 다음과 같은 내용을 포함하고 있다.      실행 상태: 프로세서 레지스터, 프로그램 카운터, 스택 포인터        스케줄링 정보: 상태, 우선순위, 프로세서 시간        계정 정보, 스케줄링 큐용 모니터들, PCB를 포함하는 포인터 등  스레드의 상태 변화프로세서 하나에 스레드가 항상 하나만 실행되므로 프로세스처럼 상태 변화가 존재한다.스레드 생성에는 운영체제가 아니라 프로세스가 스택과 레지스터를 제공하므로, 프로세스의 생성과 종료보다 오버헤드가 적다.스레드 한개가 대기 상태로 변하면 전체 프로세스의 스레드가 대기 상태로 변하지 않고 다른 스레드 하나가 실행상태되므로, 병렬처리가 가능하다.다만, 프로세스들과 달리 스레드들은 한 프로세스 내의 메모리를 공유하므로, 보호, 보안 문제가 발생할 수 있지만, 보통 프로세스 하나는 한 사용자만 사용하므로, 보통은 문제없다.다중 스레드(multithread)DOS 같은 과거의 운영체제를 제외한 현대 운영체제는 단일 프로세스에서 단일 스레드 실행과 다중 스레드 실행을 둘 다 지원한다. 그림 3-14는 이러한 스레드의 예시이다.다중 스레드(multithread)는 자원을 공유하는 특성상 자원 생성과 관리의 중복성을 최소화하여 실행능력이 향상될 수 있고, 커널이 개입하지 않고 독립적으로 실행할 수 있어 서버에서 많은 요청을 효과적으로 처리할 수 있다.다중 스레드에서는 그림 3-15와 같이 데이터를 공유한다.전역 데이터가 바뀌면 다른 스레드에서 확인 가능하며 힙의 내용 또한 공유된다. 이러한 공유 특성 덕분에 스레드 생성, 스레드 간 자원 및 문맥 교환, 스레드 종료가 훨씬 빠르고, 병렬 처리로 성능을 극대화 할 수 있다.스레드 사용례다중 스레드는 사용자 수준에서 적용하여 운영체제와 무관하여 속도가 매우 빠르고 비동기적 요소를 구현할 수 있다. 예를 들면, 스레드 하나는 사용자의 입력을 받아들이고 다른 스레드는 다음 명령을 신속하게 준비할 수 있다.그림 3-16처럼 워드 편집기에서 주기적으로 진행되는 자동저장 또한 이런식으로 동작한다.현재 실행중인 스레드를 대기 상태로 바꾸고 제어를 다른 스레드로 옮기는 상태 변화로 많은 요청을 효과적으로 처리할 수 있다. 그림 3-17은 웹 브라우저에서 스레드 한개가 이미지, 텍스트를 로딩하는 동안 다른 스레드는 네트워크에 연결하여 데이터를 검색할 수 있다.만약 공유메모리가 있는 다중 처리 시스템에서는 프로그램을 공유 메모리에 저장하고 각 프로세서에 스레드를 할당하여 병렬처리하면 더욱 크게 성능을 향상시킬 수 있다.하지만 이러한 사용자 수준 스레드는 커널 하나를 통해서만 시스템 호출이 가능하므로 시스템 호출의 경우 병목현상이 일어날 수 있다.수준별 스레드와 스레드 구현스레드는 다음과 같이 세 가지 형태로 구현할 수 있다.      사용자 수준 스레드(user-level thread) : 다대일(n:1) 매핑          스레드 라이브러리를 이용하여 작동하는 형태            커널 수준 스레드(kernel-level thread) : 일대일(1:1) 매핑          커널(운영체제)에서 지원하는 형태            혼합형 스레드(multiplexed thread) : 다대다(n:m) 매핑          이 둘을 혼합한 형태      사용자 수준 스레드(user-level thread)사용자 수준 스레드는 스레드 라이브러리를 통해 구현하며, 커널 입장에서는 다중 스레드로 보이지 않고 프로세스 하나로 본다. 즉 스레드 교환에 커널이 개입하지 않고, 커널 수준 스레드 하나에 사용자 수준 스레드 여러개가 매핑되므로 다대일 스레드 매핑이라고도 한다.스레드 라이브러리는 스레드의 생성과 종료, 메시지 전달, 스케줄링과 문맥 등 정보를 보관하며 POSIX 표준안 스레드 확장판인 Pthread, Win32 thread, JAVA thread API를 사용한다.            장점      세부                  이식성이 높음      커널 독립적 스케줄링으로 운영체제에 무관              오버헤드가 적음      커널 호출이 없으므로, 커널 영역의 오버헤드가 줄어듦              유연한 스케줄링이 가능      커널이 아닌 스레드 라이브러리에서 스레드 스케줄링을 제어하므로 응용 프로그램에 맞게 스케줄링을 할 수 있다.                  단점      세부                  시스템의 동시성을 지원하지 않음      스레드가 아닌 프로세스 단위로 프로세서를 할당해 다중 처리 환경을 갖춰도 스레드 단위로 다중처리 불가. 또한, 프로세스 내의 스레드 한개가 대기 상태가 되면 이 중 어떤 스레드도 실행 불가능              확장에 제약이 따름      커널이 한 프로세스에 속한 여러 스레드에 프로세서를 동시에 할당 불가능하여 다중 처리 시스템 규모 확장 불가              스레드 간 보호 불가능      스레드 간 보호에 커널의 보호 방법을 사용할 수 없다. 스레드 라이브러리에서 해당 기능 지원해야 함.      커널 수준 스레드(kernel-level thread)사용자 수준 스레드의 한계를 극복하고 커널이 스레드와 관련된 모든 작업을 관리하는 방식, PCB와 TCB를 커널에서 관리한다.한 프로세스에서 다수의 스레드가 프로세서를 할당받아 병행 수행이 가능하고, 스레드 한개가 대기상태가 되면 동일한 프로세스에 속한 다른 스레드로 교환 가능하다.스레드가 생성되거나 교환될 때 커널 스레드가 생성되어 사용자 스레드와 1:1 매핑된다.            장점      세부                  커널 지원 가능      사용자 수준에 비해 커널 지원이 자유롭다              스레드 병행 수행      커널이 각 스레드를 개별적으로 관리하여 동일 프로세스 내 스레드들이 병행 수행 가능              시스템 동시성 지원      동일 프로세스 내의 스레드 중 하나가 대기 상태가 되더라도 다른 스레드를 실행 할 수 있다                  단점      세부                  오버헤드 커짐      커널 영역으로 전환하는 오버헤드 발생, 스케줄링, 동기화에 더 많은 자원 필요              어려운 구현      고급 멀티스레딩 기술 필요.              이식성 낮음      각 운영체제 마다 다른 구현이 필요하고, 운영체제에 의존함      혼합형 스레드(multiplexed thread)사용자 수준 스레드와 커널 수준 스레드를 혼합한 구조로, 양 측 단점을 극복하기 위해 제안되었다.각 사용자 수준 스레드는 다수의 경량 프로세스(LWP)와 N:M 매핑되고, 이러한 경량 프로세스는 커널 수준 스레드와 1:1 매핑된다.커널 수준 스레드는 디스패치하고 스케줄링하여 프로세서에서 실행되고, 경량 프로세스는 시스템 호출로 생성해 커널 영역에서 커널이 독립적으로 스케줄링하여 다중 처리 시스템에서는 병렬로 실행한다.            장점      세부                  시스템 동시성 문제 해결      스레드 단위로 다중처리 가능. 또한, 프로세스 내의 스레드 한개가 대기 상태가 되면 다른 스레드가 실행 상태로 전환가능              성능 최적화      스레드 라이브러리의 최적 성능을 위해 커널이 경량 프로세스 수 동적 조절              자유로운 매핑 변환      병행 실행이 의미가 없는 경우 다대일 매핑으로 바꾸거나 스레드 풀링을 이용하여 일대일 매핑으로 오버헤드를 줄일 수도 있다.      경량 프로세스가 자원과 입출력 대기를 하므로, 프로세스는 입출력 완료까지 기다릴 필요없다.스레드 풀링(thread pooling): 시스템이 미리 생성한 스레드의 풀을 응용 프로그램에 제공하여 스레드를 효율적이게 사용할 수 있는 방법, 스레드 생성 오버헤드 감소, 스레드 수 제한으로 인한 일정한 성능 등의 장점이 있다.            단점      세부                                다중 처리, 다중 프로그래밍, 다중 작업, 다중 스레드"
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%204-%EB%B3%91%ED%96%89%20%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80%20%EC%83%81%ED%98%B8%EB%B0%B0%EC%A0%9C.html": {
    title: "OS 정리-Chap 4-병행 프로세스와 상호배제",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%204-%EB%B3%91%ED%96%89%20%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80%20%EC%83%81%ED%98%B8%EB%B0%B0%EC%A0%9C.html",
    tags: ["OS","CS","요약"],
    content: "4. 병행 프로세스와 상호배제title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.병행 프로세스병행 프로세스란?프로세스는 프로세서, 레지스터, 캐시, 입출력 장치 등 여러 자원을 사용하며, 이중, 메모리 같은 자원은 모든 프로세스가 동시에 병렬로 공유프로세서 자원을 시분할을 통해 여러 프로세서가 동시에 작업하는 것처럼 보이게 하는 것을 병행 프로세스라고 하며, 다음 두 가지로 나뉜다.독립 프로세스단일 처리 시스템에서 다른 프로세스와 자원을 공유하지않고, 영향을 주고받지 않는 독립적인 병행프로세스. 따라서 초깃값에 따라 항상 동일한 결과를 보여주며, 중지 후, 변동사항 없이 다시 시작 가능하다.단일 프로그래밍, 다중 프로그래밍, 다중 처리의 프로세스가 예시이다.협력 프로세스다른 프로세스에 영향과 상호작용을 주고 받으며 특정 기능을 수행하는 비동기적 프로세스.자원 효율성, 계산 속도, 모듈적 구성, 사용자 동시처리성 향상에 기여한다.예를 들어, 파일을 읽는 프로세스와 파일을 쓰는 프로세스는 파일을 쓰는 순간 파일 내용이 바뀌므로 파일을 읽는 프로세스가 영향을 받으며, 이 덕분에 사용자는 편리하고 빠르게 문서작업이 가능하다.따라서 서로 충돌이 일어나지 않게 하기위해 다음과 같은 세가지 형태의 상호작용을 한다.      경쟁 관계, 다중 프로그래밍 환경에서 디스크나 프린터를 접근할 때, 프로세스가 서로를 인식하지 못하며, 정보 교환이 없고 완전히 상호배제함.        간접적으로 서로의 관계를 인식, 입출력 버스 등을 공유 시, 다른 프로세스의 정보나 타이밍에 영향을 받고 개체 공유를 위해 협력한다.        프로세스끼리 함수를 통해 서로 인식하고 통신, 완전한 협력 관계 하에 병행하여 동작한다.  title: 병행성과 병렬성병행성 : 한 프로세서가 시분할로 공유하는 여러 프로세스를 나누어 처리병렬성 : 여러 프로세서가 동시에 여러 프로세스를 처리병행 프로세스의 해결 과제병행성은 다중, 단일 처리 시스템 등에서 시스템의 신뢰도를 높이고 처리 속도를 높이는데 중요한데, 몇 가지 문제를 가지고 있다.      공유 자원을 상호 배타적으로 사용해야 한다. 프린터, 통신망 등이 예시.        병행 프로세스 간에는 협력이나 동기화 되야 한다. 상호배제는 동기화의 한 형태.        두 프로세스 간에 데이터 교환 및 통신이 되어야 한다.        프로세스는 동시에 수행하는 다른 프로세스의 실행 속도와 관계없이 항상 일정한 실행 결과를 보장하도록 결정성(determinancy)을 확보해야 한다.        교착 상태를 해결하고 병행 프로세스들의 병렬 처리 능력을 극대화해야 함.        실행 검증 문제를 해결해야 함.        병행 프로세스를 수행하는 과정에서 발생하는 상호배제를 보장해야함. 즉, 어떤 작업을 한 프로세스가 수행 중이면 다른 프로세스는 접근 불가.  선행 그래프와 병행 프로그램이때, 상호배제를 보장하기 위해 프로세스는 선행 그래프를 이용한다.선행 그래프(precedence graph)프로세스는 프로세스 집합과 이것의 선행 제약(precedence constraint)로 정의할 수 있다.프로세스 집합은 프로세스들의 모임이고, 선행 제약은 프로세스 집합 내 원소를 순서대로 다른 상태로 옮기는 것이다.예를 들어 프로세스 집합 $P_1, P_2, \\cdots, P_n$이 있고, 선행 제약이 $P_i&lt;P_j$이면, 프로세스 상태는 $P_i$이후 $P_j$로 옮겨간다. 선행제약이 없다면, 두 프로세스는 독립적이란 의미이며, 서로 영향을 주지 않으므로 병행 실행이 가능하다.선행 그래프(precedence graph)는 선행 제약을 논리적으로 표현한 것으로, 순차적 활동을 표현하는 방향성 비순환 그래프이다. 즉, 연결선(Edge)로 연결된 노드 간에는 한쪽 방향으로만 이동할 수 있으며, 순환(Cycle)이 존재하지 않는다.선행 그래프에서의 노드 혹은 활동은 소프트웨어 작업이나 동시 실행 가능한 프로그램을 의미한다.그림 4-2 (b)의 선행 그래프를 예시로 들자면      $S_1$과 $S_2$는 서로 독립적이므로 동시 수행 가능        $S_3$은 a 값과 b 값을 할당받기 전에 수행하면 안된다.        $S_4$는 c 값을 계산하기 전에 수행 불가  그림 4-2 (a)에서 알고리즘의 일부를 병행 수행, 예를 들어 값 a와 b를 동시에 구하려면 프로세서 하나에 기능 단위를 여러 개 두거나 프로세서를 여러 개 사용한다.예를 들어 그림 4-3의 선행 관계는 비순환적이다. 아래 그림 4-4의 경우 $S_2$는 $S_3$가 끝나야만 수행가능하고, $S_3$는 $S_2$가 끝나야만 수행가능하다는 의미를 가진 순한 관계가 존재하므로, 모순이 존재하며, 수행 불가하다.fork와 join 구조실제 프로그램의 선행 관계를 표기하기에는 2차원 선행 그래프는 너무 단순하며 추가적으로 fork, join, 병행 문장 등의 구조가 필요하다.      fork 명령어    fork L 문장을 통해 병행 프로세스를 2개로 나눌 수 있다. goto 문과 비슷하기 때문에 조금 복잡하게 보일 수 있다.아래 예시 그림 4-5에서는 fork 명령어를 통해 $S_2$와 $S_3$가 병행 수행된다.        join 명령어    fork와는 반대로 병행 연산 2개를 하나로 결합하는 명령으로, 병합 대상이 되는 두 프로세스 중 먼저 끝나는 순서대로 join을 먼저 수행한다.    세개 이상의 프로세스를 join해야할 경우, 먼저 끝나는 두 프로세스가 서로 join된 뒤, 그 결과값과 다음에 끝나는 프로세스가 join된다.    그림 4-6에서 count 매개변수로 join할 프로세스 갯수를 집어넣고, join이 실행되면, 프로세스간 조인이 진행되면서 1씩 줄어들며 0이 되면 join이 종료된다.  그림 4-7은 fork와 join을 이용한 병행 수행과 병합을 표현한 알고리즘과 선행 그래프 예시다.그림 4-8은 3개의 노드가 join되므로, 유입 정도(In degree)는 3이며 count의 초기값은 3이다.병행 문장병행 문장은 하나의 프로세스가 여러 병렬 프로세스로 퍼졌다가 다시 하나로 뭉쳐지는 것을 나타내는 고급 언어 구조이다.parbegin S1; S2; .....; Sn; parend;각 $S_i$는 단일 문장이고, parbegin과 parend 사이의 모든 문장은 병행 수행가능 하다. 그림 4-9 예시처럼 최초로 시작되는 프로세스인 $S_0$와 모든 프로세스가 끝난 후에 실행되는 $S_{n+1}$ 문장을 추가해 정의할 수 있다.이때 BEGIN과 END는 순차 실행 문장으로, 이를 통해 그림 4-10와 같은 복잡한 구조를 표현할 수 있다.아래 그림 4-11은 그림 4-2를 parbegin/parend 구조를 이용해 표현한 것이다.병행 문장은 블록 구조의 고급 언어에 쉽게 추가할 수 있으며, 다른 구조적 제어 문장의 장점을 많이 보여준다.상호 배제와 동기화상호배제의 개념상호배제는 병행 프로세스에서 프로세스 하나가 공유 자원을 사용 시 다른 프로세스들이 동일한 일을 할 수 없도록 하는 방법이다. 줄여서 mutex(MUTual EXclusion)이라고도 한다.예를 들어 그림 4-13처럼 공유 메모리를 $P_1$ 프로세스가 사용하고 있으면, $P_2$는 해당 공유 메모리에 접근할 수 없도록 막아야(=상호배제) 한다.읽기 연산처럼 동시에 읽어도 문제없는 연산을 제외하고, 쓰기 연산의 경우, 프로세스 별로 차레대로 공유 자원에 접근해야 하며, 이를 제어하는 방법을 동기화라고 한다. 동기화를 통해 상호배제를 구현할 수 있지만 교착 상태와 기아 상태가 발생할 수 있다.그림 4-13의 공유 메모리 같이 두 프로세스가 동시에 사용할 수 없는 공유 자원을 임계 자원(critical resource), 이러한 임계 자원에 접근하고 실행하는 프로그램 코드 부분을 임계 영역(critical section)이라 한다.성공적인 상호배제 연산은 다음과 같은 4개의 조건을 만족해야 한다.      두 프로세스는 동시에 공유 자원에 진입할 수 없다.        프로세스의 속도나 프로세서 수에 영향을 받지 않음.        공유 자원을 사용하는 프로세스만 다른 프로세스를 차단 가능.        프로세스가 공유 자원을 사용하려고 너무 오래 기다리면 안됨.  임계 영역임계 영역의 예시로는 입출력 장치 간의 버퍼가 존재하며, 임계 영역에서는 작업을 빠르게 수행하고, 특정 프로세스가 임계 영역에 오래 머물거나 무한 루프 등에 빠지지 않게 관리해야 한다.임계 영역으로 상호배제를 구현할 수 있는데, 임계영역에 다른 프로세스가 있으면, 이 프로세스는 다른 프로세스가 임계 영역에 들어가지 못하게 진입 상호배제를 수행하며, 임계영역에서 나오는 프로세스는 탈출 상호배제를 수행하여 다른 프로세스가 임계 영역에 들어갈 수 있도록 해야한다. 물론, 이 동작은 단일 머신 사이클에서의 동작이며, 다중 처리 시스템에서는 이 행동만으로 상호배제가 되지 않는다.임계 영역의 상호배제를 자물쇠와 열쇠로 비유하면, 그림 4-14처럼 어떤 프로세스가 자물쇠를 풀 열쇠를 가지고 있는지 확인하는 검사 동작과, 다른 프로세스가 사용 못하도록 자물쇠로 잠그는 동작으로 분류할 수 있다.이를 코드로 표현하자면 다음과 같다.do &amp;#123;    while (turn != i); /* 진입 영역 */    /* 임계 영역 */    turn = j;/* 탈출 영역 */    /* 나머지 영역 */&amp;#125; while (TRUE)진입 영역에서는 각 프로세스가 임계 영역에 들어갈 수 있도록 요청한다.만약 turn 변수가 i, 즉 자기 자신의 순번 턴이 되면, 임계영역에 진입하고, 순번이 아니면 무한 루프로 대기한다.이후 임계 영역에서 작업을 처리한 후, 탈출 영역에서 다음 순번의 turn으로 바꾸어 준 뒤 임계 영역에서 나가게 된다.임계영역은 세 가지 조건을 만족해야 성공적인 상호배제가 이루어진다.      상호배제 : 어떤 프로세스가 임계 영역에서 작업 중이면 다른 프로세스는 임계 영역으로 들어갈 수 없다.        진행 : 임계 영역에서 프로세스가 없는 상태에서 여러 프로세스가 들어가려고 할 때는 어떤 프로세스가 들어갈지 적절히 결정해야 한다.        한정 대기: 다른 프로세스가 임계 영역을 무한정 기다리는 상황을 방지하기 위해 한번 들어갔던 프로세스는 다음에 임계 영역에 다시 들어갈 때 페널티를 받는다.  생산자-소비자 문제와 상호배제를 해결하는 초기의 시도생산자-소비자 문제는 운영체제에서 비동기적으로 수행하는 모델로, 생산자 프로세스가 생선한 정보를 소비자 프로세스가 소비하는 형태이다.예를 들어 그림 4-17에서 라인 프린터 드라이버는 라인 프린터가 사용하는 문자 데이터를 생산한다.생산자는 소비자에게 데이터를 전송할 때, 소비자가 받을 준비가 됬을 때만 보내게 만들기 위해 버퍼를 도입하여 해결한다.  (그림 4-16)소비자가 받지 못할 때는 생산자는 버퍼에 데이터를 보내어 쌓고, 소비자는 여유가 생기면 데이터를 버퍼에서 가져감으로써, 생산자가 소비자의 처리속도에 영향을 받지않고 작업을 할 수 있다.만약, 버퍼의 최대 크기가 유한하다면,      버퍼가 가득찼을 때는 생산자가 더 이상 데이터를 보내지 않고 소비자가 버퍼를 소모해줄 때까지 대기 하도록        버퍼가 비어있다면 소비자가 더 이상 데이터를 소비하지 않고 생산자가 버퍼를 채워줄 때까지 대기 하도록  두 가지 경우에 동기화가 필요하다.이러한 동기화에 대해 먼저 무한 버퍼일 경우 소비자와 생산자의 알고리즘은 아래의 그림 4-18과 같다.유한한 크기의 버퍼에서는 그림 4-16처럼 원형으로 순환하는 배열을 구현한 뒤, 삽입 포인터와 삭제 포인터를 구현하여 해결할 수 있다.삽입 포인터가 삭제 포인터보다 앞선 번호라면(modulo 연산의 원형 큐로 구현시 무조건 큰 번호가 앞서지 않으므로 주의), 버퍼에 값이 존재하는 것이다.유한 버퍼를 코드로 구현하면 다음과 같다.// 공유 데이터의 선언#define BUFFER_SIZE 10 //버퍼 최대 크기typedef struct &amp;#123;    DATA data;&amp;#125; item;item buffer[BUFFER_SIZE];int in = 0; // 삽입할 원소 자리 포인터int out = 0; // 삭제할 원소 자리 포인터int counter = 0; // 버퍼 내 원소의 갯수생산자와 소비자 두 프로세스는 공통적으로 while 문으로 counter 변수를 계속 확인하며 검사한다.//생산자 프로세스item nextProduced; //생산하는 새로운 원소를 위한 변수while (true) &amp;#123;    while (counter == BUFFER_SIZE); //버퍼가 가득 차 아무일도 하지 않음    buffer[in] = nextProduced;    in = (in+1) &amp;#37; BUFFER_SIZE;    counter++;&amp;#125;//소비자 프로세스item nextConsumed; //소비하기 위해 추출한 원소를 위한 변수while (true)&amp;#123;        while (counter == 0); //버퍼가 비어 아무 일도 하지 않음    nextConsumed = buffer[out];    out = (out+1) &amp;#37; BUFFER_SIZE;    counter--;&amp;#125;하지만 이러한 생산자-소비자 방식은 동시에 실행될 시 프로세스의 접근 순서에 따라 공유 데이터이 달라지게될 위험이 있다. 예를 들어 생산자-소비자 방식의 소스코드의 마지막 줄의 코드를 레지스터의 관점에서 살펴본다면 다음과 같이 작동한다.//counter++ 기계어register_1 = counter;register_1 = register_1 + 1;counter = register_1;//counter-- 기계어register_2 = counter;register_2 = register_2 -1;counter = register_2;만약 이 두 코드가 동시에 실행되어 아래 코드와 같이 섞여 버려 다음과 같은 결과를 낼 수 있다.// 초기 counter값은 5로 초기화register_1 = counter; // register_1 = 5register_1 = register_1 + 1; // register_1 = 6register_2 = counter; // register_1 = 5register_2 = register_2 - 1; // register_2 = 4 counter = register_1; // counter = 6counter = register_2; // counter = 4정상적으로 동시성을 적용하면 counter 값은 5가 되는게 정상이지만, 공유 데이터인 counter의 접근 순서에 따라 실행결과가 6 또는 4로 달라지게 된다.예시의 생산자-소비자 프로세스처럼 이렇게 공유 데이터 접근 순서가 달라지면 동일한 결과값을 보장하지 못하는 프로세스들을 경쟁 상태(race condition)에 놓여져 있다고 한다.경쟁 상태를 예방하려면 병행 프로세스들을 동기화해야 하며, 동기화는 임계영역을 이용한 상호배제로 구현할 수 있다. 즉 공유 변수 counter를 임계 영역으로 두고, 상호배제해야한다.상호배제 방법들title: 상호배제의 다양한 방법들            수준      방법      종류                  고급      소프트웨어      - 데커의 알고리즘- 크누스의 알고리즘- 램포트의 빵집 알고리즘- 핸슨의 알고리즘- 다익스트라의 알고리즘                     프로그래밍 언어, 운영체제 수준 소프트웨어      - 세마포- 모니터              저급      하드웨어 수준 원자 연산      TestAndSet(TAS)      고급 방식들데커 알고리즘데커 알고리즘은 두 프로세스가 서로 통신하기 위해 공유 메모리를 사용하여 충돌 없이 단일 자원을 공유할 수 있도록 허용하는 알고리즘이다.각 프로세스는 같은 수의 플래그를 설정한 후, 다른 프로세스의 플래그를 무한 루프로 확인한 뒤, 임계 영역 할당을 기다린다. 다만 이 또한, 우연하게 동시에 플래그를 설정 및 검사하면 교착 상태가 발생할 수 있다.데커 알고리즘은 다음과 같은 장점을 가지고 있다.      특별한 하드웨어 명령문 필요 없음        임계 영역 바깥에서 수행중인 프로세스가 다른 프로세스들이 임계 영역에 들어가려는 것을 막지 않는다.        임계 영역에 들어가기를 원하는 프로세서를 무한정 기다리게 하지 않는다.  데커 알고리즘을 코드로 구현하면 다음과 같다.// 프로세스가 공유하는 데이터 flag[] : 부울(boolean) 배열, turn : 정수flag[0] = false;flag[1] = false;turn = 0 ;                         // 공유 변수, 0 또는 1// 프로세스 P0;                     // 프로세스 P0의 임계 영역 진입 절차flag[0] = true;                    // P0의 임계 영역 진입 표시while (flag[1] == true) &amp;#123;          // P1의 임계 영역 집입 여부 확인    if (turn == 1) &amp;#123;               // P1이 진입할 차례가 되면        flag[0] = false;           // 플래그를 재설정해 P1에 진입 순서 양보        while (turn == 1) &amp;#123;        // turn을 바꿀 때까지 대기           // 바쁜 대기        &amp;#125;        flag[0] = true;            // P1이 임계 영역에 재진입 시    &amp;#125;&amp;#125;/* 임계 영역 */turn = 1;                          // P1에 진입 순서 제공flag[0] = false;                   // P0의 임계 영역 사용 완료 지정/* 나머지 영역 */                    // P0의 나머지 영역 수행// 프로세스 P1; // 프로세스 P1의 임계 영역 진입 절차flag[1] = true;while (flag[0] == true) &amp;#123;    if (turn == 0) &amp;#123;        flag[1] = false;        while (turn == 0) &amp;#123;           // 바쁜 대기        &amp;#125;        flag[1] = true;    &amp;#125;&amp;#125;/* 임계 영역 */turn = 0;flag[1] = false;/* 나머지 영역 */      $P_0$은 flag[0]을 true로 설정하여 임계 영역 사실을 고지한다.        이후 while문으로 flag[1]을 검사하여 $P_1$의 임계 영역 진입 여부를 확인한 뒤,                  false이면 $P_0$이 임계 영역으로 진입하고                    true이면 $P_1$이 임계 영역에 진입할 차례라서 자신의 플래그 flag[0]를  false로 설정한 뒤, 아래의 while문으로 이동해 바쁜 대기한다.                  공유 변수 turn을 통해 두 프로세스가 동시에 임계 영역에 들어가는 것을 방지한다.  이외의 소프트웨어 상호배제 알고리즘      다익스트라 상호배제 : 2개 이상의 프로세스 상호배제 문제 해결, 가장 짧은 평균 대기시간 제공, 실행 시간이 가장 짧은 프로세스에 할당하는 세마포 방법        크누스 : 이전 알고리즘 관계를 분석 후, 일치 패턴을 찾아 패턴 반복을 줄이는 방법으로 무한정 연기를 해결, 단 프로세스들이 평균 대기시간이 김.        램포트 빵집 : 준비 상태 큐에서 기다리는 프로세스마다 우선 순위를 부여해 그중 우선순위가 가장 높은 프로세스에 먼저 프로세서를 할당하는 방법. 빵집 번호표에 비유        핸슨 : 실행 시간이 긴 프로세스에 불리한 부분을 보완하는 것으로 대기 시간과 실행 시간을 이용하는 모니터 방법  TestAndSet(TAS) 명령어TestAndSet(TAS) 명령어는 하드웨어 명령어로, 메모리 영역의 값에 대해 검사와 수정을 원자적으로 수행할 수 있다.이를 이용하면 실행 효율이 좋고, 대기시간이 짧으면서 간단하게 구현할 수 있다.원자적 연산(atomic operation)은 중단 없이 실행하고 중간에 다른 사람이 수정할 수 없는 컴퓨터의 최소 단위 연산이다. 메모리의 1비트에서 작동한다. 따라서 중간에 다른 명령어가 들어갈 수 없어 상기했던 경쟁 상태가 벌어지지 않는다.총 2개의 명령어인 TestAndSet 명령어와, TestAndSet에 지역변수 lock을 설정하는 명령어로 이루어진다.TestAndSet 명령어는 읽기와 쓰기 모두를 제공하며, 해당 주소의 값을 읽고 새 값으로 교체하면서 해당 메모리 위치의 이전 값을 돌려준다. 동작을 코드로 설명하자면 다음과 같다.// TestAndSet 명령어의 코드 구현, 실제로는 원자적 연산이므로 기계어로 되어 있다.// taget을 검사하고, target 값을 true로 설정boolean TestAndSet (boolean *target)&amp;#123;    bloolean temp = *target; // 이전 값 기록    *target = true;          // true로 설정    return temp;             // 값 반환&amp;#125;부울 변수 lock을 이용해 프로세스의 임계영역에 존재를 1 또는 0으로 설정할 수 있다.기계가 TAS 명령어를 지원시, 부울 변수 lock을 false로 초기화하여 상호배제를 구현할 수 있다.//lock을 사용한 상호 배제do&amp;#123;    while (TestAndSet(&amp;#38;lock));     // lock을 검사하여 true이면 대기, false이면 임계 영역 진입    //임계 영역           lock = false;    // 다른 프로세스의 진입 허용의미로 lock을 false로    //나머지 영역&amp;#125; while (true);최초로 lock 변수를 검사하여 true로 초기화 되어있으면, 임계영역의 사용이 막혀있는 상태이며, false으로 초기화했으면, 임계 영역에 진입 가능하다.진입하면, TAS 명령어로 lock의 값이 true가 되므로, 다른 프로세스는 임계영역에 접근하지 못하고 while문에서 무한 대기하게 된다.이후 임계영역에서의 작업이 끝나면 lock을 false로 재설정하여 다른 프로세스가 접근가능하다.하지만 프로세스가 3개 이상일 경우, 무한대기 상태로 빠질 수 있으며, 이를 막기 위해 아래와 같은 코드로 상호배제를 구현할 수 있다.boolean waiting[n]; // flag들의 모임 배열boolean lock = false;int j; // 0...n-1boolean key;false로 초기화된 공유 배열인 waiting[0...n-1]로 선언한다. 공유 부울 변수 lock은 모든 프로세스에 전역변수로 선언하여 상호배제를 구현할 수 있다.// TestAndSet 명령어를 이용한 상호배제do                               // 프로세스 Pi의 진입 영역&amp;#123;    waiting[i] = true;    key = true;    while (waiting[i] &amp;#38;&amp;#38; key)         key = TestAndSet(&amp;#38;lock);    waiting[i] = false;        // 임계 영역        // 탈출 영역    j = (i+1)&amp;#37;n;    while ((j!=i)&amp;#38;&amp;#38; !waiting[j]) // 대기 중인 프로세스를 찾음        j = (j+1)&amp;#37;n;    if (j == i)                  // 대기 중인 프로세를 없으면        lock = false;            // 다른 프로세스의 진입 허용    else                         // 대기 프로세스가 있으면 다음 순서로 임계영역 진입        waiting[j] = false;      // Pj가 임계 영역에 진입할 수 있도록&amp;#125; while (true);프로세스 i가 임계영역을 진입하기 위해서는 자신의 flag waiting[i]를 true로 놓은 뒤, TAS를 통해 key를 가져오려 시도한다. 초기에 key 변수를 false로 초기화했으므로, key 값은 false로 변하고 lock 값은 true가 되어 while문에서 탈출하게 되고, 임계 영역에 들어서게 된다.  만약 다른 프로세서가 사용하고 있다면 key 변수는 true일 것이고, while문을 통과하지 못하고 바쁜 대기 상태에 빠질 것이다.이후 임계 영역을을 지난뒤, 자신보다 우선순위가 낮은 프로세스 중 우선순위가 높은 순서대로 waiting[j] 플래그가 true인 프로세스를 찾는다.      찾게 된다면, 해당 프로세스에게 넘겨주기 위해 waiting[j] 플래그를 false로 바꾼다. 그럴 경우 key값과 관계없이 해당 프로세스는 임계영역에 들어가게 될 것이다.        만약 없다면, 한바퀴 돌아 자기 자신의 번호인 i를 가르키게 되며, 다른 프로세스의 진입을 허용하기 위해 lock 변수를 false로 설정한다.              장단      TestAndSet 명령어의 장단                  장점      사용자 수준에서 가능        - 메인 메모리를 공유하는 다중, 단일 프로세서 환경에서 프로세스 수에 관계없음- lock 변수 수에 상관없이 구현- 구현이 단순하고 확인이 용이- 다중 임계 영역을 지원한다.              단점      - 바쁜 대기 발생 (대기 시간 증가, 비생산적 자원 소모)- 기아 상태 발생 : 프로세스가 임계 영역을 떠날 때 프로세스 하나 이상을 대기하는 경우 가능하다.- 교착 상태 발생 : 우선순위가 낮은 프로세스가 lock을 풀 때, 우선 순위가 더 낮은 프로세스로 넘겨주므로, 우선순위가 높은 프로세스는 무한정 바쁜 대기상태가 될 수 있다.      세마포앞선 상호배제 해결 방법들은 일반화가 어렵고 바쁜 대기로 인한 자원낭비가 생긴다. 이를 막기 위해 다익스트라가 세마포(semaphore)를 제안하여 해결하였다.3.1 세마포 개념과 동작세마포는 값이 음이 아닌 정수인 플래그 변수이다. 세마포는 흔히 열차 차단기에 비유되며,      세마포값이 true면 (차단기가 내려가면), 임계 영역을 진입할 수 있고,(열차가 지나갈 수 있음)        세마포값이 false면 (차단기가 올라가면), 임계 영역을 진입할 수 없다(열차가 지나갈 수 없음)  세마포는 P와 V라고 불리우는 연산과 세마포를 의미하는 정수 변수 S로 동작하며, 이를 이용해 임계 영역이나 스케줄링 제약 조건을 시행할 수 있다. 코드로 표현하자면 아래와 같다.// 프로세스를 대기하게 하는 wait 동작, 임계 영역에 진입하는 연산P(S) : wait(S) &amp;#123; // 보통 S(세마포 = 1로 초기)       while S &amp;#38;#60;= 0; // 바쁜 대기, S &amp;#38;#62; 0 때까지 대기, 나중에 바쁜대기를 없앤 코드 나옴    S--;&amp;#125;// 대기 중인 프로세스를 깨우려고 신호를 보내는 signal 동작, 임계 영역에 나오는 연산V(S) : signal(S) &amp;#123;    S++; // 다른 프로세스의 접근 허용&amp;#125;S에 대한 P 연산은 S 값을 검사하여 양수이면 1을 감소시키는 과정이며, S에 대한 V 연산은 S를 1만큼 증가시킨다.P와 V 연산을 종료할 때까지는 다른 프로세스가 두 연산을 수행하지 못하며, 중간에 인터럽트가 존재하면 안된다.관례적으로 양의 값일 경우는 사용가능하다는 의미이다. 양수 n으로 초기화하면 최대 n개의 프로세스가 동시에 임계 영역을 사용할 수 있다. 즉, 세마포가 0일 때는 임계 영역이 사용을 금지 중이거나 사용 중이라는 의미이며, 음의 값은 존재하지 않는다.세마포(아래 코드의 mutex 변수)를 공유함으로, 1개의 임계 영역에 대한 n개의 프로세스 문제도 해결할 수 있다.do &amp;#123;    wait(mutex);        // 임계 영역    signal(mutex);        // 나머지 영역&amp;#125; while(1);세마포는 동기화하는 데 사용할 수 있으며, 예를 들어 두 프로세스가 각각 명령 $S_1, S_2$를 실행해야 하고, 반드시 명령 $S_1$ 뒤에 명령 $S_2$ 순서로 수행되어야 한다면 아래와 같은 코드를 사용하면 된다.// 첫번째 프로세스 동작// 세마포 synch는 0으로 초기화S1();signal(synch)// 두번째 프로세스 동작wait(synch);S2();위와 같은 코드를 이용한다면 두번째 프로세스가 먼저 동작하여도 wait(synch)에서 세마포어 synch값이 0으로 시작되므로 S2() 함수가 시작되지 않고 바쁜 대기 상태에 빠지며, 첫번째  프로세스의 signal(synch)이 실행된 뒤부터 진행할 수 있다.3.2 세마포의 종류세마포에는 계수(counting) 세마포와 이진(binary) 세마포가 있으며, 계수 세마포는 생산자-소비자 문제처럼 상호배제와 조건부 동기화를 해결하기 위해 설계 되었으며, 이진 세마포는 임계 영역처럼 특별히 상호배제를 해결하려고 설계했다.이진 세마포이진 세마포에서는 세마포 S를 상호배제에 사용하고, 1 또는 0으로 초기화하며, S를 검사해 양수이면 S를 0으로 재설정하거나 아니면 S를 준비큐로 되돌리는 P 연산과 S를 1로 초기화하며 준비 큐에 있는 프로세스를 시작하는 V 연산을 교대로 실행한다.앞서 설명했던 세마포와 마찬가지로 S를 0으로 초기화했을 경우, signal() 혹은 P 연산을 먼저 시작하기 전에 사용하지 못하게 막을 수 있다.계수 세마포계수 세마포를 이용해 유한한 자원에 접근할 때, 여러번 자원을 획득하거나 해제할 수 있도록 count 변수를 이용한다. count는 초기의 세마포 수로 초기화한다.즉, 이진 세마포와 달리 0과 1로 이루어져있지 않다.(그림 4-20)각 프로세스가 자원을 사용하려면 S(count)를 감소시켜야 하며, 반대로 해제할 때는 증가시킨다.  count가 0이면 사용할 수 없거나 금지된 상태이며, 공유 가능한 세마포의 수는 count의 값과 같다.3.3 세마포의 구현앞서 구현했었던 세마포의 단점 중 하나는 바쁜 대기이다. 이는 프로세서 시간을 낭비하며, 코드를 수정하면 극복할 수 있다.while문 대신 프로세스를 중단하고, 준비 큐에 배치해 대기 상태로 바꾼 뒤, 재실행하는 코드를 이용하면 바쁜 대기 상태가 없는 세마포를 구현할 수 있다.이때, 프로세스 중단 및 준비 큐 배치는 스스로 가능하지만, 프로세스 재개는 세마포를 반환하는 프로세스가 signal 연산을 실행해줘야한다. 이를 코드로 구현하면 다음과 같다.struct semaphore &amp;#123;     int count; // 사용 가능한 세마포어의     queueType queue; // 해당 세마포어를 기다리는 프로세스들의 준비 큐&amp;#125;;semaphore S; //wait 연산(=P 연산) 구현wait(S) &amp;#123;    S -&amp;#38;#62; count--;    if (S-&amp;#38;#62; count &amp;#38;#60;0)&amp;#123;        add this process to S -&amp;#38;#62; queue;  // 프로세스를 준비 큐에 추가        block();                         // 프로세스 중단(일시정지)    &amp;#125;&amp;#125;//signal 연산(=V 연산) 구현signal(S) &amp;#123;    S -&amp;#38;#62; count++;    if (S-&amp;#38;#62; count &amp;#38;#60;= 0) &amp;#123;        remove a process P from S -&amp;#38;#62; queue;// 준비 큐에서 P 프로세스를 제거        wakeup(P);                         // 신호를 보내 프로세스를 실행    &amp;#125;&amp;#125;wait() 연산을 실행하게 되면, 세마포의 S 값이 줄어들며, 만약 사용 불가였으면 프로세스가 중단된다.따라서 특이하게, 기존의 바쁜 대기 세마포와 달리 개선된 세마포의 S-&gt;count 값이 음수가 될 수 있으며, 음수의 값을 통해 세마포에서 기다리는 프로세스 수를 의미한다.실제 프로세스 준비 큐는 프로세스 제어 블록(PCB)의 링크 필드 정보를 이용해 구현한다.또한 세마포의 연산들은 중간에 끼어들 수 없도록 원자적으로 수행되어야 한다. 이를 해결하기 위해 두 가지 방법을 이용할 수 있다.      단일 프로세서 환경에서는 wait와 signal 연산 수행 중에 인터럽트를 금지시킨다.        다중 프로세서 환경에서 모든 프로세서의 인터럽트를 금지시키면 성능이 크게 떨어지므로, 바쁜 대기 현상을 완전히 제거할 수 없으며, 응용 프로그램 진입 영역에서 임계 영역까지 바쁜 대기를 제거한다. 만약 응용 프로그램의 임계 영역이 너무 길면 성능이 매우 떨어질 수 있다.  다만 이렇게 구현한 세마포는 프로세스 하나가 한 세마포의 준비 큐에만 대기할 수 있으므로, 두 프로세스가 자원을 하나씩 점유하고 상대방이 점유한 자원을 대기하는 교착상태를 유발할 수 있다.모니터(monitor)세마포는 강력한 상호배제 도구지만, wait 연산과 signal 연산의 순서를 엄격하게 제한해서 실행해야 교착상태가 발생하지 않는다.하지만 이러한 연산은 프로그램 전체에 퍼져있고, 복잡하기 때문에 세마포를 잘못쓰면 오류가 많이 생겨나게 되므로, 이를 보완하기 위해 모니터가 등장했다.모니터의 개념과 구조모니터는 공유 자원과 이것의 임계 영역을 관리하는 소프트웨어 구성체로, 사용자 사이에서 통신하려고 동기화하고, 자원에 배타적으로 접근할 수 있도록 프로세스가 사용하는 병행 프로그래밍 구조이다.모니터는 공유 데이터, 임계 영역이 코딩된 프로시저, 초기화 코드로 구성되어 있다. 데이터 정의와 프로시저의 독점적 제어가 모두 포함되어 있다.      초기화 코드 : 모니터 생성시 이용하는 코드        공유 데이터 변수 : 모니터 내부의 프로시저로 접근 가능한 임계 자원        프로시저 : 동시에 한 프로세스만 접근 가능한 임계 영역이 코딩된 프로시저  프로시저를 점유 중일 때, 접근에 실패한 다른 프로세스들은 차단된 뒤, 준비 큐에서 진입을 기다리게 하여 상호배제를 실현한다. 덕분에 동기화 제약 조건을 명시적으로 작성할 필요가 없어, 상대적으로 제어하기 더욱 쉽다.조건 변수가 있는 모니터의 구조모니터의 동기화 방법은 프로세스 특성에 따라 직접 추가해줘야 한다. 예를 들어, 생산자-소비자 프로세스는 공유 버퍼가 비어 있거나 가득차 있으면 프로세스를 대기시켜야 한다.이렇게 프로세스가 대기해야할 조건에 대한 변수를 조건변수라고 한다.조건 변수는 보통 하나 이상이며 조건변수 x, y는 모니터 안에 공유 데이터 변수와 별개의 영역에 연관된 큐와 함께 놓인다.      조건변수가 하나 이상이면 마찬가지로 하나 이상의 프로세스가 모니터 내부에 존재하게 할 수 있으며, 준비 큐도 하나 이상일 수 있다.        보통 준비 큐는 선입 선출 큐를 이용하지만 이 또한 다양한 스케줄링 알고리즘으로 대체할 수 있다.  x.wait 연산은 특정 조건이 true이면 프로세스의 실행을 중단 혹은 차단한다.특정 조건이 false 이고, 어떤 프로세스가 x.signal 연산을 호출하면 프로세스 준비 큐에 중단되어 있던 다른 프로세스의 실행을 재개한다.title: 왜 굳이 조건이 바뀌면 실행하지 않고 다른 프로세스의 x.signal 연산을 기다리는가?준비 큐 내의 프로세스, 혹은 모니터가 특정 조건이 false가 되는 순간을 감시하게 만들면, 바쁜 대기가 일어날 것이다.따라서, 점유하고 있던 프로세스가 준비 큐 내의 프로세스를 깨우는 x.signal 연산을 기다리게 하는 것이다.점유 중인 프로세스 P가 대기 중인 프로세스 Q에 모니터를 넘긴 후, 즉 P가 signal() 연산을 한 후에 프로세스 P는 대기해야 하는데, 두 가지 동작의 방법이 존재한다.title: 왜 P는 Q를 대기해야 하는가?프로세스의 작업이 끝나지 않았기 때문이다. P의 작업이 끝나지 않았음에도 Q에게 프로세스를 넘기는 이유는 시분할에 의한 할당량 종료, 입출력 대기, 사용자의 요청, 조건변수의 변화 등 다양하며, 이는 작업이 완전 끝나고 넘기는 경우(=대기할 필요가 없는 경우)보다 빈도가 잦다.      점유 중인 프로세스 P가 대기 중인 프로세스 Q에게 모니터를 넘긴 후, Q가 모니터를 떠나거나(준비 큐로 돌아가거나) 다른 조건으로 변할 때까지(Q 또한 모니터를 넘기는(signal()) 대신 이전에 wait()을 부를 수 있다.) 대기한다. (즉, 프로세스 P가 즉시 멈추고 준비큐로 돌아가는 것?)        대기 중이던 프로세스 Q가 점유 중이던 프로세스 P가 모니터를 떠나거나(준비 큐로 돌아가거나) 다른 조건으로 변할 때까지 대기한다. (즉, 프로세스 P가 signal() 이후 계속 모니터를 점유하는것?)  둘다 장단점이 있으며, 이 둘을 절충한,  x.signal 연산을 보낸 점유가 끝난 프로세스 P는 모니터에 즉시 나가는 방법도 존재한다.모니터와 세마포 비교모니터의 조건 변수 x에 대해 x.wait와 x.signal 연산은 세마포의 P와 V 연산과 비슷하지만 차이점 두 가지를 가지고 있는데,      세마포의 P 연산은 언제나 차단시키지 않고 세마포가 0보다 클 경우, 자원이 충분하므로 대기하지 않고 실행되지만, x.wait 연산은 반드시 차단시킨다.        세마포의 V 연산은 무조건 세마포의 크기를 증가시키고, 대기 작업을 확인해 넘겨주지만 x.signal은 대기중인 작업이 없으면 아무런 효과가 없다, 즉 조건변수 x(보통은 호출 가능한 자원의 수)에 영향을 주지 않는다.  또한, 세마포는 세마포의 크기가 V 연산으로 인해 커지면 즉시 다음 프로세스가 실행되지만, 모니터는 조건 변수 x가 충족이 되어도 점유 중이던 프로세스가 x.signal을 명시적으로 실행해야 다음 프로세스가 실행되므로, 병렬 프로그래밍에서 오류가 적고 쉽게 작성 가능하다.하지만 모니터는 프로그래밍 언어로 구현하는 부분이 필요하며, 컴파일러가 관여하므로, 컴파일러가 운영체제의 임계 영역에 접근할 수 있어야 하므로, 자바, C# 같은 일부 언어만 지원한다.                   모니터      세마포                  모니터 x.wait VS 세마포 P      조건변수 x와 관계없이 무조건 프로세스 대기      P연산 중이여도, 세마포가 0보다 크면 프로세스 실행됨              모니터 x.signal VS 세마포 V      대기 중인 프로세스가 없으면 조건 변수 x에 변동 없음      대기중인 프로세스가 없어도 세마포의 수가 증가      세마포를 이용한 모니터의 구현모니터마다 1로 초기화된 세마포 mutex를 사용하며, 포르세스는 진입하기전 P(mutex) 혹은 wait(mutex) 연산을 실행해야하며, 모니터를 떠나려면 V(mutex) 혹은 signal(mutex)로 양도해야한다.signal을 보내는 프로세스는 재개된 프로세스가 떠나거나 대기(wait)할 때까지 기다려야 하므로  next라는 이진 세마포를 생성하고 0으로 초기화한다. (세마포가 0이면 사용불가 혹은 대기 상태임을 상기하자.)semaphore mutex; // 1로 초기화semaphore next; // 0으로 초기화int next_count = 0;// 프로세스 외부 프로시저wait(mutex); // mutex 세마포어가 1이면 진입, 0이면 대기    ...    // 프로시저(F)의 작업영역    ...if (next_count &amp;#38;#62; 0) // next_count == 중단된 프로세스 수    signal(next); //이진 세마포의 값을 1로 만들어 다음 프로세스에게 넘김else    signal(mutex);조건 변수 x에서 초기값이 0인 세마포 x_sem과 정수 변수 x_count를 사용하여 아래와 같이 x.wait와 x.signal 연산을 구현할 수 있다.semaphore x_sem; // 0으로 초기화int x_count = 0; // 조건 x의 준비 큐에서 기다리는 프로세스의 수//x.wait 연산x_count++;if (next_count &amp;#38;#62; 0)    signal(next);else    signal(mutex);wait(x_sem);x_count--;//x.signal 연산if (x_count &amp;#38;#62; 0) &amp;#123;    next_count++;    signal(x_sem);    wait(next);    next_count--;&amp;#125;중단된 프로세스 중에 고르는 것은 선입 선출로도 가능하지만, 조건 대기 구조나 우선순위 큐, 우선순위 번호를 명시해주는 방식으로도 가능하다.x.wait(c); // 우선순위 번호 할당, // x.signal 연산 시 c 값이 가장 작은 우선순위를 가진 프로세스를 실행아래는 단일 자원 할당 모니터의 예시이다.monitor resource_allocator &amp;#123;    condition_is_free;    int in_use = 0; // 0일 경우 자원이 사용가능한 상태    get_resource() &amp;#123;        if (in_use) // 만약, 자원이 사용 중이면            wait(is_free); // 프로세스 중단(일시정지)        in_use=1; // 자원이 사용중임을 표시    &amp;#125;    return_resource() &amp;#123;        in_use = 0; // 자원 사용 가능하게 변경        signal(is_free);   // 대기 중인 프로세스에 할당 허용(신호)       &amp;#125;&amp;#125;"
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%205-%EA%B5%90%EC%B0%A9%20%EC%83%81%ED%83%9C%EC%99%80%20%EA%B8%B0%EC%95%84%20%EC%83%81%ED%83%9C.html": {
    title: "OS 정리-Chap 5-교착 상태와 기아 상태",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%205-%EA%B5%90%EC%B0%A9%20%EC%83%81%ED%83%9C%EC%99%80%20%EA%B8%B0%EC%95%84%20%EC%83%81%ED%83%9C.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true5. 교착 상태와 기아 상태title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.교착 상태의 개념과 발생 원인1 교착 상태(deadlock)의 개념교착 상태 정의      시스템 자원에 요구가 뒤엉킨 상태로 여러 프로세스가 작업을 멈추고 사용하는 비공유 자원을 서로 기다리고 있는 상태        프로세스들이 임계 영역으로 진행할 수 있음을 나타내는 신호 같은 결코 일어나지 않을 사건을 서로 기다리는 상태  교착 상태의 해악교착 상태에 빠지면 작업이 정지되어 더 이상 진행하지 못하게 되며, 무한 대기, 기아 상태와 그 이상의 문제를 일으킨다.이를 해결하기 위해 운영자나 사용자가 작업을 교체하거나 종료하는 외부 간섭으로 해결해야 한다.교착 상태의 대두일괄 처리 시스템에서는 없었지만 대화식 시스템에서 동적 자원을 공유하여 자원의 사용률을 높이기 위해 병행 처리와 자원 공유를 하면서 생기는 부작용이다.프로세스는 자원 요청, 자원 사용, 자원 해제 순서로 자원을 사용하고 반납하는데, 자원의 요청과 해제는 시스템 호출로 하며 이때 운영체제가 관여할 수 있으며, 이 두 가지를 잘 관리하면 교착 상태를 막을 수 있다.교착 상태의 그림 예시title: A와 B 프로세스는 서로가 가진 자원을 할당받을 때까지 작업이 중지되게 된다.교차로에서 교통 마비의 예시, 일부 자동차를 후진하거나 네 줄의 차량 줄 중 한 줄을 없애야 한다.2 교착 상태의 예      스풀링 시스템에서 발생하는 교착 상태    할당된 스풀 공간의 출력을 완료하지 않은 상태에서 다른 작업이 스풀 공간을 모두 차지하면 교착 상태가 발생한다.스풀링 파일의 일정 포화 임계치(saturation threshold)를 설정하여 교착 상태를 예방할 수 있다. 대신 그만큼 시스템의 처리량은 줄어들 것이다.        디스크를 공유할 때 발생하는 교착 상태    디스크 사용에 제어가 없으면 프로세스들이 서로 충돌하는 명령을 요청할 때 교착 상태가 발생한다. 예를 들어                  프로세스 A가 디스크의 실린더 A를 읽으려하고, 프로세스가 실린더 B를 읽으려는 상황                    프로세스 A가 디스크 헤더를 실린더 A로 옮기고 일시 정지한 뒤, 다음 프로세스 B에 넘겨준다.                    프로세스 B에 의해 디스크 헤더가 실린더 B로 옮겨지고, 프로세스를 넘겨준다.                    프로세스 A는 실린더 A의 내용을 읽으려 했으나 디스크 헤더가 옮겨졌으므로, 다시 한번 2번~3번 과정을 무한하게 반복하게 된다.                  네트워크에서 발생하는 교착 상태    입출력 버퍼 공간이 부족한 네트워크 시스템에 메시지 흐름을 제어하는 적절한 프로토콜이 필요하다.  3 교착 상태의 발생 조건교착 상태는 다음과 같은 네가지 조건을 만족할 때 반드시 발생한다. 정확히는 순환 대기를 제외한 나머지 조건을 만족하면, 순환 대기가 생길 가능성이 생기고, 순환 대기는 점유와 대기 조건을 포함한 교착 상태이다.상호 배제(Mutual exclusion)한 번에 프로세스 하나만 해당 자원을 사용할 수 있음. 사용 중인 자원을 다른 프로세스가 사용하려면 요청한 자원이 해제될 때까지 기다려야 함.점유와 대기(Hold and wait)자원을 최소한 하나 정도는 보유하고, 다른 프로세스에 할당된 자원을 얻으려고 기다리는 프로세스가 있음.비선점(No preemption)자원은 선점, 즉 강제로 빼앗을 수 없고, 자원을 점유하고 있는 프로세스가 끝나야 해제된다.순환(환형) 대기(Circular wait)그림 5-3과 같이 프로세스들이 각자 다른 프로세스가 점유하고 있는 자원을 얻으려 기다리는 것을 의미한다.돌 하나에 한 사람만 디딜 수 있는 징검 다리 위를 두 사람이 마주친다면, 서로 상대방이 딛고 있는 돌에 발을 디딜 수 없으므로, 교착상태가 발생할 것이다.이 상황을 앞서 배운 교착 상태의 조건에 비유해보자면,      돌 하나를 한 사람만 디딜 수 잇음 👉 상호 배제        각자 돌 하나를 딛고 상대 방의 돌을 요구하고 있음 👉 점유와 대기        사람이 딛고 있는 발을 강제로 들어올리게 할 수 없음 👉 비선점        왼쪽에 오는 사람은 오른쪽 사람의 디딤돌을 요구하고, 오른쪽의 사람은 왼쪽 사람의 디딤돌을 요구함 👉 순환 대기 조건  이때의 해결 방법은 세가지가 있을 것이다.      둘 중 한 사람이 뒤로 돌아 되돌아 간다.        징검다리를 가기 전에 먼저 반대편에 사람이 있는지 확인하고 출발        한쪽에 먼저 갈 수 있는 우선순위를 둔다.  title: 선점 자원과 비선점 자원선점 자원 : 부작용 없이 소유한 프로세스에서 빼앗을 수 있는 자원. 메모리, 버퍼, 프로세서 등이 있다.비선점 자원: 부작용 없이 다른 프로세스에 할당할 수 없는 자원. 프린터, CD 드라이브, 스캐너, 디스크 드라이브, 임계 영역 등이 있다.교착 상태는 비선점 자원이 발생 시킨다.4 교착 상태의 표현교착 상태는 방향 그래프로 표현된 시스템 자원 할당 그래프으로 쉽게 파악할 수 있다.정집 집합는 프로세스 집합과 자원 집합으로 나뉘며, 간선 집합은 자원 할당선으로 나타낸다.그래프는 그림 5-4와 같이 표현된다.            프로세스 집합      자원 집합      할당 간선 집합                  ${P_1,P_2,P_3}$      ${R_1,R_2,R_3,R_4}$      ${P_1\\rightarrow R_1,P_2\\rightarrow R_3,R_1\\rightarrow P_2,R_2\\rightarrow P_2,\\ R_2\\rightarrow P_2,R_3\\rightarrow P_3}$      프로세스 상태      프로세스 $P_1$은 자원 $R_2$의 자원을 하나 점유하고, 자원 $R_1$을 기다린다.        프로세스 $P_2$는 자원 $R_1$과 $R_2$의 자원을 각각 하나씩 점유하고, 자원 $R_3$을 기다린다.        프로세스 $P_3$은 자원 $R_3$의 자원 하나를 점유 중이다.  자원 할당 그래프에서는 그래프에 사이클이 없다면 교착 상태가 발생하지 않으며, 사이클이 있다면 발생할 가능성이 존재할 수 있다. 만약 사이클이 존재하고 각 자원에는 단 하나의 자원만 존재한다면 교착상태가 발생한다.예를 들어 위의 그림 5-6에서는 총 2개의 사이클이 있다.      ① : $P_1\\rightarrow R_1\\rightarrow P_2\\rightarrow R_3\\rightarrow P_3\\rightarrow R_2\\rightarrow P_1$        ② : $P_2\\rightarrow R_3\\rightarrow P_3\\rightarrow R_2 \\rightarrow P_2$  이 두 사이클 모두 서로의 자원 얻으려고 기다리는 교착 상태이다.  $P_1\\rightarrow R_1\\rightarrow P_3\\rightarrow R_2 \\rightarrow P_1$위 그림은 사이클이 존재하지만 프로세스 $P_4$가 작업이 끝나면 $P_3$가 자원을 할당받아 차례대로 대기 상태를 벗어나므로 교착상태가 아니다.교착 상태의 해결 방법교착 상태의 해결 방법은 크게 3가지로 나눈다.      교착 상태가 발생하지 않도록 예방(prevention)하는 방법        교착 상태의 발생 가능성을 배제하지 않고 이를 적절히 회피(avoidance)하는 방법        교착 상태를 허용하되 교착 상태를 탐지(detection)하여 다시 회복하는 방법  1 교착 상태 예방앞서 설명한 교착 상태의 네가지 조건이 하나라도 발생하지 않는다면 교착 상태는 예방된다. 다만, 상호배제를 제외할 경우 자원을 효율적으로 쓰는게 불가능하므로, 다음 세가지 교착 상태 예방 방법이 있다.      각 프로세스는 필요한 자원을 한 번에 모두 요청해야하며, 요청한 자원을 모두 제공받기 전까지는 작업을 진행할 수 없다.        어떤 자원을 점유하고 있는 프로세스의 요청을 더 이상 허용하지 않으면 점유한 자원을 모두 반납하고 필요할 때 다시 자원을 요청해야 한다.        모든 프로세스에 자원을 순서대로 할당해야 한다. 모든 프로세스에 각 자원 유형별로 할당 순서를 부여한 후 순서에 따라 자원을 요청하게 한다.  먼저, 상호배제를 포함해 모든 교착 상태 조건을 예방하는 방법을 하나씩 알아보겠다.1.1 자원의 상호배제 조건 방지상호배제는 프린터, 디스크 같은 동시에 사용 불가능한 비공유 자원을 대상으로 한다.파일 읽기 같은 공유 자원의 경우 배타적인 접근이 필요없고 동시에 사용 가능하므로 교착 상태가 발생하지 않는다.하지만 상호배제의 완벽한 예방은 사실상 불가능한데, 파일 쓰기 처럼 배타적인 접근을 허용하지 않고 공유하면 교착 상태가 발생하는 경우도 있다. 즉, 상호배제는 일부 자원에만 교착 상태를 유발하며, 일부 자원의 경우 오히려 상호배제가 없으면 교착 상태가 생긴다.1.2 점유와 대기 조건 방지점유와 대기 조건을 방지하려면 두 가지 방법이 있는데 이를 DVD 드라이브에서 디스크 파일을 데이터를 복사, 정렬, 인쇄 후 테이프 드라이브에 복사하는 프로세스의 경우를 예시로 들어 보자.      프로세스가 작업을 수행하기 전에 필요한 자원을 모두 요청하고 시스템이 허용될 때 모두 획득하게(이를 최대 자원 할당 상태라고 한다.) 하여 프로세스가 대기 상태에 자원을 점유할 수 없게 한다.          프로세스가 시작할 때, DVD 드라이브와 디스크 파일, 프린터, 테이프 드라이브를 모두 요청하고 작업이 끝날 때까지 가지고 있는다.            프로세스가 자원을 전혀 갖고 있지 않을 때, 혹은 자원을 완전히 반납했을 때만 자원을 요청할 수 있도록 허용한다.                  프로세스가 처음에 DVD 드라이브와 디스크 파일을 요청해서 데이터를 복사 및 정렬                    프로세스가 작업이 끝나고  DVD 드라이브와 디스크 파일을 모두 해제한다.                    프로세스가 다시 DVD 드라이브와 프린터를 요청한 뒤, 데이터를 인쇄                    DVD 드라이브와 프린터를 모두 해제                    디스크 파일과 테이프 드라이브를 요청해서 파일을 테이프에 복사                    디스크 파일과 테이프 드라이브를 모두 해제            위 두 방법은 모두 동일한 단점을 가지고 있다.      자원 효율성이 매우 낮음, 첫번째 방법은 테이프 드라이브 같은 처음부터 사용하지 않는 자원을 모든 작업 기간동안 점유한다. 두번째 방법은 계속 사용하는 DVD 드라이브를 계속 해제 및 요청하면서 대기 상태와 오버헤드를 유발한다.        기아 상태가 발생할 수 있다. 다른 프로세스가 하나라도 자원을 점유하고 있으면, 당장 작업이 가능한 다른 자원들을 확보해도 대기 상태를 유지한다. 또 적은 수의 자원을 요청한 프로세스가 지나치게 우선순위가 높아져, 대화식 시스템에서는 사용할 수 없다.  1.3 비선점 조건 방지비선점 조건 방지를 위한 3가지 방법이다.      다른 자원을 요청할 때 요청한 자원을 즉시 받을 수 없다면, 가진 모든 자원을 해제하고 필요한 모든 자원을 요청할 수 있을 때까지 대기 후 요청.          실행한 작업의 상태를 잃을 수 있으며, 작업 상태를 쉽게 저장, 복구 할 수 있는 상황에만 좋다.            프로세스가 어떤 자원을 요청시 자원의 상태 검사                  1-1. 만약 요청 자원이 사용 가능시 바로 할당                    1-2. 자원이 사용 불가능하면 해당 자원을 가지고 있는 프로세스의 상태 확인                              1-2-1. 점유 프로세스가 대기 상태라면, 점유를 해제하고 요구 프로세스에게 할당                                1-2-2. 점유 프로세스가 실행 상태거나 기타 오류 등의 이유라면 요청 프로세스가 대기                          1-2-2-1. 대기 상태 요청 프로세스가 점유한 자원을 누가 요청하면 자원을 해제                                                두 프로세스에 우선순위를 부여 후, 높은 우선순위 프로세스가 비교적 낮은 우선순위 프로세스의 점유 자원을 선점          레지스터같이 복원과 저장이 쉬운 자원에 적용하기 좋다.      1.4 순환(환형) 대기 조건 방지모든 자원에 일련의 순서를 부여하고 각 프로세스가 오름차순으로만 자원을 요청하게 함순환 대기 조건 방지의 규칙은 다음과 같다.      각 프로세스는 오름차순으로만 자원들을 요청        프로세스가 자원 R_j를 요청할 때마다 점유하고 있는 j보다 높은 자원들 i &gt; j을 모두 해제해야 한다.  단점으로      순서와 맞지않는 작업 순으로 할당해야할 때 자원 낭비가 심해진다.        작업의 순서를 실제 자원 순서를 반영해야 낭비가 적어지는데, 예측이 힘들다        최신 운영체제는 사용자를 위해 편리한 작업환경을 위해 제약이 적어야 하는데, 작업 순서가 제약이 됨  자원 집합 $R={R_1, R_2,\\dots, R_n}$에 자연수 결과 함수를 이용해 다음과 같은 예시로 정의한다.F(CD 드라이브) = 2,F(디스크 드라이브) = 4,F(프린터) = 7만약 CD 드라이브와 프린터를 요청하려면, CD 드라이브를 먼저 요청해 할당 받고, 프린터를 요청해 할당받아야 한다.2 교착 상태 회피앞서 배운 예방 방법들은 효율성이 크게 떨어지는 문제점이 있었고, 교착 상태 회피는 덜 엄격한 조건을 요구하여 자원을 좀 더 효율적으로 사용하는 것이 목적이다.예방보다는 회피가 좀더 병행성을 허용하며, 조건을 느슨하게 허용하여 효율적인 편이다.      프로세스의 시작 중단    프로세스의 요구가 교착 상태를 발생시킬 수 있다면 프로세스 시작을 중단하는 방법    현재 수행 중인 모든 프로세스의 최대 자원 요청량과 새로운 프로세스의 최대 요청량을 합한 자원 요청량을 수용할 수 있으면 수용하는 방식.        자원 할당 거부    프로세스가 요청한 자원을 할당 시 교착 상태가 예상된다면 할당 취소, 은행원 알고리즘이라고도 한다.  2.1 프로세스의 시작 중단교착 상태를 회피할 수 있는 자원 할당의 이상적인 방법은 앞으로 할당된 자원의 순서와 사용가능한 자원, 사용하고 있는 자원등을 미리 파악하고 있는 것이다.이를 위한 가장 간단한 알고리즘은 각 프로세스가 필요한 자원의 최대치(할당 가능한 자원 수)를 미리 선언하여 시스템이 미리 파악하고, 교착 상태가 되지 않도록 할당하는 것이다.시스템은 자원 할당 사태를 사용 가능 자원 수, 할당된 자원 수, 프로세스들의 최대 요청 수로 정의하여 순환 대기 조건이 발생하지 않도록 검사하고 할당한다.자원 할당 상태안정 상태 : 각 프로세스에 최대치까지 자원을 할당할 수 있고, 안정 순서에 따라 할당할 수 있어 교착 상태를 예방할 수 있는 상태      안정 순서 : 프로세스들이 요청한 작업 할당 순서에 맞추어 모든 자원 요구를 충족할 수 있는 상태                  정확히는 프로세스 순서 $&lt;P_1, P_2,\\cdots,P_n&gt;$이 안정순서라면, 모든 $P_i$가 요청하는 자원이 현재 사용 가능한 자원과 $j&gt;i$인 모든 $P_j$가 점유한 자원들로 충족할 수 있는 상태이다.                    만약 현재 사용 가능한 자원으로 부족하다면, $P_i$는 자신의 턴을 넘기고 다음 프로세스로 넘기며 순서대로 작업을 처리하고 자원을 해제하게 하다가 충분한 자원이 확보되면 작업을 처리한다.            불안정 상태 : 안정 순서가 없는 상태, 교착 상태가 발생할 가능성이 있는 상태.시스템 상태 변화 예시동일한 자원 10개와 $P_0,P_1,P_2,P_3$을 가진 시스템의 예시를 들어보자.title: 안정 상태의 자원 예            프로세스      현재 사용량($t_0$ 시간)      최대 사용량                  $P_0$      2      7              $P_1$      1      8              $P_2$      2      4              $P_3$      2      10              여분 자원 수             3      먼저 안정 상태에서 위 그래프와 같은 상태의 시스템의 예시를 보자면 $&lt;P_2,P_0,P_1,P_3&gt;$ 순서의 안정 조건을 만족한다.      $P_2$의 자원 2개를 추가로 할당 받으면 $P_2$의 최대 사용량을 만족하므로 작업 실행 후 반납        여분 자원 수가 5개가 되므로, $P_0$가 자원 5개를 받아 실행한 후 자원 7개를 반납        $P_1$이 자원 7개를 할당 받고 실행하여 자원 8개를 반납        $P_3$도 자원 8개를 받아 실행한 후 자원 10개를 반납, 모든 작업 완료  title: 불안정 상태의 자원 예            프로세스      현재 사용량($t_0$ 시간)      최대 사용량                  $P_0$      5      7              $P_1$      1      8              $P_2$      2      4              $P_3$      2      7              여분 자원 수             1      $P_0$의 요청에 따라 사용량을 3개를 추가해 불안정 상태 시스템을 만들어 보자.  이제 여분 자원 수 1개를 어디에 할당해도 최대 사용량에 도달하는게 불가능하므로, 작업이 끝나지 않는다. 즉, 모두 대기하는 교착 상태가 유지된다.위의 두 예시를 보았듯이 자원 요청을 어떻게 들어주냐에 따라 시스템 상태가 바뀌므로, 시스템은 언제나 자원 상태를 검사하고 확인하여 할당을 허용해야 한다.2.2 자원 할당 거부(은행원 알고리즘)은행원 알고리즘은 자원의 할당 여부를 결정하기 전에 미리 모든 자원의 최대 가능한 할당량을 시뮬레이션해 할당 시, 교착 상태, 안정 상태를 검사한다.각 프로세스는 얼마큼 자원 요청이 가능하고, 얼마큼 자원을 보유하고 있으며, 얼만큼 각 자원을 사용할 수 있는지 등 정보를 얻고, 이를 여러 자료 구조에 저장한다.은행원 알고리즘 구현에 필요한 자료구조자료 구조는 자원 할당 시스템의 상태를 나타내며, $n$을 시스템의 프로세스 수, $m$을 자원 수라 하면 다음과 같은 자료구조가 필요하다.      Available : 각 형태별로 사용 가능한 자원(사용 가능량)을 표시하는 길이가 m인 벡터          예를 들어 $Available[j]=k$는 자원을 최대 k개 사용할 수 있다는 의미            Max : 각 프로세스 자원의 최대 요청량(최대 요구량)을 표시하는 $n\\times m$ 행렬이다.          $Max[i,j]=k$이면 프로세스 $P_i$는 자원이 $R_j$인 자원을 최대 k개까지 요청할 수 잇다는 의미            Allocation : 각 프로세스에 할당되어 있는 각 형태의 자원 수(현재 할당량)을 정의하는 $n\\times m$ 행렬이다.          $Allocation[i,j]=k$이면, 프로세스 $P_i$는 자원이 $R_j$인 자원을 최대 k개 점유 중            Need : 각 프로세스에 남아 있는 자원 요청(추가 요구량)을 표시하는 $n\\times m$ 행렬이다.                  $Need[i,j]=k$이면, 프로세스 $P_i$는 자신의 작업을 종료하려고 자원 $R_j$를 k개 더 요청한다는 의미,                    $Need[i,j]=Max[i,j]-Allocation[i,j]$                  Request : 프로세스 $P_i$가 작업을 위한 자원 $R_j$를 요청하는 $n\\times m$ 벡터  은행원 알고리즘 구현에 필요한 제약 조건      시간이 흐르면서 벡터의 크기와 값이 변함 $\\rightarrow$ 자료구조가 수정 가능        X와 Y는 길이가 n인 벡터이다.        $X[i]\\leq Y[i]$이고, $i=1,2,\\cdots,n일 때만 X\\leq Y$        $X=(0,3,2,1)$이고 $Y=(1,7,3,2)$이면 $X\\leq Y$ $\\rightarrow$ 자료구조 크기 비교 정의        $X\\leq Y$이고 $X\\neq Y$이면 $X&lt;Y$  은행원 알고리즘 동작프로세스 $P_i$가 자원을 요청했을 시 아래와 같이 동작한다.      Request[i] &lt;= Need[i]이면 2단계로 이동하고, 아니면 프로세스가 최대 요청치를 초과하기 때문에 오류상태이다.        Request[i] &lt;= Available이면 3단계로 이동하고, 아니면 자원이 부족하므로 $P_i$는 대기한다.        시스템은 상태를 다음과 같이 수정하여 요청된 자원을 프로세스 $P_i$에 할당          이때, 안전 알고리즘에 의해 할당 여부를 결정한다.      Available = Available - Request[i];Allocation[i] = Allocation[i] + Request[i];Need[i] = Need[i] - Request[i]; 즉, 할당 시, 자원 할당이 안정 상태라면 프로세스 $P_i$는 자원을 할당 받고, 불안정 상태라면 $P_i$는 $Request[i]$를 대기하고 이전 자원 할당 상태로 복귀한다.이를 흐름 차트로 나타내면 아래 그림 5-8과 같다.안전 알고리즘은 시스템이 안정 상태인지, 불안정 상태인지 검사하며 다음과 같은 과정을 거친다.(그림 5-9)      Work와 Finish를 각각 길이가 m과 n인 벡터라고 하자. Work = Available, Finish[i]=false, i=1, 2, ..., n이 되도록 초기화        조건 Finish[i]==false, Need[i] &lt;= Work을 만족하는 i 값을 찾고 없으면 4단계로 이동        Work = Work + Allocation[i], Finish[i]=true를 수행하고 2단계로 이동        모든 i에 대하여 Finish[i]==true이면 시스템은 안정상태이며, 아닐 경우 불안정 상태이다.  안정 알고리즘의 예시프로세스가 $P_0\\sim P_4$까지 5개 존재하며, 자원 4 종류(A,B,C,D)가 있을 때, 다음과 같은 상태의 예시를 알아보자.title: 시간 $t_0$일 때 시스템의 상태 (자원 ABCD 순서)            프로세스      Allocation      Max      Need      Available                  $P_0$      2011      3214      1203      1222              $P_1$      0121      0252      0131                     $P_2$      4003      3105      1102                     $P_3$      0210      1530      1320                     $P_4$      1030      3033      2003                     할당량      7375                           이 시스템은 $&lt;P_2, P_0, P_4,P_1,P_3&gt;$ 순서를 통해 안정 조건을 가질 수 있다.      $P_2$를 실행한 후 할당된 자원을 해제하면 Available은 5225가 되고 해당 자원으로 다음 프로세서 $P_0$의 작업을 처리할 수 있다.        이런 식으로 마지막 순서까지 문제없이 실행 가능  하지만 만약 프로세스 $P_1$이 자원을 0101만큼 추가로 요청했고, 이를 시스템에서 받아들였을 경우, 어떠한 순서로도 프로세스들이 작업을 마칠 수 없게 되며 불안정 상태가 되게 된다.따라서 이를 막기 위해 안정 알고리즘으로 미리 상태를 시뮬레이션 해야한다.은행원 알고리즘의 단점      할당할 수 있는 총 자원을 파악하고 일정량을 요청한다. 자원은 수시로 바뀌므로 파악하기가 매우 어렵다.        사용자 수가 일정해야 하지만 다중 프로그래밍 시스템에서는 사용자 수가 항상 변한다.        교착 상태 회피 알고리즘을 실행하면 시스템 과부하가 증가한다.        프로세스는 자원을 보유한 상태로 끝낼 수 없다.        작업에 필요한 최대 필요량을 파악하기 힘들다. 최근 프로그램들은 최대 작업량을 정해놓지 않아 사용자의 편의성을 확보한다.        불안정 상태를 방지해야 하므로 자원 이용도가 낮다.  3 교착 상태 회복교착 상태 회복에는 교착 상태 탐지 알고리즘과 교착 상태 회복 알고리즘이 필요하다.탐지와 회복 방법은 필요한 정보를 유지하고 탐지와 회복에 비용이 많이든다. 하지만 탐지 알고리즘을 자주 실행하면 시스템 성능은 떨어지지만, 반대로 자주 실행하지 않으면 자원의 유휴 상태가 오래 남을 수 있다.3.1 교착 상태 탐지 알고리즘은행원 알고리즘과 비슷한 자료구조를 사용한다.      Available : 자원마다 사용 가능한 자원 수를 표시하는 길이가 m인 벡터        Allocation : 각 프로세스에 현재 할당된 각 행태들의 자원 수를 표시하는 $n\\times m$ 행렬        Request : 각 프로세스의 현재 요청을 표시하는 $n \\times m$ 행렬        Work와 Finish는 각각 길이가 m과 n인 벡터이며, Work=Available로 초기화하며, (i=1,2,...,n)일 때 Allocation[i] != 0이면 Finish[i]=false, 아니면 Finish[i]=true이다.        Finish[i]==false, Request[i]&lt;=Work 조건에 맞는 i를 찾고, 조건에 맞는 i가 없으면 4단계로 이동한다.        Work=Work+Allocation[i], Finish[i]=true 조건과 일치하는지 여부를 판단해 2단계로 이동한다.        Finish[i]==false라면, 1&lt;=i&lt;=n인 범위에서 시스템 전체와  프로세스 $P_i$은 교착 상태이다.  이를 의사 흐름 차트로 표현하면 그림 5-10과 같이 표현된다  아래 차트는 for loop를 모두 표현하면 지나치게 복잡해져서 간략하게 표현되었다.시스템을 아래 표와 같이 가정하여 알아보자.title: 시간 $t_0$일 때 시스템의 상태 (자원 ABC 순서)            프로세스      Allocation      Request      Available                  $P_0$      010      000      000              $P_1$      200      202                     $P_2$      303      000                     $P_3$      211      100                     $P_4$      002      002                     total                    726      위 상황에서 안전 순서 $&lt;P_0, P_2, P_3, P_1,P_4&gt;$를 통해 모든 i에 대해 Finish[i]=true인 안정 상태를 이룩할 수 있다.교착 상태 탐지 알고리즘은 교착 상태가 자주 발생할 수록 자주 호출해야 줘야 자원들의 유휴 상태를 막을 수 있지만, 너무 자주 부르면 연산 시간 부담이 크며, 특히 객체 수가 증가하면 기하급수적으로 증가하므로 보통 1시간 마다 또는 CPU 이용률이 40%로 떨어질 때마다 호출한다.3.2 교착 상태 회복 방법교착 상태에서 회복한다는 것은 순환 대기에서 벗어난다는 것이며, 단순하게 프로세스를 중단하는 방법과 교착 상태 프로세스들에게 자원을 선점하게 하는 방법이 있다.프로세스 중단중단 방법 또한 두 가지로 나뉘며, 둘다 시스템이 정지된 프로세스에 할당되어 있는 모든 자원의 해제를 요청하는 방법이다.      교착 상태 프로세스를 모두 중단 : 교착 상태의 순환 대기를 확실히 해결하지만 자원 사용과 시간 면에 비용이 많이 들며, 프로세스의 부분 작업물들이 날아간다.        한 프로세스씩 중단 : 한 프로세스를 중단할 때마다 교착 상태 탐지 알고리즘을 호출해 교착상태를 확인해가며 중단, 탐지 알고리즘을 호출하는 부담이 크다.  프로세스를 중간에 중단하는 것은 진행 중이던 작업이 취소되거나 날아가는 문제가 생길 수 있다. 또한 한 프로세스 씩이나 부분 중단 하는 방식은 어떤 프로세스를 중단 시킬지 결정하는데 비용이 든다.중단의 기준으로 프로세스의 우선 순위, 수행 시간과 필요 시간, 사용한 자원 형태, 필요한 자원 수, 대화식, 일괄식 여부 등의 정보를 기준으로 한다.자원 선점프로세스의 자원을 선점해서 교착 상태를 해결할 때까지 선점한 자원을 다른 프로세스에 할당하는 방법, 다음과 같은 세가지 사항을 해결해야 한다.      선점 자원 선택 : 프로세스를 종료할 때 비용을 최소화하려면 적절한 선점 순서를 결정, 중단 때와 같이 점유 자원 수, 실행 시간 등이 기준이다.        복귀 : 자원을 잃은 프로세스는 안정 상태로 복귀시키고 다시 시작해야하며, 보통 완전 중단시키고 재시작하며, 가능하면 교착 상태에서 벗어날 정도만 복귀 시킨다. 프로세스의 상태 정보를 유지해야 하는 부담이 있다.        기아 : 동일한 프로세스가 자원들을 항상 선점하지 않도록 비용 기반으로 희생자를 선택한다. 하지만 오랫동안 희생자가 되면 기아 상태가 되므로, 복귀 횟수를 따져가며 기아 상태로 만든다.  실제 운영체제는 앞서 배웠던 모든 방법을 결합하여 시스템 자원 형태마다 최적의 접근 방법을 채택한다.예를 들어 PCB 내부 자원은 자원 순서화, 메인 메모리 자원은 선점, 작업 파일 등은 회피 방법을 이용한다.기아 상태교착 상태는 자유로운 자원 할당에 의한 자원 부족 상태가 원인이라면, 기아 상태는 교착 상태를 예방을 하기 위한 조치들을 원인으로 프로세스가 자원을 영원히 혹은 아주 오랫동안  기다리는 상태이다.식사하는 철학자철학자(프로세스) 다섯명이 식사(작업)하거나 생각(대기)하는데 양 손에 두개의 포크(자원)이 필요하다. 자신의 양옆의 포크만 집을 수 있으며, 이는 모두 공유(경쟁 상태)한다. 그림 5-11 처럼 음식을 중앙으로 두개씩 배치되어있으므로, 최대 2명만 식사를 하고, 식사를 마치거나 중단하면 내려놓을 수 있다.모든 철학자가 동시에 왼쪽 포크를 집은 뒤, 오른쪽 포크를 집으려면, 모두 왼손에 하나만 들게되므로 모두 식사를 하지 못한다.(교착 상태) 이 상태를 해결하기 위한 방법은      포크를 더 많이 사던가(자원 증량)        최대 4명만 동시에 앉게 하거나 (교착 상태 예방)        철학자가 양쪽 포크를 조사해 둘다 사용 가능할 때만 집을 수 있도록 허용        비대칭 해결법 : 홀수 철학자는 왼쪽을 집은 뒤 오른쪽 포크 사용, 짝수는 반대로 사용        각 포크를 세마포로 표시하는 것이다.  semaphore fork[5]; // 1로 초기철학자는l P(wait) 연산 이후 포크를 집고, 사용한 뒤 V(signal) 연산 이후 내려놓는다.이를 코드로 표현하면 아래와 같다.title: 식사하는 철학자 문제에서 철학자 $i$의 구조while (true) {    wait(fork[i]);        // 왼쪽 포크를 집음    wait(fork[(i+1)%5]);  // 오른쪽 포크를 집음    ...    // 식사한다.    ...    signal(fork[i]);      // 왼쪽 포크를 내려놓음    signal(fork[(i+1)%5]);// 오른쪽 포크를 내려놓음    ...    // 생각한다.    ...}아래는 최대 4명만 동시에 앉게 하는 방법의 코드다.title: 식사하는 철학자 문제의 해결 방안semaphore fork[5] = {1};semaphore room = {4};int i;while (true){    wait(room); // 최대 4명의 자리를 얻을 수 있는지 확인하는 P 연산    wait(fork[i]);    wait(fork[(i+1)%5]);    ...    // 식사한다.    ...    signal(fork[(i+1)%5]);    signal(fork[i]);    signal(room);    ...    // 생각한다.    ...}교착 상태의 해결책은 기아 상태의 해결책이 아니므로, 기아 상태를 해결하기 위해 프로세스의 기다리는 시간을 조사 추적하고, 조치해야 한다. 하지만 이 또한 시스템 대기로 처리량이 감소할 수 있다."
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%206-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%20%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81.html": {
    title: "OS 정리-Chap 6-프로세스 스케줄링",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%206-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%20%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true6. 프로세스 스케줄링title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.스케줄링의 이해1 스케줄링의 개념스케줄링(scheduling)은 다음을 의미한다.      다중 프로그래밍 환경에서 프로세서를 할당할 프로세스를 선택할 때의 전략        여러 프로세스가 번갈아 사용하는 자원(보통 프로세서)을 어떤 시점에 어떤 프로세스에 할당할 지 결정하는 것  이를 통해 프로세서 이용률, 처리율 증가, 응답시간 감소 등의 성능적 이득을 얻을 수 있다.단일 처리 시스템에서는 스케줄링의 개념이 없어 입출력 등을 처리하기 위해 프로세서가 유휴 상태가 되어 비효율적인 경우가 많이 생긴다.스케줄링에 대상이 되는 프로세스는 자원의 공유 순서나 일정이 중요한 사용자 프로세스나 시스템 프로세스이며, 즉시 사전 처리되는 인터럽트 처리, 오류 처리, 사용자의 시스템 호출 등은 대상이 아니다.2 스케줄링의 목적스케줄링의 목적은 다음과 같다.      자원 할당의 공정성 보장 : 모든 프로세스가 공평하게, 무한한 연기 없이 처리됨        단위 시간당 처리량 최대화 : 단위시간당 유효 시간을 줄이고 프로세서의 처리량을 최대화하여 가능한 많은 프로세스에 서비스 제공        적절한 반환시간 보장 : 느려도 2~3초 내의 적절 시간 내에 응답 요구        예측 가능성 보장 : 작업을 시스템 부하와 관계 없이 유사한 시간과 비용에 실행 보장        오버헤드 최소화 : 오버헤드는 성능 향상을 위해 필요하지만 과하면 반대로 성능 감소        자원 사용의 균형 유지 : 모든 시스템 자원의 최대 활용을 위한 지원 제공        반환 시간과 자원의 활용 간에 균형 유지 : 반환 시간과 자원 활용은 서로 반비례 관계인 경우가 많으므로 적절한 균형 유지가 필요 (ex : 실시간 시스템은 반환 시간이 빠른게 중요, 다른 형태 시스템은 자원의 활용도도 중요)        실행 대기 방지 : 실행의 무한 연기를 막기 위해 에이징 방법 등을 활용        우선순위 : 우선순위가 높은 프로세스에게 자원 배려        서비스 사용 기회 확대 : 페이지 부재율이 적은 프로세스에 더욱 자주 서비스 제공        서비스 수 감소 방지 : 시스템 과부하에 의한 서비스 수 감소 방지  3 스케줄링의 기준 요소프로세서 버스트(burst)는 프로세스를 프로세서에서 실행하는 기간을 의미한다.프로세서 버스트는 크게 사용자의 입출력을 기다리는 입출력 버스트와 작업이 처리되는 프로세서 버스트로 나뉘며 버스트 간에는 프로세스가 준비 큐(ready queue)로 이동하여 기다린다.주로 그림 6-1 예시처럼 입출력 버스트와 프로세서 버스트의 순환, 특히 프로세서 버스트로 시작해 프로세서 버스트로 끝나는 경우가 많다.프로세스 버스트는 빈도가 많은 작업들은 짧은 경향이 많으며, 반대로 빈도가 적은 작업들은 긴 경향이 있다.또한, 백그라운드 프로그램 같이 긴 프로세서 버스트가 많으면 입출력이 적다는 의미이므로 프로세서 중심 프로세스는 짧은 입출력 버스트를 가지는 경향이 있으며, 입출력 버스트가 잦으면, 그만큼 프로세스가 입출력을 기다리는 경우가 많다는 의미이므로 입출력 중심 프로세스는 짧은 프로세서 버스트를 많이 가지는 경향이 있다. 이는 그림 6-2에 묘사되어있다.이러한 프로세서 중심 프로세스는 할당 받은 프로세서 시간이 완료 시간을 결정하고, 입출력 중심 프로세스는 입출력 완료 시간으로 완료시간이 결정되므로, 이 둘의 적절한 혼합을 위해 스케줄링 알고리즘을 잘 골라야한다.  혼합이 필요한 예를 들자면, 입출력 프로세스만 너무 많으면 준비 큐가 거의 비어있어 단기 스케줄러가 할일이 없어 비효율적이다. 반대로 프로세서 중심 프로세스만 많으면 입출력 준비 큐가 비어있으면서 프로세서들이 병목 현상을 겪을 것이다.최근에는 프로세서의 발달로 프로세서 버스트가 짧아져 입출력 중심으로 바뀌고 있으며, 프로세서 중심과 입출력 중심이 서로 바뀌는 프로세스 또한 존재한다.이때 입출력 중심 프로세스를 실행 뒤 프로세서 중심 프로세스로 바뀌면 짧은 지연이 생기며, 반대의 변화의 경우 긴 지연이 걸린다.4 스케줄링의 단계스케줄링은 아래 그림처럼 수행 단계에 따라 크게 3가지로 나뉜다. 중기 스케줄링을 제외한 단기와 장기 스케줄링은 6절에 자세히 알아보자.1단계 작업 스케줄링 : 작업 선택 (장기 스케줄링)      실제 시스템 자원을 사용할 작업을 결정하는 작업 스케줄링        디스크에 있는 작업 중 프로세스화할 작업과 시스템에 들어갈 작업을 결정하는 과정  수행 빈도가 적어 장기 스케줄링에 해당한다.2단계 작업 승인과 프로세서 결정 스케줄링 : 사용 권한 부여 (중기 스케줄링)      프로세서를 사용할 권한을 부여할 프로세스를 결정하는 작업 승인 및 프로세서 할당        작업 연기할 프로세스를 결정해 메모리에서 제거, 프로세스 수 관리  수행 빈도 기준 중기 스케줄링, 스와핑(swapping, 교체) 기능의 일부로 메모리 사용성을 높이고 작업 효율성이 향상시킴.3단계 프로세서 할당 스케줄링 : 준비 상태의 프로세스에 프로세서 할당(디스패칭, 단기 스케줄링)  디스패처(분배기)가 준비 상태에 있는 프로세스 중에서 프로세서를 할당할 프로세스를 결정하는 프로세스 할당 스케줄링수행 빈도 기준 단기 스케줄링5 스케줄링 큐스케줄링에서 사용하는 큐는 시스템에 단 하나 있는 준비 큐와 입출력 장치별로 하나씩 있는 입출력 장치 큐가 있다. 그림 6-4는 프로세스의 큐 이동 방향이다.그림 6-5은 큐에 적재되어 있는 프로세스의 모습이다. 각 큐의 포인터로 head와 tail 포인터가 존재하며, 프로세스 제어 블록의 포인터 주소를 가르키고 있다. 테이프 드라이브나 시분할 단말기 같은 전용 장치는 큐에 단 하나의 프로세스만 존재 가능하지만, 디스크 같이 여러 프로세스가 줄을 서는 경우도 있다.6 스케줄링과 스케줄러스케줄러의 프로세스 변화과정에서의 역할과 종류를 알아보자.6.1 큐잉 도표(queueing diagram)큐잉 도표는 프로세스 스케줄링을 표현하는 방법이다.      큐는 사각형으로        자원은 원으로        프로세스의 흐름은 화살표로  표시한다.(그림 6-7 참조)작업이 시스템에 들어오면 프로세스 제어 블록을 생성하고, 준비 큐에 들어간 뒤, 작업 종료까지 갱신된다.  프로세스가 준비 큐에 나와 프로세서가 할당된 뒤, 다음 사항중 하나가 일어난다.      프로세스가 입출력 요청을 보내고 입출력 큐에 들어감. (이후 대기 상태, 이후 준비 큐에서 준비 상태)        프로세스가 새로운 프로세스를 생성(fork)하고 생성한 프로세스의 종료 때까지 대기 상태로 기다리다 종료되면 준비 큐로 들어간다.        프로세스가 시간 할당량을 초과(시간 종료)하면 준비 큐에 들어감.        오류 등의 이유로 인터럽트 발생으로 인해 프로세서에서 제거된 프로세스는 다시 준비큐에 들어감.  6.2 스케줄러의 종류와 역할스케줄러는 스케줄링을 통해 프로세스를 선택하며, 주로 장기 스케줄러와 단기 스케줄러가 있다.그림 6-4와 같이 단순 큐잉 도표를 기준으로 역할을 알아보자장기 스케줄러작업 스케줄러라고도 하며, 스케줄링에 따라 디스크에서 메모리로 작업을 가져와 처리할 순서를 결정한다.제출 시간, 작업 이름, 작업 길이, 메모리 사용량, 자원 등을 확인하고 프로세스 제어 블록을 생성해 메모리에 적재하여 프로세스를 준비 상태로 만든다. 이때, 메모리에 적재할 프로세스의 수를 통해 다중 프로그래밍 정도를 결정할수 있다.실행 빈도가 분 단위로, 상대적으로 드물게 수행되며, 비교적 성능에 영향이 적다.단기 스케줄러메모리에 적재된 프로세스의 요청 중 자원을 만족하는 경우, 해당 프로세스에게 프로세서를 할당해 실행 상태가 되도록 스케줄링 알고리즘으로 결정한다.실행 빈도가 비교적 수시로 일어나므로, 오버헤드를 최대한 작게 만들어야 한다.프로세스의 실행 상태에서 대기 상태로, 또는 대기 상태에서 준비 상태로 변화하는 것을 처리일부 단기 스케줄러는 그림 6-11처럼 디스패처를 가지고 있다. 디스패처의 역할은 다음과 같다.      단기 스케줄러가 준비 큐에서 선정한 프로세서에게 프로세서를 할당하는 모듈        프로세스의 레지스터를 적재(문맥 교환)        사용자 상태로 전환하고 다시 시작할 때, 사용자 프로세스가 올바른 위치를 찾도록 도와줌  중기 스케줄러      프로세스들이 프로세서를 두고 경쟁 시, 프로세스를 메모리에서 빼내 다중 프로그래밍 정도를 줄인다. (스왑 아웃)        이후, 메모리에서 빼낸 프로세스를 다시 메모리에 들이고, 실행이 중단된 곳부터 재실행한다. (스왑 인)  이 과정을 스왑이라고 하며, 작업의 혼합과 메모리 효율에 좋다.(그림 6-8)메모리에 부분적으로 프로세스를 적재하고, 일시중지된 프로세서의 원인을 해결하면 다시 준비상태로 만듦.프로세스 상태 변화와 스케줄러의 역할프로세스 상태변화에 영향을 끼치는 스케줄러는 그림 6-9를 통해 알 수 있다.실행에서 종료 상태로 변화하는 것은 장기 스케줄러와 단기 스케줄러가 처리한다.7 선점 스케줄링과 비선점 스케줄링선점은 실행 중인 작업의 자원을 다른 작업이 빼앗을 수 있음을 의미하며, 비선점은 반대로 빼앗을 수 없음을 의미한다.선점 스케줄링은      프로세스 하나가 장시간 동안 프로세서를 독점한는 것을 방지하여 모든 프로세스에 서비스 기회를 줄 수 있고,        우선순위를 통해 실시간 시스템 같은 긴급, 인터럽트 처리를 하는데 유용하다.        선점 시 오버헤드가 커질 수 있어 메모리에 프로세스가 준비 상태로 있어야 효과적        우선순위를 의미있게 부여해야 함  비선점 스케줄링은      모든 프로세스가 공정하게 관리됨        우선순위에 연연하지 않으므로, 응답 시간 예측이 쉽다.        보통 구현이 더욱 용이하다.  8 스케줄링 알고리즘의 선택 기준스케줄링 알고리즘은 프로세서 사용률과 처리율을 최대화하고, 반환시간, 대기시간, 응답시간을 최소화하는 것이 바람직하다.그림 6-10을 보고 환경에 따른 시간의 정의를 알아보자.      프로세서 사용률 : 프로세서를 항상 실행 상태로 유지해 유휴 상태가 되지 않도록 함. 입출력 중심 작업보다는 프로세서 중심 작업을 실행으로 상승        처리율 : 단위 시간당 완료하는 작업수, 인터럽트를 줄이거나 짧은 작업을 우선 처리하면 상승한다.        반환시간 : 작업이 메모리에 들어가기까지 걸리는 시간 + 준비큐에 머무는 시간 + 실행시간과 입출력 시간 등의 합이므로, 이들을 최소화해야 한다.        대기시간:  준비 큐에 머무는 시간. 사용자 수를 제한하면 상승한다.        반응시간 : 작업 요청부터 첫 응답까지의 시간 대화형 시스템에서 중요함.  스케줄링 알고리즘단일 처리 시스템 기준으로 주요 스케줄링 알고리즘을 소개한다.1 선입선처리(FIFO, First In First Out) 스케줄링비선점 방법으로 큐 구조를 활용해 줄처럼 먼저 요청된 프로세스가 먼저 처리되는 방식이다.            장단      항목                  장점      - 스케줄링의 이해와 구현이 단순하다.- 준비 큐에 있는 모든 프로세스가 결국 실행되므로 기아 없는 공정한 정책이다.- 프로세서가 지속적으로 유용한 프로세스를 수행하여 처리율이 높다.              단점      - 비선점식이므로 대화식 프로세스(작업)에는 부적합하다.- 장기 실행 프로세스가 뒤의 프로세스(작업)를 모두 지연시켜 평균 대기시간이 길어져 최악의 대기시간이 됨.- 긴 프로세스(작업)이 실행되는 동안 짧은 프로세스(작업)이 긴 대기시간으로 호위 효과 발생      일괄 처리 시스템에서는 매우 효율적이나 빠른 응답을 요청하는 대화식 시스템에 적합하지 않다.그림 6-11처럼 시스템에 새로운 프로세스가 들어오면 해당 프로세스 제어 블록을 준비 큐의 마지막에 연결한다.선입선처리는 대부분 성능이 좋지 않고 들쭉날쭉하며, 평균 대기 시간이 길다.또한, 호위 효과라는 현상이 생기는데, 처리 속도가 빠른 프로세스들이 처리 속도가 느린 프로세스를 끝나기를 기다리며 몰려다니는 현상이다.(그림 6-12)title: 호위 효과 예시      정상적인 시스템 상태    프로세서를 차지하고 있는 느린 프로세스를 대기하면서 준비 큐에 빠른 프로세스들이 쌓인다.          주로 느린 프로세스는 프로세서 중심 프로세스이며, 빠른 프로세스는 입출력 중심 프로세스이다.            이번엔 입출력장치를 사용하고 있는 느린 프로세스 때문에 빠른 프로세스들이 입출력 장치 큐에 쌓이기 시작한다.        2번과 같이 역시 프로세서를 사용하는 느린 프로세스를 기다리느라 빠른 프로세스들이 준비 큐에 쌓인다.    이러한 상황이 계속 반복되면서 느린 프로세스 뒤로 빠른 프로세스들이 쌓이기 시작한다.또한, 비선점식이므로 정기적으로 프로세서를 공유하는 시분할 시스템에서는 부적합하다.2 최소작업 우선 스케줄링최소작업 우선(SJF, Shortest Job First) 스케줄링은 각 작업의 프로세서 실행 시간을 이용해 프로세서가 사용가능 시 실행 시간이 가장 짧은 작업(프로세스)에 할당하는 방법이다. 만약, 실행 시간이 동일하다면 선입선처리 스케줄링을 적용한다.            장단      항목                  장점      항상 실행 시간이 짧은 작업을 신속하게 실행하여 평균 대기시간이 짦음              단점      - 초기의 긴 작업을 짧은 작업을 종료할 때까지 대기시켜 기아 유발- 기본적으로 짧은 작업이 항상 실행되도록 설정하므로 불공정한 작업을 실행- 실행 시간을 예측하기가 어려워 실용적이지 않음      가장 짧은 작업 먼저 실행하도록 한다면, 짧은 작업들의 대기 시간은 줄어들고 긴 시간의 작업들의 대기시간은 늘어나며, 보통 평균 대기시간을 줄일 수 있다.      만약 5, 11, 7, 3의 실행시간을 가지는 프로세스들($P_1,P_2,P_3,P_4$)이 순서대로 실행한다고 가정하자.    선입 선출의 경우 평균 대기 시간은 다음과 같다.\\[\\displaylines{\\frac{W(P_1)+\\dots+W(P_4)}{4}=\\\\ \\frac{0 + 5 + (5+11) + (5+11+7) + (5+11+7+3)}{4}=11}\\]    만약 최소작업 먼저 우선하여 스케줄링하였다면 순서는 $P_4,P_1,P_3,P_2$가 되며 평균 대기 시간은 다음과 같다.\\[\\displaylines{\\frac{W(P_4)+\\dots+W(P_2)}{4}=\\\\\\frac{0 + 3 + (3+5) + (3+5+7) + (3+5+7+11)}{4}=6.5}\\]  물론 실제로는 가장 짧은 작업이 먼저 자원을 요청하지 않을 수 있으므로, 이상적인 상황이 나오진 않을 것이다.최소 작업 우선 스케줄링은 선점과 비선점 방식 두 가지 방식으로 구현 가능하다.(그림 6-13)title: 프로세스 도착 시간 예시 그래프            프로세스      도착 시간      실행 시간                  $P_1$      0      10              $P_2$      1      28              $P_3$      2      6              $P_4$      3      4              $P_5$      4      14      title: 비선점 최소 작업 우선 스케줄링 VS 선점 최소 작업 우선 스케줄링비선점 최소 작업 우선 스케줄링의 평균 반환시간은 26이며 평균 대기시간은 13.6이다.반면에 선점 최소 작업 우선 스케줄링의 평균 반환시간은 25이며 평균 대기시간은 12.6이다. 다만, 실제로는 문맥 교환에 의한 오버헤드를 고려해야한다.비선점 방식은 실행시간이 짧더라도 이미 실행되고 있는 프로세스를 중단시킬 수 없으므로, 비선점 방식보다 대기시간이 긴경우가 많으며, 시분할 시스템에 사용하기 어렵다.선점 방식은 문맥 교환 시간이 소요되므로 언제나 비선점형보다 유리하진 않으며, 따로 최소 잔여 시간 우선(SRTF, Shortest Remaining Time First) 스케줄링이라고도 부른다.최소 작업 우선 스케줄링은 실행 시간을 예측하기 힘들고, 긴 작업들의 기아 상태를 유발시킬 수 있다는 단점이 있다.따라서 장기 스케줄링에 많이 사용되지만 단기 스케줄링에서는 예측이 어려워 최소작업 우선 스케줄링의 근사치를 이용한다.3 우선순위 스케줄링우선순위 스케줄링(prioirty scheduling)은 준비 큐에 존재하는 프로세스들의 우선순위를 비교해 우선순위가 가장 높은 프로세스에 프로세서를 할당하는 알고리즘이다. 우선순위가 동일하다면 선입선출 순서로 스케줄링한다.            장단      항목                  장점      - 각 프로세스의 상대적 중요성을 정확히 정의할 수 있다.- 다양한 반응으로 실시간 시스템 사용 가능하다.              단점      높은 우선순위 프로세스가 프로세서를 많이 사용하면 우선순위가 낮은 프로세스는 무한정 연기되는 기아 발생      보통 우선순위의 기준은 예측된 다음 프로세서 버스트의 역이며, 실행 시간이 길수록 우선순위가 낮고, 짧을 수록 높으며, 보통 0~7 또는 0~4095 범위의 수를 사용한다. 단, 0을 최상위나 최하위로 정하지 않는다.하지만 우선순위는따로 내부적, 또는 외부적 요인을 기준으로 정의할 수 있으며, 내부적 우선순위로는 제한 시간, 기억장소 요청량, 사용 파일 수, 입출력/프로세서 처리 비율 등이 있으며, 외부적 우선순위는 프로세스의 중요성, 고객 등급, 부서, 정책적인 요인 등이 있다.우선순위 스케줄링 또한 선점과 비선점이 존재하며,      선점일 경우, 실행 중인 프로세스보다 방금 도착한 프로세스의 우선순위가 최고로 높다면 중단 뒤, 문맥교환이 일어난다.        비선점일 경우, 단순히 준비큐의 맨 앞 순번으로 해당 프로세스가 배치된다.  보통 그림 6-14처럼 우선순위 별로 준비큐를 가지고 있으며 높은 우선순위의 준비큐가 비면 다음 준비큐의 프로세스가 처리되는 방식이다.우선순위 스케줄링의 주요 단점은 무한 정지와 기아이다. 실행 준비는 했으나 우선순위가 높은 프로세스가 계속 들어오면 우선순위가 낮은 프로세스는 무한정 기다려야한다.이를 막기 위해 오래 대기하는 프로세스들의 우선순위를 점차 높이는 에이징(aging) 방법을 이용한다.4 라운드 로빈 스케줄링시분할 시스템을 위해 설계된 라운드 로빈(round-robin) 스케줄링은 작업을 규정 시간량(time quantum) 또는 시간 할당량(time slice)로 나누어 순환 큐(circular queue)에 존재하는 프로세스들을 돌아가며 한번에 한 프로세스에 정의된 규정 시간량만큼 프로세서를 제공하는 선점 방식이다.            장단      항목                  장점      - 기아상태 발생하지 않음- 실행 큐에 프로세스 수를 알고 있으면 구현이 쉬움- 강한 상호작용과 프로세스의 짧은 응답시간, 특히 프로세스 최악의 응답시간을 예측 가능- 작업 길이가 다양하면 작업을 마치지않고 규정 시간량을 마치고 다음 작업으로 이동하는 경우가 많아, 평균 대기시간이 선입선처리와 최소작업 우선 스케줄링보다 적음              단점      - 성능은 규정 시간량의 길이에 따라 달라짐, 너무 길면 선입선처리 성능, 너무 짧으면 문맥교환 오버헤드, 규정 시간량은 작업의 길이와 비슷해야 적절함- 하드웨어 타이머 필요- 미완성 작업은 각 규정 시간량을 마친 후 프로세서를 기다리므로 평균 처리 시간이 높음      보통 규정 시간량은 20밀리초~ 1000밀리초이며, 순환 큐인 준비 큐에는 선입선출 큐로 되어있어 새로 진입한 프로세스는 꼬리에 붙이게 된다.  리눅스의 경우 100밀리초, 윈도우 XP는 20밀리초이며, 우선순위 동작, 전면 작업, 백그라운드 작업에 따라 크기가 변한다.준비큐 맨 앞의 프로세서가 할당된 프로세스는 두 가지 경우가 있다.      규정 시간안에 작업을 마친 경우, 준비큐의 다음 프로세스에게 넘긴다        현재 실행 중인 프로세스의 실행 시간이 규정 시간량보다 긴 경우, 규정 시간량이 지나면 운영체제에 의해 인터럽트가 발생하고 프로세스의 레지스터들은 해당 PCB에 저장되고 준비 큐의 꼬리에 입력된다. 이후 스케줄러는 다음 프로세스의 작업을 가져온다.  규정 시간량이 5인 라운드 로빈 스케줄링의 예시를 들어보자.(그림 6-15)            프로세스      도착 시간      실행 시간                  $P_1$      0      10              $P_2$      1      28              $P_3$      2      6              $P_4$      3      4              $P_5$      4      14      title: 라운드 로빈 스케줄링 예시만약 더 이상 대기하는 프로세스가 없다면 인터럽트없이 연속해서 프로세스가 작업할 수 있다.라운드 로빈의 평균 반환 시간은 36.8이며, 평균 대기 시간은 24.4이다.라운드로빈은 규정 시간량을 넘길 경우 인터럽트 이후 다음 프로세스로 대치하므로 선점 스케줄링이며, 프로세스의 수를 n, 규정 시간량을 q라고 할 때, 각 프로세스는 대기 시간을 최대 $(n-1)\\times q$ 이상 넘기지 않는다. 따라서 적절한 규정 시간량 설정은 성능에 크게 관여한다.대부분의 대화식 프로세스는 입출력을 제외하면 규정시간량 이상 사용하지 않으며, 입출력 요청에 프로세서를 사용하고 대기한다. 따라서 규정 시간량은 입출력까지의 계산 시간 보다 커야 입출력 활용도가 극대화되고 반응 빠르다.규정시간량이 매우 크면 작업을 완료할 충분한 시간을 얻게되므로, 선입선출 방식과 비슷한 성능을 가지게 되고, 너무 작으면 프로세서를 나누어 공유하는 것처럼 보이지만 오버헤드가 커진다. 또한 적어도 문맥 교환에 소요하는 시간보다는 커야 한다.최적 규정 시간량은 시스템 특성, 오버헤드, 프로세스에 따라 다르며, 보통 대화식 프로세스가 적절한 시간에 반응을 보여주도록 보장하게 설정한다.프로세스의 반환시간 또한 규정시간량에 영향을 받으며 보통 프로세스들이 단일 규정 시간 내에 작업을 끝마칠 수 있으면 반환시간이 줄어든다.  예를 들어 실행시간이 10인 프로세스 3개가 있을 경우, 규정시간이 1이면 평균 반환 시간은 29이지만, 규정시간이 10이면 평균 반환시간은 20이다.5 다단계 큐 스케줄링다단계 큐(MLQ, MultiLevel Queue) 스케줄링은 각 작업을 서로 다른 묶음으로 분류하여 분류하는 방식이다.            장단      항목                  장점      응답이 빠르다.              단점      - 여러 준비 큐와 스케줄링 알고리즘 때문에 추가 오버 헤드가 발생한다.- 우선순위가 낮은 큐의 프로세스는 무한정 대기하는 기아가 발생      예를 들어 전면 작업(포어그라운드 작업, foreground task)과 후면 작업(백그라운드 작업, background task)는 두 유형의 요청 반응시간이 다르므로 서로 다르게 스케줄링되며 보통 전자가 우선순위가 높다.다단계 큐 스케줄링은 준비 큐를 종류별로 여러개 분할해 둔뒤, 메모리 크기나 프로세스 형태에 따라 프로세스를 큐잉하며, 각 큐는 독자적인 스케줄링 알고리즘을 가질 수 있다.예를 들어 전면 작업 큐는 즉각적인 반응을 위해 라운드로빈 알고리즘을, 후면 작업 큐는 선입선처리를 할 수 있다.큐 사이의 스케줄링은 보통 선점 우선순위 스케줄링을 이용한다.(그림 6-15)무조건 우선순위 높은 프로세스가 남아있는한 낮은 프로세스가 실행되지 않는 절대적인 우선순위 스케줄링이나 낮은 우선순위의 큐는 더 적은 시간을 할당받는 스케줄링도 가능하다.6 다단계 피드백 큐 스케줄링다단계 피드백 큐(MLFQ, MultiLevel Feedback Queue)에서는 다단계 큐에 작업이 큐 사이를 이동할 수 있어 융통성을 확보한 방식이며, 가장 일반적인 스케줄링 방식이다.            장단      항목                  장점      - 매우 유연하여 스케줄러를 특정 시스템에 맞게 구성할 수 있다.- 자동으로 입출력 중심과 프로세서 중심 프로세스를 분류한다.- 적응성이 좋아 프로세스의 사전 정보가 없어도 최소 작업 우선 스케줄링의 효과를 보임              단점      설계와 구현이 복잡하다.      예를 들어 실행시간이 긴 작업은 평균 대기시간을 낮추기 위해 낮은 우선순위로 보내거나, 입출력이나 전면 작업같이 빠른 반응이 중요한 작업은 우선순위가 높은 큐에 보낼 수 있다.기아상태를 방지하기 위한 에이징 또한 우선순위 큐를 높은 곳으로 옮기는 방법으로 구현할 수 있다.또한 그림 6-16 처럼 우선순위가 높은 큐는 규정 시간량을 적게두고, 우선순위가 낮은 큐는 규정 시간량을 크게 두게 설정한다. 이를 통해 자원의 공정성을 어느 정도 해결한다.            프로세스      실행 시간                  $P_1$      30              $P_2$      20              $P_3$      10      title: 다단계 피드백 큐 예시프로세스가 오랫동안 점유할 경우 큐가 이동됨에 따라 간트차트 아래 시간의 차이가 벌어진다.다단계 피드백 큐의 평균 반환시간은 48.333이며, 평균 대기시간은 28.333이다.보통 다단계 피드백 큐는 다음에 따라 정의한다.      큐 수        각 큐에 대한 스케줄링        높은 우선순위의 큐로 격상시키는 시기의 결정법        낮은 우선순위의 큐로 격하시키는 시기의 결정법        프로세스들이 어느 큐에 들어갈 것인지 결정하는 법        프로세스가 서비스를 받는 시기를 결정하는 법  단점은 설계와 구현이 복잡하다.7 HRN 스케줄링HRN(Highest Response-ratio Next) 스케줄링은 우선 순위를 서비스 시간과 대기 시간에 대한 함수로 계산하여 적용 하는 비선점 스케줄링 방식이다.            장단      항목                  장점      - 자원을 효율적으로 활용한다.- 기아가 발생하지 않음              단점      오버헤드가 높을 수 있음      최소작업 우선 스케줄링의 약점인 긴 작업과 짧은 작업 간의 지나친 불평들을 보완하기 위해 만들어졌다.\\[우선순위=\\frac{서비스\\ 받을\\ 시간 + 대기\\ 시간}{서비스\\ 받을\\ 시간}\\]우선 순위는 위와 같은 함수를 통하여 결정되며 따라서 서비스 받을 시간이 길수록 우선순위가 낮으며, 대기시간이 길수록 우선순위가 차차 높아진다.8 다중 프로세서 스케줄링프로세서가 여러 개가 되면 스케줄링은 더욱 복잡해진다. 각 프로세서마다 독자적인 큐와 스케줄링이 있으므로 프로세서가 다르면 선택 가능한 프로세스가 제한되므로, 프로세서 별로 분류하고 스케줄링해야 한다.예를 들어 IBM 시리즈/1에서는 Vax 어셈블리어로 작성한 프로그램이 돌아가지 않는다.하나의 시스템에 종류가 같은 프로세서가 여러 개이면 부하 공유 문제가 생긴다. (9절 스레드 스케줄링 참조)다중 프로세스 스케줄링 구조단일 프로세서가 아닌 다중 프로세서 환경에서의 스케줄링 기법에 대해 알아보자.준비큐 독립 구조공유 부하를 프로세서 별로 준비 큐를 독립시켜 해결할 수 있다.+ 각 프로세스는 각 프로세서의 큐에 분산되기 때문에 스케줄링 오버헤드가 적다.  프로세스가 40개이면 스케줄링 비용이 기하급수적으로 커지지만 이를 4개 프로세서가 10개씩 나누면 오버헤드가 줄어든다.- 준비 큐 상황이 서로 달라, 어떤 프로세서는 준비 큐가 비어 유휴, 어떤 프로세서는 바쁠 수 있다.공동 준비큐 구조각 프로세서는 공통의 준비 상태 큐에서 실행할 프로세스를 유일하게 하나 선택하여 프로세스 큐로 보낸다.+ 공통 준비큐는 강결합된 공유 메모리 구조에서는, 모든 프로세서가 모든 프로세스의 문맥 정보를 이용할수 있다.+ 프로세스 큐의 존재로 인해 프로세스가 메모리에 적재되는 동안 다른 프로세서에서 작업을 실행할 수 있으며, 각 프로세서가 독립적인 스케줄링과 오버헤드를 가질 수 있다.이때 스케줄링 방법은 크게 두 가지가 있다.      프로세서 자신이 스스로 스케줄링, 각 프로세서는 공동 큐에서 가져온 프로세스 하나를 겹치지 않게 가져오고, 단일 프로세서 스케줄링을 실시한다.    - 이 방법은 스케줄링이 복잡하고 오버헤드가 증가할 수 있다.        비대칭 다중 처리(AMP, Asymmetric MultiProcessing): 프로세서 하나가 다른 모든 프로세서의 스케줄러를 지정하는 주종(Master/Slave) 구조    시스템 호출, 핵심 커널 기능 등의 시스템 프로세스와 스케줄링은 주 프로세서가 담당, 사용자 프로세스는 나머지 종 프로세스가 담당. 사용자가 입출력 호출 등이 필요하면 주 프로세스에 종 프로세스가 요청    + 기존의 단일 프로세서 다중 프로그래밍과 비슷한 방법    + 주 프로세서가 자원 관리를 통해 자원 충돌 해결    - 주 프로세서에 문제 생기시 전체 시스템 정지    - 주 프로세서에 과중하게 오버헤드가 일어날 수 있음  9 스레드 스케줄링동일한 주소 공간에 동시에 실행할 수 있는 스레드들로 응용 프로그램의 입출력과 프로세서 처리를 중첩할 수 있다.스레드의 문맥교환은 프로세스 문맥 교환보다 오버헤드가 적어 성능을 항상시킬 수 있다.멀티 프로세싱, 스레딩 좀더 알아보기!부하 공유(load sharing)부하 공유는 단일 프로세서 환경에서 주로 사용하며, 프로세서를 특정 프로세스(스레드들의 집합) 하나에 할당하지 않고 전역 큐에서 프로세서를 유지하며, 대기 중인 프로세서는 전역 큐에서 스레드 한개를 선택하는 방법이다.            장단      항목                  장점      - 부하는 프로세서에 균등하게 분산하며, 실행 대기 중인 작업에는 분산하지 않음- 스레드를 제어하는 중앙 제어 스케줄러 없이 사용할 때는 현재 작업을 진행한 프로세서에서 다음 작업을 선정- 전역(공유) 큐는 선입선처리나 우선순위 방법을 이용하여 구성              단점      - 중앙 큐는 상호배제 방법으로 접근하는 메모리 영역으로, 많은 프로세서가 동시에 작업을 찾는다면 중앙 큐는 병목 지점이 될 수 있음- 선점된 스레드들은 동일한 프로세서 상에서 실행 재개가 힘듬- 프로그램의 스레드 사이에 밀접한 협조가 요청되면, 관련된 프로세스 문맥 교환은 성능을 심각하게 저하      갱 스케줄링(gang scheduling)갱 스케줄링은 관련된 스레드의 집합을 일대일 대응 원칙에 따라 프로세서 집합에서 동시 실행하게 하는 스케줄링 방법이다. 즉, 단일 프로세스에 속한 스레드들을 한꺼번에 스케줄링한다.주로 스레드들이 병행 실행되지 않으면 다른 스레드들이 기다려야 하는 프로그램에 사용한다. 또한 한 번의 스케줄링 결정이 다수의 프로세서와 프로세스에 영향을 주므로 오버헤드도 적다.아래의 전용 프로세서 할당과 함께 프로세서 단편화 문제를 회피할 수 있다.  프로세서 단편화 : 대기하고 있는 프로세스의 요청을 받기에는 프로세서의 자원이 충분하지 않아 프로세서를 할당할 수 없는 단편화전용 프로세서 할당부화 공유와는 반대로 스레드들을 실행 점담 프로세서에 할당해 정의된 스케줄링을 제공하는 방법이다.다중 스레드에서의 문제는 다음과 같다.      각 프로그램은 실행되는 동안 스레드와 동일한 수의 프로세서를 할당받으므로, 스레드 수가 적다면 프로세스가 낭비된다.        한 응용 프로그램의 스레드를 다른 스레드의 동기화나 입출력 대기 때문에 보류되면 해당 스레드의 프로세서가 쉬게 된다.  따라서 활성화된 스레드 수를 프로세서와 동일한 수로 제한해 효율성을 높인다.효율적인 할당으로 프로세서 단편화 문제를 회피할 수 있다.동적 스케줄링동적 스케줄링은 스레드 수를 실행 도중 동적으로 변경하여 시스템 이용률을 높일 수 있도록 부하 조절을 허용하는 방법이다.운영체제는 프로세서들을 각 프로세스에 할당하고 프로세스들은 스레드 처리를 위해 할당된 프로세서를 사용한다.프로세스의 자원을 선점할때, 어느 스레드를 일시 중지할 것인지는 각 응용 프로그램 실행 라이브러리 루틴들이 결정한다."
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%207-%EB%A9%94%EB%AA%A8%EB%A6%AC%20%EA%B4%80%EB%A6%AC.html": {
    title: "OS 정리-Chap 7-메모리 관리",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%207-%EB%A9%94%EB%AA%A8%EB%A6%AC%20%EA%B4%80%EB%A6%AC.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true7. 메모리 관리title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 메모리 관리의 개요1 메모리 관리의 개념과 정책메모리란 디스크에 있던 프로그램을 적재하여 실행하는 작업 공간메모리 관리는 프로세스들을 위해 메모리를 할당, 제거, 보호하는 기술메모리 관리자는 운영체제의 관리 모듈과 메모리 관리 장치(MMU, Memory Management Unit)이 협업하여 적재 정책, 배치 정책, 대치 정책을 통해 메모리를 관리메모리 관리 정책  적재 정책(fetch policy) : 디스크에서 메모리로 프로세스를 반입할 시기를 결정하는 것          요구 적재 : 운영체제, 프로그램 등의 참조 요청에 따라 다음 프로세스를 메모리에 적재      예상 적재 : 시스템의 요청을 미리 예측에 메모리에 적재, 보조기억장치의 탐색 시간, 회전 지연 특성 또한 참조.        배치 정책(placement policy) : 디스크로 부터 반입한 프로세스의 메모리 상 적재 위치를 결정하는 것          최초 적합      최적 적합      최악 적합        교체 정책(replacement policy) : 메모리 부족 시 제거할 메모리 내의 프로세스를 결정하는 것          선입선출, 최소사용 알고리즘 등이 존재 (8장 참조)      2 메모리의 구조와 매핑(사상)메모리는 주소의 연속으로 이루어져 있으며, 주소는 크게 두가지 관점으로 해석 가능      프로그래밍에 사용되는 논리전 관점의 논리적 주소 (그림 7-1 (a)), 가상 주소라고도 함    목적 코드(object code), 자료 구조등이 해당        실제 데이터나 프로그램을 저장하는 물리적 관점의 물리적 주소 (그림 7-2(b)), 실제 주소  논리적 주소와 물리적 주소는 메모리 관리 장치(MMU)에 의해 변환되며, 그림 7-2와 같이 동여러 번환 방법이 존재한다. 이때, 변환을 위해서는 논리적 주소에 대응 하는 물리적 주소를 알아야 한다.예를 들어 프로그래밍 언어에서 변수 라는 논리적 주소의 값을 알기 위해서는 메모리 주소라는 물리적 주소를 알아야 한다.프로세스의 논리적 주소에 대응하는 물리적 주소를 연결, 즉 매핑(mapping)하는 것을 바인딩(binding)이라고 하며, 바인딩은 변환 시점에 따라 3가지로 나눌 수 있다.      컴파일 시간    컴파일러가 메모리에서의 프로세스 위치를 파악하고 물리적 주소에 배치한 뒤, 확장시켜 나간다. 만약, 시작 위치가 바뀌어야 한다면, 코드를 다시 컴파일 해야한다.        적재 시간    만약, 컴파일러가 적재 위치를 모른다면, 대신 시작 주소가 0인 상대 주소를 생성한 뒤, 바인딩은 적재 시간에 이루어 지게 한다. 시작 주소가 변하면 사용자 코드를 재적재 한다. 이를 정적 대치라고 한다.        실행 시간    가장 주요한 바인딩 방법, 기본 및 경계 레지스터 등의 하드웨어를 이용해 수행시간에 바인딩, 이를 이용해 프로세스가 메모리의 세그먼트 간에 이동하면서 실행할 수 있다. (동적 연결)  title: 컴파일러의 동작  컴파일러(compiler)가 원시 코드를 컴파일해 목적 파일을 생성  링커(linker)가 목적 파일에 라이브러리 파일이나 다른 목적 파일을 결합  로더(loader)가 지정 위치에서 시작해 메모리에 프로그램을 배치, 배치 방법은 다음이 존재          절대 적재 : 프로그램을 메모리의 지정된 위치에 적재      교체 적재: 프로그램을 메모리의 여러 위치에 적재      3 메모리 관리 관련 용어3.1 동적 적재(dynamic loading)필요한 일부 루틴만 메모리에 적재하고 나머지는 적재 가능 상태로 디스크에 저장하다가 필요 시 메모리의 다른 루틴과 교체하여 주소 테이블을 갱신하는 방법바인딩을 실행 직전에 실행하는 동적 적재로 효율적인 메모리 관리가 가능케 되었다.🟢 PROS: 효율적인 메모리 사용🟠 CONS : 오류 발생 가능성, 구현의 어려움3.2 중첩(오버레이, overlay)특정 시기에만 사용하는 일부 데이터를 메모리 내의 중첩 영역에 순차적으로 호출하여 그림 7-4 처럼 적재 및 교체 하는 적재 방식운영체제 영역과 필수적인 공통 루틴은 메모리에 무조건 포함된다.🟢 PROS      운영체제 지원없이 메모리에 따른 분기와 파일구조를 이용해 구현 가능        규모가 큰 프로세스를 메모리에 효율적이게 적재 가능  🟠 CONS  프로그램 구조, 코드, 자료구조를 완전히 이해해야 구현 가능, 현 시대의 큰 규모의 프로그램에 적용 힘듦, 마이크로 컴퓨터나 메모리 크기가 극히 제한된 임베디드 시스템에서만 사용그림 7-5는 패스 1과 2가 존재하는 2패스 어셈블러 예시이다. 어셈블러 프로세스의 총 크기는 210KB, 메모리의 최대 크기는 200KB, 즉, 메모리 초과로 모두 적재가 불가능하다.패스 1과 패스 2의 기능이 다르므로 중첩 영역으로 설정한 뒤, 교체해가며 실행 가능하다.3.3 스와핑(프로세스 교체)스와핑은 다중 작업과 효율적인 메모리 사용을 위해 수행이 완료된 프로세스는 보조 기억장치로 보내는 스왑 아웃, 새롭게 시작하는 프로세스는 메모리에 적재하는 스왑 인을 반복하는 것이다.(그림 7-6)프로세스가 빈번하게 교체되는 순환 할당 알고리즘이나 우선순위 알고리즘을 사용하는 다중 프로그래밍 환경에서 사용한다.스와핑을 통해 우선순위에 따른 프로세스 처리가 가능하며, 시스템에 유연성을 제공한다.스와핑의 과정스와핑은 주로 중기 스케줄러가 실행준비 큐에서 대기 상태 👉 스왑 인에 의해 프로세스가 준비큐 메모리에 적재실행 중인 프로세스가 할당 시간 종료, 작업 종료, 입출력 등에 의해 대기 상태 👉 스왑 아웃을 통해 교체된다.이때 스왑을 위해 스왑 준비와 스왑 대기 상태를 거쳐 우선순위, 조건에 따라 스왑되게 된다. (그림 7-7)스와핑 유의할 점      실행 시간 바인딩에서만 사용 가능        스와핑 시 생기는 주요 오버헤드로 문맥 교환시간이 존재, 프로세스가 많을 수록 커짐        따라서 보조 기억장치의 전송속도가 중요하다.        메모리를 요청하는 정보의 변화를 관리하면 효과적으로 제어 가능        스와핑은 프로세스가 유휴(일부 대기, 중단) 상태여야 가능          입출력 버퍼를 액세스 중이거나 실행 중이면 불가능 하다.      나중에 페이지로 발달하게 된다.3.4 메모리 적재 방법메모리 적재 방법은 그림 7-6과 같이 나뉜다.      연속 메모리 할당: 프로세스의 데이터가 메모리 주소상 연속적으로 위치함    다만, 내부 단편화, 외부 단편화를 초래하며, 메모리 압축을 통해 이를 해결 시, 대신 실행 시간 낭비가 초래됨                  고정 분할 : 크기가 각기 다른 프로세스에도 같은 크기의 메모리를 할당해 메모리 낭비 초래                    가변 분할: 프로세스 별로 각기 다른 메모리 크기 할당                    이외에도 직접 배치, 중첩, 버디 시스템 등이 존재                  비연속(분산) 메모리 할당 : 프로세스 데이터가 페이지나 세그먼트 단위로 나눠 여러 곳에  적재, 연속 메모리 할당의 단점을 극복                  고정 분할(페이징) : 프로그램 하나를 분할하는 기준에 따라 동일한 크기의 페이지로 나누어 적재                    가변 분할(세그먼테이션) : 일정치 않은 크기의 세그먼트로 나누어 적재            02 연속 메모리 할당1 단일 프로그래밍 환경에서 연속 메모리 할당단일 프로그래밍 환경의 특성      초기 프로그래밍 환경 시스템        한명의 사용자만 사용 가능        컴퓨터 자원을 독점        메모리 보다 작은 크기의 프로세스만 사용 가능        메모리에 직접 배치 과정을 통해 항상 같은 메모리 위치에 적재        메모리 제어 권한을 사용자가 전적으로 가짐  연속 메모리 할당 시스템 구조단일 프로그래밍 환경에서는 메모리를 다음과 같이 나누었다. (그림 7-9)      메모리 상위나 하위에 위치한 운영체제 상주 영역(모니터 영역)          경계 레지스터 : 사용자 영역과 모니터 영역 사이를 표시, 사용자가 임의로 운영체제 영역을 건드려 오류가 나지 않게 방지, 메모리 주소시 마다 이를 검사            사용자 영역        사용하지 않는 미사용 영역  연속 메모리 할당 시스템의 동적 적재동적대치의 문제점과 해결 방안문제점만약, 운영체제의 동적 크기 변화 등의 원인으로 기준 레지스터의 값을 바꿔야 한다면, 모든 프로그램을 재적재 해야함해결방안      그림 7-9의 (a) 처럼 운영체제 변화에 영향없도록 언제나 상위 부분에 프로그램 적재          중간의 사용하지 않는 공간을 통해 운영체제와 사용자 프로그램 확장 가능            이후, 기준 레지스터를 이용하여 주소 바인딩을 연기          이를 통해, 프로그램이 확장, 축소 되는 경우 기준 레지스터만 바꾸면 메모리 범위를 쉽게 알 수 있다.      기준 레지스터를 이용한 동적 대치기준 레지스터 : 메모리 상의 물리적 시작 주소, 프로그램의 시작 주소를 저장경계 레지스터 : 프로그램 영역이 저장되어 있는 크기, 논리적 주소예를 들어, 어떤 프로그램의 기준 레지스터값이 300이고, 경계 레지스터 값이 400이면, 메모리 내의 물리적 주소 300~700까지의 범위는 해당 프로그램의 범위이다.(그림 7-10)연속 메모리 할당 시스템의 장단점🟢 PROS: 단순하고 이해하기 쉬움🟠 CONS      메모리 효율성이 떨어짐        한번에 하나의 프로그램만 이용 가능        메모리보다 큰 프로그램은 스와핑을 사용해야 함          단, 스와핑 사용시 프로세서 중심 작업이 아니라 입출력 작업이 교대로 발생한다면, 프로세서의 유휴 상태가 자주 일어나 자원 낭비가 심하고 문맥 교환 비용이 급증(그림 7-11)      2 다중 프로그래밍 환경에서 연속 메모리 할당여러 사용자가 여러 프로세스를 사용 가능한 다중 프로그래밍 환경에는 다음과 같이 두 방법이 사용 되었다.2.1 고정 분할 방법고정 분할 방법은      메모리를 고정된 크기의 여러 구역으로 분할        이후, 분할된 구역별로 프로세스를 하나씩 배정한다.  고정 분할의 물리적 주소 구하기(그림 7-12)      분할 기준 레지스터(PBR)에 프로세스 별로 구분된 값을 기준 레지스터로 사용 불러옴        논리적 주소를 더해 프로세스 별 메모리 범위 한정  즉, 프로세스 $P_1$의 메모리 범위는 $PBR_{P_1}$ ~ $PBR_{P_1}+ 경계\\ 레지스터(논리적\\ 주소)$까지이다.고정 분할 방법의 문제      분할된 메모리의 경계보다 논리적 주소가 크면(=프로세스 크기가 고정 분할 크기보다 크면) 오류 발생        분할된 메모리의 경계보다 논리적 주소가 작으면(=프로세스 크기가 고정 분할 크기보다 작으면) 내부 단편화 발생        큐의 분할 방법과 분할 영역의 크기에 따라 성능이 크게 바뀜                  예를 들어 그림 7-16는 Q2 준비큐는 2KB 고정 분할 주소로Q7 준비큐는 6KB 고정 분할 주소로Q12 준비큐는 12KB 고정 분할 주소로 할당 시,Q2 준비큐는 가득차고, 나머지 큐는 대기하는 프로세스가 없어도6KB와 12KB 고정 분할 주소는 사용되지 않고 낭비되게 됨.                    위 경우를 막기위해 그림 7-17처럼 하나의 큐로 만들 경우, 7KB 작업이 충분히 12KB 공간에 5KB 이하의 작업과 동시에 처리 가능함에도, 낭비가 된다.            위와 같은 문제를 해결하기 위해 시스템을 분석하여      적절한 분할 영역의 개수와 크기를 결정        효율적인 작업 스케쥴링이 필요하다.  내부 단편화란?다른 프로세스가 들어가기 너무 작아 낭비되는 메모리 공간페이징에서는, 특정 페이지가 모종의 이유로 패이지 내 일부 공간만 활용하고 나머지 공간은 활용하지 못하고 낭비되는 공간프로세스 고정 분할 크기(18,464 바이트) - 프로세스 크기 (18,462 바이트) = 2바이트 (그림 7-13)👉 이 크기는 다른 프로세스를 넣거나 기능 확장을 하기에 너무 작은 크기며, 그냥 낭비(=내부 단편화)로 남게 됨내부 단편화의 예시내부 단편화 비효율성에 영향을 주는 요소들      각기 다른 크기의 프로세스들의 스케줄링 방법        분할 크기  고정 분할에서의 프로세스 메모리 보호여러 프로세스가 같은 메모리에 상주하면서 다른 메모리를 침범을 막기 위해 기준 레지스터와 경계 레지스터를 사용그림 7-15에서 기준 레지스터 값(물리적 시작 위치)은 300040, 경계 레지스터 값(메모리 크기)은 120900이다. (a)이를 이용해, 그림 7-15(b)처럼 프로세서가 주소에 접근 시, 접근 주소의 크기 $S$를 다음과 같이 설정시 메모리를 보호할 수 있다.\\[기준\\ 레지스터\\ 값 \\leq S &lt; 기준\\ 레지스터\\ 값+경계\\ 레지스터\\ 값\\]2.2 가변 분할 방법고정된 경계를 없애고 각 프로세스가 필요한 만큼 메모리를 할당하는 것기준 레지스터와 경계 레지스터를 이용해 분할 영역 표현(그림 7-18)의 분할 테이블을 살펴보면, 기준은 기준 레지스터 값, 크기는 경계 레지스터 값을 의미한다.분할 테이블을 통해 프로세스의 사용 부분을 알 수 있다.해당 프로세스가 기준 레지스터 값보다 낮은 주소나 기준 레지스터 값 + 경계 레지스터 값 초과의 주소를 참조하려 하면 오류가 발생한다.(=이를 통해서 메모리 보호)가변 분할의 배치 알고리즘가변 분할은 그림 7-19처럼 효율적인 배치 알고리즘을 이용해 공간활용이 가능하다.배치 알고리즘 : 비어있는 메모리 공간의 크기와 위치를 파악해 하나 이상의 특정 작업을 나누어 할당하는 알고리즘      최초 적합(first-fit) : 사용 가능 공간 중 충분히 큰 첫번째 공간에 할당 (그림 7-20)                  🟢 PROS : 사용 가능 공간의 첫 공간, 혹은 이전 검색의 인덱스 활용, 빨리 검색 가능                    🟠 CONS : 공간 활용도가 크게 떨어짐                  최적 적합(best-fit) : 프로세스를 충분히 큰 사용 가능 공간중 들어갈 수 있는 가장 작은 공간에 할당 (그림 7-21)                  🟢 PROS : 사용 가능 공간 이용률 향상                    🟠 CONS : 크기 순으로 사용 가능 공간을 정렬 및 관리 필요, 할당 과정에 많은 시간 소요                  최악 접합(worst-fit) : 프로세스를 가장 큰 사용 가능 공간에 할당 (그림 7-22)                  🟢 PROS: 새로운 사용 가능 공간이 생기므로 메모리 활용 효율이 최고, 가장 많이 사용됨                    🟠 CONS: 크기 순으로 사용 가능 공간 정렬 및 관리 필요, 할당 과정에 많은 시간 소요            앞서 말했던 것처럼 기준 레지스터와 경계 레지스터를 통해 메모리를 보호하며, 디스패처는 이 값들을 이용해 적재하고, 검사한다.외부 단편화전체의 사용 가능 공간의 합은 충분히 다음 작업이 적재될 수 있으나 다른 프로세스 공간들 사이에 여러 작은 공간으로 단편화되어 사용할 수 없는 문제가변 분할 방법은 내부 단편화 문제는 없으나 외부 단편화 문제가 존재한다.이를 해결하기 위해 다음 두 가지 해결방안이 있다.      메모리 통합(coalescing) 방법    작업 종료시, 다른 빈 공간과 인접을 확인하고 하나로 합침, 일부 공간을 통합 되지 않기도 함(그림 7-24)        메모리 압축(compaction) 방법    그림 7-25처럼 작업들의 메모리 적재 위치를 변경하여 하나의 커다란 공간으로 만든다.    🟢 PROS: 남김 없이 모든 흩어진 빈공간 통합, 이론상 모든 메모리 사용 가능    🟠 CONS                  주소 대체가 정적이거나, 컴파일 중이거나 적재 중인 작업은 압축을 수행할 수 없다.                  즉 동적 대체, 작업이 실행 상태일 때, 중단시켜야일때만 가능하다.                            추가로, 압축동안 전체 시스템이 모든 일을 중지해야하며, 이때 대화형 사용자는 응답시간이 일정치않고, 실시간 시스템은 멈추게 된다.                    메모리의 작업들의 정보를 일시적으로 액세스 가능한 형태로 보관 후, 복구 해야함.                    압축 작업을 자주 요구하며, 시스템 자원의 소모가 큼                    메모리 테이블을 유지해야 압축이 가능하므로, 오버헤드가 발생하며, 외부 단편화 조각들이 작을 수록 비효율적이다.                  예를 들어 고작 1byte 크기의 외부 단편화를 해결하기 위해 작업을 옮기는 것은 비효율적이다.                                    그림 7-26 처럼 같은 압축 방법에 따라 효율이 달라진다.            3 다중 프로그래밍 환경의 버디 시스템(buddy system)버디 시스템은 고정, 동적 분할의 단편화 문제를 해결하기 위해 제안되었다.큰 버퍼들을 반복적으로 이등분하여 작은 버퍼들을 만들며, 가능할 때마다 인접한 빈 버퍼들을 합치는 방식으로 동작한다.버퍼를 나눌 때 각각 분할을 서로의 버디라고 한다.그림 7-27처럼 예시를 들자면 만약 (a) 경우처럼 최소 크기 버디보다 요청한 메모리(8KB)가 더 작다면 들어갈 수 있는 크기내에 최소한 작게 이등분으로 나눈 뒤 할당한다.이후 (e)처럼 요청이 해제될 때 다시 합치면 된다.고정 분할이나 가변 분할 방법의 단점을 해결했지만, 최근에는 페이징이나 세그먼테이션 가상 메모리를 이용하며, 유닉스의 커널 메모리 할당이나 병럴 처리 시스템에서 주로 사용한다.03 분산 메모리 할당 1: 페이징프로세스를 크기가 동일한 페이지들로 나누고, 메인 메모리도 (페이지) 프레임이라는 고정 크기 블록으로 나눠 프레임에 페이지를 적재하는 방법동적 대치의 한 형태이며, 논리적 주소를 페이징 시스템을 이용해 물리적 주소로 변환한다.1 페이징의 개념페이징에서는 그림 7-28 (a)와 달리 같은 프로세스의 내용이라고 해도 (b) 처럼 여러 위치에 같은 크기의 페이지로 분산 적재된다.따라서 페이징을 이용하는 프로그램은 다음 작업이 필요하다      프로세스에 필요한 페이지를 결정해 페이지 번호 부여        메모리의 빈 프레임을 조사해 프로세스를 적재할 위치를 파악        프로세스의 페이지를 빈 프레임에 적재하도록 준비  🟢 PROS      빈 프레임에는 어떤 페이지든 적재 가능, 메모리 효율적        프레임 간 외부 단편화 없음        시분할 환경에서 중요한 공통 코드 공유 가능(공유 페이지)        압축 기능 제거  🟠 CONS      분산 적재된 페이지들의 관리 부담          특히 메모리에 페이지들의 정보가 담긴 페이지 테이블을 따로 준비해야한다.            언제나 마지막 페이지에 할당된 프레임이 차지않아 내부 단편화가 발생                  페이지 크기가 100이고, 프로세스 크기가 101이면, 페이지 1개는 99가 낭비됨                    페이지 크기가 작아지면 작아질수록 내부 단편화 정도가 약해지지만, 대신 페이지 관리 부담 증가                  하드웨어 비용 증가        연속 적재에 비해 속도 저하  2 페이징 시스템의 하드웨어 구조와 원리페이징 시스템의 하드웨어 구조는 그림 7-29와 같다.페이지의 물리적 주소 구하기프로세스의 논리적 주소는 페이지 번호 p와 오프셋(변위) d로 구성      논리적 주소의 페이지 번호 p를 페이지 테이블에서 찾아본다.        페이지 테이블에서 페이지 기준 주소이자 메인 메모리 프레임 번호인 f를 얻는다.          페이지 테이블은 별도의 레지스터 혹은 메인 메모리에 배치되며, 프로세스 당 하나씩 관리해야 한다.            논리적 주소의 오프셋 d를 기준 주소 f와 더하면(f+d) 메인 메모리의 물리적 주소가 된다.          이때 오프셋 d 크기가 페이지 프레임 크기보다 작아야 한다.      페이지와 논리적 주소의 구조페이지의 크기는 보통 하드웨어에 따라 다르며, 512Byte ~ 1GB 사이이며, 2의 배수만큼의 크기이다.title: 페이지가 2의 계승에 비례한 크기인 이유?논리적 주소를 페이지 번호와 오프셋으로 쉽게 변환할 수 있기 때문이다.아래의 논리적 주소의 설명에 따라서 오프셋의 크기의 최대값은 이진법 표기로 2의 계승 - 1이므로, 임의의 수보다 비교하기 쉽다.논리적 주소의 구조논리적 주소의 상위 n 비트는 페이지 번호, 나머지 하위 비트들은 오프셋을 나타낸다.예를 들어 그림 7-31의 16비트 논리적 주소는 6비트의 페이지 번호와 나머지 10비트의 오프셋으로 이루어져있으므로, 페이지는 최대 64개로 이루어져 있고, 페이지 크기는 최대 1024bit(=1KB)이다.그림 7-32는 IBM 370의 32비트 논리적 주소로, 앞의 8비트는 사용되지 않는다.title: Why?2의 계승을 유지하면서, 가능한 페이지 번호나 오프셋이 너무 크면 생기는 문제를 막기 위함그림 7-33과 7-34는 메모리 페이지의 예시이다.3 다중 단계 페이징 시스템의 구조와 원리기존 페이징 시스템의 문제점논리적 주소가 크면 관리해야할 페이지 테이블 크기가 증가하며, 이는 곧 더 큰 메모리 사용량 오버헤드로 이어진다.자세하게 예를 들어보자      32비트 프로세서의 메모리는 $2^{32}$인 4GB가량까지 지원        페이지 크기 : 4KB(=$2^{12}=4096$)로 가정        총 페이지의 수는 $4GB/4KB=1,048,576$개        페이지 하나당 관리를 위한 페이지 테이블의 항목이 하나 필요        페이지 테이블 하나당 4Byte이므로, 페이지 테이블 하나는 4MB이다.        프로세스 별로 페이지 테이블이 하나씩 필요        프로세스가 100개만 되도 (보통 백그라운드 프로세스가 많다.), 400MB가 낭비, 이는 4GB의 1/10 수준          심지어 10KB짜리 작은 프로세스를 위해서 4MB의 페이지 테이블을 유지해야 한다.      다중 단계 페이징 시스템을 이용한 해결다중 단계 페이징 시스템  실제 참조할 페이지가 있는 영역에서만 단계별 페이지 테이블을 설정하는 페이징 시스템기존의 페이지 주소와 오프셋으로 구성되는 논리적 주소를 페이지 항목, 페이지 테이블, 페이지 오프셋으로 구성되게 함(그림 7-35)      페이지 항목은 1차 수준 테이블에서 2차 수준 테이블의 주소를 찾는데 사용          1차 수준 테이블은 2차 수준 테이블들의 물리적 주소(디렉토리)들이 적혀있다.            페이지 테이블은 2차 수준 테이블에서 페이지의 주소를 찾는데 사용          2차 수준 테이블은 페이지들의 주소들이 적혀있으며, 해당 디렉토리에 데이터가 있는 경우에만 생성된다.            페이지 오프셋은 기존의 오프셋과 동일  title: 최근에는 최대 4단계 페이징 시스템까지 사용(모토로라 68030)다중 페이징 시스템에서의 페이지 테이블 메모리 계산      페이지 항목과 페이지 테이블이 각각 10비트        페이지 항목이 10비트이므로 1차 수준 테이블의 전체 항목수는 $2^{10}=1024$        1차 수준 테이블 한 항목의 크기는 4Byte, 즉 1차 수준 테이블의 전체 크기는 4KB        페이지 테이블이 10 비트, 2차 수준 테이블 또한 마찬가지로 전체 항목 수 $2^{10}=1024$        2차 수준 테이블의 한 항목 의 크기는 4Byte, 즉 2차 수준 테이블의 전체 크기 또한 4KB        2차 수준 테이블은 해당 1차 수준 테이블에서 지정한 주소(디렉토리)에 데이터가 존재하는 경우에만 생성                  2차 수준 테이블의 수를 넉넉하게 200개라고 하자. (최대 1024개 가능)                    페이지 하나 크기가 4KB라고 가정하면,\\[200(2차\\ 수준\\ 테이블\\ 수) \\times \\\\1024(2차\\ 수준\\ 테이블\\ 최대\\ 크기)\\times4Kbyte=800MB\\]        800MB 크기의 엄청난 크기의 프로세스(창을 18개 정도 열어놓은 구글 크롬 브라우저 수준?)이다.                  페이징 시스템에 필요한  $4+4 \\times 200=804KB$로 기존의 4MB에 비해 1/5 수준으로 유지          만약 10mb 수준의 간단한 프로세스(백그라운드 실행중인 메신저 프로그램 수준) = 2차 수준 테이블 수는 3~4개 = 대략 16kb(기존 4mb에 비해 1/20 이하)      추가적으로 다중 단계 페이징에서는 페이지가 연속적으로 배치되어 있지 않아도 된다.4 페이지 테이블의 구현페이지 테이블은 아래 그림처럼 구성되어 있다.프로세스 하나에는 페이지 테이블이 하나씩 존재한다.      액세스 비트 : 메모리 액세스 여부 표시        수정 비트 : 메모리에 적재한 이후 수정했는지 여부        참조 비트 : 액세스 여부        보호(RWX) : 읽기, 쓰기, 실행 권한        저장 비트 : 참조 페이지의 메모리 저장 여부        프레임 번호 : 메인 메모리에서 페이지 프레임 번호  페이지 테이블 기준 레지스터(PTBR, Page Table Base Register)      페이지 테이블는 계산 및 입출력이 빠른 전용 레지스터에 저장하는 것이 이상적        하지만 대게 페이지 테이블의 크기가 커서 적합하지 않음. 👉 따라서 페이지 테이블은 메모리에 두고 이를 관리하기 위한 페이지 테이블 기준 레지스터(PTBR, Page Table Base Register)를 이용        문맥 교환 시 실제로 메모리 정보를 모두 교환하는 것보다, 레지스터의 값을 서로 바꾸는 편이 경제적  title: But!하지만 이 방법에서 특정 주소에 액세스 하려면 메모리 접근이 필요하고, 이는 속도를 느리게 한다.  구체적인 예를 들자면, 임의의 주소 i에 접근하려면 i의 페이지 번호와 PTBR 기준 주소 값을 찾아서 이 둘을 결합한 프레임 번호를 제공해야 한다. 이때, 페이지 번호 접근에 메모리 접근이 필요함👉 이 단점을 해결하기 위해 연관 레지스터나 변호나 우선 참조 버퍼(TLB, Translation Look-aside Buffer, 8장 참조)를 이용한다.연관 레지스터를 이용하는 방법은 다음 세가지로 나뉜다.      직접 매핑(direct mapping) 주소 변환        연관 매핑(associative mapping) 주소 변환        연관 직접 매핑을 결합한 주소 변환  4.1 직접 매핑(direct mapping)으로 주소 변환메모리나 캐시에 프로세스 메모리를 구성하는 모든 페이지 항목이 존재하는 페이지 테이블을 유지하며, 이를 PTBR의 값과 페이지 번호를 통해 물리적 주소를 구하는 방식이다.🟢 레지스터만 변경해도 페이지 테이블을 변경 가능🟠 사용자 메모리 위치에 액세스하는 데 시간이 걸림      주소 p에 액세스 하기 위해 페이지 테이블에서 한번($p’$값 읽기), 해당 주소 p의 값(워드, 원하는 값)를 읽기 위해 한번 총 2번의 메모리 액세스가 필요해서 2배로 시간이 걸림        이를 해결하기 위해, 연관 레지스터 또는 보조예비 기억장치라고 하는 하드웨어 메모리 사용  그림 7-37의 예시를 들자면, 논리적 주소 V = (p, d)일 때,      페이지 테이블 기준 레지스터에 프로세스 페이지 테이블의 메모리 주소 b를 적재한다.        페이지 테이블 시작 주소 b에 참조 페이지 번호 p를 더함        b+p 주소를 통해 페이지 번호 p에 존재하는 프레임 $p’$를 얻는다.        $p’$에 오프셋 d 값을 더해 물리적 주소 $p’+d$를 얻는다.  4.2 연관 매핑(associative mapping)으로 주소 변환논리적 주소를 프로세스의 페이지 번호와 프로세스에 대응하는 프레임 번호가 있는 연관 레지스터의 집합으로 표현하고, 페이지 번호를 모든 키값과 동시 비교하여 프레임 번호를 얻어내는 방식각 레지스터는 키와 값으로 구성되며, 연관 레지스터에 항목을 추가할 수 있다.🟢 매우 빠르게 검색 가능, PTBR 불필요🟠 모든 항목을 동시 비교하는 하드웨어가 비쌈, 보통 일부 연관 레지스터만 구축함4.3 연관 직접 매핑을 결합한 주소 변환앞선 두 방법을 혼용하는 방법, 최근에 사용한 페이지는 연관 레지스터에, 연관 레지스터에 없는 페이지는 직접 매핑 하는 방법이다.프로세스가 메모리의 정보를 균일하게 액세스하는 것이 아니라,      많이 참조하는 정보        방금 참조하는 정보(시간 지역성)        방금 참조한 정보의 주변 메모리에 존재하는 정보(공간 지역성)  이상을 자주 참조하는 경향이 있으며 이를 지역성(locality)이라고 한다.그림 7-39로 구체적인 예를 들자면, 논리적 주소가 V=(p, d)일 때,      먼저 페이지 번호 p를 부분 연관 사상표에서 모든 항목을 동시에 조사해 페이지 번호 p를 찾는다.    1-1. 존재한다면, 프레임 번호 p’를 찾아 오프셋 d를 더해 물리적 주소 p’+d를 얻는다.        만약 부분연관 사상표에 존재하지 않는다면, PTBR의 페이지 테이블 주소와 페이지 번호 p를 합쳐 페이지 테이블 번호 b+p를 찾는다.        직접 사상표(페이지 테이블)에서 프레임 번호 p’를 얻어 물리적 주소 p’+d를 얻는다.  과정 1에서 부분 연관 사상표(연관 레지스터)에서 페이지 번호를 발견하는 비율을 적중률이라고 한다.    연관 레지스터 탐색이 50나노초, 메모리 접근이 750 나노초 걸린다고 할때, 과정 1-1(적중 성공 시), 800나노초가 걸린다.만약 과정 2(적중 실패 시)에는 페이지 테이블 액세스 (b), 프레임 번호 액세스(p’), 총 2번 액세스 해야하므로 50+ 750+750=1550나노초가 소요된다.이를 유효 접근 시간을 적중률 80%에 대입해 계산하면 그림 7-40-1처럼, 적중률 90%에 대입해 계산하면 그림 7-40-2처럼 계산된다.5 공유 페이지페이징 시스템에서는 프로세스를 메모리에 연속적으로 할당할 필요가 없기 때문에 여러 프로세스가 일부 페이지를 공유할 수 있다.공유를 위해서 각 프로세스는 메모리의 같은 페이지(지역)을 가리키게 해야하며, 재진입을 허용하므로 재진입 코드(reentrant code) 혹은 순수 코드(pure code)라고도 한다.동시에 접근이 가능하게 하기 위해 수정이 불가능한 읽기 전용이며, 주로 공유 라이브러리 코드 등에 사용됨공유 페이지는 스레드가 텍스트와 메모리를 공유하는 방법과 비슷한데, 모든 프로세스에서 논리적 주소 공간의 동일한 위치(논리적 주소)는 동일한 물리적 주소로 매핑된다.예를 들면, 그림 7-41 에서      물리적 주소 3번 위치는 프로세스 1과 프로세스 3이 공유 (공유 페이지 3)        물리적 주소 6번 위치는 프로세스 1과 프로세스 2가 공유 (공유 페이지 6)        프로세스 1과 프로세스 3에서 공유 페이지 3번은 페이지 테이블 2번 위치에 동일하게 존재해야 한다.        프로세스 1와 프로세스 2에서 공유 페이지 6번은 페이지 테이블 3번 위치에 동일하게 존재해야 한다.  이렇게 설계한 이유는 공유페이지를 다른 프로세스가 공유하려면 프로그래밍 시, 공통 코드로 접근하는 코드가 접근하는 위치가 모두 같아야 하기 때문이다.      만약, 동일한 공유페이지를 접근하는 코드를 가지고 있는 두 프로세스 A, B가 있다면,                  A 프로세스의 공유 페이지 접근 코드는 get_share_from_3()                    B 프로세스의 공유 페이지 접근 코드는 get_share_from_2()                    위 두 코드의 결과는 같아야 하므로, 공유 페이지의 내용은 물리적 주소의 2번과 3번에 동일하게 존재해야할 것이다. (=같은 데이터가 메모리 내에 2개, 즉 공유가 아님)            그림 7-42처럼 각 프로세스가 ed1~ed3를 공유한다면 이를 공유 페이지로 만들고 data 1~3만 메모리에 적재하면 되므로, 이용 메모리가 크게 줄어든다.페이지 공유는 주로 컴파일러, 윈도우 시스템, 데이터베이스 시스템 등에 사용된다.6 페이징에서 보호프로세스 하나가 다른 프로세스의 데이터 영역을 무단으로 읽거나 쓰게 하면 안되므로, 페이지 별로 액세스 권한을 다르게 부여할 수 있다.그림 7-43에서 각 페이지 테이블 항목에 보호 비트를 추가해 페이지 보호를 할 수 있다.보통 1의 경우 접근 가능, 0의 경우 접근 불가능이다.그림 7-44는 불법적인 시도를 막는 과정이다.04 분산 메모리 할당 2 : 세그먼테이션(segmentation)1 세그먼테이션의 개념메모리의 사용자 관점을 지원하는 비연속 메모리 할당 방법으로, 논리적 영역을 세그먼트의 집합으로 인식한다. 즉 세그먼테이션은 그림 7-45 처럼 프로세스 관점에서 페이징과 달리 크기가 다양한 세그먼트로 메모리를 나눈다.각 세그먼트는 컴파일러가 원시 프로그램을 실행 프로그램으로 변환하면서 프로그램을 구성하는 서브루틴, 프로시저, 함수나 모듈 단위로 구성함.즉, 세그먼트는 연관된 기능을 수행하는 하나의 모듈 단위가 많으며, 세그먼트는 연속된 위치에 구성하되 서로 인접할 필요는 없다.그림 7-46은 일정한 크기의 페이지로 나누는 페이징과 가변 분할이 가능한 세그먼테이션의 비교이다.하드웨어 보호나 관리 또한 페이징과 비슷하거나 동일2 세그먼테이션에서 하드웨어 구조와 원리그림 7-47은 프로세서로 생성한 세그먼트의 논리적 주소로, IBM 370 예처럼 세그먼트 번호 s와 오프셋 d로 구성세그먼트 번호는 8비트이므로 총 256개가 가능하며, 각 세그먼트는 64KB가 최대 크기이다.페이징과 마찬가지로 각 프로세스는 고유의 세그먼트 테이블을 메모리에 유지.세그먼트 테이블의 항목은      세그먼트가 적재될 메모리의 시작 주소(세그먼트 기준)        잘못된 주소 방지를 위한 세그먼트 길이(세그먼트 경계)이다.  추가로 세그먼트 테이블을 위한 레지스터가 둘이 존재한다.      세그먼트 테이블 기준 레지스터(STBR)        세그먼트 테이블 길이 레지스터(STLR)  세그먼트 테이블은 각 세그먼트 경계와 기준 항목을 가지고 메모리 주소의 시작과 세그먼트의 끝을 나타내느 기준과 경계 레지스터의 쌍들로 구성된 배열이다.세그먼트 테이블 레지스터는 주로 위의 논리적 시작 주소를 검사하는데 사용한다.그림 7-48의 예시를 보자면,      접근하려는 세그먼트 번호 s를 세그먼트 길이 레지스터(STLR)과 비교    1-1. 만약 실패한 경우 (0 &gt; s || s &gt; STLR), 오류 발생        이하일 경우 (0&lt;= s &lt; STLR), s + STLR을 이용해 세그먼트 테이블 주소 산출        세그먼트 테이블에서 보호 비트를 확인해 접근 가능한지 확인        세그먼트 테이블에서 세그먼트 경계(SL)과 세그먼트 기준(SB) 값을 읽는다.        접근하려는 세그먼트 주소 오프셋 d가 세그먼트 경계(SL)과 비교    3-1. 만약 실패한 경우 (0 &gt; d || d &gt; SL), 세그먼트 실패        이하일 경우 (0&lt;= d &lt; SL), SB + d를 통해 원하는 워드 주소에 접근  세그먼테이션도 일련의 과정이 페이징과 비슷하며, 메모리 액세스 과정으로 인해 속도가 매우 느리다.3 세그먼트 공유세그먼트 공유는 페이징 시스템과 달리 페이지 테이블의 공유 항목을 표시 할 필요 없이 해당 항목을 공유로 선언하면 된다. 즉, 다른 프로세스에서 동일한 메모리 주소를 지정해 접근하면 된다.추가로 공유를 위한 보호체계와 사용자 승인을 통과하면 액세스 할 수 있다.그림 7-50은 세그먼테이션에서 세그먼트를 세그먼트 테이블 주소를 동일하게 표시하여 공유하는 예시이다.4 페이징과 세그먼테이션 비교공통점  프로그램을 나눈 세그먼트나 페이징에서 메모리의 빈 공간을 찾아 할당차이점      페이징과 달리 프로그램을 나누는 크기가 변함.        세그먼테이션은 최적, 최초 적합 알고리즘 같은 동적 메모리 할당 방법을 이용        따라서 세그먼테이션은 외부 단편화 발생 가능          압축, 대기, 프로세스 큐에서 충분히 작은 프로세스 찾기, 세그먼트 크기 줄이기 등으로 해결            페이징은 크기가 일정하므로 물리적 주소 없이도 큰 가상 주소 공간이 가능        세그먼트는 프로그램과 데이터를 논리적으로 독립된 주소 공간으로 나누고 쉽게 공유,보호 할 수 있음  5 페이지화된 세그먼테이션페이지화된 세그먼테이션은페이지의      내부 단편화 발생        메모리의 효율적 사용        작업 크기 동일에 의한 다양한 알고리즘  세그먼테이션의      외부 단편화 발생        가변적인 데이터 구조와 모듈 처리        공유와 보호의 지원 편리  특성을 합친 시스템이다. 즉, 외부 단편화 문제와 할당 과정의 어려움을 해결했다.멀틱스 시스템과 인텔 386 계열에서 사용했다.멀틱스 시스템의 논리적 주소는 그림 7-51 처럼 총 32 비트이며,      18 비트의 세그먼트 번호        16비트의 세그먼트 오프셋 : 페이지 크기보다 작으면, 해당 세그먼트는 페이지가 하나                  6비트의 페이지 번호 : 프레임 번호 제공을 위한 페이지 테이블 인덱스                    10비트의 페이지 오프셋 : 페이지 테이블 내의 위 페이지 번호 주소에서 구할 수 있는 프레임 번호와 더해서 페이지 주소 찾음            으로 구성되어 있다.엄밀히 말하면 멀틱스의 18비트의 세그먼트 번호는 너무 커서 추가로 세그먼트테이블을 페이징했다.8비트 페이지 번호와 10비트 오프셋으로 분할해 세그먼트 페이지의 물리적 주소를 색인해야 한다.이를 통해, 논리적 주소를 세그먼트들로 나누고, 이 세그먼트를 고정된 페이지들로 나눌 수 있다.페이지화 세그먼트의 주소 변환 과정그림 7-52는 페이징 세그먼테이션의 주소 변환 과정이다. 초기의 논리적 구조는 {s, d}로 시작되며, 또 d는 {p, d’}로 나눌 수 있다.      프로세서는 논리적 주소의 세그먼트 번호(s)와 세그먼트 테이블 기준 레지스터(STBR)를 이용해(s+STBR) 세그먼트 테이블 확인        세그먼트 오프셋을 세그먼트 길이와 비교하여 (d &lt;= 세그먼트 길이) 오류를 확인한다.        오프셋(d)을 나누어 세그먼트 페이지 테이블을 찾기 위한 논리적 주소({p, d’})로 만든다.        해당 세그먼트 페이지 테이블에서 페이지 번호(페이지 테이블 기준 + p)를 찾아 이에 대응하는 페이지 프레임(f)을 찾기        페이지 프레임(f)을 오프셋(d’)과 결합해(f+d’) 메모리 주소 생성  그림 7-53은 위의 주소변환 과정을 논리적 주소 4, 2, 64를 변환하는 과정이다.세그먼트 4 테이블의 페이지 번호 2에서 프레임 번호 1을 찾은 뒤, 이를 페이지 오프셋 64를 이용해 프레임 1로 매핑한다.각 프로세스는 세그먼트 테이블과 세그먼트 테이블 시작 주소를 가진 레지스터가 있음각 세그먼트는 자신의 페이지 테이블이 있다.각 세그먼트의 마지막 페이지는 내부 단편화 존재, 외부 단편화는 없음.복잡하지만 할당 과정을 쉽게 해결했다.그림 7-2는 이를 정리한 장단점 표이다."
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%208-%EA%B0%80%EC%83%81%20%EB%A9%94%EB%AA%A8%EB%A6%AC.html": {
    title: "OS 정리-Chap 8-가상 메모리",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%208-%EA%B0%80%EC%83%81%20%EB%A9%94%EB%AA%A8%EB%A6%AC.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true8. 가상 메모리title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 가상 메모리의 이해사용자와 논리적 주소를 물리적으로 분리해 사용자가 메인 메모리 용량을 초과한 데이터를 보조 기억장치에 할당 후, 주소를 지정해 메모리를 제한 없이 사용할 수 있도록 하는 것가상 주소(논리적 주소, 프로그램 주소)는 실행 중인 프로그램이 직접 참조하며, 이 가상 주소를 이용해 물리적 주소를 얻어내는 과정을 매핑(Mapping)이라 하며, 성능에 큰 영향을 미친다.:green_circle: PROS+ 가상 메모리 주소를 이용해 다중 프로그래밍 환경 등에서 메인 메모리보다 더 큰 저장 공간을 제공할 수 있다. + 프로세스의 전체를 동시에 적재하지 않아도 스왑과 캐시를 통해 실행 가능+ 중첩을 고려하지 않고 더욱 쉽게 프로그래밍 가능+ 프로세서 이용률과 처리율 향상, 단 응답시간과 반환 시간은 향상되지 않음.:orange_circle: CONS- 메모리와 디스크 사이의 이동량과 스와핑 증가와 이를 위한 공간 필요- 페이지 적재 및 복귀를 위한 알고리즘 구현 필요- 페이지 부재 시 처리를 위한 알고리즘 구현 필요- 페이지 부재가 잦을 경우 심각한 성능 저하1 가상 메모리의 개념과 원리실제 메모리 사용 시, 프로세스는 다음과 같은 방식으로 사용하므로, 메모리를 동시에 적재할 필요 없다.  모든 메모리를 동시에 사용하지 않는다.  예외 처리, 오류 처리 코드는 자주 혹은 전혀 사용하지 않음  배열, 리스트, 테이블 등은 실제 사용한 크기보다 넉넉히 크게 사용한다.      문서 편집기의 복사하기, 붙이기, 잘라내기, 삽입하기 메뉴는 선택한 메뉴 하나만 메인 메모리에 적재해도 됨    라인 : 메인 메모리와 캐시 사이에서 데이터 이동시 캐시의 공간 지역성을 활용하기 위해, 캐쉬는 라인 단위의 데이터를 가져오게 된다.  위 그림처럼 보조 장치에 있는 데이터는 논리적 주소 공간에 할당되며, 데이터가 불려질 경우, 메인 메모리로 이동된다.  가상 주소를 물리적 주소로 매핑하는 과정은 위 그림처럼 표시한다.  매핑 방법 중 동적 주소 변환(DAT, Dynamic Address Translation)을 나타낸 그림.          가상 주소가 연속적으로 배치된다고 하여, 메인 메모리에서도 연속적일 필요가 없어(인위적 연속성), 사용 시, 데이터의 적재 위치를 고려하지 않아, 프로그래밍이 쉽다.        2단계 메모리 관리 방법 : 2차 기억 장치에 저장된 프로세스를 실행하려면 사용할 코드와 데이터를 모두 메인 메모리에 옮겨야 하며, 이를 통해 여러 사용자가 메모리를 효율적으로 공유 가능2 가상 주소와 테이블 항목페이지는 디스크에서 메인 메모리로 블록 단위로 옮겨져 페이지 프레임에 적재됨.이때 페이지 프레임은 가상 페이지 크기와 동일하며, 어떠한 페이지든지 들어갈 수 있다.이렇게 가상주소와 물리적 위치는 페이지 테이블 항목을 통해 매핑되게 됨.페이징 시스템에서 가상 주소는 아래 그림과 같이 구성됨.  가상 페이지 번호(p)  페이지 오프셋(d)예시32 비트의 가상 주소, 4KB($2^{12}$) 크기의 페이지일 경우,  페이지 크기 만큼의 오프셋이 필요하므로 12 bit는 페이지 오프셋  32비트 중 나머지 중 20 bit로 가상 페이지 번호 (20bit) = 총 프레임 $2^{20}$(1048576)개로 구성  예를 들어, 가상 주소가 0x009B18A0일 경우, 가상 페이지 번호는 2481, 페이지 오프셋은 2208가상 주소 변환을 위한 테이블 항목은 위 그림과 같다.보통 한 항목에 32비트이며, 세부적으로 메인 메모리 저장 여부를 표시하는 제어 비트와 페이지 프레임 번호 비트로 구성됨.위 그림은 가상 주소를 물리적 주소로 매핑하는 과정이다.  가상 주소 페이지 번호 0x2로 페이지 테이블 항목 찾음  페이지 테이블 항목에서 프레임 번호 0x8을 얻음  프레임 번호와 가상 주소의 오프셋을 결합해 물리적 주소 획득  프레임 번호 $\\times$ 페이지 크기 + 오프셋 = 데이터의 물리적 위치02 요구 페이징1 요구 페이징의 개념요구 페이징은 프로세스 시작 시, 디스크에서 메인 메모리로 모든 프로세스 데이터를 스와핑하는 순수 스와핑과 달리, 실행 중인 프로세스들의 요구 페이지만 반입한다.이를 지연 기술 또는 지연 스와퍼(lazy swapper)이라 하며, 마찬가지로 페이지 테이블을 유지한다.주로 가상 메모리에서 많이 사용하며, 프로그램들이 모듈을 순차적으로 사용하는 특징을 이용한다.요구 페이징의 페이지 테이블은 해당 페이지가 메모리에 적재되어 있는 지를 1과 0으로 표기하는 타당-비타당 비트가 추가로 존재한다.  1은 유효, 0은 유효하지 않은 페이지이다.위 그림의 경우 페이지 A,C,F는 메인 메모리에 존재하므로 타당 비트가 1이며, 나머지는 0이다.2 페이지 부재만약, 프로세스가 요구한 페이지의 타당 비트가 1(=메인 메모리에 적재 중)일 경우, 신속히 사용할 수 있다. 하지만 페이지가 메모리에 적재되어 있지 않을 경우 이를 페이지 부재(page fault)라고 하며, 운영체제에 하드웨어가 트랩을 걸게 되며, 아래 그림과 같은 과정을 거쳐 처리한다.      프로세스 B가 가상 주소의 페이지 번호가 1인 가상 페이지에 액세스        페이지 테이블의 타당-비타당 비트 검사, 1일 경우 명령 처리, 0일 경우 페이지 부재        페이지 부재로 트랩이 발생하여 제어권이 운영체제로 넘김, 운영체제는 해당 페이지를 디스크에서 가져오기로 함        메모리에서 빈 프레임 하나 선택, 없을 경우 나중에 배울 교체 혹은 큐잉 필요        할당된 프레임에 요구된 페이지를 디스크에 가져오며, 그 동안 프로세서는 다른 프로세스의 동작 스케줄링        해당 페이지를 디스크에서 메모리로 가져옴        요구된 페이지가 메모리에 적재 완료, 페이지 테이블 항목의 타당 비트 설정        주소 트랩을 발생시켜 인터럽트 명령어 재시작  🟢 PROS+ 다중 프로그래밍의 정도를 증가, 액세스하지 않을 시 적재하지 않으므로 메모리 효율적+ 초기 시작 시 프로그램의 적재 지연 감소, 디스크 오버헤드 감소+ 적재된 페이지 중 하나를 수정할 때까지 페이지들은 여러 프로그램이 공유하므로 쓰기복사(COW, Copy-On-Write) 기술로 더 많은 자원을 저장할 수 있다.+ 프로그램을 실행할 충분한 메모리가 없는 시스템에서도 대용량 프로그램 실행 가능+ 프로그래머가 중첩 구조보다 더욱 쉽게 구현 가능:orange_circle: CONS- 나중에 배울 프리 페이징(미리 페이지 가져옴)에 비해 약간의 초기 지연 발생- 낮은 비용, 낮은 성능의 시스템에서는 페이지 대체를 지원하기 위한 하드웨어 없음- 페이지 교체 알고리즘을 포함하는 메모리 관리가 복잡- 페이지 액세스 시간은 페이지 부재로 인해 예측이 어려움쓰기복사(COW, Copy-On-Write)여러 작업이 메모리나 디스크에 저장된 동일한 자원을 사용 시, 별도의 복사본을 만들지 않고 동일한 자원 포인터를 부여할 수 있다.공유 페이지와 다른 점은 쓰기복사는 읽기 전용 뿐만 아니라 쓰기 작업이 가능하다는 점이다.또한 다중 스레드 시스템의 잠금을 이용한 방법 대신 내부 참조 카운터의 증가와 감소를 이용해 구현(참조를 아무도 안하면(= 참조 카운터가 0이면) 안전히 해제 가능):green_circle: PROS+ 따라서 효율적인 메모리 사용과 쓰기 작업이 동시에 가능+ 동일한 자원을 사용하는 프로세스 간의 메모리 공유 가능+ 메모리를 불연속적으로 사용 가능:orange_circle: CONS- 수정이 많아지면 메모리 사용량 증가- 커널 수준 쓰기 복사를 구현하기 복잡함- 파일 디스크상 연속성(인접성)을 차단, 단편화로 인한 디스크 탐색량 증가여러 운영체제에서 사용 중이며, 데이터베이스의 스냅샷(snapshot) 등에도 이용위의 그림은 쓰기 복사의 예시이다.  각 논리 파일 블록 3곳($L_0, L_1, L_2$)은 물리적 비연속 블록 3곳($P_0, P_2, P_3$)을 가리키고 있다.  이때, $L_1$이 새로운 쓰기를 실행하였다.  새로운 내용을 가진 $P_2$은 이를 빈 새로운 물리 블록 $P_6$에 할당한다.  논리 파일 블록 $L_2$가 가르키는 번호를 $P_6$로 옮긴다.          만약 $P_2$를 참조하던 다른 프로세스는 아무변화 없이 사용할 수 있다.      3 페이지 성능페이지 부재 확률을 $p$, 메모리 액세스 시간을 MAT(Memory Access Time, 보통 10~200나노초)이라고 할때 유효 액세스 시간(EAT, Effective Access Time)은 다음과 같이 계산된다.\\[페이지\\ 부재\\ 확률: 0 \\leq p \\leq 1 \\\\유효\\ 액세스\\ 시간 = (페이지\\ 적중\\ 확률) \\times MAT + (페이지\\ 부재\\ 확률) \\times (페이지\\ 부재\\ 해결\\ 시간)\\\\ = (1-p)\\times MAT + p \\times (페이지\\ 부재\\ 해결\\ 시간)\\]페이지 부재 해결 시간은 크게 다음 항목들의 합이다.  인터럽트 처리 시간 : 아래에 포함  프로세스 재시작 시간 : 인터럽트 처리시간을 합쳐도 명령어 수백개, 1~100 마이크로초  페이지 교체 시간 : 디스크의 헤드 이동에 따른 탐색 시간(5ms)+ 지연시간(3ms)+ 전송시간(0.05ms) = 약 8밀리초($10^{-3}$)즉, 페이지 부채를 처리하는데는, 인터럽트 처리와 프로세스 재시작은 오래걸리지 않지만, 페이지 교체 시간이 가장 많이 차지한다.이를 이용해 유효 액세스 시간을 계산하면 $200+p\\times7,999,800$이다.액세스 1000개 중 하나의 페이지 부채만 생겨도 40배의 시간이 지체되며, 10퍼센트 정도의 성능만 희생 시키려면 페이지 부채는 399,990번 중 한번만 발생해야 한다.즉, 페이지 부재를 적게 일으키기 위한 알고리즘과 정책이 필요하다.4 페이지 성능을 높이는 페이지 대치만약, 프로세스들이 요구하는 페이지가 갑자기 늘어나 메모리의 프레임이 부족해지게 되면 페이지 대치로 해결.페이지 대치는 페이지 부재 발생 시, 메인 메모리에 있는 사용하지 않는 페이지를 새로운 페이지로 바꾸며, 페이지 대치를 담당하는 핸들러(서비스 루틴)은 프로그램에게 이를 통보한다.페이지 대치 과정은 위 그림과 같다.페이지 부재로 발생한 페이지 대치 과정은 페이지 2개가 스왑 인, 아웃하는 과정이므로, 액세스 시간이 증가, 시스템에 부담하지만 위의 CONs을 수정된 비트를 활용해 감소 가능하며, 보조기억장치를 활용할 수 있게 된다.03 페이지 대치 알고리즘1 페이지 부재와 프레임 수메인 메모리에 프레임 수가 많을 수록, 많은 페이지를 메인 메모리에 담을 수 있으므로, 페이지 부재가 줄어든다.위 그림은 (a)와 같은 참조가 이루어질시 프레임 수에 따른 페이지 부재 횟수이다.2 선입선출(FIFO, First In First Out) 대치 알고리즘대기 큐 내에서 가장 오래 기다린 페이지부터 대치하는 알고리즘.새로운 페이지는 페이지 테이블 항목을 변경 후, 선입선출 큐의 마지막 위치에 삽입한다.큐의 길이는 곧 프레임의 수이다.:green_circle: PROS+ 프로그램 작성이 쉬움🟠 CONS- 성능이 항상 좋진 않다.  벨레디의 변이 (Belady’s anomaly) : 프레임이 오히려 많아지는데 페이지 부재 횟수가 늘어남(위 그림의 프레임 수 3~4 구간)3 최적 페이지 대치(OPT, OPTimal replacement algorithm) 알고리즘모든 알고리즘 중 페이지 부재 비율이 가장 낮은 이상적인 알고리즘.앞으로 가장 오랫동안 사용하지 않을 페이지를 대치한다.하지만 이는 미래에 참조할 페이지를 정확히 알 수 없는 현재 기술로는 구현 불가능한 이상적인 알고리즘이며, 비교 연구에 주로 사용된다.4 최근 최소 사용 대치(LRU, Least Recently Used) 알고리즘오랫동안 사용하지 않은 페이지를 최우선적으로 대치하는 알고리즘과거에 오랫동안 사용하지 않았다면, 미래에도 오랫동안 사용하지 않을 것이라는 근사치와 지역성을 이용한 정책이다.최근 최소 사용 대치 알고리즘은 하드웨어를 요구하는 경우가 많은데, 아래 두 가지 방법으로 많이 구현한다.4.1 카운터(계수기)를 이용한 순서 결정 방법      각 페이지 테이블 항목에 사용 시간 레지스터를 매핑        프로세서에 논리 클록을 추가 후, 카운터 필드를 덧붙임        메모리 관리 장치(MMU)가 페이지 참조 때마다 클록 증가        클록 레지스터의 내용은 복사되어 페이지의 해당 페이지 테이블에 있는 사용시간 레지스터에 추가, 즉 최후 참조 시간을 페이지에 넣게 된다.        가장 낮은 카운터 값(=가장 오랫동안 사용하지 않음)의 페이지 제거  페이지 탐색 시간과 페이지 테이블 변화 과정 만큼의 오버헤드가 존재4.2 스택을 이용한 순서 결정 방법페이지 번호를 스택에 넣어 관리하고 페이지 참조마다 페이지 번호를 스택의 톱에 두어 순서를 결정하는 방법따라서 스택의 가장 위는 가장 최근에 사용한 페이지가 된다.가장 아래에 있는 페이지가 가장 오래된 페이지가 되며, 페이지 부재 발생 시 교체된다.+ 필드 비교, 업데이트, 계수기 등이 필요 없음- 스택의 중간에 내용을 꺼내야 하므로 이중 연결 리스트로 구현해야 하며, 따라서 오버헤드가 생긴다.5 최근 최소 사용 근접 알고리즘최근 최소 사용 알고리즘의 구현에 필요한 하드웨어와 성능 때문에 참조 비트를 이용한 근접 알고리즘을 이용함.  참조 비트(reference bit) : 참조되거나 참조에 관련된 페이지는 1로, 아니면 주기적으로 0으로 초기화하여 대치의 대상이 되게 한다.5.1 참조 비트 알고리즘  각 페이지마다 8비트(1바이트) 정보에 일정한 간격으로 참조 비트를 기록  가장 최상위 비트에 참조를 표기하고, 매 참조 시마다 비트를 오른쪽으로 쉬프트한다.  각 페이지마다 참조 비트를 비교하여 크기가 클수록 참조를 최근에 했거나 참조 횟수가 빈번한 것이다.  따라서 가장 작은 참조 비트를 가진 페이지는 교체된다. 크기가 같은 경우 선입선출로 결정된다.각 페이지의 참조 비트의 크기만큼 참조 기록을 유지하게 된다.5.2 시계(2차적 기회 대치) 알고리즘각 프레임의 사용 여부를 나타내는 참조 비트와 원형 버퍼, 포인터를 이용한 알고리즘.:green_circle: PROS+ 자주 사용하는 페이지의 대치를 방지할 수 있음+ 최근 최소 사용 알고리즘과 성능은 비슷, 오버헤드는 더 적음+ 시계 알고리즘에 사용 비트를 추가 시, 수정 여부, 디스크 저장 여부 등을 이용해 더 강력한 페이지 대치 알고리즘을 작성 가능  페이지가 메모리에 처음 들어오면 참조 비트를 1로 설정한다. 이후, 참조될 때마다 참조 비트를 1로 설정한다.  페이지가 프레임에 할당되면 해당 프레임은 원형 큐로 들어가며, 큐 내의 다음 프레임을 가르키는 포인터를 가지게 되며, 다음 프레임을 가리키던 프레임은 방금 추가된 프레임을 가르키게 된다.  페이지 대치가 일어나면, 원형 큐 내의 버퍼를 조사한다.  만약, 프레임의 참조 비트가 1이라면, 0으로 조정하고, 현재 시간으로 고친다.(2차적 기회)  만약, 프레임의 참조 비트가 0이라면, 즉시 프레임을 새로운 페이지로 대치하고 1로 돌아간다.  만약, 원형 큐를 한바퀴 조사하는 동안에 참조 비트가 모두 1이였다면, 3으로 돌아간다.위 그림은 간단한 시계 알고리즘의 예시이다.선입선출 알고리즘보다 나은 성능을 보여준다.5.3 NUR(Not Used Recently) 알고리즘참조 비트과 수정 비트을 이용해 최근 사용하지 않은 페이지를 교체하는 알고리즘  참조 비트(R) : 해당 페이지의 액세스 여부를 확인해서 최근에 사용한 페이지들을 유지(초기값 : 0=참조 안함)  수정 비트(M) : 페이지의 수정 여부 확인(초기값 : 0=수정 안함)이때, 두 비트 (R, M)를 확인해 (0, 0), (0, 1), (1, 0), (1, 1) 순서로 대치된다.  참조한 페이지는 미래에 다시 사용될 가능성이 높음  페이지 부재 발생 시, 수정한 페이지는 해당 페이지를 디스크에 복사한 뒤 제거 되어야 하므로 디스크 엑세스가 한번 더 필요함.          수정되지 않은 페이지는 원본이 디스크에 그대로 있으므로 복사하지 않아도 된다.      따라서, 최대한 수정이나 참조가 되지 않은 페이지를 고른다.5.4 최소 사용 빈도수(LFU, Least Frequently Used) 알고리즘각 페이지마다 참조 횟수 카운터가 있으며, 수가 가장 작은 페이지를 대치시간이 지나면 참조 횟수가 점점 줄어들게 만들어, 과거에 참조가 많았던 쓸모없는 페이지를 대체되게 할 수 있다.5.5 최대 사용 빈도수(MFU, Most Frequently Used) 알고리즘위와 반대로, 계수가 높으면 충분히 사용했다고 생각하여 제거하는 방식.보통 성능도 떨어지고 비용도 많이 들어서 사용하지 않는다.5.6 페이지 버퍼링페이지 버퍼링은 대체 대상으로 선택된 페이지를 즉시 교체하지 않고, 포인터 리스트 2개를 이용해 잠시 동안 메인 메모리에 유지하는 방식이다.다른 대체 알고리즘과 병행하여 성능을 높일 수 있다.  수정 안된 가용 페이지 리스트, 혹은 자유 프레임 풀은 비어있거나 수정한 적이 없는 페이지 프레임의 리스트며, 페이지 테이블 항목만 삭제하고 메모리 데이터를 해제하지 않고 대기한다.          수정한 적없는 프레임은 다른 페이지를 넣기 위해 대치 요청이 들어오면 해제한다.        변경 페이지 리스트는 수정한 적이 있는 페이지 프레임들의 모임으로, 매 페이지는 페이지 테이블 항목만 삭제하고 메모리를 해제하지 않고 대기한다.          변경 페이지 프레임들은 일정 시기마다 일괄적으로 디스크에 복사된 후, 가용 페이지 리스트로 빈 프레임을 보내어 효율적인 디스크 액세스를 달성한다.        페이지 부재가 일어나면, 운영체제는 해당 두 리스트의 대기하고 있는 프레임들을 확인한 뒤, 리스트 내에 존재한다면, 리스트에서 제외하고, 페이지 테이블 항목만 추가하여, 디스크에 엑세스하지 않고 즉시 적재할 수 있다.    페이지 대치 알고리즘의 비교  페이지 대치 알고리즘은 가용한 프레임 수가 적을 때 더욱 유용하다는 것을 알 수 있다.04 프레임 할당 알고리즘1 프레임 할당 알고리즘의 필요성여러 프로세스를 한 시스템에서 사용할 때, 프레임 할당 방법으로  모두 한꺼번에 공유하면, 불공정하게 일부 프로세스가 프레임을 독점할 수 있다.  균일하게 배분하면 메모리를 크게 사용하는 프로세스는 스래싱이, 메모리를 적게 사용하는 프로세스는 낭비가 일어날 수 있다.따라서, 공정하고 페이지 부재를 줄이면서, 효율적인 메모리 사용을 위해 프레임 할당 알고리즘 또한 중요하다.보통 프로세스들은 작업에 필요한 최소 프레임 수를 정의하고 있으며, 명령어 하나를 실행 중에 페이지 부재가 일어나면 프레임을 할당받고 처음부터 다시 시작한다.  예를 들어 메모리 참조 명령은 직접 참조는 프레임 하나, 간접 참조이면 프레임 셋이 필요해야 실행 가능하다.따라서 최소 프레임 수와, 최대 할당 가능한 프레임 수 사이에 적절한 프레임 수를 할당해야 한다.2 균일, 비례 프레임 할당 알고리즘앞서, 균일 프레임 할당은 낭비와 페이지 부재가 일어날 수 있다고 하였다.비례 프레임 할당 알고리즘은 프로세스의 가상 메모리 크기에 비례에 프레임을 할당 하는 방법으로 예시를 들자면,최대 프레임 갯수가 개인 프로세스 $P_i$의 가상 메모리 크기를 $s_i$라고 하면, 전체 프로세스가 갖는 가상 메모리 공간의 크기 $S$와 비례 프레임 할당 갯수 $a_i$는 다음과 같다. \\(S=\\sum s_i\\\\a_i=s_i/S\\times m\\)- 단, 이 방법은 미리 소프트웨어의 메모리 정보를 알고 있어야 하므로 오버헤드가 생긴다.추가로 우선순위를 결합하여, 우선순위가 낮은 프로세스는 우선순위가 높은 프로세스에 의해 페이지 대치가 될 수 있게 할 수 있다.현대에서는 균일과 비례 프레임 할당 방법 둘을 혼합하여 사용한다.05 메모리를 관리하는 프로세스 적재 정책메인 메모리에 너무 적은 수의 프로세스가 상주하면 프로세스가 메모리에 적재되기 위해 대기시간이 너무 길어지고, 효율성이 떨어진다.반대로 너무 많은 수의 프로세스가 상주하면 프로세스들이 차지하는 평균 페이지 감소로 페이지 부재가 자주 발생할 수 있다.1 스래싱(thrashing)1.1 스래싱의 개념할당된 프레임 수가 너무 부족해 실제 프로세스 작업 수행보다 페이지 교환에 더욱 시간을 보내게 되어 시스템 성능이 급격히 낮아지는 현상.1.2 스래싱의 발생 원인운영체제는 프로세서의 이용률을 감시하며, 이용률이 적다면 다중 프로그래밍 수준을 올리기위해 프로세스 수를 늘리게 되고, 따라서 페이지 부재가 생겨난다. 심지어, 페이지 부재를 프로세서 대신 페이징 처리 장치가 처리하면서, 프로세서의 이용률이 낮아지고, 이를 감지한 운영체제가 더욱 더 프로세스를 늘려 페이지 부재가 점점 심화될 수 있다.따라서 위의 그림처럼 어느 순간부터 오히려 프로세서 이용률이 떨어지는 스래싱이 발생하며, 이를 막기 위해서는 다중 프로그래밍의 정도를 낮추어야 한다.1.3 스래싱의 예방시스템이 프로세서 이용률과 시스템 페이지 부재 비율을 동시에 감시하면 스래싱을 감지할 수 있다.주로 프로세서 이용률은 낮고 시스템 페이지 부재 비율이 치솟으면 스래싱이다.지역 교환 알고리즘이나 우선순위 교환 알고리즘을 통해 미리 완화할 수 있다.  지역 교환 알고리즘 : 자기에게 할당된 프로세스 이외에는 대치 불가, 다른 프로세스로의 스래싱 전염을 막음.  우선순위 교환 알고리즘 : 우선순위가 높은 프로세스에서 일방적으로 낮은 우선순위의 프로세스의 프레임을 뺏음.다만 위 두 경우는 여러 프로세스가 동시에 페이지 부재에 들어가 스래싱이 발생하는 것을 막을 수 없다.따라서 다음 두가지 방법으로 적절한 프레임 수를 파악해 막을 수 있다.  작업 집합(Working set) 모델 : 프로세스가 실제로 얼만큼 프레임을 많이 사용하는지 검사해 지역 모델을 정의  지역 모델 : 프로세스 실행 시 프로그램은 보통 지역 몇 개로 중첩해서 구성하므로 한 지역에서 다른 지역으로 이동할 때 해당 지역의 크기만큼 프레임 할당          지역 : 적극적으로 동시에 사용하는 페이지의 집합(서브루틴, 함수, 변수, 리스트 등…), 보통 하나의 지역에서 수행을 마치고 다른 지역으로 이동하는 방식으로 사용됨.      데닝은 다중 프로그래밍 정도를 50% 정도로 제안2 지역성(구역성, 국부성, locality)  실행 중인 프로세스에서 나타나는 동일한 값이나 관련 저장 위치를 자주 액세스 하는 현상  한번 참조한 동일한 데이터나 저장 위치 근처의 데이터를 짧은 시간 안에 다시 참조하는 현상  어느 실행 단계 동안 메모리의 정보를 균일하게 액세스하는 것이 아니라 선호하는 특정 페이지만 집중적으로 참조하는 현상잠시동안 적은 양의 데이터만 집중적으로 참조하다 데이터의 또 다른 작은 규모의 데이터 덩어리로 이동하는 경향이 존재주로 프로그램들의 루프, 서브 프로그램, 스택, 변수들의 계산, 합계, 배열 순례, 순차적 코드 실행 등으로 발생크게 시간 지역성과 공간 지역성으로 분류한다.  시간 지역성          특정 동일한 자원, 메모리 위치를 상대적으로 짧은 시간 안에 재사용한다.순환(루프), 서브 프로그램, 스택, 계산이나 합계에 사용하는 변수 등이 예시        공간 지역성          프로세스가 메모리의 어떤 위치를 참조하면 그 근처를 이후에도 계속 참조할 가능성이 높다.데이터 정렬, 1차원 배열의 요소 탐색 같은 순차적 지역성, 배열 검색, 순차적 코드 실행, 근처의 관련 변수 선언 등이 예시      이를 이용해 적절한 프레임 수를 할당하여 스래싱을 예방할 수 있다.예를 들어 위의 그림은 프로세스의 메모리를 참조하는 실제 그래프이며, 특정 시간에 특정 번호와 그 주변을 집중적으로 참조함을 알 수 있다.3 작업 집합 모델(WSM, Working Set Model)작업 집합 모델을 이용하면 많이 참조하는 페이지 집합을 메모리 공간에 상주시켜 페이지 대치 현상 줄일 수 있다.작업 집합(Working set)이란, 가장 최근의 페이지 참조에 있는 페이지 집합을 의미하며, 프로그램의 지역 근사치가 된다.프로세스의 작업 집합 모델 내의 페이지들, 즉 최근에 참조한 페이지만 메인 메모리에 유지시키는 방법이다. 그리고 새로운 프로세스들은 메인 메모리에 자신들의 작업 집합을 적재할 수 있는 공간이 있을 때만 시작할 수 있다.작업 집합 모델 생성을 위해 :arrow_right: 작업 집합의 크기 정보 필요 :arrow_right: 작업 집합의 크기 정보는 작업 집합 창으로 구함.작업 집합 창은 현재 시간부터 특정 과거까지 참조한 페이지를 포함하는 범위를 의미한다.위 그림의 w이 작업 집합 창의 크기이다. w가 커질 수록, 과거 어느 시간까지 작업 집합에 포함할지 결정된다. 따라서 작업 집합의 크기는 위 그림처럼 일정하게 증가하다 프로그램 크기에 가까워질수록 증가량이 둔화된다.너무 크면, 작업 집합이 전체 프로그램이 되어 비효율적이며, 너무 작으면 페이지 부재 비율이 늘어날 것이다.작업 집합 창과 시간에 따른 작업 집합의 예시이다.매드닉과 도노반은 100000을 제안했다.작업 집합에서 가장 주요한 성질은 작업 집합 크기(WSS, Working Set size)로, 각 프로세스들의 작업 집합 크기의 합을 통해 전체 요구 프레임을 다음과 같이 구할 수 있다.\\(D = \\sum WSS_i\\)사용 가능한 유효 프레임 수보다 D값이 더 커지면 스래싱이 발생된다.운영체제에서는 이를 최대한 막기위해 작업집합을 예상하고 충분한 프레임을 할당한다.하지만 일시적으로 D가 커져 스레싱이 발생할 순간에는 프로세스 하나를 선택해 일시중단한 후, 해당 프로세스의 프레임을 다른 프로세스에 할당하여 스래싱을 예방한다.프로세스를 수행하는 동안 작업 집합의 크기는 위 그림처럼 계속 변하는데, 작업집합 크기가 일정한 안정기와 급격하게 변하는 과도기로 나뉜다.주로 새로운 페이지를 참조하거나, 다른 지역으로 이동, 프로세스의 전환 등에 의해 급격히 커지며, 새로운 지역성이 이전 지역을 대체해 급격히 작아진다.운영체제는 각 프로세스의 작업 집합을 감시하고 각 프로세스에 작업 집합 크기에 맞는 충분한 프레임을 할당한다.여분의 페이지 프레임이 있을 때는 준비 상태에 있는 다른 프로세스를 불러들인 후 프레임을 할당해 다중 프로그래밍의 정도를 증가시킨다.전체 작업 집합 크기의 합이 전체 유효 프레임 수보다 커지면 잠시 중지시킬 프로세스를 선정해 페이지를 회수하여 스래싱을 방지한다.🟢 PROS+ 페이지 부재 비율 감소+ 유동적이고 효율적인 메모리 사용+ 프리페이징(prepaging)에 유용:orange_circle: CONS- 작업 집합이 미래의 참조 페이지를 보장하지 않음(=페이지 부재는 사라지지 않음, 스래싱 조절 힘듦)- 작업 집합 크기와 구성 페이지들은 주기적으로 감시되고 변해야 한다.- 각 프로세스에서 작업 집합을 모두 측정하는 것은 현실적으로 불가능- 참조한 페이지 시간과 시간 순서로 된 페이지 큐 유지 관리 필요- 최적의 작업 집합 창의 크기 정하기 어려움작업 집합의 크기와 페이지 부재 비율의 그래프는 위와 같다.작업 집합의 크기가 클수록 부재 비율이 감소하지만, 메모리 효율이 나쁘다.작업 집합의 크기가 작으면 부재 비율이 높지만, 메모리 효율이 좋다.부재 비율 대 메모리 효율이 적절한 중간 지점을 찾는게 중요하다.4 페이지 부재 비율(PFF, Page Fault Frequency)페이지 부재가 일어난 비율, 페이지 환경에서 프로세스의 실행 측정 기준으로 사용페이지 부재 비율이 증가하는데 유효 프레임이 없으면 프로세스를 중단한 후, 페이지 부재 비율이 높은 프로세스에 할당한다.만약 페이지 부재 비율이 너무 낮다면 유효 프레임이 너무 많다는 것이므로, 프레임을 줄인다.그림 위 처럼 상한 값과 하한 값 사이의 적절한 값을 유지하는것이 관건이다.+ 작업 집합 모델 비해 페이지 부재 시마다 프레임 수를 조절할 수 있게 해주어 오버헤드가 적음- 단, 페이지 부재 비율 알고리즘은 페이지 참조가 새로운 지역으로 이동하는 과도기에는 제대로 작동하지 않는다.06 메모리 관리와 관련된 기타 이슈1 대치 범위대치가 일어나 희생될 페이지를 고르는 기준은 모든 프로세스의 페이지가 대상인 전역 대치(리눅스)와 프로세스 개별로 제한된 지역 대치(윈도우 XP)가 있다.보통 전역 대치는 구현이 쉽고 부담이 적고, 지역 대치는 성능 분석이 쉽다.1.1 전역 대치메인 메모리에 있는 모든 페이지를 자유롭게 선택하여 대치, 즉 다른 프로세스에게 프레임을 뺏길 수 있다.소프트웨어로 유지관리되며, 프레임이 모두 조사 가능하도록 잠금 해제 되어 있어야 함.+ 작업집합이 더 많거나 더 적은 페이지 프레임 할당에 따라 증가 및 감소됨.+ 구현이 쉽고 부담이 적다.+ 다량의 프레임이 필요한 커다란 프로세스가 존재하는 경우 유용, 대형 시스템에서 주로 사용+ 프로세스당 필요한 할당 추측 가능+ 대기 프로세스의 메모리가 낭비되지 않음- 뺏는데 시간이 천차 만별이므로 프로그램의 수행 시간이 불안정해진다.- 프로세스 별 차별적인 조절 불가1.2 지역 대치페이지 부재를 발생한 프로세스의 상주 페이지에서 대치할 페이지를 선택. 즉, 자기에게 할당된 프레임만 사용 가능.+ 고정 분할이나 작업 지합 모델 기반 균형 설정으로 메모리 할당을 조정한다.+ 프로세스의 중요도나 크기에 따라 메모리 할당을 조정해 성능을 향상시킬 수 있다.+ 성능이 일관되며 프로세스가 자신의 페이지 부재 비율을 월등히 제어할 수 있다.+ 확장성이 있으며, 프로세스가 공유 자원을 두고 경쟁하지 않아도 된다.- 단, 구현이 어렵고 부담이 크다.2 프리 페이징(prepaging)시작시 지역성 형성을 위해 많은 페이지 부재가 생기는 요구 페이징의 단점을 방지하는 방법예상되는 모든 페이지를 사전에 한꺼번에 메모리 내로 가져온다.+ 입출력 인터럽트 한번만 발생시켜 연속된 페이지를 한번에 메모리로 가져오므로 요구 페이징보다 성능이 좋다.예를 들어 어떤 프로세스가 입출력이나 프레임 부족 등으로 쉬게 되었다면, 작업 집합을 기억했다가 시작하기 전에 모든 페이지를 자동으로 가져온다.- 만약, 사용할 페이지를 잘못 예측하면 오히려 요구 페이징보다 성능이 떨어질 수 있으므로, 예측 알고리즘 성능이 중요하다.3 페이지 크기페이지 크기 또한 하드웨어 디자인에 매우 중요한 요소이다.페이지의 크기는 보통 2의 지수승이며, 256~4096 바이트나 워드이며, 메인 메모리의 크기와 프로그램 크기 자체에 영향을 받는다.페이지 크기가 작은 경우, 전송속도가 빠르지만, 전송 속도는 워낙 빨라 성능에 큰 영향을 끼치지 않는다.입출력 회전 지연시간과 오버헤드 때문에 페이지의 입출력의 경우 페이지 크기가 큰 편이 유리한다.페이지 크기는 컴퓨터 종류별, 프로세서 속도와 메모리 용량 별로 천차 만별이고, 이는 적절한 페이지 크기를 결정하는 것은 어려운 일임을 알 수 있다.|작은 페이지|큰 페이지||—–|——||내부 단편화 감소, 지역성 증가|페이지 테이블 크기 감소, 디스크 입출력 감소, 페이지 부재 비율 감소|4 페이지 테이블의 구조각 프로세스에는 페이지 테이블을 가지고 있으며, 이를 이용해 가상 주소를 프레임 번호와 오프셋으로 구성된 물리적 주소로 변환한다.페이지 테이블 또한, 메모리에 저장해야 하므로 페이징의 대상이며, 프로세스마다 하나씩 가지므로 크기를 줄이는게 관건이다.예를 들어 64비트 가상 주소(64bit 운영체제), $2^{12}$ 비트의 페이지 크기, 4GB=$2^{32}$ 비트의 메모리인 시스템에서 단일 수준 페이지 테이블을 유지하는데 필요한 공간은 다음과 같이 계산한다.  페이지 테이블 항목 수 : 가상 주소 논리적 주소 수 = 64 비트 중 12비트는 페이지 크기 4KB 때문에 오프셋으로 사용 = 52비트의 논리적 주소 수 = $2^{52}$개  페이지 테이블 항목 크기 :          물리적 페이지의 수(메모리 크기 $2^{32}$ / 페이지 크기 $2^{12}$) = $2^{20}$개      액세스 제어 비트(1비트) + 물리적 페이지 번호(20비트) = 21 비트 = 약 4 바이트 = 32비트(2의 지수승이어야 하므로)        전체 페이지 테이블 크기 : $2^{52} \\times 2^2$바이트 = $2^{54}$바이트 = $2^{4}$PB = 16PB4GB의 메모리를 유지하기 위해 백그라운드 프로세서를 포함해서 10개의 프로세스가 있다면 메모리 160PB가 소모되며, 연속적인 메모리 배치도 불가능하다.4바이트 테이블에 10비트 주소 공간의 계층 구조 테이블을 이용하면, [52/10]=6단계에 걸쳐 액세스할 수 있지만, 디스크 접근 성능이 떨어진다.이를 대신해, 메모리의 물리적 주소를 이용하는 역 페이지 테이블이 존재하다. + 역 페이지 테이블 항목은 가상 주소의 논리적 주소 대신 물리적 주소를 이용하며, 시스템 당 하나만 존재해도 된다.같은 시스템에서 역 페이지 테이블이 차지하는 크기를 알아보자  페이지 테이블 항목 수 : 물리적 주소 수 = 메모리 용량 $2^{32}$ / 페이지 크기 $2^{12}$ = $2^{20}$개  페이지 테이블 항목 크기 : 물리적 페이지 주소(20비트) + 액세스 제어 비트 포함 기타 정보(12 비트) + 프로세스 ID (16비트) = 48비트 = 8바이트(2의 지수승)즉 역 페이지 테이블의 크기는 $2^{20}\\times 2^3Byte$=$2^{23}Byte = 8MB$이다. 뿐만 아니라 시스템에 단 하나만 존재해도 된다.(프로세스 100개 기준 8MB VS 160PB)- 2차 저장소(디스크)에 있는 페이지와 관련된 어떤 정보도 유지하지 않아 운영체제가 관리해야 한다.상단의 그림은 선형 역 페이지 테이블을 이용한 변환 절차이다. 역 페이지 테이블의 가상 주소는 프로세스 식별자, 가상 페이지 번호, 오프셋으로 구성된다.가상 페이지 번호와 프로세서 식별자를 이용해 색인 값인 물리 페이지 번호를 알 수있다.하지만, 가상 페이지 주소를 찾으려면 테이블 전체를 탐색하거나 페이지 크기의 메모리를 액세스 해야하므로, 탐색 시간을 줄이려면 해싱 테이블을 이용해 시간을 줄일 수 있다.+ 해시를 이용하면 해시 함수를 적용시 바로 색인값이 나오기 때문에 곧바로 접근할 수 있다.해시 테이블에는 프로세서 ID와 VPN 값에 해쉬 함수를 적용해 생성되는 색인값이 존재하며, 이를 통해 페이지 테이블에 접근할 수 있다. 해시테이블은 충돌의 가능성이 있으므로, 페이지 테이블의 항목들은 연결 리스트 체인을 통해 연결되어 있고, VPN 값을 비교해 찾은 색인 값을 물리 페이지 번호로 이용한다.위 그림은 해쉬 역 테이블에서 물리 페이지를 찾아내는 과정이다.만약 탐색 결과가 Null이라면 페이지 부재이므로 디스크에서 해당 값을 가져온다.+ 이곳에 추가로 캐쉬를 적용하면 더욱 빨라진다.변환 우선 참조 버퍼(TLB, Translation Look-aside Buffer)는 프로세스의 전체 페이지 테이블 중 가장 최근에 참조한 페이지 일부를 저장하는 캐쉬이다.  가상 주소 [V=(p,d)]를 참조 시 먼저 변환 우선참조 버퍼를 조사해 페이지 P를 찾는다.  존재한다면 저장한 프레임 번호(PFN)을 이용해 접근한다.  존재하지 않는다면 페이지 테이블 항목을 조사해 접근한다."
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%209-%EC%9E%85%EC%B6%9C%EB%A0%A5%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EA%B3%BC%20%EB%94%94%EC%8A%A4%ED%81%AC%20%EA%B4%80%EB%A6%AC.html": {
    title: "OS 정리-Chap 9-입출력 시스템과 디스크 관리",
    date: " Aug 10, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%209-%EC%9E%85%EC%B6%9C%EB%A0%A5%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EA%B3%BC%20%EB%94%94%EC%8A%A4%ED%81%AC%20%EA%B4%80%EB%A6%AC.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true9. 입출력 시스템title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 입출력 시스템 관리1 입출력 시스템과 입출력 모듈입출력 시스템은 모니터, 키보드 같은 하드웨어인 입출력 장치와 입출력 모듈까지 포함한다.  입출력 장치 : 실제 사용자와 입출력 수행  입출력 모듈 : 메모리나 프로세서, 레지스터 등 내부 저장장치와 물리적 입출력 장치 사이의 이진 정보를 전송          입출력 채널(I/O channel), 입출력 프로세서(I/O Processor) : 프로세서를 대신해 입출력과 관련된 복잡한 일 처리      입출력 제어기(I/O Controller), 장치 제어기(device controller): 단순 프로세서의 입출려과 관련된 일을 담당      그림 9-1처럼 주변 장치를 직접 시스템 버스에 연결 하지 않고 입출력 제어기로 연결한다.입출력 장치에 따라 입출력 속도, 제어 및 운용법이 다르기 때문이다.주로 정보 단위(워드, 문자, 블록), 처리량 등을 동기화한다.2 입출력 모듈의 구성아래 그림은 일반적인 입출력 모듈의 구성이다.  데이터 레지스터 : 입출력 모듈로 출입하는 데이터를 일시적으로 저장  제어 레지스터 : 상태 레지스터를 프로세서의 명령에 따라 동작  상태 레지스터 : 현재의 상태 정보를 저장, 제어 레지스터에 의해 동작  입출력 논리회로 : 상태 레지스터의 정보를 제어하는 장치의 주소로 인식하고 각 장치와 인터페이스를 제어  제어 버스: 프로세서와 교신3 입출력 모듈의 기능  내부 자원과 데이터 입출력 등의 다양한 동작 제어 및 타이밍          입출력 모듈이 외부 장치 타이밍, 데이터 형식, 기계적 동작 등을 처리하므로 프로세서는 단순히 파일 열기와 닫기로 장치를 제어할 수 있다.      프로세서가 입출력 장치에 명령을 보내기 위해 입출력 장치의 식별자를 주소로 지정하는 방법 두 가지                  전용 메모리 주소 공간에 주소 지정하는 전용 입출력 방법          메인 메모리 주소 공간 일부를 입출력 주소 공간으로 공유하는 메모리 매핑 주소 방법, 유연성, 인터럽트 신호가 특징                      프로세서에서 명령을 전달받고, 관련된 메시지를 인식          메시지 인식 순서                  명령 해독 : 프로세서에서 명령들을 받아 해독한다.          데이터 교환 : 데이터 버스로 프로세서와 데이터를 교환          상태 보고 : 저속 주변장치의 상태를 확인하여 프로세서에 보고          주소 인식 : 모듈에 연결된 여러 장치를 구분할 주소를 인식                      버퍼링을 이용해 전송속도 조절          메인 메모리와 입출력 장치 간의 속도 간극을 해결하기 위해 버퍼를 이용해 데이터 전송        오류 검출          종이걸림 같은 기계적 오류, 전송 오류 등의 검출을 위해 패리티 비트 같은 오류 검출 코드 사용      4 프로세서 역할에 따른 입출력 방법프로세서와 입출력 장치의 입출력 방법에 따라 다음과 같이 나눌 수 있다.4.1 프로그램 제어 입출력 방법프로세서 내부의 입출력 데이터와 주소 레지스터를 입출력 모듈과 직접 연결, 전송하는 형태프로세서가 입출력 장치의 상태 비트(status bit)를 주기적으로 확인하므로 폴링(polling) 방법이라고도 한다.입력 시, 워드 단위로 입출력 장치, 입출력 모듈을 거쳐 입출력 데이터 레지스터로 이동한 뒤, 프로세서의 산술 논리 연산 장치로 보내져 처리된다.반대로 출력 시, 산술논리 연산장치의 결과를 입출력 데이터 레지스터로 이동하고 프로그램을 이용해 입출력 모듈로 전송한다.데이터 전송 시에는 장치의 포트 상태 레지스터에 저장되있는 입출력 장치의 상태 비트를 폴링 순환 (그림 9-4)을 통해 검사한다.폴링 순환이 너무 잦으면 시간과 자원 낭비가 생기고, 너무 적으면 입출력 장치가 너무 오래 쉬게 된다.+ 하드웨어가 적게 필요+ 입출력 장치가 직접 메모리에 접근하지 않음. 보안상 유리- 프로세서가 데이터 입출력에 관여하므로 프로세스 부담이 큼- 데이터 처리속도 느림, 프로세서의 명령어 인출 및 실행이 데이터 입출력이 끝난 뒤 실행  키보드, 문자 모드의 프린터와 모니터 등 저속장치에 적합  🔵 상태 비트 뿐만 아니라 메인 메모리에 3비트로 구성된 채널 상태 워드(channel status word)가 존재하며, 각각 채널, 제어장치, 각 장치의 사용 가능 여부가 표시되어 있다.4.2 인터럽트 기반 입출력 방법입출력 장치가 작업을 완료시 상태와 결과를 메모리에 저장 후, 인터럽트를 발생시켜 프로세서에게 알리는 방법프로세서는 이후 메모리에서 작업 결과를 확인하고 다른 입출력 명령을 전송할 수 있다.위 그림은 인터럽트 기반 입출력 방법의 과정이다.+ 폴링 절차 불필요, 입출력 연산 대기시간 없음+ 불규칙적이고 빠른 응답에 적합- 작업 내용을 스택에 저장해야 하는 오버헤드- 프로세스 레지스터 내 모든 데이터 전송 필요4.3 DMA(Direct Memory Access, 직접 메모리 제어) 입출력 방법입출력 모듈이 프로세서의 도움 없이 메인 메모리를 직접 제어해 데이터를 전송DMA 입출력의 기본 구성은 위와 같다.입출력 장치를 위한 메모리에 연속된 일부분을 할당한다.DMA 제어기  입출력 중 모든 버스 제어 신호를 생성 및 제어하므로 버스 마스터 역할을 맡기도 함.  버스 우선순위가 높기 때문에 버스와 메모리를 프로세서가 사용할 때 사이클 스틸링 하기도 함.          사이클 스틸링(cycle stealing) : 프로세서 도움없이 메모리 접근, 독점적으로 액세스      프로세서의 속도를 떨어뜨리지만, 입출력 작업을 DMA에 전담시켜 전체 성능 향상        통용적으로 사용되는 물리적 주소 말고 직접 가상 주소 액세스(DVMA, Direct Virtual Memory Access)를 이용해 메모리에 액세스 하기도 함+ 프로세서는 이 과정에 개입하지 않으므로 프로세서 부담이 적어+ 디스크 데이터 전성과 멀티미디어 같은 대용량 데이터 전송에 적합- 외부 제어기, DMA 제어기 등의 하드웨어가 필요  프로세서가 읽기와 쓰기 정보, 입출력 주소와 메모리 주소, 길이를 DMA 제어기에 전달해 입출력 요청  DMA 제어기가 입출력 처리  입출력 모듈이 메모리로 블록 단위로 데이터 전송  DMA 제어기가 프로세서에게 인터럽트로 종료 통보  프로세스가 메모리에 보관된 데이터를 사용입출력 채널 : IBM에서 DMA 제어기가 존재하는 입출력 모듈, 채널 프로그램을 통해 DMA 입출력을 수행한다.4.4 입출력 채널을 이용한 입출력입출력 채널 : 프로세서와 메인 메모리를 입출력 장치에 결합해 프로세서의 명령으로 입출력을 제어하는 장치  지역 기억 장치 또한 가지고 있으며, 많은 입출력 장치를 공유하고 제어  입출력의 모든 동작은 프로세서의 개입없이 진행하고 종료 후, 인터럽트를 보내 완료를 보고한다.  DMA 제어기(메모리 접근) + 입출력 명령어(입출력 동작 실행)          이때 입출력 명령어는 메인 메모리에 존재한다.      그림 9-9는 입출력 채널 구조의 예이다.채널 경로: 여러개 존재 가능, 프로세서와 메인 메모리의 액세스를 조정할 수 있다.채널 : 사이클 스틸을 사용해 메모리에 직접 접근입출력 장치 : 제어장치를 사용해 채널과 연결제어 장치 : 채널과 통신하고 장치를 제어, 유사한 장치끼리는 이를 공유함.입출력 채널 또한 그림 9-10처럼 여러개로 나뉜다.  실렉터 채널(selector channel)          어떤 장치를 고르고 입출력 종료할 때까지 다른 장치를 실행하지 않도록 한다.      자기디스크, 테이프 같은 장치가 고속으로 전송할 때 대량의 데이터를 전용 채널로 이동할 때 다중화하는 방법        멀티플렉서 채널(multiplexer channel)          바이트 단위로 시분할해 여러 장치의 출력을 처리      주로 저속, 중속 장치(카드리더, 프린터)등에 사용      채널-메모리 연결이 다수의 느린 장치의 데이터 전달보다 훨씬 빠르므로 동시에 처리 가능하다.        블록 멀티플렉서 채널(block multiplexer channel)          실렉터 채널 + 멀티 플렉서, 여러 대의 고속 입출력 장치를 블록 단위로 동시 처리 가능      장치 하나와 입출력 명령어 수행 후, 다른 장치로 빠르게 변환      5 커널 입출력 서브시스템입출력 모듈 : 프로세서와 장치 드라이버, 장치 간의 데이터를 전송 제어커널 : 입출력 인터페이스를 제공한다.5.1 커널 입출력 구조커널 : 입출력 서비스와 입출력 인터페이스 제공응용 프로그램 : 커널이 제공하는 서비스를 각자 모델이나 제조사에 따라 액세스 가능하도록 표준함수로 제공(소프트웨어 드라이버?)장치 드라이버 (하드웨어 드라이버?) :  응용 프로그램에서 제공한 함수를 수행  입출력의 결과의 장치마다 차이를 표준 인터페이스로 변환하여 상위 커널 입출력 서브 시스템에 제공하드웨어: 소프트웨어 드라이버의 함수에 입출력 실행 후, 장치 드라이버에 돌려줌.5.2 커널이 제공하는 입출력 서비스커널 입출력 서브 시스템은 아래와 같은 다양한 서비스를 제공해 컴퓨터 효율성을 증진시킨다.      입출력 스케줄링  입출력 요구를 전반적인 시스템 성능 향상을 위해 공평하게 처리하도록 실행 순서를 결정        버퍼링  입출력 장치와 응용 프로그램 사이에 전송되는 데이터를 버퍼에 임시로 저장하는 방법      그림 9-12의 (a)의 경우 데이터가 디스크에 도착할 때까지 기다려야 한다.  (b)의 경우, 메모리에 버퍼를 만들어, 버퍼가 찬 이후 한꺼번에 전송하기 때문에 디스크는 그 동안 다른 일을 할 수 있다.  (c)의 경우, 추가로 버퍼를 하나 추가하여, 버퍼의 데이터가 디스크에 쓰여지고 있는 와중에도 전송을 할 수 있게 해준다.    추가로 버퍼를 통해 패킷 등의 데이터의 단위를 변환하기도 한다.        캐싱  명령어와 데이터를 캐시에 일시적으로 저장하여 프로세서와 메모리 간의 액세스 속도 차이를 줄여 컴퓨터 성능을 향상    버퍼와의 차이점은 캐시는 데이터의 복사본을 저장한다는 점이다.    때로는 버퍼가 캐시 역할을 수행하기도, 캐시가 버퍼 역할을 수행하기도 한다.    일반적으로 캐시는 RAM 보다 빠르고 크기가 작다.        스풀링  하나의 출력장치로 여러 요청이 동시에 간다면, 커널은 이러한 요청을 각각 나누어 디스크 파일에 스풀링한 뒤, 큐에 삽입하여 차례대로 처리한다. 보통 커널 스레드를 이용한다.        오류 처리  입출력 장치 고장, 네트워크 전송 오류로 발생하는 일시적인 고장을 해결        자료 관리  입출력 구성의 상태 정보를 유지.  커널 대신 독립 프로그램이 상태 정보를 유지할 때의 장단점은 다음과 같다.    + 입출력 시스템의 구조와 설계가 간단, 운영체제 커널의 크기가 작고 유연함  - 장치 드라이버와 커널의 정보 공유로 메시지를 교환하려는 오버헤드가 증가    상태 정보의 예시는 다음과 같다.          장치 이름과 장치 액세스 제어 관리      장치 할당과 입출력 스케줄링 관리      버퍼링, 캐싱, 스풀링 관리      장치 상태 모니터링과 오류 처리, 고장 복구 관리      02 디스크의 구조와 스케줄링1 디스크의 구조  보조기억 장치의 목적: 방대한 데이터를 낮은 가격으로 영구히 저장하는 것.  메모리에 비해 접근 시간이 느리고 순차 접근만 허용  주로 자기 디스크가 사용된다. 최근에는 SSD로 넘어가는 추세지만 여전히 자기 디스크가 싸다.  디스크는 컴퓨터 장치 중 가장 느려 시스템 전체의 성능과 신뢰성에 많은 영향을 끼치므로, 메모리나 캐시 등이 발달해도 여전히 중요함디스크의 원리  양 표면에 자기 물질을 입힌 원판 모양의 디스크에 정보를 기록  표면을 여러 논리적 트랙으로 나누고 입출력 헤드를 통해 기록한다.헤드 이동 방식그림 9-13(a)의 경우 트랙마다 고정 헤드가 존재하는 고정 헤드 디스크이며, 그림 9-13(b)은 이동 헤드 디스크로, 헤드를 암 이동 장치를 이용해 이동시키는 방식이다.디스크 성능 높이기그림 (b)처럼 디스크를 한축에 여러개 쌓거나 양면으로 이용해 저장 공간을 늘리거나, 램  디스크, 고속 디스크를 이용해 빠르게 데이터 전송을 할 수 있게 한다.디스크 시스템 구성  디스크 드라이버 : 디스크의 기계적 동작 제어 담당          구동 모터      액세스 암 이동장치      입출력 헤드        디스크 프로세서 : 원하는 데이터 위치(주소)와 버퍼, 판독, 기록 등 을 관리  디스크 제어기 : 디스크 드라이버의 인터페이스디스크 작동 순서  디스크 제어기가 프로세서에게 명령을 받음  디스크 제이기를 통해 디스크 드라이버 작동  디스크 드라이버가 탐색, 기록, 판독 등을 수행          이때, 디스크의 주소는 드라이버 번호, 표면 번호, 트랙 번호로 구성됨    - 트랙(track): 원형 평판 표면에 데이터를 저장 할 수 있는 동심원, 자기장 간섭을 막고, 헤드 정렬을 위해 일정한 이격으로 구분    - 실린더(cylinder) : 동일한 동심원으로 구성된 모든 트랙의 집합, 헤드의 움직임 없이 액세스할 수 있는 모든 트랙들    - 섹터(sector) : 트랙을 부채꼴 모양으로 나눔, 섹터는 트랙을 하드웨어적으로 일정 크기로 고정되어 나뉘면서 생긴다. 기록의 기본 단위로 보통 512바이트이며, 고유 번호가 존재해 위치 판별에 쓰임      2 디스크 액세스 시간디스크의 섹터를 액세스할 수 있는 시간, 탐색 시간 + 회전 지연시간 + 전송 시간으로 이루어져 있다.  탐색 시간(seek time) : 헤드를 해당 트랙이나 실린더에 위치시켜 원하는 섹터에 접근하는데 걸리는 시간          고정 헤드 디스크는 각 트랙마다 헤드가 있으므로 0        회전 지연 시간 : 헤드가 지정된 트랙의 원하는 섹터가 입출력 헤드 아래로 회전할 때까지 기다려야 하는 시간  전송 시간 : 디스크와 메인 메모리 간의 섹터를 주고 받는데 걸리는 시간디스크 주소 표현메모리와 디스크 사이의 입출력은 하나 이상의 섹터 단위로 전송하며, 디스크 주소 b는 다음과 같이 표현한다.트랙당 섹터 수=s실린더당 트랙 수=t실린더 번호=i표면 번호=j\\(b= k+s\\times(j+i\\times t)\\)블록 b+1을 액세스 할대는 헤드가 한 트랙만 옮기면 된다.디스크에는 어떤 파일이 있는지 표시하는 장치 디렉터리가 있으며, 장치 디렉터리는 파일 이름으로 리스트를 나타낸다.  파일의 길이  파일의 종류  파일의 소유주  생성된 날짜  사용한 시간  부호등의 파일 정보를 포함한다.    3 디스크 스케줄링의 개념과 종류  입출력장치(디스크 드라이버)에는 요청 큐가 있으며, 다음과 같은 정보를 포함한다.  입력, 출력 구분 정보  디스크 주소  메모리 주소  전송할 정보의 총량(바이트, 워드 기준)이후, 디스크 드라이버와 제어기를 사용할 수 있을 때까지 큐에서 대기하며, 다음과 같은 기준으로 순서를 평가하는 알고리즘을 쓴다.  처리량 최대화: 시간당 처리한 서비스 요청 수  탐색 시간 최소화: 디스크 헤드 이동 시간  평균 반응시간 최소화: 요청 후 서비스 할 때까지 대기시간  반응(응답) 시간 변화 최소화: 반응시간 예측 정도, 무기한 연기 방지, 예측 가능한 응답시간디스크 스케줄링은 크게 탐색 시간 최적화와 회전 지연시간 최적화 알고리즘으로 분류된다.최근 디스크는 탐색 시간과 평균 지연 시간이 비슷하고 대화식 프로세스가 많아 회전 최적화로 성능을 개선한다. 일괄 처리 프로세스는 데이터 트랙 전체를 액세스 하기 때문에 탐색 시간 최적화가 낫다.탐색 시간 최적화4 선입선처리(FCFS, First Come First Served) 스케줄링요청이 도착한 순서에 따라 처리하는 가장 쉽고 간단한 알고리즘+ 구현이 쉽다+ 무기한 연기 없이 공평하다.+ 실행 시간 오버헤드가 적다- 서비스 지연 요청(헤드에서 먼 요청)에 의한 탐색 시간 증가- 처리량 감소5 최소 탐색 시간(SSTF, Shortest Seek Time First)우선 스케줄링현재 헤드 위치에서 가까운 요청부터 처리하는 방법+ 비교적 처리량 증가, 반응시간 감소+ 일괄 배치 처리 시스템에 적합- 예측 불가한 처리 시간, 대화형 시스템에 부적합- 공정성 없음, 디스크 요구의 기아 발생 가능  만약, 0 위치에 헤드가 존재할 때, 1~10 위치의 요청이 무한히 발생한다면, 120 위치의 요청은 영원히 실행되지 않을 것6 스캔(SCAN) 스케줄링헤드가 디스크의 한쪽 끝과 다른 끝 사이를 계속해서 왕복하면서 한 방향으로만 서비스하다 디스크 가장 자리에 도달하면 역방향으로 전환하는 스케줄링엘리베이터의 동작과 유사해 엘리베이터 스케줄링이라고도 한다.위 그림은 스캔 스케줄링의 예시로, 반대 방향으로 먼저 가도 스캔 스케줄링이다.만약, 새로운 요청이 헤드의 경로 바로 앞에 생성되면, 다른 앞선 요청들 보다 먼저 처리한다.반대로 새로운 요청이 헤드의 경로 바로 뒤에 생성되면, 다시 돌아올 때까지 오랫동안 기다려야 한다.7 순환 스캔(C-SCAN, Circular-SCAN) 스케줄링헤드가 한쪽 방향으로 이동하다 한쪽 끝에 다다르면 반대쪽 끝으로 즉시 이동한 뒤, 같은 방향으로 진행하는 방식이다.스캔 스케줄링의 변형이며, 대기시간을 좀더 균등하게 하고, 처리량을 향상 시킨다.8 N-단계 스캔 스케줄링디스크 요청 큐를 N개의 서브큐로 분할 하고, 각 서브 큐 내부 스캔을 사용해 하나씩 처리하는 방법N이 크면 스캔 알고리즘 N=1이면 선입 선처리 알고리즘과 동일하다.유연하지만 복잡하다.9 룩(LOOK, C-LOOK) 스케줄링순환 스케줄링처럼 이동하다, 현재 방향 경로에 요청이 없다면 경로를 바꾸는 알고리즘룩은 진행 방향으로 움직이기 전에 요청을 검사하기 때문에 생긴 이름이다.스캔 스케줄링 보다 더 효율적이다.회전 지연시간 최적화10 최소 지연시간 우선(SLTF, Shortest Latency Time First) 스케줄링모든 요청 중 회전 지연 시간이 가장 짧은 요청부터 처리하기 위해 디스크 헤드가 특정 실린더에 도달 시, 해당 실린더의 모든 요청을 먼저 처리한다.정확히는, 섹터 위치에 따라 큐에 넣은 후, 가장 가까운 섹터 요청을 먼저 처리한다. 따라서 섹터 큐잉(sector queuing) 알고리즘이라고도 한다.+ 탐색 시간이 없는 고정 헤드 디스크에 효과적이다.+ 특별한 트랙마다 실린더 내에 처리 요청이 하나 이상일 때는 이동 헤드 장치에도 쓸만하다.+굳이 대기큐 앞부분에 잇지 않더라도 헤드가 해당 섹터를 지나간다면 요청을 처리하도록 하여 처리율을 늘릴 수 있다.11 최소 위치 결정 시간 우선(SPTF, Shortest Positioning Time First) 스케줄링탐색 시간과 회전 지연시간의 합이 가장 짧은 요청을 먼저 고른다.+ 처리량과 평균 반응 시간 성능이 좋다.- 가장 안쪽과 바깥쪽 실린더의 요청이 무기한 연기될 수 있다.(중간 쯤의 실린더는 평균적으로 가깝기 대문이다.)위 그림 같은 경우 긴 시간이 걸리는 회전 지연시간이 더욱 긴 A보다는 다른 트랙에 있어 탐색 시간이 더 걸리더라도 회전 지연시간이 짧은 B를 먼저 처리할 것이다.  🔵 에센 바흐 방법(Eschenbach Scheme)헤드는 순환 스캔 스케줄링 처럼 진행, 한쪽 방향으로 디스크를 회전 시키면서 만나는 요청을 처리+ 회전 지연시간 최소화- 요청 2개가 실린더의 동일 섹터에 있으면 하나만 처리하고 나머지 하나는 한바퀴 돈 뒤에 처리12 디스크 스케줄링 알고리즘의 선택스케줄링 선택법최소 탐색 시간 우선 스케줄링은 일반적 선택스캔, 순환 스캔 스케줄링은 디스크 부하가 클 경우 선택선입선출 스케줄링은 디스크 요청이 극단적으로 적을 때 선택스케줄링 성능에 영향을 끼치는 요인      요청의 형태와 수에 따라 최적의 알고리즘은 달라진다.    파일 할당 방법 또한 영향을 끼침          연속적으로 할당된 파일을 읽으면, 헤드 이동이 해당 연속으로 강제됨      링크 파일이나 색인 파일은 디스크 여러 위치에 흩어져 반응 시간은 느리지만, 저장 효율은 높음        디렉토리 구조 탐색에 많은 디스크 사용이 있으므로, 디렉토리는 디스크 중간 부분에 두는 것이 효율적이다.03 RAIDRAID의 소개  RAID(Redundant Array of Inexpensive Disks(혹은 Independent Disks))는 운영체제로 여러 대의 물리적 디스크를 하나의 논리적 디스크로 인식하는 기술  다수의 장치를 병렬로 구성, 처리하여 성능과 안전성을 향상 시킬 수 있음    2 RAID 계층    방법과 용도에 따라 총 6계층으로 분류  구성하기 위해 소프트웨어 RAID 카드나 물리적 RAID 카드 필요  최근에는 RAID 0, RAID 1, RAID 0+1, RAID 5를 제외하고 거의 사용되지 않거나 지원하지 않는다.2.1 RAID 0(스트라이핑)일련의 데이터를 논리적 디스크 배열 하나에 일정한 크기로 나눠서 분산 저장하고, 이를 논리 디스크 하나에 저장된 것처럼 이식하는 방법일정한 섹터 혹은 블록 단위인 스트립(strip)으로 나누어 연속적인 배열 첨자와 대응되도록 순환 할당.그림 9-28에서 데이터 ABCD를 A, B, C, D로 병렬로 분산 저장했는데, 이 각각의 A, B, C, D를 스트립(strip)이라고 하며, 연속된 데이터 ABCD는 스트라이프(stripe)라고 한다.+ 동일한 하드디스크 n대로 데이터를 병렬로 저장하여 입출력 전송시간을 n배로 늘릴 수 있다.- 한 디스크 이상에서 장애 발생 시, 데이터 손실, 즉 전송시간 n배로 늘리는 대신 안전성은 1/n배주로 빠른 데이터 입출력 성능을 요구하는 중요하지 않은 동영상 편집 등에 적합하다.2.2 RAID 1(미러링)데이터 스트라이핑을 사용하면서 배열 내의 모든 디스크를 미러링(mirroring)을 통해 동일한 데이터가 있는 미러 디스크를 가진다.+ 드라이브 장애 발생 시 즉각 두번째 디스크로 복구 가능 안전성이 n배+ 읽기 성능 n배 향상 됨- 전체 용량의 절반을 여분의 데이터 기록에 사용하므로 디스크 공간이 두배가 필요- 쓰기 성능은 단일 드라이브와 같음  그림 9-29의 경우에는 쓰기 성능 또한 4배 증가하는게 맞다. 다만 맨 앞에 두 디스크만 있는 경우는 쓰기 성능은 1배이다.쓰기가 적고 읽기가 중요하며, 안전성이 중요한 시스템 드라이브 같은 중요한 파일에 적합2.3 RAID 2(허밍 코드를 이용한 중복)데이터 스트라이핑과 오류 감지 및 수정을 위한 허밍 오류 정정 코드(Hamming Error Correcting Code, Hamming ECC)를 저장하는 드라이브로 이루어져 있다.허밍 오류 정정 코드는 패리티 비트를 이용해 디스크에 전송된 데이터 오류가 있는지 확인하고 정정한다.오류 정정 코드는 각 데이터 디스크에 대응하는 비트를 계산한다. 그림 9-30은 데이터 A를 A0, A1, A2, A3로 스트라이핑 한다.이후 생성한 허밍코드를 패리티 디스크에 ECC/Ac, ECC/Ay, ECC/Az로 저장한다.+ 실시간 오류를 감지 및 수정 가능+ 빠른 기록 속도 (스트라이핑 하므로)+ 장애 복구 능력 (오류 수정으로)+ 디스크 두개의 작은 시스템에도 적용 가능- 디스크 공간 낭비- 최신 SCSI 드라이브는 자체적으로 오류 검출 능력 존재title: 허밍 코드패리티 비트를 필요한 수만큼 정해진 위치에 두어 오류가 발생했을 때 오류 발생 비트를 알아내어 정정이 가능도록 함.  허밍 코드의 비트 수 : 정보 비트의 수가 m이면 패리티 비트 수 p는 2p &gt;= m+p+1  패리티 비트의 위치 : 허밍 코드의 왼쪽부터 1, 2, 4, 8, 2n-1에 위치ex) 정보 비트 4, 패리티 비트 3 (P1, P2, D1, P3, D2, D3, D4)(1,2,4번 자리에 패리티 비트 위치)2.4 RAID 3(비트 인터리브된 패리티)스트라이프를 이용한 분산 저장, 별도의 드라이브 한대를 패리티 드라이브로 이용해 오류 검출과 수정에 사용함데이터 복구는 다른 드라이브에 기록된 정보에 XOR을 계산해 수행나머지 디스크에는 스트라이프하여 분산 저장한다.+ 스트라이프에 의한 높은 데이터 전송률+ 패리티 비트를 이용한 오류 탐지 및 수정- 입출력 작업을 한번에 한 요청만 실행하므로 빈번한 입출력에 좋지 않음  즉, 속도는 빠르지만 한번에 한번만 실행대형 레코드를 많이 사용하는 단일 사용자 시스템, CAD, 이미지 작업에 적합2.5 RAID 4(블록 인터리브된 패리티)RAID 3와 비슷하지만 데이터를 블록 단위로 분산 저장하여 각 디스크에서 독립적으로 입출력을 할 수 있다.기존의 비트 단위 저장의 경우 데이터 출력과 단위가 달라 의미없이 나누었지만, 블록 단위 저장의 경우 각 디스크가 데이터의 의미가 있도록 나누어, 각기 다른 입출력이 가능하다.쓰기는 패리티 비트를 수정해야 하므로 느리지만 읽기는 RAID 0와 비슷한 성능을 보인다.+ RAID 3의 장점에 추가로 각 디스크가 각기 입출력 진행 가능+ 저렴한 가격으로 장애복구 능력과 빠른 읽기 속도- 크기가 작은 입출력의 성능이 떨어짐 (작은 변화에도 패리티 정보를 갱신해야 하므로)  따라서 높은 데이터 전송률이 필요할 때는 성능이 떨어짐2.6 RAID 5(블록 인터리브된 분산 패리티 블록)별도의 패러티 드라이브 대신 모든 드라이브에 패리티 정보를 나눠서 저장한다. 다른 모든 장점을 가지고 있어 가장 널리 사용되는 RAID 방법이다.+ 쓰기에서 패리티 수정 동작이 디스크 병목 현상을 일으키지 않않음+ 병렬 입출력, 동시 읽기 기록, 데이터 입출력 성능 빠름, 안정성 높음- 최소 3대, 보통 5대 이상의 드라이브가 필요, 소규모 시스템 부적합- 읽기 요청은 각 드라이브에서 패리티 정보를 건너뛰어 RAID 4보다 비교적 느림2.7 RAID 0+1스트라이핑과 미러링 방법을 혼합한 형태+ 안전성과 빠른 속도 모두 확보- 최소 4개의 디스크 필요- 미러링으로 인해 용량이 절반으로 줄어듦"
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2010-%ED%8C%8C%EC%9D%BC%20%EA%B4%80%EB%A6%AC.html": {
    title: "OS 정리-Chap 10-파일 관리",
    date: " Aug 11, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2010-%ED%8C%8C%EC%9D%BC%20%EA%B4%80%EB%A6%AC.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true10. 파일 관리title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 파일 시스템과 파일1 파일 시스템의 개념정보를 저장하는 논리적인 관점과 저장장치의 물리적인 특성을 고려해 논리적 저장 단위인 파일을 정의하고 메모리에 매핑시키는 운영체제 기능  데이터를 실제로 저장하는 파일  이를 계층적으로 연결하고 파일의 위치와 속성을 기술하는 디렉터리로 이루어짐.2 파일 시스템의 기능  파일 구성 : 사용자가 각 응용 업무에 적합하게 파일 구성  파일 관리 : 파일 생성, 수정, 저장, 참조, 제거, 보호, 파일 공유 및 다양한 액세스 제어 방법 제공  보조 메모리 관리 : 다양한 형태의 저장 장치 지원  파일 무결성 보장 : 파일에 저장된 정보 손상 방지  파일 액세스 방법 제공 : 기본 연산(파일 생성, 기록, 판독, 삭제) 기능을 활용한 구조(순차, 인덱스)에 따른 다양한 액세스 방법(순차, 직접, 인덱스 등) 제공  장치 독립성 유지 : 물리적 장치 별 기호화된 이름으로 장치와 독립성 유지  파일 백업과 복구 : 정보 손실 및 고의 손상에 대비해 데이터 사본 생성 및 복구 기능 제공  파일 보호 : 정보 암호화 및 해독 기능 제공  정보 전송 : 파일 간 정보 전송 명령  편리한 인터페이스 제공종합적으로 다양한 형태의 저장장치에 입출력을 지원하는 것, 데이터 보호 및 처리율 향상과 성능 향상이 목적이다3 파일 시스템의 구조파일 시스템은 크게 두가지로 구분할 수 있다.  논리적 파일 : 파일의 개념, 속성, 디렉토리 구조, 파일에 허용하는 연산 등을 정의하는 논리적 파일  디스크에 이런 논리 파일 시스템을 매핑하는 부분계층으로 나누자면 아래와 같으며, 각 계층은 낮은 계층의 서비스를 사용한다.      장치 드라이버 : 운영 체제의 일부 기능으로, 장치와 통신하여 입출력 연산 및 요구 등을 수행, 논리적 블록을 물리적 블록 주소로 변환        입출력 제어기 : 장치 드라이버 루틴과 인터럽트 처리기로 구성, 명령어를 해석해 메모리와 디스크 시스템 간에 정보 전송        기본 파일 시스템 : 물리적 블록을 읽거나 쓰려고 장치 드라이버를 호출, 데이터 블록 처리        파일 구성 모듈 : 디스크 빈 공간 파악, 파일의 논리 블록 주소를 물리 블록 주소로 변환        논리 파일 시스템 : 보호 및 보안 관련 기능, 파일 이름과 디렉터리 등 정보를 이용한 레코드 처리 기능 제공  4 파일 시스템의 관리4.1 블록메모리와 디스크 간의 전송 단위이며, 파일은 블록 하나 이상으로 이루어져 있고, 블록은 섹터 하나 이상으로 구성된다.논리 블록 번호는 입출력 제어기의 명령으로 장치 드라이버에서 물리적 디스크 주소로 변환된다.물리적 디스크 주소는 트랙, 실린더, 표면, 섹터 등으로 구성되어 있다.그림 10-3은 논리적 파일인 가상 블록 번호(VPN, Virtual Block Number)가 물리적 파일인 논리 블록 번호(LBN, Logical Block Number)로 매핑시키는 과정이다.4.2 메타 데이터(Metadata)파일 시스템은 메타 데이터를 저장한다.메타 데이터의 예시는 다음과 같다.  파일 시스템 크기  저장 장치의 빈 공간 정보(비트맵, 가용 블록, 불량 섹터 등)  루트 데이터 위치와 파일 소유자  파일 크기, 마지막 수정 시간유닉스 시스템은 메타 데이터를 디렉토리에 데이터 파일로 저장하며, 기본 저장 장치인 i 노드를 이용한다.보통 사용자가 직접 수정하지 않고 파일 시스템의 무결성을 유지한다.4.3 마운팅(mount)새로운 장치의 파일 시스템에 접근하기 위해 기존의 파일 시스템의 디렉토리에 설치하는 과정운영체제에 마운트하려는 파일 시스템의 저장위치에게 새로운 파일 시스템의 설치 지점인 마운트 포인트를 제공하여 이루어진다.이후, 마운트 테이블을 통해 마운트된 디렉토리들을 관리한다.마운트되면, 기존의 디렉토리 처럼 자유롭게 액세스가 가능하다.그림 10-4의 경우, \\dev\\sda6가 \\mnt에 마운트 되는 과정이다.다만, 마운트포인트에 존재하던 \\cdrom 등의 기존 디렉토리는 언마운트 될때까지 사용 불가능하다.윈도우의 경우, C 드라이브와 D 드라이브가 예시이다.5 파일의 개념과 구성  보조 저장 장치에 기록된 프로그램과 데이터 등 정보의 모음  파일 제작자와 사용자가 정의한 비트, 바이트, 레코드의 집합  사용 목적과 기능에 따라 구조와 형식이 다양하다.운영체제에 의해서 필드, 레코드, 블록 등으로 매핑된다.      필드(항목)레코드를 구성하는 바이트의 모임, 의미있는 데이터의 가장 작은 단위.고정 길이나 가변 길이로 정해진다.        레코드파일을 구성하는 요소, 레코드가 모여 블록을 형성하기도 한다.마찬가지로 고정 길이와 가변 길이가 존재하며, 고정 길이 레코드는 크기에 따라 문자열이 잘리거나 저장 공간이 낭비되는 단점이 있고, 가변 길이 레코드는 레코드의 액세스가 힘들다는 단점이 있다.  6 파일의 이름 명명파일명은 같은 디렉토리 내에선 유일해야 한다.루트 디렉토리에서 해당 파일까지의 경로를 절대경로라고 한다.만약 절대경로가 너무 길면, 현재 폴더 위치에서의 상대경로를 이용할 수 있다.예를 들어 그림 10-6의 p3.c의 절대 경로는 \\수학과\\교수\\김정수\\교육과정\\570\\프로그램\\a3\\p3.c이며, 현재 위치가 수학과일 때 상대 경로는 \\교수\\김정수\\교육과정\\570\\프로그램\\a3\\p3.c이다.7 파일의 속성파일 속성이란 시스템이 파일을 관리하는데 필요한 정보를 의미한다.파일 속성은 운영체제가 파일 관리에 사용하며, 메인 메모리에 유지하여 파일을 열 때 탐색 시간을 줄인다.  파일 이름: 사용자들이 이해할 수 있는 형태  파일 식별자: 각 파일에 할당한 고유 번호, 사용자가 보통 판독 불가  파일 유형: 다양한 파일 형식을 지원  저장 위치: 장치와 장치 내의 위치를 표시하는 포인터  파일 크기: 파일의 현재 크기, 허용 가능한 최대 크기 포함  액세스 제어 데이터 : 파일 읽기, 쓰기, 실행 등 권한 정보  소유자 : 파일을 최초로 생성한 사용자  레코드 크기 : 고정된 크기나 최대 크기 등 레코드 종류에 따라 다름  시간, 날짜, 사용자 식별 정보 : 생성 시간, 수정 변경 시간, 최근 사용 시간 등으로 파일을 보호, 보안, 감시디렉토리 내의 각 파일은 파일 속성과 내용이 담긴 파일헤더를 가진다. 유닉스에서는 이를 i 노드라고 부른다.8 파일의 유형파일 시스템은 지원 가능한 파일 구조와 해당 구조에서만의 특별한 연산을 제공한다.예를 들어 텍스트 파일은 프린트 가능하지만, 실행 파일은 불가능 한 대신 실행 가능하다.다음은 파일의 유형들이다.      일반(정규) 파일텍스트나 이진 형태의 파일, 텍스트는 아스키 형식으로 사용자가 읽거나 인쇄할 수 있는 정보이며, 이진 파일은 컴퓨터로 읽을 수 있는 정규파일로, 작업을 수행하려는 시스템에 지시하는 실행 파일이다.        디렉터리 파일모든 유형의 파일에 액세스할 수 있는 정보를 포함하며, 실제 본인의 데이터는 포함하지 않음. 파일이나 하위 디렉터리를 가지고 있다.        특수 파일시스템 장치를 정의하거나 프로세스로 생성한 임시 파일로 파이프라고도 하며, 일시적으로 다른 프로세스와 통신하기 위해 생성된다.  운영체제는 파일의 구조를 엄격히 따지지 않고 응용 프로그램이나 사용자가 적절히 해석하고 운용해야 한다.보통은 파일 이름 뒤에 마침표 (ex) .txt)를 이용해 파일명과 확장자를 구분한다.            파일 유형      확장자      기능                  실행 가능      exe, com, bin 등      이진 수행 기능 프로그램              소스 코드      c,p,pas,f77,asm,a      다양한 언어로 된 소스코드              배치      bat, sh      명령어 해석기에서 명령              문서      txt, doc      텍스트 데이터, 서류              워드 프로세서      wod,doc,hwp      다양한 워드 프로세서 형식              라이브러리      lib,a,DLL      프로그래머들이 사용하는 라이브러리 루틴              백업, 보관      arc, zip, tar      관련된 파일을 하나로 묶거나 압축해서 보관      9 파일의 연산      파일 생성 : 파일 시스템에 있는 공간을 찾는다. =&gt; 새로 생성한 파일 항목을 디렉터리에 만들어 파일 이름과 파일 시스템 내의 위치를 기록        파일 열기 : 먼저 프로세스를 열고, 메인 메모리에 디스크 주소 속성과 리스트를 가져와 시스템 호출을 빠르게 한다.        파일 쓰기 : 파일을 기록하려고 파일 이름과 파일에 기록할 정보를 표시하는 시스템 호출을 수행.        파일 위치 재설정: 현재의 파일 위치 포인터 조정.        파일 삭제 : 디스크 공간을 확보        파일 크기 조절        속성 설정하기 : 보호 모드 같은 일부 속성은 사용자가 설정할 수 있고, 파일을 생성한 후 변경할 수 있다.        파일 이름 바꾸기 : 사용자가 기존 파일의 이름을 변경 시, 새 이름으로 파일 복사 후 삭제        파일 닫기 : 모든 액세스를 완료하면 속성과 디스크 주소는 이제 필요하지 않아 메모리에서 내보낸다.  위 연산들을 응용해서 파일을 편집, 수정, 복사할 수 있고, 파일이 목적 코드 형태이면 실행도 할 수 있다.10 파일 디스크립터(descriptor)  파일을 액세스하는 동안 운영체제에 필요한 정보를 모아놓은 자료구조  각 파일마다 독립적으로 존재하며, 파일을 열 시 프로세스가 생성          정확히는 디스크에 파일과 함께 저장되어 있다가 열면 메인 메모리로 적재, 닫으면 메인 메모리에서 폐기        음이 아닌 고유의 정수인 ID로 파일을 액세스 하고 열린 파일 테이블을 식별하는데 사용한다.열린 파일 테이블 : 프로세스나 프로세스 그룹이 현재 열린 파일에 액세스 하는 방법이 기록되어 있음.파일 디스크립터의 자세한 내용은 그림 1-8(a)와 같다.11 파일에 액세스하는 방법파일의 데이터를 순회하는 방법을 의미하며, 다음이 존재한다.11.1 순차 액세스레코드 단위의 순서로 파일을 일을 읽는 방식읽기 동작 : 파일의 다음 부분을 읽은 후 자동으로 파일 포인터 증가쓰기 동작 : 파일의 끝에 내용을 추가하고, 포인터를 쓴 내용(파일의 새로운 끝)으로 이동주로 프로 그램이 생성하는 임시 작업 파일에 사용11.2 직접 액세스모든 블록을 순서없이 직접 읽거나 쓰는 방식|순차 액세스 방식| 직접 액세스 방식||—|—||reset(초기화)|cp = 0;||read next(다음 것 읽기)|read cp; cp = cp + 1;||write next(다음 것 기록)|write cp; cp = cp + 1;|직접 액세스는 대규모 데이터베이스 등에 유용하다.11.3 인덱스 순차 액세스(ISAM, Indexed Sequential Access Method)디스크의 물리적 특성에 따라 인덱스를 구성하여 인덱스를 탐색하고, 포인터를 사용해 파일에 직접 액세스 하는 방식규모가 클 경우 2차 이상의 인덱스 파일을 구성해 처리할 수 있다.  먼저 마스터 인덱스에서 이진 탐색을 실행해 2차 인덱스 블록 번호 탐색(첫번째 직접 액세스)  2차 인덱스 블록을 이진탐색 하여 원하는 레코드를 포함하는 블록 탐색(두번째 직접 액세스)  해당 블록을 순차 액세스위 처럼 최대 2번의 직접 액세스와 1번의 순차 액세스로 탐색이 가능하다.+ 적은 입출력으로 탐색이 가능02 파일을 관리하는 디렉터리 시스템1 디렉터리의 개념디렉토리는 파일 시스템에서 다른 파일들의 이름과 위치 정보(파일 인덱스)를 담은 파일로 다른 파일들과 달리 사용자 데이터를 저장하지 않음.디렉토리의 정보는 주로 시스템이 사용해 간접적으로 사용자에게 도움을 줌디렉터리는 다음과 같이 두 부류가 있다.      장치 디렉터리: 각 실제 장치에 저장, 장치에 있는 파일의 물리적 속성(파일의 위치, 파일의 크기와 할당 과정)        파일 디렉터리: 모든 파일의 논리적 구성으로 이름, 파일 유형, 소유한 사용자, 계정 정보, 보호 액세스 코드 등을 기술  운영체제는 심벌 테이블(symbol table)로 디렉토리 내의 파일 이름을 찾는다.디렉토리 구조를 이용하면 수많은 파일들을 디렉토리 별로 분류하여 빠르게 찾을 수 있다.디렉터리 내의 정보는 공통적으로 다음과 같다.  파일 이름 : 기호로 된 각 파일의 이름은 사람이 읽을 수 있는 형태이어야 하고, 특정 디렉터리 내에서는 유일  파일 형태 : 이진 파일, 텍스트, 적재 모듈 등  위치 : 파일이 위치한 장치와 위치 포인터, 주로 경로명  크기 : 바이트, 워드, 블록 등으로 표현, 파일의 현재 크기와 최대 가능 크기가 포함  현재 위치 : 파일에서 현재 읽기나 쓰기를 행하는 위치의 포인터  보호 : 액세스 제어 정보는 사용자에 따라 읽기, 쓰기, 실행하기 등을 할 수 있는지 여부를 제어  사용 수 : 현재 열린 파일을 사용하는 프로세스 수  시간, 날짜, 처리 식별 : 생성 시간, 수정 시간, 마지막 액세스 시간 등을 유지하며 보호와 사용, 감시에 이용2 디렉터리의 구현2.1 디렉터리의 구조디렉터리는 계층적으로 구성되어 있다.그림 10-13은 디렉터리와 속성(파일 헤더)로 구현한 예이다.수학과\\교수\\programs\\a3\\p3.c 경로의 계층 구조 예시파일 헤더 액세스와 디스크 블록 액세스(논리 블록 번호를 통한 변환으로 디스크 블록을 알 수 있음) =&gt; 각각 디스크 입출력 6번 씩 진행만약 파일 헤더를 메모리에 유지했다면 파일 헤더 액세스 6번은 메모리 액세스 6번, 디스크 전송 1번(마지막 파일 내용 전송)으로 대체돼 더욱 빠를 것이다.즉, 파일 헤더를 메모리에 유지, 최근에 사용한 디스크 블록 메모리에 유지하면 빨라진다.디렉터리 구현은 아래와 같이 두 방법이 있다.2.2 선형 리스트를 이용한 디렉터리 구현디렉터리에 파일 이름, 포인터들의 선형적 리스트를 구성하여 파일의 생성과 삭제 등을 실행하는 방법- 선형 탐색을 통한 오버헤드 증가+ 소프트웨어 캐시, 이진 연결 트리의 이진 탐색을 통한 이진 탐색으로 오버헤드 줄일 수 있음2.3 해시 테이블을 이용한 디렉터리 구현파일 이름을 해쉬에서 값을 얻어 리스트를 직접 액세스하도록 디렉토리를 구현+ 디렉토리 탐색 시간 감소 + 성능 향상+ 연결리스트 체이닝을 이용한 해쉬 충돌 방지- 해시 테이블 크기 고정에 의한 해시 기능 제한  예를 들어 해시 테이블 크기가 64 였는데 65번째 파일을 생성하려면 기존의 해시 테이블 크기를 128로 바꾸고 기존의 디렉터리 값들 또한 새로운 해시테이블에 맞게 값을 재구성해야함.적절한 해시 테이블의 크기는 장치의 파일 수 평균값을 이용해 산정한다.3 디렉터리의 연산다음은 디렉토리의 가능한 연산들이다.  탐색하기 : 파일의 심볼릭 네임(symbolic name)을 통해 파일 간의 연관성을 나타내 특정 파일을 찾으려고 디렉토리 탐색  파일 생성하기  파일 삭제하기  파일 열람하기 : 파일과 파일의 디렉터리 항목값 열람  파일 이름 변경하기  파일 시스템 순회하기 : 다른 디렉터리를 순회하며 파일 열람  백업하기 : 신뢰성을 위해 내용 복사해 백업4 디렉터리의 구조논리적 파일의 디렉터리 구조4.1 1단계 디렉터리(single level)모든 파일이 동일한 디렉토리에 존재한다.장치 디렉토리가 사용하는 구조+ 가장 간단하고 쉽다.- 파일 수가 증가하거나 다수의 사용자가 있을 시 고유한 파일명 짓기 힘듦, 시스템이 정하는 파일명 길이 제한4.2 2단계 디렉터리사용자 별로 자신의 서브디렉터리(subdirectory)를 생성해 그곳에 자신의 파일을 구성사용자는 자신만의 파일 디렉터리(UFD, User File Directory)를 가진다.유닉스나 도스가 대표그림 10-5의 트리의 리프가 파일들이며, 사용자 이름을 경로로 사용한다.+ 파일 이름이 겹쳐도 사용자가 다르면 상관없다.  파일 탐색 시, 각 항목의 포인터가 있는 사용자 고유의 마스터 파일 디렉토리(MFD, Master File Directory)를 먼저 탐색  삭제, 생성에도 마찬가지+ 허용하지 않으면 다른 사용자가 함부로 파일 액세스 불가- 액세스 하려면 상대방의 허용 + 정확한 파일명을 알아야 가능  사용자 디렉터리는 새로 추가될 수 있다.      파일을 지명하는 문법이 특이하다.    ```  u:[sst.jdeck]login.com;1  u: 파티션 명  sst: 디렉터리명  jdeck: 서브디렉터리 명  맨뒤 1: 버전  ```  VMS에서의 지명 문법      파일 탐색 시, 사용자 파일 디렉토리가 최우선 탐색되며, 없을시 이후로 여러 특수 시스템 디렉토리를 탐색하는데, 이 순서를 탐색 경로라고 하며, 마음대로 바꿀 수 있다.(유닉스, 도스 사용)  #### 4.3 트리 구조 디렉터리루트 디렉토리를 루트로 삼고, 이후 서브 디렉토리와 파일을 노드로 삼는 구조앞선 2단계 디렉터리는 높이가 2인 트리구조 디렉터리이다.모든 디렉터리와 파일은 형식상 동일하고, 한 비트를 활용해 파일(0), 서브디렉토리(1)로 나눈다.  사용자 디렉터리 포인터가 계정 파일에 존재해 개인 디렉토리 사용 가능  현재 디렉토리라는 개념 존재          탐색 등이 현재 디렉토리를 기준으로 시작됨        앞서 말한 절대경로와 상대경로 개념  파일 삭제시 디렉토리를 비우거나(=모든 자식노드 옮기거나), 그냥 함께 삭제되도록 구현4.4 비순환 그래프 디렉터리(acyclic graph)트리 구조 디렉터리에 추가로 서브디렉터리와 파일을 공유할 수 있게 허용공유는  링크라는 파일이나 디렉토리를 가르키는 포인터로 구현, 포인터에는 경로명으로 적혀있다.          바로가기와 비슷한가?        공유 파일의 정보와 속성을 그대로 복사하는 방법          단, 복사본과 원본의 무결성을 유지하기 힘듦      공유된 파일은 복사되는 것이 아니라 참조되는 것이므로, 누군가가 내용을 바꾸면 전부 바꾼 모습으로 보인다.+ 트리 구조 보다 융통성 있음- 트리 구조 보다 복잡함  공유 파일 삭제 시 고아 포인터(dangling pointer) 문제: 한쪽에서 삭제하면 공유된 쪽에 빈 포인터가 남음          해결방법 : 해당 파일의 참조 리스트를 추가해 복사나 공유되면 항목 추가, 삭제되면 리스트에서 삭제하여 리스트가 비면 완전 삭제                  유닉스에서는 i 노드에 참조 카운터를 두어 파일 참조 작업을 수행하면 참조 카운터 증가, 끝나면 참조 카운터 감소, 0이 되면 파일 아예 삭제                      파일 탐색 시 공유된 파일이나 폴더가 같이 검색됨4.5 일반 그래프 디렉터리추가로 상위 디렉토리로의 링크를 가지게 하여 순환을 허용한 그래프 구조기존의 구조보다 더욱 유연성 있다.  전역 탐색 시 순환에 의해 반복적으로 탐색되지 않도록 유의  삭제 시, 본인을 참조하는 경우 유의(참조 카운터 혹은 참조 리스트가 0이 되지 않을 수 있다.)          이를 방지하기 위해 가비지 컬렉션(garbage collection)이 필요하다.      가비지 컬렉션  첫번째 탐색 : 전 파일 시스템을 탐색, 액세스 가능 파일 표시  두번째 탐색 : 표시하지 않은 메모리(=사용 가능한 메모리)를 사용 가능 메모리 리스트에 추가+ 참조하지 않는 파일은 지우고, 사용가능한 메모리를 알 수 있음- 모든 파일 탐색에 따른 성능 저하, 디스크 시스템에서는 너무 느려서 사용 불가- 순환이 존재하면 안됨      또는 순환 발생을 찾는 알고리즘을 이용하면 되지만 비용이 많이 듦    03 파일의 디스크 할당  1 파일의 디스크 할당 방법파일 디스크 할당에 따라 액세스 속도와 공간 효율이 달라지며, 일반적으로 세가지 방법 중 하나를 이용1.1 연속 할당디스크의 연속적인 주소에 할당하는 방법.디스크 헤드 이동이 효율적임블록 단위로 연속 할당하며, 각 파일의 항목은 시작 블록의 번지와 영역의 길이를 가지고 있다.+ 블록의 위치를 예측하기 쉬우므로 직접 액세스 가능+ 다음 블록의 위치를 찾기 쉬우므로 순차 액세스 가능+ 좋은 성능 기대할 수 있음, 특히 작은 파일에 특화- 연속적인 빈공간 찾기 힘듦(메모리의 적합 정책 이용)- 외부 단편화 발생  세그먼테이션의 메모리 압축과 비슷한 재포장 루틴(routine)을 이용해 커다란 연속 가능 공간을 생성하여 해결 가능- 파일 공간 크기 결정 힘듦  만약 파일의 크기가 계속 커져 다른 파일의 영역을 만나게 된다면?          해결 방법 1: 오류 메시지를 출력하고 종료한 뒤, 사용자가 재포장 루틴이나 저장공간을 확보하도록 함                  사용자의 저장 공간의 과도한 추정으로 공간 낭비 단점                    해결 방법 2: 새로운 충분히 큰 공간을 찾아 복사한 뒤, 이전의 공간 해제                              추가적인 성능을 요구한다는 단점            1.2 연결 할당            각 파일을 디스크 블록들의 리스트에 연결하고, 디스크 블록들은 디스크 내에 흩어져있는 방법각 블록은 다음 블록의 디스크 주소를 포함하기 위한 공간이 필요하다.디렉터리는 파일의 첫 번째 블록 포인터를 가지고 있다.그리고 각 포인터는 다음 블록으로 연결되는 포인터를 가지고 있어, 이를 읽으면서 순차 액세스가 가능하다.마지막 포인터는 리스트의 마지막 포인터를 의미하는 nil 값을 가지게 된다.그림 10-21은 연결 할당의 예시이다.                              + 파일 생성이 쉬움, 장치 디렉터리에 새로운 항목을 생성하고, nil 값을 가진 첫번째 디스크 블록을 만들고, 용량을 키워나갈 수 있음+ 외부 단편화가 존재하지 않음- 순차 액세스 파일에만 가능, n번째 블록으로 직접 접근은 바로 되지 않고 순서대로 포인터를 따라가야함.- 내부 단편화 발생 가능  블록이 크면 내부 단편화 심화, 대신 입출력 연산의 횟수 감소 및 연속 배치로 인한 읽기 성능 증가, 블록이 작으면 반대- 블록 포인터 공간 필요, 대략 512 워드의 블록이라면 2워드 정도가 필요- 신뢰성 유지가 어려움 한 워드가 블록 포인터 내용을 잃으면 그 뒤부터 모든 내용 손상- 탐색 시간 증가 파일들이 여러 위치로 흩어져 디스크 헤드의 탐색 시간 증가, 임의 주소 계산 힘듦신뢰성과 순차 액세스, 탐색 시간 증가의 경우 이중 연결 리스트나 관련 블록 캐싱으로 어느 정도 해결 가능하나, 여전히 많은 부담을 준다.파일 할당표(FAT, File Allocation Table)파일 할당표는 사용자가 해당 블록의 포인트를 실수로 지우지 않게 백업 기능과 빠르게 접근하기 위한 포인터를 모아 놓은 곳이다.각 디스크 블록 내에 항목이 하나 있고, 블록 번호로 참조해, 연결 리스트로 많이 사용한다.마지막 블록은 EOF(End of File)값을 가지고 있으며, 사용하지 않는 블록들은 제로 테이블 값으로 표시한다.주로 도스와 OS/2 운영체제에서 사용한다.1.3 인덱스 할당모든 포인터를 인덱스 블록에서 관리하여 해당 인덱스 블록으로 직접 액세스를 지원인덱스 블록은 각 파일마다 가지고 있는 디스크 블록 주소의 배열이다.디렉토리에는 각 파일의 인덱스 블록 주소가 존재한다.그림 10-23은 인덱스 할당의 예시로, 각 배열마다 디스크 데이터 블록을 가리키고 있다.  처음 인덱스를 만들면 모든 인덱스 블록 포인터가 null값이다  i번째 블록 처음 사용시, 그 블록을 빈 공간 리스트에서 제거하고, i번째 인덱스 블록 항목에 주소를 기록+ 직접 액세스 속도 빠름(인덱스 탐색 + 블록 탐색)+ 구현하기 쉬움+ 외부 단편화 없음+ 인덱스 블록의 최대 크기만큼 확장 쉬움- 순차 액세스 효율성 떨어짐- 인덱스 블록 크기 이상으로 확장 어려움- 인덱스 블록 크기 정하기 힘듬(너무 크면 수많은 파일당 부담, 너무 작으면 파일 크기 제한)- 사용된 인덱스 블록의 포인터 크기가 연결 할당 포인터보다 메모리 부담이 큼- 사용되지 않은 인덱스 블록은 nil 값을 가지며 공간 부담이 큼단점을 극복하기 위해 크기가 작은 파일은 연속 할당을, 크기가 크면 인덱스 할당을 이용하게 하는 방법도 사용한다.  유닉스에서는 여러 항목에 파일 주소를 사용하는 다중 인덱스 파일 할당(Multilevel Indexed Allocation), i 노드를 이용함  다중 인덱스 파일 할당은 처음 항목 12개의 데이터 블록 주소(LBN)를 담는다  나머지 배열 항목은 각각 한 단계, 두 단계 그리고 세 단계 간접 포인터를 통해 또 다른 간접 블록을 참조한다.  모든 간접 블록은 마지막에 데이터 블록을 가르킨다.3 디스크의 빈 공간 관리 방법디스크는 빈 공간을 표시해놓은 빈 공간 리스트가 있다.  파일 생성 시 리스트에서 빈 공간을 빼어 할당  파일 삭제 시 비어진 공간을 리스트에 추가빈 공간을 관리하는 방법은 크게 세가지로 구분된다.3.1 비트맵빈 공간 리스트는 비트맵(bitmap) 또는 비트 벡터(bit vector)로 구현할 수 있다.디스크의 블록 중 빈 공간은 1, 사용된 공간은 0으로 표시한다.블록 크기가 2KB, 디스크 크기가 1GB이면 $2^{30}/2^{12}=2^{18}$개의 비트맵이 필요하며 이는 총 $2^{15}$바이트 =32KB를 의미한다.+ 구현이 간편함+ 연속적인 빈 블록 n개를 찾기 편함- 비효율적인 메모리 사용량따라서 보통 대형 컴퓨터 보다는 마이크로 컴퓨터 환경에 많이 사용된다.3.2 연결 리스트디스크의 빈 디스크 블록을 첫 번째 빈 블록 내에서 다음 빈 디스크 블록의 포인터를 갖도록 연결 리스트로 구현빈 공간 리스트 탐색 시 각 블록을 모두 읽어야 하므로 비효율적3.3 인덱스 블록(그룹핑)빈 블록의 포인터를 인덱스 블록에 보관해 서로 연결하는 방법인덱스 블록은 빈 블록 중에 선정되며, 다른 빈 블록의 인덱스를 저장하는 역할을 한다.그리고 인덱스 블록의 마지막 인덱스 또한 다른 인덱스 블록이 된다.이 방법을 통해 사용 가능한 블록 주소를 여러개 쉽게 찾을 수 있고, 빈 블록을 활용하여 메모리 소모도 줄인다.04 파일 보호1 파일 보호의 필요성컴퓨터 시스템은 다수의 사용자가 사용하는 파일들을 다음으로부터 보호해야 한다.  물리적인 손상 : 헤드 파손, 먼지, 온도, 전원 장애  부적합한 액세스파일이 허용하는 액세스 종류는 다음과 같다.  읽기  쓰기  실행하기  추가하기  삭제하기파일 이름 다시 지정, 복사, 편집 등은 위 기능을 조합해서 가능하다.보호 방법은 파일 자체 보호와 액세스 경로 보호로 나뉨하지만 액세스 경로 보호가 좀더 일반적인데, 디렉토리를 보호하면 파일은 자동으로 보호되기 때문2 파일 보호 방법2.1 파일 명명(file naming)액세스하려는 파일의 명명 권한이 없는 사용자를 제외하여 파일을 보호이는 파일명을 알지못하면 접근 및 사용이 힘든다는 점을 이용함.- 하지만 파일 이름을 추측하거나 정해져있는 경우 추측이 쉽다는 단점이 있음2.2 암호(비밀번호, password)각 파일마다 암호를 정하여 제한- 단, 수많은 파일마다 암호를 정하는 것은 현실적으로 불가능하고, 하나의 암호로 통일하면 보안이 취약해짐다중 암호로 보완해야함2.3 액세스 제어(access control)사용자에 따라 액세스할 수 있는 파일이나 리스트를 두어 사용자 신원에 따라 서로 다른 액세스 권한 부여액세스 요구시 이를 참조해서 보호- 리스트의 길이가 너무 크다  수많은 사용자에게 수많은 파일의 액세스 리스트에 등록하면 크기가 어마어마해짐  디렉터리 항목이 고정 크기이므로 액세스 리스트 크기를 유동적으로 조절하기 힘듦2.4 액세스 그룹(access group)액세스 제어의 단점을 보완하기 위해 각 파일과 연관된 사용자를 다음 세가지로 분류하여 해결  소유자 : 파일 생성한 사용자  그룹 : 파일을 공유하고 비슷한 액세스가 필요한 사용자의 집합  모든 사람 : 시스템에 있는 모든 다른 사용자유닉스의 경우 그림 10-31처럼 각 그룹을 읽기, 액세스, 쓰기를 rwx 3비트로 표현한다.파일당 총 9비트로 보호 정보를 사용한다.2.5 사용자 권한(user permission) 지정사용자가 계정을 받을 때부터 특정한 디렉터리와 파일만 액세스할 수 있도록 시스템 관리자가 허락하고 그 외의 영역은 액세스를 불허하는 방법예를 들어 읽기 권한만 받았다면 파일의 읽기만 가능하고 삭제 등을 행하지 못한다.+ 침입자의 손상 정도를 줄일 수 있다.아래는 네트워크와 호스트 컴퓨터 시스템 관리자에 대한 사용자 권한이다.|권한|설명||—|—||삭제|파일 삭제 허용||생성|새로운 파일 생성 허용||쓰기|파일에 새 정보 저장 및 수정 허용||읽기|파일을 열어 정보 읽기 허용||검색|권한 검색 명령을 이용하여 파일 검색을 허용|"
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2011-%EB%B6%84%EC%82%B0%20%EB%B0%8F%20%EB%8B%A4%EC%A4%91(%EB%B3%91%EB%A0%AC)%20%EC%B2%98%EB%A6%AC%20%EC%8B%9C%EC%8A%A4%ED%85%9C.html": {
    title: "OS 정리-Chap 11-분산 및 다중(병렬) 처리 시스템",
    date: " Aug 11, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2011-%EB%B6%84%EC%82%B0%20%EB%B0%8F%20%EB%8B%A4%EC%A4%91(%EB%B3%91%EB%A0%AC)%20%EC%B2%98%EB%A6%AC%20%EC%8B%9C%EC%8A%A4%ED%85%9C.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true11. 분산 및 다중(병렬) 처리 시스템title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 분산 시스템1 네트워크와 분산 시스템분산 처리  컴퓨터 사용자 간에 서로의 자원, 장치, 데이터를 교환해 처리할 수 있도록 네트워크로 상호 연결한 것이다.네트워크 연결 시스템의 분류  분산 시스템 : 메모리와 클록을 공유하지 않고, 지역 메모리를 유지하는 독자적 프로세서들로 구성  다중 처리 시스템(병렬 처리 시스템) : 메모리와 출력을 공유하는 하나 이상의 프로세서로 구성2 네트워크의 구성  네트워크란, 서로 독립 된 시스템들이 적절한 영역 안에서 속도가 빠른 통신 채널을 이용해 상호 통신할 수 있도록 지원하는 데이터 통신 시스템  분산 시스템에서 네트워크는 원거리의 하드웨어나 소프트웨어 자원에 액세스할 수 있는 기능 제공  크게 강결합과 약결합 시스템으로 이루어짐    2.1 강결합(tightly coupled) 시스템      그림 11-1과 같이 프로세서들이 메모리를 공유하는 다중 처리 시스템.공유 메모리를 이용해 통신하므로, 결합 교환을 통해 경쟁을 최소화title: 결합교환(combining switch)이란?메모리를 요청한 프로세서들 중 하나에게만 액세스 허용2.2 약결합(loosely coupled) 시스템운영체제, 메모리, 프로세서, 입출력 장치 등이 독립된 시스템들이 통신을 통해 메시지 전달, 원격 저 호출로 통신.➕ 시스템 하나가 고장나도 다른 독립된 시스템에 장애가 영향을 미치지 않음3 네트워크의 구조(topology)네트워크의 노드나 링크 요소를 물리적으로 연결하는 방법들title: 노드(Node)사이트, 터미널, 컴퓨터 등 통신하는 객체를 의미3.1 망(mesh) 구조각 노드를 시스템의 모든 노드와 직접 연결하는 완전 연결(fully connected) 방법➕ 두 노드를 직접 연결하므로 매우 빠름➕ 시스템 분할에 필요한 장애의 수가 많음 = 신뢰성이 높음➖ 노드가 많을 수록 기하 급수적으로 설치 비용이 많이 들고, 통신선 낭비가 많음.3.2 트리(tree) 구조, 계층(hierachy) 구조주로 회사 등에서 사용하는 방법, 네트워크의 각 노드가 트리 형태로 구성됨.➕ 기본 비용이 망 구조보다 낮음➖ 부모 노드가 고장나면 그 아래 모든 자손 노드들은 자신들을 제외한 다른 노드들과 고립됨3.3 성형(star) 구조모든 노드를 중앙 노드에 직접 연결하고, 중앙 노드 외의 다른 노드는 서로 연결하지 않음.그림 11-5 중앙의 모든 노드의 연결을 중계하는 노드를 중앙 노드라고 하며, 메시지 교환을 담당.➕ 기본 비용이 노드 수에 비례하고 비교적 저렴➕ 비교적 간단한 구조로, 집중 제어 및 유지 보수 수월➖ 중앙 노드에 병목 현상에 의한 성능 감소➖ 중앙 노드 장애 발생 시 전체 시스템 마비3.4 링(ring) 구조각 노드를 정확히 다른 노드 2개와 연결하는 방법단 방향 구조와 양 방향 구조가 있다.단방향 구조는 한 노드나 링크가 고장 나면 네트워크 분할양 방향 구조는 연결이 2개 고장 나면 네트워크 분할➕ 기본 비용이 저렴한 편이고, 노드 수에 비례함➖ 메시지가 링을 순환하면서 성능 저하3.5 버스(bus) 구조연결 버스(중앙 통신 회선) 하나에 모든 노드를 연결하는 방법➕ 기본 비용이 노드 수에 비례하고 경제적➕ 각 노드의 고장이 노드 간 통신에 영향을 주지 않음, 신뢰성 높음➕ 노드의 추가 변경 제거 등이 비교적 쉬움➖ 버스에 장애가 발생하면 네트워크 전체에 영향을 미침4 원격 프로시저(RPC, Remote Procedure Call) 호출분산 시스템에서 한 컴퓨터에서 실행할 프로세스를 다른 컴퓨터에서실행하는 프로세스의 프로시저(함수)를 호출하는 방법.클라이언트/서버 모델이 전제.주로, 원격지 노드에 필요한 데이터가 있을 때 요청하는 방법으로 사용한다.간단한 클라이언트 서버 간 RPC 원리  클라이언트와 서버 사이에 프로시저를 호출하는 프로그램을 허용하여 지역 프로시저를 호출  잠시 클라이언트의 프로시저를 멈추고 네트워크를 통해 지역적으로 분산된 원격 프로시저를 실행  반환 값을 네트워크 통신 응답을 통해 클라이언트로 받는다.스터브(stub)          데이터를 전송하려고 다양한 형태의 데이터로 서로 변환하는 기능을 수행하는 모듈➕ 프로세스들이 원격 컴퓨터에 있는 프로시저를 지역 컴퓨터에 있는 프로시저 처럼 쉽게 호출하여 상호 운용성 증진➖ 통신의 신뢰성과 보안, 전역변수의 미지원        5 분산 시스템의 구조와 구축 목적        분산 시스템은 네트워크로 연결된 여러 노드, 즉 개인 메모리와 클록이 없는 프로세서들의 집합을 이용해 프로그램 하나를 분산하여 실행해 서로 정보를 교환하는 구조이다.분산 시스템은 다음과 같은 목적으로 구축한다.            자원 공유 용이 : 분산 시스템으로 다른 노드의 자원, 장치, 데이터, 함수에 접근할 수 있다.      연산 속도 향상              특정 연산을 동시 수행 가능한 여러 연산으로 분할해 여러 노드에서 동시에 수행하여 성능 향상      특정 노드가 너무 작업이 집중되면 다른 노드로 작업 이동(부하 분담, load sharing)     - 신뢰성 향상      한 노드가 고장 나도 나머지 노드가 계속 작동하여 신뢰성 보장      시스템이 고장을 감지하고 복구하는 작업 수행    - 통신 기능      다수의 노드를 통신 네트워크를 이용해 연결하면 다른 노드에 있는 사용자들이 정보를 교환      분산 시스템은 다음과 같은 투명성(transparency)을 보장해야 한다. 일종의 성능 지표?다만, 모든 투명성을 실현하는 것은 복잡하고 비용이 많이 들므로, 각 응용 시스템 마다 필요한 투명성만 구현한다.  액세스 투명성(access transparency) : 프로세스가 다른 노드의 차이(컴퓨터 구조, 운영체제 등)과 관계없이 동일한 형식으로 액세스할 수 있게 하는 능력          SQL 쿼리, 웹 내비게이션 등        위치 투명성(location transparency) : 자원 위치와 각 컴포넌트가 상호작용는 위치를 사용자가 몰라도 지역 파일처럼 액세스 가능          웹페이지, NFS(Network File System, 네트워크 파일 시스템)        고장 투명성(failure transparency) : 시스템 구성 요소(컴포넌트)와 통신 오류 때문에 시스템을 수행하는 데 장애를 받지 않도록 하며, 이를 빠르게 복제나 복구할 수 있도록 함.          보통 고장의 여파를 성능 감소 정도로만 느끼게 만든다.      데이터베이스 관리 시스템(database management system)        중복 투명성(replication transparency) : 복제한 자원 그룹에서 모든 액세스가 자원이 하나만 있는 것처럼 보이게 해 신뢰성과 유용성을 높일 수 있음          분산 DBMS, 웹페이지 미러링(mirroring)        이동 투명성(migration transparency) : 자원을 한 시스템에서 다른 시스템로 이동해도 사용자는 의식하지 않고 사용 가능          웹페이지, NFS        영속 투명성(permanence transparency) : 자원이 저장된 위치 정보를 감춤.  자원 투명성(resource transparency) : 구성 요소에서 자원의 배당과 해제 정를 감춤.  트랜잭션 투명성(transaction transparency) : 공유 공간에서 동작하는 트랜션 연산 조정과 자원 집합 사이의 결합을 숨겨 데이터 무결성과 일관성 확보  재배치 투명성(reassignment transparency) : 한 객체의 재배치를 이와 통신하는 다른 객체에 감출 수 있게 함.  규모 투명성(scale transparency) : 구성 요소를 추가하거나 제거하는 등, 규모가 바뀌어도 사용하자가 의식하지 않음  병행 투명성(concurrency transparency) : 사용자와 응용 프로그램이 서로 간섭 없이 공유 데이터 또는 객체에 동시에 액세스 가능.          NFS, 금융 자동화 기기(ATM) 네트워크        02 네트워크 운영체제        네트워크 운영체제는 단일 사용자 환경인 네트워크 운영체제와 시스템에 연결된 컴퓨터들의 원격 자원을 공유하기 위한 분산 운영체제로 구분        1 네트워크 운영체제(NOS, Network Operating System)        1.1 네트워크 운영체제의 원리        네트워크 운영체제는 메모리 관리, 프로세서 관리, 장치 파일 관리 등 전역 제를 수행하지 않고 네트워크에 있는 서버를 사용하도록 지원하여, 자율적인 컴퓨터들이 편리하고 효율적인 방법으로 서로를 이용할 수 있도록 하는 분산 시스템 소프트웨어이다.            네트워크 운영체제는 통신을 제어하고 분산된 자원을 공유하며 독립된 시스템을 서로 연결하기 위해 개발됨.그림 11-11처럼 네트워크 운영체제는 서버에 저장된 대용량 파일이나 서버에 연결된 공유 프린터 등 자원을 요청한 클라이언트에 제공하려고 설계함.일반적인 운영체제와의 차이점은 컴퓨터 한대 뿐만 아니라 LAN(근거리 통신망, Local Area Network)의 전체 자원을 관리하여 클라이언트가 서버와 상호작용할 수 있도록 도와준다.1.2 네트워크 운영체제의 주요 기능네트워크 운영체제는  네트워크 사용자마다 동질성이 부족하므로 가능하면 표준을 유지하며 모든 컴퓨터를 지원하도록 새로운 기술을 용이하게 적용할 수 있는 구조이어야 함.  프린터나 파일 관리는 물론 네트워크 전체를 항상 관리, 감시하면서 장애가 발생하면 복구도 수행할 수 있는 환경을 보장해야 함.  다수의 네트워크 클라이언트가 신속하게 데이터에 액세스 할 수 있도록 효율적이고 신뢰성 있어야 한다.주요 기능들은 다음과 같다.  자원 공유 : 네트워크에 연결된 서버와 클라이언트 간에 하드디스크나 프린터 등 자원 공유 가능  액세스 권한 부여 : 사용자는 원격 사이트의 자원을 사용할 수 있도록 액세스 가능.  파일 전송 : 한 컴퓨터에서 다른 컴퓨터로 데이터를 전송  데이터 보호 : 사용자 별 적합한 권한(읽기 권한, 쓰기 권한 등)을 설정하고 데이터를 관리, 보호  관리 제어 : 각 클라이언트의 네트워크 이용 정보와 네트워크에서 발생하는 여러 문제를 해결하고 조절,관리하는 기능 제공    1.3 네트워크 운영체제의 운영 방법    네트워크 운영체제는 크게 P2P(peer-to-peer) 모델과 클라이언트/서버 모델로  구분P2P 방법그림 11-14와 같이 동등하게 작동되는 LAN에 연결된 각 컴퓨터가 상황에 따라 클라이언트, 서버로 동작한다.즉 파일 서버나 중앙 집중식 관리 자원이 없으며, 모든 컴퓨터가 동일한 액세스 권한과 네트워크에 있는 사용 가능한 자원에 같은 권한을 가짐.➕ 적은 초기 비용: 클라이언트/서버의 기능을 같은 컴퓨터에 구현➕ 자원 : 자원의 활용 극대화와 자원을 각각 균일하게 공유➖ 분산 : 파일과 응용 프로그램에 중앙 저장소 없이 분산되므로 관리 힘듦.➖ 보안 : 서버와 클라이언트에 보안 적용 힘듦클라이언트/서버 방법네트워크에 연결된 컴퓨터가 각각 클라이언트나 서버로서 역할을 수행하는 방법.    서버 : 작업을 수행하는 시스템, 프린터 서버, 파일 서버 등의 서비스 제공  클라이언트: 서비스를 요청하는 시스템, 워크스테이션 모델의 경우 클라이언트는 예외적으로 동시에 한명만 사용 가능그림 11-15처럼 네트워크와 독립적이며, 네트워크에 접속하지 않은 컴퓨터 시스템은 자신의 환경에 맞게 변경, 관리할 수 있음.➕ 중앙 집중식: 서버에서 자원과 데이터 보안 제어➕ 유연성 : 새로운 기술을 시스템에 쉽게 통합➕ 접근성 : 서버를 여러 플랫폼이나 원격으로 액세스➖ 비용 : 전용 서버에 초기 투자가 필요➖ 소프트웨어 : 네트워크 운영체제 소프트웨어가 필요➖ 의존성 : 서버가 다운되면 네트워크가 작업을 중지➖ 유지 관리 : 대형 네트워크 작업을 효율적으로 동작할 수 있는 관리자 필요2 분산 운영체제(DOS, Distributed Operating System)의 연산분산된 컴퓨터 간에 자원을 쉽게 공유하고 액세스할 수 있는 운영체제네트워크 운영체제의 지역적임을 극복하기 위고 전역적으로 제어 관리하기 위해 발전됨.다음과 같은 기능을 수행한다.  데이터 이동(data migration)  사용자가 데이터를 요청하면 이를 전송하는 방법은 크게 두 가지가 있다.          파일 전체를 요청자에게 보내는 방법, 이후 수정 여부와 관계없이 요청자는 원 데이터 보유자에게 파일을 되돌려 보내야 한다.      작업에 필요한 부분만 요청자에게 보내는 방법, 이후, 수정된 부분만 원 데이터 보유자에게 되돌려 보낸다.  주로 전체 파일을 액세스해야 한다면 첫번째, 일부분만 접근해도 되면 두번째가 효율적        연산 이동(computation migration)  사용자가 함수나 연산을 요청하여 그 결과 값을 받는 방법.  동시 수행이 가능하여 이쪽이 더욱 효율적일 때가 많다.  프로세스 이동(process migration)  프로세스 전체를 이동시켜 실행하는 방법  다음과 같이 두가지 방법으로 분류한다.          사용자에게 프로세스 이동 사실을 숨김  ➕ 사용자가 이동을 위한 외부 명령을 코딩할 필요 없음  ➕ 부하 균등화와 연산 속도 상승 효과      사용자가 직접 이동 방법을 명시하거나 허가하게 함  ➕ 효율적인 하드웨어나 소프트웨어 이용 가능  프로세스 이동은 다음의 장점이 존재한다.  ➕ 부하 균등화 : 프로세스는 네트워크로 작업량을 분산  ➕ 연산 속도 향상 : 다른 사이트에서 동시에 수행 가능한 여러 서브 프로세스로 나눈다면 전체 프로세스의 반환 시간이 줄어듦  ➕ 하드웨어 이익 : 예를 들면 행렬 계산 등은 특화된 배열 프로세서에 맡기는 게 좋다.  ➕ 소프트웨어 이익 : 소프트웨어 규모가 너무 크거나 보안, 인증 등의 이유로 연산 이동이 불가한 경우      3 분산 운영체제의 구현분산 운영체제는 대부분 마이크로 커널 기반 구조이다.프로세스와 스레드 관리, 메모리 관리, 프로세스 간 통신, 장치 관리와 시스템 인터럽트 처리 등의 기본적인 기능만 유지하며, 나머지는 응용 프로세스로 처리한다.크게 프로세스 기반과 객체 기반으로 나눠서 구현한다.  프로세스 기반 분산 운영체제          시스템의 모든 프로세스와  자원을 시스템 서비스를 제공하는 프로세스의 집합으로 해석.      프로세스 간 동기화와 시스템 상태, 사용자 프로세스의 제어, 프로세스 생성과 스케줄링, 인터럽트 등 프로세스 관리는 프로세스 간에 메시지를 교환해 수행        객체 기반 분산 운영체제          시스템의 각 하드웨어와 소프트웨어들을 독립된 객체로 해석      각 객체에는 고유한 이름과 식별자, 상태가 있으며, 객체의 생성, 제거 등을 커널이 관리한다.        4 분산 시스템에서 프로세스 관리        분산 시스템 교착 상태는 복잡한 문제를 일으킨다.교착 상태는 자원 할당 교착 상태와 통신 교착 상태로 구분할 수 있다.        4.1 자원 할당 교착 상태        프로세스가 서버에 있는 데이터 객체나 입출력 자원에 액세스하려 할 때, 이미 다른 프로세스에 할당된 자원을 요구하면 발생            교착 상태는 다음 네 가지 조건이 모두 성립할 때 발생한다.(교착 상태 단원 참조)  상호 배제  점유와 대기  비선점  순환 대기따라서 이 중 하나만 방지 돼도 교착 상태가 생기지 않는다.다음은 이러한 조건을 방지하는 방법들이다.교착 상태 예방  자원의 종류에 따라 선행적인 순서를 정의하면 순환 대기 조건이 예방 된다.          예를 들어 B 자원은 무조건 A 자원을 먼저 이용한 뒤 얻을 수 있다면, B 자원을 이용하고 있는 프로세스는 A 자원을 이미 사용해 다시 요구할 리 없으니, A자원을 가진 프로세스가 B자원을 요구하고, B 자원을 가진 프로세스가 A 자원을 요구하는 순환 대기가 일어나지 않는다.        작업 수행 전에 필요한 모든 자원을 모두 확보 가능할 때만 할당 받게 한다면 점유와 대기 조건이 예방 된다.교착 상태 탐지  시스템에서 모든 자원의 점유와 요구 사항을 자원 그래프로 표현해 사이클 여부를 검사, 이후 프로세서가 실행에 필요한 자원 확보 시까지 실행을 지연시킨다.  다음과 같은 방법들로 교착 상태 탐지 가능          중앙 집중형 제어 : 노드 하나가 교착 상태 탐지 전담, 모든 요구와 해제 메가 중앙 노드로 전송되어 파악.  ➕ 구현하기 쉬움  ➕비교적 작고 통신 속도가 높은 LAN에 효율적  ➖ 자원 요청이 중앙 프로세서에 집중되어 병목 현상 발생  ➖ 중앙 프로세스 문제 시 시스템 마비      계층형 제어 : 노드를 계층 구조로 구성하고 한 노드가 루트 역할을 담당하게 한 후 자원 할당 정보를 수집해 교착 상태 탐지, 중앙 집중형 제어와 비슷      분산형 제어 : 모든 프로세스가 협력해 교착 상태 탐지  ➕ 노드 장애 발생에 대한 장애 내구성 높음  ➖ 정보 교환에 의한 오버헤드 발생      교착 상태 회피자원 할당 요구 허용 뒤, 교착 상태 여부를 결정하여 판단➖ 모든 노드가 시스템 전역 상태를 파악 및 유지를 위한 오버헤드가 심함➖ 안전 상태를 검사하는 것도 상당한 처리 오버헤드 발생따라서 구현이 어렵다.교착 상태 복구다른 프로세스에 필요한 자원을 많이 사용하는 프로세스를 선택하여 제거한 후 다시 처음부터 재실행 하도록 조치4.2 메시지 전송 교착 상태프로세스가 메시지를 기다리는 상태다음과 같은 두 가지 이유로 발생한다.상호 대기동일한 그룹에서 한 프로세스가 다른 프로세스가 보낸 메시지를 기다리고 있고 전송 중인 메시지가 없을 때 발생그림 11-17은 시스템 교착 상태를 WFG(wait for graph)로 표현한 것이다.그림 (a)는 교착 상태가 아니며, 그림 (b)는 상호 대기에 의한 교착 상태이다.메시지 할당 버퍼 불충분그림 11-18은 메시지 할당 버퍼 불충분에 의한 교착 상태 예시이다.(a)는 서로의 버퍼가 가득 차 패킷을 전송하거나 수신할 수 없는 상태로, 메시지 직접 저장-전송 교착 상태라고 한다.(b)는 어떤 노드의 버퍼가 패킷로 가득 차 메시지를 수신할 수 없어, 버퍼를 비우지 못하게 되어 순환 대기 상에 빠진 상태로 메시지 간접 저장-전송 교착 상태라고 한다.이러한 교착 상태는  각 링크 별로 별도의 버퍼를 하나씩 사용하게 하여 예방  버퍼가 가득차지 않게 전송 메시지 수에 상한 값을 부여하여 예방  계층화된 버퍼 풀(pool)을 이용해 교착 상태 제거          버퍼가 부족 시 버퍼 풀에서 가져오도록 하여 버퍼가 꽉 차지않게 예방      5 클라이언트/서버 분산 컴퓨팅5.1 클라이언트/서버 시스템의 정의클라이언트 역할과 서버 역할을 하는 프로그램을 나누어 구성하는 시스템  클라이언트 : 다른 프로그램에 서비스를 요청하는 프로그램, GUI, 입출력, 요청 등을 담당(웹 브라우저(크롬))  서버 : 그 요청에 응답하는 프로그램, 데이터 저장, 처리, 제공을 담당(웹서버(IIS, APache))클라이언트 서버 분산 컴퓨팅 환경은 사용자, 응용 프로그램, 자원들을 네트워크(단일 LAN이나 WAN)으로 연결함.title: WAN(Wide Area Network, 광역 네트워크)?지역적으로 넓게 분산된 네트워크 시스템, 통신 링크가 비교적 느리고 신뢰성이 낮음. 전화선, 통신위성 채널 등이 예시➕사용자에게 편리한 인터페이스와 응용 프로그램 제공➕ 데이터베이스, 네트워크 관리, 유틸리티 기능의 중앙 집중화로 유지 부담 감소➕데이터와 정보에 액세스 하는데 필요한 컴퓨터와 인터페이스 유형 선택이 자유로움➕ 여러 지역에 걸쳐 분산된 프로그램을 서로 연결시키기 편리함5.2 2계층의 일반적인 클라이언트/서버 구조클라이언트는 원활한 상호작용을 위해 GUI 사용자 인터페이스 설계가 매우 중요하다. 이러한 역할을 프레젠테이션 서비스라고도 한다.2계층 구조는 그림 11-21과 같다.클라이언트와 서버가 물리적으로 독립된 시스템에 존재하며, 운영체제, 플랫폼 등이 서로 다를 수 있다. 이를 통해 공유 자원과 작업을 분할 할 수 있다.이 둘을 서로 연동하기 위해 TCP/IP나 OSI 참조 모델 같은 통신 소프트웨어와 분산 응용 프로그램을 이용하여 연동한다.title: OSI? TCP/IP?OSI(Open System Interconnection reference model)  이기종 시스템간 네트워크 연결을 위해 네트워크를 계층 7개로 나눠 네트워크 를 이용해 응용 프로그램 정보를 다른 컴퓨터의 응용 프로그램에 전달하는 방법 설 명TCP/IP(Transmission Control Prtocol/Internet Protocol)  가장 널리 사용되는 네트워크 계층 프로토콜복잡한 계산이나 유형이 다른 데이터를 분석하는 소프트웨어 위치에 따라 팻 클라이언트와 씬 클라이언트로 구분한다.      팻(fat) 클라이언트주요 응용 프로그램 처리 모듈과 프레젠테이션 로직이 클라이언트에 위치그림 11-22의 (a)에 해당한다.        씬(thin) 클라이언트주요 응용 프로그램 처리 모듈과 데이터베이스가 서버에 위치하고, 클라이언트는 프레젠테이션 로직만 수행즉, 클라이언트는 사용자 인터페이스와 메시지 전달만 담당하는 가벼운 역할이다.그림 11-22의 (b)에 해당  2계층 모델의 장단점은 다음과 같다.➕ 서로 다른 환경의 클라이언트와 서버를 연동 가능➖ 네트워크 통신량 증가로 인한 병목 현상 발생\t- 캐시 등으로 완화 가능하나, 캐시의 일관성 문제 발생➖ 응용 프로그램의 논리적 구조나 물리적 구조가 완전히 분리되지 않아, 프로세스나 업무 환경 변경 시, 소프트웨어 수정이 힘들다.➖ 대부분 응용 프로그램이 특정 데이터베이스에 종속되어 응용 프로그램, 데이터 통합이 힘들고 유연성이 떨어짐 (확장성 한계)➖ 이식성 한계로 다른 서버로 프로그램 기능 일부 이동시키기 어려움5.3 3계층의 클라이언트/서버 구조데이터베이스가 위치한 서버 부분과 사용자가 주로 사용하는 클라이언트를 완전히 분리하고, 응용 프로그램 로직을 모듈화해서 중간 계층에 별도로 두어 유연하고 확장 가능하게  한 3 계층의 클라이언트/서버 구조 시스템클라이언트 증가에 따른 성능 저하 문제를 해결하기 위해 서버에 중재 소프트웨어인 미들웨어를 적용.미들웨어의 역할 예시  서버의 부하를 균등하게 분배하는 부하 분산(로드 밸런싱, load balancing)이 가능  응용 프로그램 로직을 분산시키는 모듈화해서 중간 계층에 두어 분산 환경에서 일관성있게 액세스 가능  클라이언트의 데이터 가공 작업을 맡음 -&gt; 클라이언트 요구에 따라 데이터 관리 서버와 연동해 클라이언트의 요구를 처리 후 결과 전달위와 같은 역할을 통해서 클라이언트와 서버 기능을 분리해 비대해지는 것을 막음.➕ 파일 캐시의 일관성과 장치 간의 환경 불일치를 극복해줌.➕ 서버 확장에 용이, 사용자 수 급증 시 부하 분산 및 추가 확장으로 일정한 응답 시간 보장➕ 서버, 클라이언트의 작업 처리 부담을 덜고, 여러 기능을 제공하여 시스템 성능과 융통성이 향상➕ 응용 프로그램 로직을 모듈화하여 유지 관리 쉬움5.4 클라이언트/서버 구조와 미들웨어미들웨어(middleware)는  클라이언트/서버 사이에 교량 역할  다른 환경에서 실행되는 응용 프로그램이 상호 원만하게 통신하게 함.  모든 플랫폼으로 시스템 자원에 액세스할 수 있게 하는 소프트웨어  서로 다른 시스템 간에 상호 운영을 하는 데 필요한 소프트웨어  네트워크가 연결된 분산 컴퓨팅 환경에서 사용자 컴퓨터와 네트워크에서 실행되는 응용 프로그램 간에 자유롭게 데이터를 이동하여 응용 프로그램 개발을 지원하는 소프트웨어주로 운영체제와 분산 응용 프로그램 사이에 존재한다.미들웨어는 데이터베이스 엔진과 클라이언트를 연결하여 호환성을 제공하기도 하고, GUI, 각종 통신 소프트웨어 역할을 하기도 한다.이외에도 그림 11-26처럼 개발 도구, 실행 환경, 관리 도구 등으로 구성되며, 통신 서비스, 코어 서비스, 응용 프로그램 서비스로 나뉜다.03 다중 처리 운영체제1 다중 처리(multiprocessing) 시스템의 구조와 원리다중 처리 혹은 병렬 처리는 다수의 프로세서를 동시에 수행하여 시스템 성능을 향상 시키는 방법이다.다중 프로그래밍은 소프트웨어 관점, 다중 처리는 하드웨어 관점병렬 프로그래밍의 어려움많은 프로세스를 통한 병렬 연산을 통해 처리량을 늘릴 수 있으나, 다음과 같은 이유로 병렬 프로그래밍은 힘들다.  인간의 사고는 병렬적으로 생각하기 어려움  인간의 언어는 병렬성으로 적절히 표현하지 못한다.  다중 처리는 병렬성을 통한 성능 향상을 염두에 두고 만들지 않고, 다중 사용자를 위해 만들었으므로, 병렬성 경험이 부족함.  컴퓨터 하드웨어는 순차적 처리에 익숙함  병렬 프로그램은 오류를 검색하기 어렵고, 정확성 증명 방법이 복잡하다.병렬 프로그래밍의 장점  많은 프로세서를 이용해 신뢰성 증가  가용성 증가 : 프로세서가 고장나면 성능이 조금 줄어들 뿐 시스템 사용 가능          시스템이 고장난 프로세서를 파악하고, 다른 프로세서에게 일을 맡김      시스템이 고장이 나지 않도록 자원 할당 방법과 부담을 분배        컴퓨터 성능의 증가  비교적 저비용으로 단일 프로세서 컴퓨터 시스템의 계산 능력을 높일 수 있음다중 처리 시스템의 특징  다중 처리 시스템은 성능이 거의 비슷한 2개 이상의 프로세서를 포함  모든 프로세서는 동일한 메모리를 공동으로 사용  모든 프로세서는 입출력 채널과 제어 장치, 그 외의 장치들을 공동으로 사용  전체 시스템은 하나의 운영체제로 운영, 해당 운영체제는 각 프로세스, 데이터들의 상호작용을 도움2 다중 처리 시스템의 연결 방법다중 처리 시스템은 다양한 방법으로 연결할 수 있다.2.1 공동 버스(common bus) 시스템그림 11-28과 같이 프로세서, 메모리, 입출력 장치 등 각종 장치 간에 하나의 공동 버스를 제공하는 방법  공동 버스 : 각종 장치 간 정보 전송을 장치의 버스 인터페이스를 통해 제어하는 수동 장치버스를 통한 데이터 통신 방법데이터 전송을 원하는 프로세서와 상대방 장치를 사용할 수 있는지 검사하고, 상대방 장치에 데이터 처리 방법을 알린 후 데이터를 전송제어 신호 인식 방법데이터를 전송 받는 장치는 버스에 실린 메시지가 자신에게 오는 정보라는 것을 알기 위해 전송 장치의 제어 신호들을 다음과 같은 방법으로 인식  어드레싱 : 버스에서 모듈을 구분해 데이터 목적지 결정  중재 : 모든 입출력 모듈은 일시적으로 마스터 기능 수행, 버스 제어는 우선순위 방법을 사용해 경쟁적 요청을 중재하는 데 제공  시간 공유 : 하나의 모듈이 버스를 제어할 때 다른 모듈은 잠김 및 버스 액세스를 달성할 때까지 작업을 일시 중단➕ 버스에 장치를 추가하여 손쉽게 장치 연결 가능➕ 경제적임➕ 융통성 있음➕ 구조가 간단함➖ 버스에 이상이 생기면 전체 시스템 불능➖ 시스템의 전체 전송량이 버스 전송률에 따라 제한➖ 시스템이 바빠지면 버스 경쟁에 의한 성능 감소➖ 따라서 작은 규모의 시스템에만 사용2.2 크로스바 교환 행렬(crossbar switch matrix) 시스템그림 11-29처럼 메모리와 프로세서의 수만큼 공동 버스 시스템의 버스 수를 켜 격자 모양으로 크로스 스위치 포인트를 지정하는 방식이다.  크로스 스위치 포인트: 프로세서의 경로를 결정하는 메모리 모듈➕ 메모리마다 회선이 달라 서로 다른 메모리 2개를 동시에 참조하며, 충돌이 없음➕ 각종 장치의 인터페이스를 간단하게 할 수 있음➕ 전송 경로의 경우의 수가 여러 개라 전체 전송률이 매우 높음➕ 장치를 추가하여 단위 시간 당 처리량을 더욱 높일 수 있음➕ 추가 논리 회로를 두어 신뢰도를 높이고 시스템 분할이 가능➖하드웨어, 교환기가 복잡해질 수 있음2.3 다중 포트 메모리(multiport storage) 시스템그림 11-30과 같이 제어 논리회로, 교환 논리회로, 우선순위 조절 논리회로를 각 메모리 인터페이스에 설치하여 포트를 이용해 메모리를 할당하고, 장치가 메모리를 할당하는 방식이다. 다중 포트 메모리 시스템 메모리 접근 충돌 방지 대책  메모리 포트에 영구적인 우선순위를 할당해 동시에 메모리에 접근 시 우선순위 순으로 할당  그림 11-31 처럼 적은 수의 프로세서와 입출력 프로세서만 메모리에 접근하도록 함➕ 프로세서와 메모리 사이의 다중 경로로 높은 전송 속도를 얻을 수 있음➕ 프로세서 메모리 할당 확보가 쉬워 높은 안정성➖ 비싼 메모리 제어 논리와 케이블, 커넥터의 수가 필요2.4 하이퍼큐브(hypercube) 시스템N개의 주소로 프로세서 $2^n$개로 인접한 다른 프로세서와 큐브 모양의 연결하여 할당하는 약결합 시스템주소의 길이가 1이면 2개의 프로세서, 주소의 길이가 2이면 4개의 프로세서, 주소의 길이가 3이면 8개의 프로세서를 상호 연결한다.3 다중 처리 시스템의 운영체제3.1 주종(master/slave) 운영체제그림 11-33 같이 프로세서 하나를 주(M, 마스터)로 지정해 운영체제를 실행하고, 나머지 프로세서는 사용자 수준의 프로그램을 실행할 수 있는 종(S)으로 지정하는 구현하기 쉬운 구조  주 프로세서는 범용 프로세서로 연산 뿐만 아니라 입출력도 담당  종 프로세서는 연산만 담당만약 종 프로세서가 운영체제에 개입할 필요가 있다면 인터럽트를 보내고, 주 프로세서가 해당 인터럽트를 처리함이때 한 순간에 하나의 인터럽트만 처리하므로, ➕ 상호 배제 문제가 간단해짐➖ 대기 큐가 생겨 프로세스 활용도와 응답 시간에 영향  따라서 시스템 부하가 잘 알려져 있어 스케줄링이 쉬운 경우에 유리  특히 입출력이 많은 작업은 주 프로세서만 가능하므로 부담 균형이 불균형➖ 또한, 주 프로세서가 고장 나면 모든 시스템이 사용 불가(낮은 신뢰성)  주 프로세서가 압도적으로 성능이 좋은 비대칭적 시스템에 적당➖ 하드웨어 비대칭성 : 프로세서들이 동등하지 못하여 하드웨어 최적으로 사용 불가3.2 분리 실행각 프로세서마다 운영체제가 서로 다르며, 각 프로세서에서 발생하는 인터럽트는 해당 프로세서에서 해결하는 구성 방법물론, 전체 시스템을 위한 정보는 상호 배제를 이용해 접근한다.➕ 공동 테이블 경쟁 적음➕ 주 프로세서가 없으므로, 신뢰성이 높음➖ 전체 프로세서를 감시하는 프로세서가 없으므로 고장 탐지 및 복구가 힘듦➖ 입출력 장치는 수동으로 재구성 필요➖ 한 프로세서가 바빠도 다른 프로세서로 부하 분배 불가능3.3 대칭여러 프로세서가 하나의 운영체제를 공유하는 방식, 모든 프로세서가 동등한 에 있으며, 공유 운영체제가 모든 프로세서와 입출력 장치, 기억 장치를 사용할 수 있도록 관리.운영체제 동시 진입을 방지하기 위해 재진입 코드와 상호 배제 필요여러 프로세서가 하나의 작업을 협력할 수 있음-&gt; 이를 위해 시스템 테이블과 시스템 기능을 책임지는 운영 프로세서를 하나 지정-&gt; 통일성 있고 일관성 있게 운영할 수 있다.➕ 작업 부하를 가장 효과적으로 분산, 다른 프로세서로 작업 이전 가능➕ 신뢰성이 가장 높음  신뢰성 구축 방법  한 프로세서가 고장 나면 운영체제는 그 프로세서를 제거하고, 이 사실을 오퍼레이터에 알림 ➕ 프로세서들의 병행 작업 가능, 자원을 더욱 효율적으로 활용➖ 구축이 복잡하고 높은 기술 요구➖ 협업 시, 동시에 여러 프로세서가 운영 프로세서가 될 시, 성능 저하  이를 막기 위해 시스템 데이터를 여러 독립적인 프로세서에 분산 수용한 뒤, 개별적으로 폐쇄할 수 있게 한다.4 클러스터(cluster)클러스터는 사용 목적에 따라 고성능, 부하분산, 고가용성 클러스터로 구분4.1 클러스터의 구조컴퓨터 여러 대를 연결하여 단일 컴퓨터처럼 동작하도록 한다. 대칭형 다중 처리가 가능하며, 클러스터를 구성하는 노드는 단일 시스템, PC, SMP, 프로세서 등이 될 수 있으며, 물리적으로 분리되어 있으며 LAN으로 연결되어 있다.시스템 전반을 위한 서비스 (예를 들어 백업과 사용자 관리, 고장 탐지 복구, 네트워크 감시, 단일 시스템 관리 등)가 필요-&gt; 클러스터 시스템 소프트웨어를 제공해야 하며, 이를 연결하는 서버가 필요클러스터 내부의 통신네트워크 인터페이스 하드웨어는 통신 프로세서처럼 작동하며 클러스터 노드 간의 데이터 패킷을 전송, 수신하게 함.통신 소프트웨어는 빠르고 신뢰성 있게 통신하게 해주는 액티브 메시지 등을 사용  액티브 메시지(active message) : 병렬 분산 시스템에서 널리 사용하는 메시지 기반 통신 시스템          메시지 핸들러를 내용과 함께 포함해 전송하여, 도착하자마자 핸들러가 메시지 처리      이를 통해 버퍼링 중의 오버헤드가 줄어들며, 비동기적 수행으로 병렬화 가능      외부에서 해당 클러스터 네트워크와의 통신을 위해 공인 IP를 사용하며, 내부의 클러스터 간의 통신은 사설 IP를 이용함클러스터 미들웨어는 사용자가 통합된 하나의 시스템으로 인식하도록 SSI(single system image) 기능을 이용한다.4.2 고성능 클러스터(HPC, High-Performance Clusters)대규모 연산을 계산하는 데 사용하는 가장 일반적인 구조클러스터의 노드에 다양한 연산 작업을 분할하여 향상된 성능을 제공, 한 노드의 계산한 중간 결과를 다른 노드에게 제공해 미래 계산에 적용 가능주로 과학 분야나 슈퍼 컴퓨터 등에 사용된다.그림 11-37은 고성능 클러스터의 자원 관리 시스템의 예시이다.자원 관리 시스템은 제한된 계산 자원의 효율성과 활용도를 높이고, 경쟁을 방지하는 작업 처리를 관리함.크게 작업 스케줄러와 자원 관리자로 구성된다.스케줄러가 가용성 자원의 스케줄링 결정과 연산 노드에 할당 등의 활동을 하며 이를 위해 작업 큐 정보를 얻기 위해 자원 관리자와 통신하는 구조자원 관리자  노드 상태의 모니터링  요청한 작업을 수신  작업 요청을 노드에서 실행할 수 있도록 할당  일부는 기본 스케줄링이나 정책 제어자원 관리자를 통해 시스템 사용률을 향상 가능작업 스케줄러  우선 작업, 작업 스케줄링, 정책 및 조직의 목표 등을 고려해 작업 우선순위 결정  사용 가능한 자원과 작업 큐를 자원 관리자에게 입력받아 우선순위 결정에 고려함기본형 클러스터 : 모든 노드의 하드디스크에 독립적으로 운영체제를 모두 설치하여 자체적으로 파일과 라이브러리를 해결디스크 없는 클러스터 : 서버 노드 한 대에만 하드디스크가 있어 다른 노드는 서버 노드의 파일 시스템을 사용4.3 부하분산 클러스터(LBC, Load-Balancing Clusters)그림 11-38과 같이 외부 클라이언트의 작업 요청을 부하 분산 서버, 혹은 부하 분배기(load balancer)가 받아들여 부하량이 가장 적은 노드에 요청을 전달하는 방식이다.따라서 단일 서버를 사용 시 발생하는 부하를 다른 노드로 분산시켜 웹 서버의 과부하를 해결 할 수 있으며, 사용자가 많은 서비스에 어울리는 구조이다.부하분산 클러스터는 구현 방법에 따라 DR, NAT 등으로 구분  DR(Direct Routing)  클라이언트에 들어온 요청을 부하 분산 서버가 다른 컴퓨터에 분배하고, 요청을 할당 받은 컴퓨터가 직접 응답하는 방법  실제 서버의 공인된 IP를 통해 서로 직접 연결한다.  NAT(Network Address Translation)  클라이언트의 요청을 처리하는 실제 서버는 부하 분산 서버를 경유해 사용자에게 전송하는 방법  ➕ 부하 분산 서버와 실제 서버 사이는 사설 IP를 사용하기 때문에, 공인 IP가 실제 서버 수만큼 필요치 않다.  ➖ 하지만 부하 분산 서버가 고장 나면 전체 서비스가 불가  이를 극복하기 위해 부하 분산 서버를 여러 대 두는 고가용성 클러스터를 함께 이용하기도 한다.부하 분산 서버의 실제 서버 스케줄링 방법  순환 할당 스케줄링(라운드 로빈, Roundrobin)  모든 노드에 동등하게 돌아가며 할당  가중치(weighted) 기반 순환 할당 스케줄링  각 서버마다 가중치를 부여하고 가중치에 따라 요청 할당  최소 연결 스케줄링  가장 연결이 적은 서버에서 요청을 직접 연결하는 방법, 각 서버에서 동적으로 실제 연결한 숫자를 계산해야 함.  가중치 기반 최소 연결 스케줄링  가중치 비율을 실제 연결자 수에 따라 설정해 네트워크 연결 할당4.4 고가용성 클러스터(HAC, High-Availability Clusters)시스템이 실패하면 중복 노드를 운영하여 서비스를 제공하는 방식이를 통해 서비스의 고가용성을 제공할 수 있다.그림 11-39와 같이 컴퓨터 두 대에 같은 소프트웨어와 하드웨어를 설치하고, 컴퓨터 두 대 사이를 고속 네트워크로 연결하여 서로 상대방의 현재 상태를 검사한다.두 가지 방법으로 구현될 수 있다.  두 대의 동일한 서버가 전체 서비스를 분할하여 제공해 성능 향상에 이용되다가 한 쪽이 고장 시, 나머지 정상 서버가 모두 인계 받아 실행  두 대의 동일한 서버 중 한 대는 아무 작업도 하지 않다가 서비스를 제공하는 컴퓨터가 고장이 나면 모든 서비스를 넘겨 받음4.5 클러스터의 성능에 미치는 요소  클러스터의 노드 수가 많으면 관리하기 힘들다.  고장난 노드 수가 많으면 시스템 성능이 떨어진다.  네트워크의 성능 : 노드 간의 수시로 주고받는 정보 전송 속도와 신뢰도  프로세서의 성능 : 노드 작업의 요청이 많고 고연산을 요구하면 작업이 쌓이면서 네트워크 병목 현상이 생김          이를 해결하기 위해 노드를 추가하거나 RAID 시스템 활용        입출력 응용 프로그램은 어느 노드에서든 입출력 장치, 메모리에 액세스할 수 있도록 단일 입출력 공간 (SIOS, Single I/O Space)를 구현한다.  클러스터 시스템을 통홥된 자원으로 사용할 수 있도록 SSI 기능 제공"
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2012-%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EB%B3%B4%EC%95%88%EA%B3%BC%20%EB%B3%B4%EC%95%88%20%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C.html": {
    title: "OS 정리-Chap 12-시스템 보안과 보안 운영체제",
    date: " Aug 11, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2012-%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EB%B3%B4%EC%95%88%EA%B3%BC%20%EB%B3%B4%EC%95%88%20%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C.html",
    tags: ["OS","CS","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true12. 시스템 보안과 보안 운영체제title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 보안의 개념과 보안 위협의 유형1 보안의 개념  보호: 컴퓨터 시스템에 저장된 프로그램과 데이터 액세스 제어 등 내적인 문제만  다룸.  보안          시스템, 시스템이 작동하는 외부 환경과 내부 조직 구성원의 액세스까지 보호하는 것.      title: 표 12-1 보안의 구분            구분      설명                  기술적 보안      - 하드웨어, 소프트웨어적 조치, 기술, 정책, 절차 포함 - 시스템(네트워크, DB, 프로세스, 컴퓨터, 장비 등)이 대상 - 액세스제어, 암호화, 인증, 침입 탐지 등 존재              물리적 보안      - 시설에 허가 받지 않은 접근 차단 및 모니터링- 물리적 출입 제어(스마트카드, 감사 추적, 접근 이력 관리)              관리적 보안      - 조직 내부의 정보 보호 체계 정립 - 절차, 감시 조직, 사고 대책 수립 등이 존재      여기서 다루는 것은 기술적 보안 중의 컴퓨터 보안이며, 그림 12-1과 같이 컴퓨터 액세스 보안, 네트워크 보안, 데이터 액세스 보안이 있다.2 보안의 요구 사항  기밀성(confidentiality): 인가된 사용자만 정보 자산에 접근(조회, 복사)하게 하는 것,  방화벽, 암호 등이 존재  무결성(intergrity): 적절한 권한을 가진 사용자가 인가된 방법으로만 정보를 수정하게 하는 것, 정보의 변조, 파괴 방지, 액세스 제한 등이 존재  가용성(availabilty): 정보 자산을 필요한 시간에 접근할 수 있는 것, 빠른 접근이 예시  인증(authentication): 사용자가 누구인지 확인하는 절차, 이후 액세스 제어로 넘어감  액세스 제어(access control): 사용자가 정보를 사용할 권리가 있는지 판단  권한 부여(authorization): 각 사용자가 적절한 권한으로 적절한 정보에 액세스할 수  허용 범위를 정의하는 과정  자격 증명(credential): 식별자 및 신분 증명이 완료된 정보로 자원에 액세스를 얻는 것    3 보안 위협의 유형        중단(흐름 차단, Interruption) : 시스템 일부를 차단하거나 사용할 수 없도록 가용성에 피해를 주는 공격  ex) 하드웨어 파괴, 통신 회선의 절단, 파일 관리 시스템 무력화 등  도청(가로채기, Sniffing): 권한이 없는 공격자가 컴퓨터 자원에 액세스, 피해자가 눈치 채지 못한다.  ex) 패킷 스니핑  변조(수정, Modification): 권한 없는 공격자가 컴퓨터 자원에 액세스 + 내용 변경하는 무결성 공격  ex) 파일의 내용 수정, 프로그램 변경, 전송 메시지 변경  위조(조작, Fabrication): 권한 없는 사용자가 정보를 교체, 제거, 데이터 순서 변경을 통해 위조 객체를 삽입하는 공격, 무결성과 인증에 피해를 줌  ex) 허위 메시지 삽입, 파일 레코드 추가 등  사칭(가장, Masquerade) : 비인가 사용자가 인가 사용자로 가장, 인증에 피해공격을 분류하면 능동적 공격과 수동적 공격으로 구분할 수 있다.  수동적(Passive) 공격: 사전 계획 없이 도청, 모니터링을 통해 정보를 얻어 공격, 공격을 알아채기 힘들지만, 암호화로 쉽게 해결  ex) 스니핑, 스캐닝(공격할만한 대상의 취약점 등을 탐색), 트래픽 분석  능동적(Active) 공격:  시스템 내부의 정보를 변경, 시스템 상태와 작동을 변경하여 공격  ex) 사칭, 변조, 서비스 거부(수 많은 연결 요청 등의 메시지를 보내 액세스 자원을 확보하여 정상적인 사용을 막음) 등이 존재그 외에 내가 조사해서 추가한 유형들  부인(Repudiation): 공격자가 자신이 보낸 메시지를 부인하여 서비스에서 이득을 취하거나 자원을 획득  백도어(Backdoor): 공격자가 미리 시스템에 침투해 향후 있을 공격을 위해 취약점을 생성해 놓음.4 소프트웨어 위협크게 호스트 프로그램이 필요한 경우와 자체적으로 위협을 가할 수 있는 경우로 나뉨4.1 컴퓨터 바이러스(computer virus)  정의 : 자기 자신을 스스로 복제할 수 있는 실행 가능한 명령 집합  피해 : 컴퓨터의 프로그램이나 프로세서의 서비스 영역에 자신 또는 자신의 변형을 복제하여 시스템에 피해를 줌  전파 : 사용자 권리와 네트워크를 이용해 다른 컴퓨터 시스템으로 퍼뜨릴 수 있음  프로세서의 서비스가 필요하므로 하드웨어의 파괴는 불가능하며 시스템이 중단 시 활동 불가  컴퓨터 바이러스의 주 기능          자기 복제(증식) 기능: 다른 파일과 시스템을 감염시켜 복제됨, 네트워크를 통한 이메일이 주요 매체      은폐 기능: 파일 크기나 내용을 감염 이전과 똑같이 보이게 하여 은폐, 일부 바이러스는 빠른 확산 속도 때문에 은폐하지 않음      파괴 기능: 미리 작성한 프로그램으로 특정 신호 이후 데이터 파괴, 시스템 오작동 유발      4.2 웜(worm)  컴퓨터 시스템에 해를 끼칠 수 있는 장소에 위치하는 바이러스나 복제 코드의 일종  시스템 자원을 이용해 자신의 복사본 생성 후, 네트워크를 통해 전파title: 컴퓨터 바이러스과 웜의 차이점컴퓨터 바이러스는 다른 프로그램이나 파일을 매개로 감염시켜 복제웜은 자기 자신이 프로그램이므로 자신을 복제함4.3 트로이 목마(trojan horse) 프로그램  해로운 기능을 수행하는 은폐된 코드를 포함한 프로그램  자기 복제 능력은 없고 고의적으로 포함된 파괴 기능이 존재4.4 트랩 도어(trap door)  서비스 기술자나 유지 보수 프로그래머가 액세스 편의를 제공하려고 시스템 설계자가 만든 통로  이를 통해 공격자가 공격할 수 있으므로 반드시 개발 완료 후 닫아야 함.4.5 사이버 테러(cyber terror)  정부 기관이나 민간 기관의 정보 시스템에 침입하여 중대한 장애를 발생하거나 파괴는 범죄 행위          이메일 폭탄(email bomb): 이메일 사용자의 이메일 프로그램을 마비 시키거나 정상적인 메일 수신을 방해하기 위해 다량의 대용량 쓰레기 메일을 발송      논리 폭탄(logic bomb): 특정 데이터의 출현과 소멸에 따라 부정 행위를 동작하는 프로그램      시한 폭탄(time bomb): 특정 사건이나 시스템 타이머의 시점에 바이러스나 트로이 목마 등을 운반하는 프로그램      02 시스템 보안의 개념과 액세스 제어1 시스템 보안의 개념권한이 없는 사용자가 파일, 폴더, 장치 등을 사용하는 것을 제한하여 보호하는 시스템 기능크게 여섯 가지 주제로 집약  계정과 패스워드 관리 : 가장 기본적인 인증 수단  세션 관리: 사용자와 시스템 또는 두 시스템 간에 활성화된 접속을 관리, 세션 가로채기를 통해 인증 공격을 받을 수 있다.  액세스 제어: 시스템을 다른 시스템에서 적절히 보호할 수 있도록 접근 통제  권한 관리: 시스템 각 사용자가 적절한 권한으로 적절한 정보 자산에 접근할 수 있도록 통제  로그 관리: 시스템 내부 또는 네트워크의 활동 사항 기록  취약점 관리: 시스템 자체의 결함 관리    2 액세스 제어(access control)    적절한 권한을 가진 인가자만 특정 시스템이나 정보에 접근할 수 있도록 통제  2.1 사용자 액세스 제어식별과 인증을 통해 사용자를 인증하고 비인가자를 막는다.그림 12-3은 이러한 시스템 로그인의 예시이다.시스템 자원을 보호하는 가장 기초적인 첫단계로, 식별자와 암호를 암호화하고 가용성을 위해 잘 관리해야 함.추가적인 보안과 관리를 위해 다음을 고려할 수 있다.  단일 사인 온(SSO, Single Sign On) : 사용자의 편의성을 위해 한번 인증으로 여러 자원에 접근 가능  2단계 액세스 제어 : 네트워크 액세스와 자원 액세스에 따로 식별자와 암호를 두게 함그림 12-4는 수많은 식별자와 암호를 중앙에서 효율적으로 관리하기 위한 네트워크 액세스 분산 서비스 그림이다.2.2 데이터 액세스 제어액세스 행렬(access matrix) : 파일이나 데이터베이스에서는 각각의 객체에 대한 엑세스 권한을 관리하기 위한 권한의 집합  주로 행은 주체, 열은 객체(파일, 테이블, 프로그램, 메모리 세그먼트), 각 항은 액세스 권한의 집합title: 액세스 행렬 예시            사용자 \\ 객체      파일 $F_1$      파일 $F_2$      파일 $F_3$      프로그램      프린터                  A      r             r                            B                           x      r              C             r      x                            D      rw             rw                    r: 읽기 권한, w: 쓰기 권한, x: 실행 권한액세스 행렬은 위와 같이 간단하지만 빈 공간이 많아 저장 공간이 낭비됨이를 막기 위해 희소행렬의 자료구조 저장법을 이용할 수 있지만, 보호 설비의 특성상 사용 힘듦다음은 액세스 행렬을 표현하는 다른 방법들이다.전역 테이블(global table)&lt;사용자, 객체, 권한&gt; 3개의 순서쌍을 이용해 집합을 구성하는 방법title: 전역 테이블 예시            사용자($D_i$)      객체($O_j$)      권한                  A      $F_1$      r              A      $F_3$      r              B      프로그램      x              B      프린터      r              C      $F_2$      r              C      $F_3$      x              D      $F_1$      rw              D      $F_3$      rw      ➕공백으로  인한 저장 공간 낭비 해결➖ 테이블이 여전히 매우 큼\t- 어떤 객체가 모든 사용자에게 사용 가능하다면 모든 사용자마다 한 항목이 존재\t- 메모리에 담을 수 없으며, 가상 메모리를 이용하기 힘듦액세스 제어 리스트(access control list)각 객체 리스트는 &lt;사용자, 권한&gt; 순서쌍으로 구성되어 해당 객체에 액세스 권한이 있는 모든 사용자 영역을 정의➕ 사용자 입장에서 효율적으로 사용 가능\t- 사용자가 객체를 만들면 해당 객체에 대한 사용 가능 사용자와 연산을 만들어 추가하면 됨➖ 액세스 권한이 지역화 되지 않음\t- 사용자 A만 사용하고 있으면 나머지 사용자 B, C 정보는 필요 없으나 분리하지 못하고 전부 가져와야 함➖ 객체 접근 마다 액세스 제어 리스트를 모두 탐색해야 함.기준치 집합: 탐색 시간을 줄이기 위해 미리 정의한 기본적인 권한에 대한 집합?(🤔)권한 리스트액세스 제어 리스트와 비슷하지만 각 행을 사용자로 놓고 객체와 그 객체에 허용된 동작의 리스트를 생성하는 방식.➕ 특정 사용자의 리스트만 읽어 들인 후, 해당 파일의 권한만 찾으면 되므로, 지역화가 쉬움(가상 메모리 활용이 쉬움)➕ 해당 사용자의 리스트만 읽으면 되므로 탐색 불필요➖ 사용자가 객체를 만들면, 해당 객체에 권한을 가진 모든 사용자의 리스트를 생성해야 함권한 리스트는 모두 운영체제가 유지하고 사용자가 간접적으로 액세스할 수 있다.➕ 권한을 운영체제가 보호하므로 객체 또한 보호할 수 있다.➖ 권한의 변경이 성능적으로 비효율적, ∵ 해당 권한 데이터를 담은 객체를 다른 객체와 구별하기 위해 고수준의 소프트웨어로 번역해야 함.  크게 두 가지 방법으로 데이터를 구분한다.          각 개체의 권한과 액세스 가능 여부를 비트로 표현한 태그로 표현, 운영체제만 접근, 변경 가능      프로그램의 메모리 부분을 두 부분으로 나눔                  보통 데이터와 명령을 담은 부분          운영체제만 접근 가능한 권한 리스트를 포함한 부분                    락/키(lock/key) 방법  위 두 방법의 절충안, 대부분 시스템에서 사용  락(lock) : 각 객체들이 가지고 있는 유일하고 독특한 비트 패턴의 리스트  키(key) : 각 사용자들이 가지고 있는 유일하고 독특한 비트 패턴의 리스트  락/키 방법 순서  1 . 프로세스가 운영체제에 객체에 접근 요청          해당 프로세스를 실행한 사용자의 키의 비트 패턴들과 해당 객체의 락의 비트 패턴들을 모두 비교      일치하는 패턴이 존재하면 접근 가능      ➕ 키의 길이에 따라 융통성 있고 효율적임  비트 연산이 빨라서 효율적인가?🤔➕ 사용자 영역 간에 자유롭게 키 교환 가능(사용자 그룹 기능을 의미하는가? 🤔)➕ 권한 리스트처럼 운영체제가 관리하므로, 동일한 장점 공유(객체 보호 가능)03 시스템 보안 방법1 암호화암호화는 신뢰할 수 없는 연결(=즉, 탈취나 도청 가능성이 있는)을 이용할 때 정보를 보호하는 방법이다.복호화를 위한 키가 없는 공격자는 정보를 알 수 없다.다음은 암호의 기능이다.  비밀성: 통신망을 이용한 정보 전송이나 시스템에 저장된 정보 노출 방지  인증: 액세스 하려는 사용자의 확인 및 사용자의 권한 방지  무결성 검사: 메시지나 파일 변조가 없음을 보장  전자 서명: 인증과 무결성 검사 기능으로 신원 확인 + 해당 정보에 대한 동작(생성, 전송)을 보장크게 대칭 암호화와 비대칭 암호화로 나뉜다.1.1 대칭 암호화대칭 암호화 혹은 단일키 암호화는 동일한 키를 이용해 암호화와 복호화하는 방식이다. 대표적으로 IBM이 개발하고 NIST가 표준화한 DES(Data Encrpytion Standard)가 있다.➕ 비대칭 암호화에 비해 성능에 부담이 적고, 직관적이다.➖ 비밀키를 상대방에게 건네주는 방법이 필요1.2 비대칭(공개키) 암호화  1976년 디피(Diffie), 헬먼(Hellman)이 발표한 알고리즘  각 사용자는 공개키와 개인키를 별도로 가지고 있다.  공개키는 모든 사용자에게 공개되어있으며, 암호화할 때 사용한다.  개인키는 오직 개인만이 가지고 있으며, 복호화할 때 사용한다.title: 개인키와 공개키의 변환반대로, 개인키를 암호화할 때 사용하면, 공개키로 복호화할 수 있다.이렇게 서로가 암호화한 암호문을 복호화할 수 있는 능력으로 디지털 서명에도 쓰인다.  RSA 암호화 알고리즘이 대표적➕ 대칭키를 암호화하여 건네주기, 자격 증명, 전자 서명 등 다양한 곳에 사용될 수 있음➖ 대칭키 방식에 비해 성능 부담이 큼2 인증서로 교환하는 정보의 무결성을 확인하는 메시지 인증과 송수신자의 정당성을 확인하는 사용자 인증으로 구분한다.2.1 메시지 인증(MAC, Message Authentication Code)메시지 인증 코드(MAC) 메시지의 끝에 넣어 인증하는 방식다음과 같은 변경을 탐지할 수 있다.  메시지 전송 중에 수신자의 변경 여부  메시지 내용의 변경 여부  메시지 순서의 변경 여부          MAC 알고리즘에 두 사람이 공유하는 비밀키와 메시지를 이용해 MAC를 만든다.      해당 MAC를 전송할 메시지에 덧붙인다.      이후 수신자는 비밀키와 메시지를 이용해 만든 MAC와 메시지 뒤편에 붙은 MAC를 비교해 변경 여부를 확인한다.      만약, 중간에 메시지가 변경되었다면 다른 MAC값이 나올 것이다.MAC 또한 조작하려면 공격자가 비밀키를 가지고 있어야 한다.HMAC는 MAC를 만드는 함수로 해시 함수를 이용하는 방법이다.2.2 사용자 인증사용자 인증은 아이디, 암호 입력 등 특별한 과정을 거쳐 서버의 특정 디렉터리를 사용할 수 있도록 하는 방법으로 사이버 공간에서 신원을 확인할 수 있게 함주로 암호를 이용하지만 그림 12-13처럼 공개키를 이용하여 인증할 수 있다.송신자가 개인키로 암호화한 암호문은 A의 공개키를 이용해서만 인증되므로, A의 개인키를 가진 A임이 인증된다.3 디지털 서명(digital signature)디지털 서명은  부인 봉쇄 : 메시지를 송수신 시 해당자가 송수신 행위를 부인하는 행위 방지  인증  데이터 변조 검증에 사용주로 비대칭키를 이용하여 진행된다.  송신자 B가 자신의 개인키로 암호화  송신자 B가 송신자 A의 공개키로 다시 암호화  수신자 A에게 전송  수신자 A가 먼저 자신의 개인키로 복호화, 이를 통해 자신을 위해 보낸 것이 증명  수신자 A가 송신자 B의 공개키로 복호화, 이를 통해 송신자 B가 보낸 것이 증명    4 네트워크 보안    4.1 방화벽(firewall)    방화벽 혹은 침입 차단 시스템은 다른 네트워크의 사용자에게서 네트워크 자원을 보호하는 프로그램  방화벽의 기능  도메인 확인: 확인된 도메인 이름이나 IP 주소인지 확인해 접속 허용  원격 접속 확인: 보안 접속 절차나 인증 확인 등을 이용해 네트워크 원격 접속 허용방화벽의 목적  비공개 자원 외부 액세스 방지  내부에서 접속해야 할 외부 자원 통제방화벽의 원리네트워크 패킷의 헤더와 페이로드를 검사하여 판단방화벽의 한계악의적인 내부 사용자의 공격 방지 못함조사 시간으로 인해 통신 병목 지점이 될 수 있음4.2 침입 탐지 시스템(IDS, Intrusion Detection System)정보 시스템의 비밀성, 무결성, 가용성을 침해하는 모든 행위를 탐지하는 적극적 보안 시스템IDS의 목적침입 사실을 빠르게 검출하고 침입자를 봉쇄하여 시스템 손실과 데이터 훼손 최소화IDS의 기능  시스템에서 의심나는 점을 감시, 조사하여 필요한 조치를 취함  비정상적인 사용이나 잘못된 사용 등 기준이나 규정을 벗어나는 행위 발견 시, 침입 과정을 로그에 기록하고 위험을 알림  방화벽 경계선을 통과한 공격이나 내부자 오남용 탐지IDS 설치 위치에 따른 역할  라우터와 외부 인터넷 사이: 모든 공격 탐지 가능, 너무 많은 패킷으로 인해 비효율적  라우터 뒤: 라우터의 패킷 필터링 이후에 탐지, 좀더 의지가 분명한 공격자 탐지  방화벽 뒤: 영향을 받지않고 공격을 막는 마지막 부분, 내부에서 외부로 향하는 공격도 탐지 가능, 가장 많이 설치되는 부분  내부 네트워크: 내부의 악성 사용자의 해킹을 감지 가능  서버 네트워크(DMZ 내): 중요 데이터와 자원을 보호하기 위해 IDS 별도 운영title: DMZ(DeMilitalized Zone)?네트워크의 내부 사용자와 외부 사용자가 모두 사용하는 네트워크철저한 보안과 인증이 필요하며, 서비스의 중축이 되는 경우가 많다.예를 들어, 페이스북의 백엔드 서버와 프론트엔드 서버는 페이스북 개발자들은 유지보수 및 개발을 위해 접근하며, 외부 사용자들은 서비스 사용을 위해 접근한다.4.3 트래픽 패딩(traffic padding)트래픽양 분석 공격을 방어하기 위해 정상적인 메시지에 추가로 가짜 암호 메시지를 보내는 것.  만약, 아무런 암호문을 보내지 않는 상황에는 의미없는 암호문을 생성해 주기적으로 송신 👉 트래픽 사용 주기 파악 불가능  암호문을 보낼 때는 일정한 길이가 되도록 의미없는 데이터를 덧붙인 뒤에 암호화하여 송신 👉 패킷 길이 정보를 통한 메시지 내용 예측 불가능이때, 의미 없는 데이터를 덧붙인 암호문의 길이는 실제 데이터 트래픽에 발생 가능한 패킷의 길이보다 길어야 함.04 보안 운영체제1 보안 운영체제(secure OS)의 개념운영체제에 내재된 결함을 발생할 수 있는 각종 해킹에서 보호하기 위해 보안 기능이 통합된 보안 커널을 추가로 이식한 운영체제이러한 보안 운영체제를 탑재한 컴퓨터를 신뢰 시스템(trusted system)이라고 한다.보안 운영체제의 기능  시스템 사용자 식별, 인증  강제적이고 임의적인 액세스 제어  감사 추적(audit &amp; trail)  위협 모니터링(threat monitoring)을 통한 침입 탐지특히, 내부 사용자 보안을 강력하게 유지 가능커널의 참조 모니터 모듈을 이용해 사용자가 파일, 디렉터리, 디바이스 등에 연산 요청 시, 액세스를 제어해 승인과 거절을 수행한다.title: TCSEC(Trusted Computer System Evaluation Criteria)신뢰성 컴퓨터 시스템 평가 기준, 일곱 가지 등급(D~A1)으로 분류컴퓨터 보안 요구사항을 규정하여 보안 커널로 구현하게 함.2 보안 운영체제의 기능강력하고 완벽한 보안을 구현하려면 시스템 구조부터 뒤바뀌어야 한다.보안 운영체제는 기존의 실행 환경을 유지하면서 강력한 보안 기능을 제공하기 위해 다음 등의 기능을 가지고 있다.2.1 영역 분리응용 프로그램이나 사용자 역할에 따라 액세스할 수 있는 정보 영역과 시스템의 제어 권한을 제한ex) 웹 서버와 관련된 프로세스가 해커의 공격을 받아 뚫려도 DB에는 접근 못하게 원천적으로 권한 제한2.2 강제적 액세스 제어(mandatory access control)사용자나 프로세스가 객체에 액세스 시, 신분이나 규칙으로 해당 객체 액세스를 커널 수준에서 제어사용자에게는 액세스 가능한 범위를 나타내는 보안 등급을,객체에는 허용 할 수 있는 기밀 수준을 할당한다.시스템 내부의 요소를 좀 더 세분화하여 여러 정책을 가진다.  액세스 제어 정책: 주체가 객체에 액세스 하는 방법 지정  인증 사용 정책: 사용자 인증에 사용  암호 사용 정책: 저장 전송하는 데이터를 보호하는 데 사용➕ 메모리, 커널 수준의 액세스 제어를 통한 강력한 보안➕ 시스템 과부화 감소2.3 신뢰할 수 있는 경로(보호된 경로,  trusted path)  소프트웨어와 함께 상호 동작하는 사용자를 허가하는 기능  주체가 객체에 접근하는 방법을 운영체제가 강제함소프트웨어 요소 간에 상호 보증할 수 있고 신뢰할 수 있는 보호된 경로의 채널을 사용하기 위함이다.만약, 이가 없다면, 철저한 보안이 갖춰지더라도 공격자가 사용자에게 취약한 경로를 이용하게 하여 공격할 수 있다.2.4 참조 모니터(reference monitor)객체의 모든 액세스를 제어하는 추상적인 장치주체와 객체 사이의 중계자 역할을 함주체와 객체의 보안 매개변수를 이용해 자원 액세스 제어 기능을 수행이때 액세스 여부는 다른 보안 방법(인증, 영역 분리 등)들과 데이터를 교환하면서 결정참조 모니터 구현 요구 사항  액세스 제어 방법은 변조 불가  모든 액세스 요청은 참조 모니터를 통해야 함  다른 많은 보안 방법과 상호작용 하면서 모든 액세스 요청을 분석과 시험해야 함 ∴ 성능 과부화를 줄이기 위해 작아야 하며, 보통 추가적인 하드웨어 구성을 함2.5 감사 로그 추적(audit log trail)사용자가 시스템에 액세스한 후 입력한 검색어, 검색 대상의 활동 내역을 기록하고 저장하여 일련의 기록을 조사하는 것  감사 로그(audit log): 단순히 시간, 사용자, 객체에서 모든 액세스 형태를 기록한 파일, 특정 서버의 로그 디렉터리에 위치          감사 로그를 통해 피해 정보와 시간과 장소를 알 수 있고 이를 통해 예방 가능      침입 탐지 시스템이 읽으며 침입을 탐지      감사 로그 필드 예시  주체  행위: 주체의 동작(로그인, 읽기, 입출력 수행, 실행)  객체  예외 조건  자원 사용량: 프로세서 시간, 출력 행 수, 읽은 레코드 수  시간 검인(timestamp): 행위가 발생한 시간title: (a) 스미스가  디렉터리의 GAME 실행 프로그램 복사 명령 감사 로그 예시COPY GAME.EXE TO GAME.EXE  위 예시는 실패하여 아래 (b)와 같은 감사 로그를 생성한다.title: (b)생성된 감사 로그(Smith, execute, COPY.EXE, 0, CPU=00002, 11058521678)(Smith, read, GAME.EXE, 0, RECORDS=0, 11058521679)(Smith, write, GAME.EXE, write-viol, RECORDS=0, 11058521680)감사 자료 또한 공격자가 조작할 수 없도록 보안성이 필요하며, 이는 커널 수준에서 구현하여 차단 가능하다."
  }
  , 
  
  "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2013-%EC%9C%A0%EB%8B%89%EC%8A%A4%20%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C.html": {
    title: "OS 정리-Chap 13-유닉스 운영체제",
    date: " Aug 11, 2022 ",
    url: "/articles/computer_science/OS/IT_COOK_BOOK_OS_%EC%A0%95%EB%A6%AC/OS%20%EC%A0%95%EB%A6%AC-Chap%2013-%EC%9C%A0%EB%8B%89%EC%8A%A4%20%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C.html",
    tags: ["OS","CS","요약","CRUDE","HIDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true13. 유닉스 운영체제title: 출처  IT COOK BOOK 운영체제 (개정 3판, 구현회 저, 한빛 아카데미)를 정리한 내용입니다.01 유닉스의 탄생과 구성1 유닉스의 탄생과 발전 과정2 유닉스의 설계 원칙3 유닉스의 특징4 유닉스의 구성 요소4.1 커널4.2 셸4.3 유틸리티와 파일 시스템02 유닉스 프로세스의 관리1 유닉스 프로세스의 종류2 유닉스 프로세스의 상태3 유닉스 프로세스의 구조4 유닉스 프로세스의 스케줄링03 시스템 호출 인터페이스1 파일 조작2 프로세스 제어3 시그널04 유닉스의 메모리 관리1 유닉스의 메모리 관리 개요2 대치그림 13-13 초기의 대치 맵과 대치 공간의 할당  (a) :(b) :(c) :3 페이징05 유닉스의 파일 시스템1 디스크 블록의 구조2 유닉스에서 연속 파일 할당3 i 노드의 할당과 반납4 유닉스의 디렉터리5 유닉스의 시스템 파일 테이블6 유닉스의 디스크 구조"
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%201-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%99%80%20%EC%9D%B8%ED%84%B0%EB%84%B7.html": {
    title: "네트워크 정리-Chap 1-컴퓨터 네트워크와 인터넷",
    date: " Aug 21, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%201-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%99%80%20%EC%9D%B8%ED%84%B0%EB%84%B7.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 1. 컴퓨터 네트워크와 인터넷 (Computer network and Internet)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/e1.1 인터넷이란?1.1.1 Component 관점Hardware 관점수 많은 연산 장치가 통신선과 패킷 스위치를 통해 연결된 네트워크      통신선(communicatioin link): 전선, 광섬유, 라디오 전파, 위성 등을 통해 비트 데이터를 보내는 선        연산 장치: 호스트(host) 혹은 종단 장치(end system)라고 부르며, 컴퓨터, 서버, 스마트폰, 인터넷 어플리케이션 등이 존재          호스트는 추가로 PC 같은 서비스 이용자들의 호스트인 클라이언트(client)와 데이터 서버 같은 데이터를 저장하고 클라이언트에게 서비스를 제공하는 서버(server)로 나뉨            패킷(packet) : 데이터가 담긴 페이로드(payload) + 프로토콜에 따른 헤더(header)가 포함된 정보        패킷 스위치(switch): 패킷을 적절한 방향의 링크로 연결하며, 라우터와 연결 계층 스위치 등이 존재한다.        경로(route, path) : 패킷이 목적지로 도달하기 위해 지나가는 통신선과 패킷 스위치들의 나열        ISP(Internet Service Providers): 모뎀, DSL, 무선, 랜 등 여러 네트워크 서비스를 제공하는 인터넷회사, 각 호스트 간 연결을 위한 인터넷망을 제공함.  [[#1.3.3 네트워크들의 네트워크(A Network of Network)]] 참조Software 관점프로토콜(Protocol)둘 이상의 통신 객체 간에 전달되야할 메시지들의 형식과 순서, 전송 간에 벌어질 행동, 규정 등을 정의한 것.인간이 대화하기 전에 대화 가능 여부를 살피고 질의응답을 받는 것과 비슷.  프로토콜 3 요소          구문(syntax): 데이터 형식, 부호화, 신호레벨의 규정      의미(semantic): 전송의 조작이나 오류 제어를 위한 제어 정보에 대한 규정      타이밍(timing): 접속되어 있는 개체 간의 통신 속도 조정이나 메시지 순서 제어 규정      두 통신 객체(호스트, 스위치 등)가 통신하려면 같은 프로토콜을 사용해야 하며, 이를 통해 인터넷 시스템을 구축대표적으로 TCP와 IP 프로토콜이 널리 사용, IETF에서 만든 RFCs(Requets for comments), IEEE 802 등에 여러 프로토콜이 적혀있다.서비스 관점종단 장치의 웹, e-mail 등의 인터넷 어플리케이션에게 연결이 가능하게 끔 서비스를 제공하는 기반종단 장치간에는 규칙과 서비스의 집합체인 소켓 인터페이스를 통해 데이터를 주고받는다.1.2 네트워크 말단(Network Edge)1.2.1 접속망(Access Network)접속망은 상단 그림의 파란 부분처럼 호스트와 첫 라우터를 연결하여 다른 망과 연결해주는 망이다.거주지 연결(Home Access, Broadband residential acess): DSL, 유선, FTTH, 5G 고정 무선DSL(Digital subscriber line)DSL Modem과 ISP의 전화선을 통해 DSLAM(Digital subscriber line acess multiplexer)가 위치한 지역 중앙 사무소(Local Central Office)에 연결되는 방식최신 DSL 기준으로 새로운 기준은 전송속도의 합(다운+업)이 1Gbps 이상.🟠 CONSISP 가격 정책, 사무소와의 거리, 연선의 상태와 전자기파 간섭 등에 의해 거리가 멀수록 속도가 많이 떨어짐.  DSL Modem : 디지털 데이터를 고주파로 변환해 전화선을 통해 통신할 수 있게 만듬.  DSLAM : 받아온 아날로그 수신 데이터를 디지털 데이터로 바꾸어 돌려줌  전화선(telephone line): 3가지 다른 주파수 영역으로 동시에 전달            채널      속도                  고속 다운스트림 채널(high-speed downstream channel)      50 kHz ~ 1MHz, 24~52Mbps              중속 업스트림 채널(medium-speed upstream channel)      4kHz ~ 50kHz, 3.5~16Mbps              기존 양방향 전화 채널(ordinary two-way telephone channel)      0 ~ 4kHz, 전화용        보통 다운로드가 여러 사람을 대상으로 하므로 더 빠른 채널을 사용하며, 속도가 다른 연결을 비대칭 접속(asymmetric access)라고 함유선(cable) 연결, HFC(hybrid fiber coax)각 사용자간 방송회사의 동축 케이블(Coaxial cable) 회선을 이용해 트리 형태로 연결 뒤, Fiber node를 통해 Cable head end와 연결하는 방식  Cable Head End:  CMTS가 존재하는 사무소          CMTS(Cable Modem Termitnation System): 아날로그 수신 데이터를 디지털 데이터로 변환, DSLAM 역할        Cable modem: 장치의 Ethernet Port와 케이블을 연결  공유 전파 매질(shared broadcast medium) 속성 - 회선을 여러사람이 사용(업, 다운)하면 그만큼 속도가 느려짐          이를 보완하기 위해 Distributed Multiple Access Protocol 필요      FTTH(fiber to the home)중앙 사무소에서 각각 사용자에게 직접 광섬유로 잇는 최신 기술연결 방법 종류: AONs(Active Optical Networks)와 PONs(Passive Optical Networks)  상단 그림은 PONs  Optical Splitter: 각 가정에 배치된 ONT(optical network terminator)의 정보를 규합해서 OLT로 보냄  OLT(Optical line terminator): 광섬유 정보를 디지털 신호로 변경, Packet을 나누어 각기 Splitter로 보내는 역할도 겸5G 고정 무선(5G fixed wireless)선없이 무선으로 모뎀을 사무소와 연결해 통신, [[#무선 랜(Wireless LAN), WiFi|WiFi]] 비슷속도가 빠르고 유선 설치가 필요 없음.기업망: Ethernet and WiFi여러 장치와 라우터 등의 장비를 효율적으로 통제하고 연결하기 위해 LAN(Local Area Network) 주로 이용시간이 지나며 가정 내에서도 라우터를 통해 여러 장치와 연결하여 구현함.이터넷(Ethernet)구리 연선(twisted-pair copper wire)으로 장치와 스위치를 연결각 스위치는 해당 집단 대표 라우터(router)에 연결, 인터넷과 통신장치의 경우 100M~10GBps, 서버의 경우 1~10Gbps 속도무선 랜(Wireless LAN), WiFi특정 접근 지점(Access Point)에 무선으로 송수신하는 이터넷IEEE 802.11 기술로 몇십미터 내에 100Mbps이상의 속도를 제공광역 무선 연결(Wide-Area Wireless Access): 3G, LTE 4G, 5G라디오 대역을 매개로 수십 킬로 미터 내의 기지국을 통해 휴대전화 통신 기반 연결4G망 기준으로 60Mbps 속도이며, 5G는 더욱 빠르다. 7장 참조1.2.2 물리적 매체(Physical Media)송수신자 간에 비트 정보가 지나가는 통로로, 전자기, 빛 등 여러 형태로 변형되며 지나간다.유도 매체(guided media)파장이 물질을 통해 특정 방향으로 퍼짐연선(Twisted-pair Copper wire)2개의 1mm 절연 구리선을 나선형으로 꼬은 유선, 전화선에서 시작되었으며 값싸고 적절한 속도로 LAN, UTP(Unshielded twststed pair) 등에 사용됨.나선형 모양을 통해 다른 가까운 전선의 전자기 간섭을 줄이고 추가로 여러 겹의 나선선을 감싸 부도체로 만든다.6a cable 등 선의 두께와 거리에 따라 종류가 다양하면 10Mbps ~ 10Gbps 정도동축 케이블(Coaxial Cable)두개의 절연된 관 모양의 구리선이 같은 방향으로 붙어있음.            유선 TV 시스템과 [[#유선(cable) 연결, HFC(hybrid fiber coax)      HFC]]를 구현하는데 사용되며, 대역을 여러 장치와 공유 가능      광섬유(Fiber Optics)빛의 파동을 이용해 비트를 전달하는 얇고 유연한 선수백 Gbps 수준의 빠른 속도, 전자기 간섭과 손실 없음, 도청이 힘듬 대신 관련 장비(transmiter, receiver, switch)등이 아주 비쌈.title: Optical Carrier(OC) standard광섬유의 속도는 OC 기준에 따라 51.8 Mbps ~ 39.8 Gbps가 존재하며, $OC-n$ 형태로 광섬유 등급이 표현되며,  $n \\times 61.8Mbps$ 속도 전후이다.비유도 매체(unguided media)파장이 대기 중에 파장의 형태로 퍼짐지상파 라디오 채널(Terrestrial Radio Channels)지상의 기지국이 주사하는 전자기파 스펙트럼을 통해 신호 전달🟢 PROS: 전선 등의 설치 불필요, 벽, 건물 등을 통과, 장거리 전달.🟠 CONS: 전파 환경과 거리에 따라 다음과 같은 문제 발생\t- 경로 손실\t- Shadow fading : 신호 강도가 점점 줄어듬\t- Multipath fading : 간섭 물체에 의해 신호 난반사\t- Inference: 다른 신호에 의해 간섭다음과 같은 그룹으로 채널을 나눔            거리      용도                  1 ~ 2m      무선 헤드셋, 키보드, 이어폰(블로투스)              수백m      무선랜, WIFI              수십km      휴대폰 통신      위성 라디오 채널(Satelite Radio Channels)위성과 지상의 기지국(Ground stations)이 극초단파(microwave)를 이용해 통신, 이후 여러 대역폭으로 전환되 이용됨.정지궤도 위성(geostationary satelite)  지구 궤도 36000Km의 한 지점에 언제나 머물러 있음🟢 PROS: 시간에 구애 받지 않고 수백 Mbps의 전송을 별도 장치 없이 가능🟠 CONS: 먼 거리에 의한 280ms 정도의 지연저궤도 위성(Low-earth orbiting(LEO) satelite)  비교적 가까운 궤도에서 공전함  정지궤도 위성의 정 반대의 장단점          빠른 지연, 일부 시간에만 통신 가능 하거나 이를 보완하기 위해 여러 위성을 올려야 함      1.3 네트워크 핵심(The Network Core)1.3.1 패킷 스위칭(Packet Switching)패킷은 장치들이 주고 받는 데이터 단위로, 네트워크 내 패킷 스위치 장치(스위치, 라우터 등)를 통해 전달된다.연결의 전송 속도(transmission rate)를 $R\\ bit/sec$, 보낼 정보의 양을 $L\\ bits$라고 할때, 전송에 필요한 시간은 $L/R$이다.축적 교환 (Store-and-Forward Transmission)패킷 하나를 완전히 전송 받은 뒤(적재 과정) 전송(포워딩)하는 방법, 대부분의 장치에서 사용한다.이로 인해 적재 시간($L/R$)이 추가로 필요하므로 실제 전송에 필요힌 시간 $L/R$ 만큼 지연 된다.(기타 지연 시간 무시)  예를 들어 패킷 한개는 $2L/R$, 패킷 3개는 $4L/R$이 걸림또한 중간에 적재되어야할 장치가 늘어나면 그만큼 배로 적재 지연이 늘어난다.  예를 들어 중간에 스위치 장치가 3개라면 패킷 한개는 $4L/R$, 패킷 3개는 $6L/R$이 걸린다.\\(d_{end-to-end}= N\\frac{L}{R}\\)큐 지연과 패킷 손실(Queuing Delays and Packet Loss)  두 컴퓨터가 너무 빠른 속도로 한 스위치에 보내면 큐 지연이 생기다가 패킷 손실이 일어난다.큐 지연(queuing delay)  앞선 패킷이 적재 중, 이후 패킷이 기다리기 위한 출력 버퍼(output buffer, output queue)에서의 지연패킷 손실(packet loss)  만약 이러한 큐가 가득차게 되면 패킷을 저장할 수 없어 손실되며 이를 패킷 손실이라고 함.            자세한 내용은 [[#큐 지연(Queuing Delay)      이곳]] 참조      포워딩 테이블과 라우팅 프로토콜(Forwarding Tables and Routing Protocols)패킷 포워딩(packet forwarding)  스위치에 도착한 패킷을 올바른 링크로 지정해주는 것포워딩 테이블(forwarding table)  패킷 포워딩을 구현하기 위한 방법  각 스위치에서 패킷의 목적지와 보낼 링크를 포워딩 테이블에서 대조하여 보낼 방향을 결정  각 포워딩 테이블은 라우팅 프로토콜(Routing protocol)에 의해 자동으로 결정 5장 참조1.3.2 서킷 스위칭(Circuit Switching)송수신 장치 간의 연결의 전송 대역을 미리 선점하여 뒤, 지속적인 연결을 보장하는 통신 방법, 전화 연결 등이 존재패킷 스위칭 : 예약 안하고 가도 되는 분식집서킷 스위칭: 예약 필수 레스토랑            비교      패킷 스위칭      서킷 스위칭                  구현      비교적 쉬움      비교적 힘듬              전송 효율성      3배 이상 좋음      나쁨              일정한 품질(지연시간, 보장 등)      불가      가능              사용      이메일, 문서 교환, 현재는 대부분      라이브 스트리밍, 게임, 최근에는 전화만        최근에는 성능과 비용의 효율 문제로 왠만한 것은 전부 패킷 스위칭으로 해결title: 효율성 예시전체 전송 속도가 1 Mbps, 한 유저가 사용하는 전송 속도가 100 kbps, 전체 연결 시간의 10%만 사용한다고 가정 시,  서킷 스위치의 경우, 10명의 유저만 사용 가능(100kbps * 10 = 1Mbps)  패킷 스위치의 경우, 35명의 유저 사용 중 11명의 유저가 동시에 사용해 전송 속도를 오버하는 일이 생길 확률은 다음과 같다.\\[1-\\sum_{n=0}^{10}{(\\frac{1}{10})^n(\\frac{9}{10})^{35-n}=\\frac{35!}{{n!}(35-n)!}}=\\frac{2121487977254128602547305755957}{5000000000000000000000000000000000}\\approx 0.000424298\\]즉, 아주 적은 확률의 패킷 손실을 감수한다면 대략 3배 이상 효율적임서킷 스위칭에서의 멀티플렉싱(다중화)(Multiplexing in Circuit-Switched Networks)주파수 분할 다중화(FDM, Frequency-Division Multiplexing)  각 연결의 주파수 대역(spectrum)을 대역폭(bandwidth)로 나누는 방법          전화 연결의 경우 보통 한 연결당 4kHz      FM 라디오파의 경우 88MHz ~ 108MHz 사이를 여러 라디오가 나누어 가짐      시간 분할 다중화(TDM, Time-Division Multiplexing)  각 연결의 일정 프레임을 일정 슬롯으로 나누는 방법          즉, 아주 빠르게 사용자를 전환하는 방법      1.3.3 네트워크들의 네트워크(A Network of Network)ISP(Internet Service Provider, 인터넷 서비스 제공자)  각 사용자간의 연결, 다른 ISP와의 연결을 통해 인터넷을 제공하는 회사들  지역 소비자를 위한 접근(access) ISP, 이들과 Tier 1 ISP를 연결하기 위한 지역적(Regional, Tier 2) ISP, 국가 간 혹은 거대 통신 회사 간의 연결을 위한 Tier 1 ISP 등 다양하다.Points of Presence (PoPs)  고객과 ISP를 연결하는 라우터들의 모임  고객 서비스 차별화를 위해 속도를 구별하기도 함Multi-home  개인 소비자나 ISP가 연결 실패나 혼잡 등을 대비하기 위해 여러 ISP와 연결하는 것Peering  같은 계층의 ISP끼리 상호 연결하여 하위 계층 ISP나 소비자에게 빠르고 추가 비용 없이 연결가능하게 함.  보통 상위 ISP와 연결 시, 비용이 필요한데 Peering은 상호 연결을 조건으로 보통 비용을 안냄Internet eXchange Point(IXPs)  여러 ISP들이 서로 Peering 할 수 있도록 중간 지점 역할을 하는 스위치 장치Content-Provider-Networks  구글 등 거대한 회사의 데이터센터를 ISP를 통하지 않고 직접 저티어 ISP와 연결해 통신하여 비용을 아끼고 성능은 높인다.1.4 패킷 교환 네트워크의 지연, 손실과 처리율(Delay, Loss, and Throughput in Packet-Switched Networks)1.4.1. 패킷 교환 네트워크 지연(Delay)처리 지연(Processing Delay)에러 검사, 헤더참조, 포워딩 등에 걸리는 시간수 마이크로초 이하큐 지연(Queuing Delay)버퍼에 들어간 패킷이 전송을 기다리는 시간보통 수 마이크로초 ~ 수 밀리초전송 지연(Transmission Delay)패킷이 라우터에서 연결로 적재되는데 걸리는 시간title: 라우터를 톨게이트, 패킷을 자동차 행렬로 비유하면 모든 자동차가 톨게이트를 완전히 빠져나가는데 걸리는 시간보통 수 마이크로초 ~ 수 밀리초전파 지연(Propagation Delay)패킷이 연결을 통해 다음 라우터로 도착할때 걸리는 물리적 전달 시간title: 자동차 행렬이 다음 톨게이트로 도착하는데 걸리는 시간연결 매개 종류에 따라 다르며 보통 수 밀리초이다.기타 지연매질, 응용 프로그램에 의해 추가적인 지연이 존재할 수 있다.WiFi, 동축 케이블은 다른 장치와 대역폭이 공유되므로 지연될 수 있다.VOIP 응용 프로그램은 음성 데이터를 패킷으로 인코딩하는 시간만큼 지연된다.1.4.2 큐 지연과 패킷 손실(queuing Delay and Pacekt Loss)큐 지연은 다른 지연과 달리 도착 순서나 시간 간격 등에 따라 다르게 측정된다.  100개의 패킷 중 가장 첫번째 패킷은 큐지연이 없고, 마지막 패킷은 큐지연이 클 것이다.  100개의 패킷이 충분한 시간 간격을 두고 천천히 도착하면 모두 없을 수 있다.패킷 하나의 크기를 $L$, 패킷의 도착 빈도를 $a$, 전송 처리율을 $R$일 때,  $La/R$이 1 초과일 경우 점점 큐 지연이 무한대로 수렴  $La/R$이 1 이하일 경우, 패킷의 도착 간격 등에 따라 큐 지연이 달라짐다만, 실제로는 큐 지연이 무한대로 커지지 않고, 버퍼가 넘쳐나 패킷 손실이 점점 증가한다.1.4.3 단말간 지연(End-to-End Delay)출발 호스트에서 도착 호스타까지 동일한 성능의 라우터 N-1개를 지나치는 N개의 혼잡하지 않은 링크의 단말간 지연은 다음과 같다.\\(d_{end-end}=N(d_{proc}+d_{trans}+d_{prop})\\)title: traceroutetraceroute는 현재 장치에서 입력된 목적 호스트까지의 패킷의 경로와 왕복 지연 등을 측정해주는 프로그램이다.목적 호스트로 패킷을 라우터 갯수만큼의 패킷을 보낸 후 각 라우터마다 하나씩 패킷을 되돌아오게 하여 측정한다.1.4.4 처리율(Throughput in Computer Network)시간 당 비트 데이터가 처리되는 양  순간 처리율: 일정 시간 당 비트데이터가 처리된 양, 예시: 다운로드 속도          두 장치 간의 경로 중 가장 성능이 낮은 구간에 구애받게 됨( 병목(bottleneck) 연결 지점)      좋지 않은 장치, 너무 많은 장치가 공유하는 지점 등이 해당        평균 처리율 : $F/\\min{R_1,\\dots,R_N}$1.5 프로토콜 계층과 서비스 모델(Protocol Layers and Their Service Models)네트워크는 마치 해외여행 시 공항의 수속 처럼 여러 구조에서 처리한 뒤 다음으로 넘기는 계층 구조를 가지고 있다.서비스 모델  각 계층에서 다음 계층에 제공할 수 있는 행동을 서비스라고 하며, 이러한 방식을 서비스 모델이라고 한다.  예를 들어, n 계층에서 자신보다 낮은 계층이 암호화 전달 기능을 제공한다면, 자신은 그 서비스를 이용해 암호화를 보장받을 수 있다.1.5.1 계층 구조(Layered Architecture)각 계층의 프로토콜은 소프트웨어, 혹은 하드웨어를 통해 구현되어 제공되며, 서비스를 다대다 구조로 제공하기도 한다.  응용 계층의 HTTP는 소프트웨어, 물리 계층의 통신은 네트워크 카드 형식의 하드웨어로 구현  1개의 랜카드로 다수의 웹 어플리케이션이 서비스를 제공 받을 수 있음응용 계층(Application Layer)웹 어플리케이션과 end system 간의 통신을 담당하는 프로토콜들이 존재하는 계층title: 응용 계층 프로토콜 예시HTTP(웹 문서), SMTP(e-mail), FTP(파일 전송), Domain Name System(DNS)패킷을 message라고 부름전달 계층(Transport Layer)어플리케이션 layer의 message들을 application endpoint를 통해서 전송하는 계층TCP와 UDP가 존재패킷을 segment라고 부름네트워크 계층(Network Layer)IP 프로토콜의 주소를 통해 다른 곳에 위치한 호스트의 Network Layer 간의 datagram을 전송하는 계층포워드 테이블을 생성하는 라우팅 프로토콜 등이 존재패킷을 datagram이라고 부름연결 계층(Link Layer)한 노드(지점, host, router)에서 다른 노드간의 datagram 통신을 담당하는 계층노드 간의 신뢰성 통신 보장, 여러 프로토콜들 간의 호환성을 제공하는 프로토콜들이 존재title: 연결 계층 프로토콜들WiFi, Ethernet, DOCSIS(cable access network용), PPP패킷을 frame이라고 부름물리 계층(Physical Layer)각 1개의 bit를 노드 간에 통신하는 것을 담당하는 계층매질(medium)에 따라 구별되는 프로토콜이 존재여기서는 패킷이 아닌 비트(bit)를 이용한다.1.5.2 캡슐화(Encapsulation)모든 장치에서 5개 계층 전부 구현하진 않는다.캡슐화  계층 간에 패킷을 주고 받으면서 이전 계층의 패킷을 페이로드 필드에 집어넣고 새로운 헤더를 붙여 현재의 새로운 패킷을 빠구는 과정수신 받은 계층은 반대로 페이로드에서 자신의 프로토콜에 맞도록  꺼내어 사용한다.이 과정 중에 용량상, 프로토콜 상 이유로 패킷이 여러개로 나뉘거나 합쳐지기도 한다.마치, 사장에게 보낼 것을 지시받은 메모를 비서에게 보내고, 해당 비서는 해당 메모를 봉투에 넣어, 우편함에 넣는 등의 과정1.6 네트워크 공격(Networks Under Attack)기본적인 네트워크는 초창기에 신뢰할 수 있는 사용자 간의 통신을 전제로 하였기 때문에 보안상 취약하다.멀웨어(malware) 공격네트워크르 통해 멀웨어를 받도록 함.피해개인정보 탈취, 시스템 무력화, DDOS 공격, botnet화 등이 가능하며, 자가 복제를 통해 다른 호스트에게 전염되게함.DoS 공격(denial-of-service attacks)적법한 사용자가 시스템을 사용하기 힘들게 다음과 같은 종류의 공격을 함  취약점 공격 (vulnerability attack)          위장된 메시지를 보내 중단, 충돌, 무력화        대역 범람 (bandwidth flooding)          수 많은 패킷을 목표 호스트에 보내 자원 낭비        연결 범람 (Connection flooding)          쓸모 없는 가짜 TCP 연결들을 보내 정상적인 연결 방해      단일 호스트가 보내는 공격은 해당 호스트를 차단하면 되지만, 멀웨어에 감염된 수많은 PC를 botnet화 하여 DOS 공격하면 특정 host 차단이 힘들고, 공격이 쉽고 효율적이다.(DDOS, Distrbuted denial of service)패킷 스니핑(packet sniffing)네트워크 장치(스위치, 라우터, 리시버 등)에 패킷 스니퍼를 심어 중요 정보를 탈취하고 정보를 뺏어갈 수 있다.탐지가 어렵고, 손쉬운 공격이며, 이를 막기 위해 암호화를 이용한다.패킷 위장패킷의 헤더 정보를 조작하여 특정 호스트가 보낸 패킷으로 위장할 수 있다.가짜 패킷에 가짜 출발 아이피 주소를 이용한 공격을 IP 스푸핑(IP spoofing)이라고 한다.이를 막기위해 엔드 포인트 인증(end-point authentication)을 이용할 수 있다.title: 인터넷의 역사 부분은 생략했음정리하지 않은 정리를 보고 싶다면 이곳을 참조"
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%202-%EC%9D%91%EC%9A%A9%20%EA%B3%84%EC%B8%B5.html": {
    title: "네트워크 정리-Chap 2-응용 계층",
    date: " Aug 21, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%202-%EC%9D%91%EC%9A%A9%20%EA%B3%84%EC%B8%B5.html",
    tags: ["CS","NETWORK","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/eChapter 2. Application Layer페이스북, 웹 메일, 인터넷 쇼핑, MMORPG, 뉴스 웹사이트 등 수많은 인터넷 어플리케이션의 덕분에 많은 사람이 필요성을 느끼고 인터넷을 사용하면서 커다란 규모를 이루게 되었다.이번 챕터에서는 network application layer의 개념과 구현 측면으로 바라보며, Web, e-mail, DNS, P2P file distribution, video streaming을 살펴보고 TCP와 UDP, Socket interface에 대해서 알아본다.2.1 Principles of Network Applications각 Application은 다른 end system의 application과 소통하여 서비스를 제공한다.이때, Application layer가 구현되어 있지 않은 end system이나, 다른 host의 하위 Layer의 프로그램들과는 소통할 수 없다.2.1.1. Network Application ArchitecturesApplication 계층의 구조는 Network 전체의 구조인 기존의 5계층과는 다르며, 비교적 좀더 유연한 선택이 가능하며, 정형화되지 않은 서비스를 제공한다.예를 들어, Application의 End system 간의 통신 구조는 크게 2가지로 나뉜다.  Client-server architecture : client의 요청에 응답해주는 언제나 켜져 있으며, 고정된 주소값을 가진 server와 사용자로부터 요청을 받아 처리하고 서버에서 결과를 받아 다시 사용자에게 반환해주는 client로 나뉜다.          절대로 client 간은 통신하지 않으며, FTP, Web, Telnet, e-mail 등이 존재한다.      하나의 서버로는 수많은 요청을 처리할 수 없으므로, 보통 Data center에 수많은 서버를 생성해서 서비스하며, 이때 연결과 bandwidth 유지를 위한 막대한 유지비가 든다.        P2P(peer-to-peer) architecture : dedicated server를 거의 또는 전혀 사용하지 않고, 사용자의 host(컴퓨터, 스마트폰 등) 또는 Peer라고 불리우는 application 간의 직접 연결을 통해 서비스를 제공한다.          self-scalability(자기확장성) : 사용자가 늘어나면 비용이 늘어나는 Client-Server 구조와 달리, 사용자가 늘음은, 동시에 서버 연결과 자원을 제공해 줄 Peer가 늘어남을 의미하므로, 이용 가능 대역폭과 연결 수의 한계가 늘어나게 된다.      단, 비집중화된 구조로 인한, 보안, 성능, 신뢰성 저하 문제는 풀어야할 과제이다.      2.1.2. Processes Communicating각 호스트에서 어떻게 여러 프로그램을 돌릴 수 있는지는 OS의 시점에서 보면 알 수 있다.program, OS에서는 Process라고 부르는 작업의 단위에 작업을 할당하며 이루어지며, 각 Process간의 통신은, OS가 지원하는 Process 통신(interprocess communication) 을 통해 가능하다.하지만 우리가 궁금한 것은, 네트워크를 통해 연결된 다른 End system 간에 속해있는 Process간 통신이며, 이는 네트워크를 통한 message 교환을 통해 통신이 이루어지게 된다.Client and Server ProcessesClient와 Server는 어떻게 구분될까?단순히 packet을 송신하느냐 수신하느냐로 구분할 경우, 만약 사용자가 홈페이지에 파일을 업로드하면 서버가 되게 된다.Client와 Server의 정의는,의사소통을 위해 세션을 요청하면 Client,기다리다가 요청이 들어오면 세션을 시작하면 Server이다.예를 들어, 서로 간에 거의 구분이 없는 P2P 구조에서는, Peer B에게 세션을 요청한 Peer A가 Client, Peer A를 위해 세션을 열어준 Peer B가 Server인 셈이다.The Interface Between the Process and the Computer Network위에 설명한 네트워크를 거리에 둔 다른 End system 간의 Process가 상호작용하기 위해서는,  Socket을 이용해야한다.Socket은 Network와 Application의 상호작용을 위해 제공하는 일종의 API(Application Programming Interface)로, Transport Layer와 Application Layer 사이에 걸쳐있으며, Application Layer 측의 socket의 모든 기능과 Protocol을 통해서 Transport Layer 측의 socket의 일부 기능(사용할 transport protocol, 최대 buffet 사이즈, sement size 등)을 제공받아 Application을 만들 수 있다.Addressing Processes소포가 목적지에 도착하기 위해 주소지와 수령자 이름이 필요한 것처럼, message가 도착 process에 도착하기 위해서는  host의 주소  host 내부의 process 중에서 process의 식별 번호가 필요하다.1은 IP 주소를 통해 이루어지며, 2는 Port 번호를 통해서 주어지게 된다.IP 주소는 32 bit 번호로 이루어져 있으며, port number는 2^16개의 번호가 할당된다.web server의 80번가, SMTP 매일 서버의 25번처럼 유명한 process 들은 미리 포트번호가 정해져있기도 하다.2.1.3. Transport Services Available to Applicationssocket은 앞서 말하듯이 transport-layer protocol을 제공하며, application 구현 시, 적절한 protocol을 선택하여 구현해야한다.예를 들어, 소포를 비행기로 보낼지, 기차로 보낼지 장단점을 고려해 선정하는 것과 같다.이러한 Protocol의 차이는 대략 4가지로 구분할 수 있다.Reliable Data Transfer(데이터 전송 보장)데이터가 해당 Process에 손실 없이 도착하는 것을 보장하는 특징을 Reliable Data Transfer(데이터 전송 보장)이라고 한다.주로 금융 정보나, 파일 전송 처럼 손실이 존재해서는 안되는 응용 프로그램에에 필요한 특성이며,반대로 영상, 음성 전송 처럼 어느 정도 데이터 손실이 존재해도 상관 없어 Reliable Data Transfer Protocol이 필요없는 프로그램을 loss-tolerant applications라고 한다.Throughput(전송 속도)데이터의 최소 Throughput(전송 속도)가 보장되는 특성을 의미한다.최근의 영상, 음성, 전화 멀티미디어 응용프로그램이 해당되며, 이러한 특성이 필요한 프로그램을 bandwidth-sensitive application이라고 한다.몇몇 프로그램은 최소 전송속도를 맞추지 못하면 잠시 서비스를 포기(32kbps &gt; 인터넷 전화 연결불가)하거나, 품질을 낮추는 방식(영상 화질 옵션, 더욱 낮은 용량의 encoding rate 사용)을 이용한다.또한, 네트워크 상황에 따라 필요한 Throughput(전송 속도)을 가변적으로 변경 가능한 프로그램을 elastic applications이라고 하며, 파일전송, 웹 문서 전송, 메일 전송 프로그램 등이 포함된다.Timing(전송 도착 시간)전송 시작 시간과 도착 시간의 차이를 보장이 필요한 경우도 있다.예를 들어, 화상 회의나 온라인 게임 중의 지연은 부자연적인 서비스를 초래할 수 있다. 이러한 application을 real-time application이라고 한다.파일 전송처럼, 전송시간 delay가 늦어어도 되는 경우는 non-real-time application이라고 한다.Security(보안성)정보의 기밀 성을 유지해야 하는 경우도 있다.주로, data encryption과 data integrity, end-point authentication을 통하여 packet이 감시, 탈취되는 상황에도 기밀성을 유지할 수 있다.2.1.4. Transport Services Provided by the Internet사람들이 많이 사용하는 TCP/IP 환경에서 보통 TCP와 UDP, 2가지의 Transport layer Protocol을 선택할 수 있다.위의 표와 같이 자신의 어플리케이션 조건을 고려하여 선택한다.TCP ServicesTCP는 다음 두가지 서비스를 제공한다.  Connection-oriented service :          clinet와 server간의 통신이 일어나기 전에, 소켓간의 handshaking 절차(일종의 통신 가능 확인)를 거친 뒤, TCP connection이 형성되게 한다.      이렇게 형성된 connection을 통해 쌍방향 통신이 가능하게 하고, packet들의 쇄도를 방지할 수 있게 한다.        Reliable data transfer service:          통신 간의 전송 동안의 packet의 변질 방지와 손실 방지, 중복 방지를 보장한다.      추가적으로, 양 application 간의 이득은 없지만, 전체적인 network 상황 개선을 위해 congestion-control(혼잡 제어)가 이루어져, sender와 receiver가 최대로 전송, 수신할 수 있는 패킷의 양을 제어할 수 있다.      UDP Services추가적인 서비스를 최소한으로만 제공하는 경량 Protocol이다.혼잡제어, 데이터 전송 보장 등이 존재하지 않아, 제약을 줄이고, 커스터마이징 하거나, 성능을 늘릴 수 있지만,전송이 보장되지 않거나, packet 순서등이 변경될 수 있다.추가적으로, 암호화, data integrity, end-point authentication 등을 제공하여 보안성을 늘린 TCP-enhanced-with-TLS(Transport Layer Securty)이 존재하며, TCP의 업그레이드 버전이라고 보면 되며, UDP를 기반으로 만들어졌다.새로운 소켓 API와 Library를 통해 구현한다.Services Not Provided by Internet Transport Protocols우리가 앞서 말했던 보안성, 데이터 전송 보장을 보장할 수 있지만, 다른 특성 2개, 최소 전송 속도와 도착시간은 보장하지 못한다.현대까지 개발된 Prtocol 중에는 저러한 특성을 보장하는 Protocol은 없으며, 네트워크 환경의 개선에 의한 빠른 전송 속도와 도착시간을 보장없이 사용되고 있다.위 fig2.5 경우, 보통 사용되는 protocol들이 적혀있다.이렇나 보장이 필요한 어플리케이션(인터넷 전화, 화상회의, 게임 등)들은 packet의 overhead를 줄이고, congestion control에서 자유롭게 되기 위해, UDP를 이용하는 경우가 많으나, 일부 방화벽 등에 의해 UDP가 차단되는 경우에 대비하여 TCP로도 통신 가능하도록 하도록 옵션을 제공하는 어플리케이션이 많다.2.1.5 Application-Layer ProtocolsApplication Layer Protocol에서는 다음과 같은 것을 결정한다.      메시지 타입 (ex) request, response message)        message packet의 field들, field들의 역할        위의 field에 들어갈 값들        message를 주고 받을 시간과, 방법에 대한 규칙  HTTP 같이 RFC에 공개 되어있는 Application-Layer Protocol도 존재하지만 Skype의 상용 프로토콜 처럼 공개되지 않은 경우도 있다.또한, 많은 Application들은 단순히 Application-layer protocol 뿐만 아니라 여러 layer의 protocol로 이루어져 있다.Netflix의 경우, 페이지를 보여주기 위한 HTTP 부터, 스트리밍을 위한 DASH 까지 여러 Layer, 여러 Protocol로 이루어져있다.2.1.6 Network Applications Covered in This Book수많은 Protocol이 존재하지만 그 중 에서 HTTP, FTP(E-mail), DNS, P2P, CDN에 대해서 알아보자.2.2 The Web and HTTP과거 연구용, 학습용, 군용으로 제한적으로 나마 쓰이던 인터넷 기술은 WWW(world wide web), 웹 이라는 어플리케이션의 등장과 함께 대중으로 퍼져나갔다.특히, 수동적인 자세로 정보를 접할 수 있던 라디오, TV와 달리, 원하는 시간에 원하는 정보를 쉽게 접근하고, 생산해 배포할 수 있게 해주는 웹은 오늘 날에 점점 수요가 늘고 있다.2.2.1 Overview of HTTPHTTP(HyperText Transfer Protocol)은 client, server side 양쪽에 구현되는 웹의 application-layer-protocol이다.web page(또는 document)는 object로 불리우는, 일종의 파일(HTML, CSS, JPEG 이미지, Javascript file, video 등)들로 이루어져있으며,대부분의 웹사이트는 URL로 주소를 표시하며, URL은  base HTML URL (hostname)과 다른 object들의 URL(path name)로 이루어져 있다.\\[\\underset{base URL,\\ hostname}{\\underbrace{http://www.someSchool.edu}}\\underset{path\\ name,\\ reference\\ url}{\\underbrace{/someDepartment/picture.gif}}\\]Web application은 보통 client side는 Web browser(chrome이나 인터넷 익스플로러)가 맡으며, Sever side는 Web server(Apache, Spring)들이 맡는다.HTTP는 메시지의 구조와 메시지 교환방식을 정의하며, TCP 연결을 기반으로 진행된다.대략적인 방식은 사용자가 웹페이지를 요청하면, 브라우저가 socket을 통해 TCP 연결로  HTTP 요청 메시지를 보내고, 서버는 socket을 통해 TCP 연결로 부터 요청(request) 메시지를 받고 object 파일이 포함되어있는 HTTP 응답(response) 메시지를 다시 socket을 통한 TCP 연결 반대편의 브라우저에게 보내주는 방식이다.또한, HTTP는 TCP의 특징인 전송 보장으로, 확실히 메시지가 전달되며, 추가적으로 client의 상태를 저장하지 않는 stateless protocol의 성격을 띈다.예를 들어, client가 같은 요청을 2번 보내도, 서버는 첫번째 요청을 기억하지 못하므로, 이미 해당 정보를 Client가 받아갔다고 해도, 같은 정보를 한번 더 전송하게 된다.처음 개발된 HTTP/1.0, 2020년대의 주류인 HTTP/1.1, 최근 지원중인 HTTP/2가 존재한다.extra : HTTP Authentication_http://frontier.userland.com/stories/storyReader$2159_에서 발췌HTTP protocol은 서버측의 접근 인가를 위한 간단한 framework를 가지고 있다.만약, 특정한 credential 정보를 제공하지 않으면 401 Unauthorized response를 돌려주도록 할 수 있다.크게 2가지 scheme이 있는데,  Basic Access Authentication유저명과 암호를 입력하여 접속하는 방법, 첫 접속 시도시 돌려받는 401 Unauthorized response message에 다음과 같은 header가 추가되어 있다.    해당 페이지는 \"Basic\" 방법으로 접근 제어가 되어있고, 보호되고 있는 realm(일종의 보안 지정 지역?)의 key:value 값이 존재한다.    이후, 브라우저는 유저명과 암호명을 물어보게 되고, 입력된 정보를 이용해 bas64로 인코딩한 뒤 다음과 같은 헤더에 포함하여 request message를 다시 보내게 된다.서버는 base64 decoding을 거친 뒤, 유저병과 암호를 비교하고, 맞을 경우, 요청한 페이지를 response 해준다.이 방법은 간단하지만 패킷 도청에 유출된다는 단점이 있다.이를 막기 위해 아래 MD5 암호화를 통해 유저명과 암호를 숨길 수 있지만, 그저 도청자가 해쉬된 암호를 해독하지 않고, 그냥 해쉬 결과값 통째로 보내는 방법으로 뚫을 수 있다.(replay attack)  Digest Access Authentication위의 보안 취약점을 막기 위한 방법으로, 두번째 방법은 추가적으로 nonce를 이용한다.nonce는 각 401 response 마다, WWW-Authenticate header에 추가되어 있는, 이전에 사용된 적 없는 유일한 값으로, 사용자는 다음과 같은 식으로 nonce를 이용해 새로 request를 보내게 된다.A1 = string.hashMD5 (username + &amp;#34;:&amp;#34; + realm + &amp;#34;:&amp;#34; + password)A2 = string.hashMD5 (paramTable.method + &amp;#34;:&amp;#34; + paramTable.uri)requestdigest = string.hashMD5 (A1 + &amp;#34;:&amp;#34; + nonce + &amp;#34;:&amp;#34; + A2)nonce 값은 이번 response에서만 유효했던 값이므로, 감청자가 위의 패킷을 replay attack으로 똑같이 보낸다고 하더라도, nonce 값이 중복되어 버려지게 된다.이외에도 digest 방법은 3자의 메시지 body 조작을 막는 방법이 마련 되어 있다.(RFC 2617)HTTP 보안은 아쉽게도, digest 방법을 사용해도 비밀번호를 제외한 다른 정보들은 여전히 감청 가능한데다, 공격자의 서버 사칭을 막을 수 없고, 일부 브라우저들은 제대로 지원하지 않는 경우도 있다.2.2.2 Non-Persistent and Persistent Connectionsclient는 server와 통신할 때, 어플리케이션의 성격에 따라 요청 메시지를 한꺼번에 여러개, 주기적으로 하나씩, 또는 간혈적으로 필요시마다 메시지를 보내게 된다.이때 중요하게 고려해야 할 점으로, TCP 연결을 요청때마다 서로 열어야하는가? 혹은 TCP 연결을 계속 열어놓은 채로 통신해야하는 가이다.전자는 non-persistent connections, 후자는 persistent connections라고 부른다.보통은 후자 persistent connection이 기본 값이지만, 장단점을 알고 비교해보자.HTTP with Non-persistent Connections비지속성 연결(Non-persistent Connections)에서는 웹사이트가 1개의 base html file과 10개의 image로 이루어져있다면, 총 11번의 TCP 연결을 개폐하게 된다.HTTP/1.0의 경우 비지속성 연결을 기본으로 하며, 브라우저 설정에 따라 최대 message 갯수 만큼 통신 후 닫게 끔 설정하여 속도를 늘릴 수도 있다.packet이 client side에서 출발해서 server side에 도착한 뒤 돌아오는데 걸리는 시간을 RTT(round-trip time)이라고 하면,연결을 시작할 때, connection을 만들기 위해 3way handshake를 통하여 연결을 수립하고, 목적인 file의 경우 이후, response message로 보내지게 된다.이 과정을 통해 총 걸리는 시간은 2RTT + file transmission time 만큼의 시간이 걸리게 된다.비 지속성 연결(Non-persistent Connections)의 경우 2가지의 단점을 가지고 있다.먼저 매 오브젝트마다 연결이 생성되므로, TCP buffer와 TCP variable이 Client와 Server 양측에 저장되어 기억되어야 한다는 점이다.이는, 다음에 보내야될 파일이 뭔지 기억하기 위함이며, (HTTP는 stateless 이므로 기억하지못하므로), 이는 양측, 특히 서버에 큰 부담이 될 것이다.두번째 단점은 매 message 마다 2RTT 만큼의 handshaking 과정의 딜레이가 추가로 생긴다는 점이다.HTTP with Persistent Connections하지만 HTTP/1.1 부터 지원하는 지속성 연결(Persistent Connections)을 사용하면, 위에 예시로 든 11개의 object message 전부를 연속된 resonse message를 순차적으로 보내는 pipelining 방법을 통해 한번의 연결만에 받을 수 있을 뿐만 아니라, 설정한 timeout interval 내에 새로운 request를 보내면 다음 오브젝트들 또한 한번에 받을 수 있어 비교적 성능이 더 좋아질 수 있다.2.2.3. HTTP Message FormatHTTP message의 포맷은 request, response 두개이며, 이는 RFC 1945, RFC 7230, RFC 7540에 정의되어있다.HTTP Request MessageGET /somedir/page.html HTTP/1.1\\c\\nHost: www.someschool.edu\\c\\nConnection: close\\c\\nUser-agent: Mozilla/5.0\\c\\nAccept-language: fr\\c\\nASCII 문자로 구성되어 있어, 사람이 읽을 수 있으며, 케리지 리턴과 줄바꿈(lf)으로 구분된 여러 줄로 이루어져있다, request message가 적혀있는 절대 필수의 request line인 가장 첫번째 줄을 제외하고는 나머지 부가적인 정보로 선택적으로 더욱 정보를 추가할 수 있으며, 이를 header lines라고한다.request line은  method field  URL field  HTTP version field 총 3부분으로 나뉘어져있다.method field는 GET, POST, HEAD, PUT, DELETE로 이루어져 있다.GET이 가장 많이 사용되며, 보통 browser가 object를 request할때 사용한다.URL field는 보통 작업이 이루어지는 object의 url을 의미한다.version field는 말그래도 HTTP의 버전을 의미한다.header line은      Host : 헤더의 경우는 object를 포함하는 host를 적는데, 이 부분은 나중에 web proxy cache를 위해 필수이다.        Connection: close header는 포함할 경우, 이 request에 response한 뒤, TCP 연결을 종료하게 해준다.        User-agent: header는 브라우저 타입과 사용 유저에 대한 정보를 적는다. 서버는 이를 보고, 각 버전이나 브라우저에 따라 호환되는 다른 object를 보내줄 수 있다.        Accept-language: header는 유저의 사용 희망언어를 구별하여, 서비스를 제공한다.  이 외에도 수많은 헤더를 추가할 수 있다.request Message format 좀더 자세히 살펴보면 아래 그림과 같다.Entity body의 경우, 현재 GET method 에서는 비어있지만, POST method에서는 유저가 form에 작성한 정보가 들어간다(로그인 아이디, 암호화된 비밀번호 등…)하지만 보안이 중요하지 않은(들켜도 되는 정보, 예를 들어 검색 단어, 조회하고 싶은 글의 번호) 경우에는 GET 요청에, URL 뒤에 변수를 삽입하여 요청 메시지를 보내기도 한다. (ex)www.somsite.com/animalsearch?monkeys&amp;maxsearch?3)      HEAD method의 경우 GET과 비슷하나, 요청한 object를 반환하지 않는다. 보통 개발자가 디버깅을 위해 사용한다.        PUT method의 경우, 보통 object를 서버에 upload할 때 사용한다.        DELETE method의 경우, 웹 서버에서 object를 지우는데 사용한다.(자신의 글, 회원 탈퇴 등)  HTTP Response MessageHTTP/1.1 200 OKConnection: closeDate: Tue, 18 Aug 2015 15:44:04 GMTServer: Apache/2.2.3 (CentOS)Last-Modified: Tue, 18 Aug 2015 15:11:03 GMTContent-Length: 6821Content-Type: text/html(entitiy body data field~)위에서 본 request message의 response message 예시이다.역시나 세가지 부분으로 나뉜다.  status line  header line 여섯줄  entitiy body : request된 object data가 들어가는 자리status line 또한 세 부분으로 나뉘며, 각각, Protocol version, status code, status message가 공백 문자로 나뉜다.header line을 다시 살펴보자면,      Connection: close는 해당 메시지를 전달 받은 뒤, TCP 연결이 끊겼음을 의미하며,        Date: 헤더는 server에서 해당 message를 보낸 시각이며,        Server: 헤더는 서버의 종류를 알려주고, request message의 User Agent 헤더와 비슷한 역할을 한다.        Last-Modified 헤더는 Object의 최신 생성시간이나 수정 시간을 알려주며, 이는 local client나 network cache server(proxy server) caching에 사용된다.        Content-Length: 헤더는 전체 사용된 bytes의 길이를 의미한다.        Content-Type: 헤더는 entitiy body에 담긴 object의 type을 알려준다.  status code의 경우 예시를 들면 다음이 있다.      200 OK : 요청에 대한 정상 응답, 문제 없음        301 Moved Permantly: 해당 요청을 위한 URL이 예전에 변경됬을 시, 응답되는 코드, 새로 변경된 URL 정보가 있는 Location : Header 필드가 포함되어 있으며, client는 보통 자동으로 해당 URL로 재요청을 하게 됨.        400 Bad Request: server가 이해할 수 없는 요청에 대한 응답, 보통 request message 자체에 오류가 있을 때 발생        404 Not Found: 해당 하는 object나 document가 server에 존재하지 않음.        505 HTTP Version Not Supported : 해당 하는 HTTP protocol 버전을 서버가 지원하지 않을 경우 발생.  telnet 명령어를 통해 쉽게 message를 살펴볼 수 있지만, 자세한 것은 과제와 wireshark에서 참조2.2.4.User-Server Interaction:CookiesHTTP의 Stateless 특성은 간단한 서버 디자인과 높은 성능을 가져왔지만, 때때로, 유저 특화 서비스나 보안을 위해 request message를 구분해야할 필요가 있다.이는 HTTP의 Cookie를 이용해 해낼 수 있으며, 대다수의 상용 웹사이트에서 사용하고 있다.쿠키의 설정은 다음 4개의 상호작용으로 이루어진다.  HTTP response message의 cookie header          서버가 처음 들어온 손님을 위한 유니크 번호가 적혀있는 쿠키를 헤더(Set-cookie)에 포함해 전달한다.      DB에 유저의 정보를 입력한다.        HTTP request message의 cookie header          유저가 사이트 내에서 한 행동들이 cookie 헤더가 있는 request에 의해 보내지고 DB에 기록된다.        사용자의 end system에 존재하고, browser에서 관리하는 cookie 파일          시간이 지난 후, 다시 유저가 사이트에 방문할 때, 브라우저에서 cookie 파일을 가져와 request message에 포함한다.        backend server database이러한 cookie는 유저 맞춤형 서비스(장바구니, 추천 시스템)나 편의성 개선(자동 로그인) 등이 가능해지지만, 대신 유저 privacy 침해나 보안 관련 이슈가 있을 수 있다.2.2.5.Web CachingWeb cache 또는 proxy server는 web server 이전 단계에서 HTTP request 들을 처리하는 network entity이다.고유의 저장공간과 response object의 복제본을 가지고 있으며,  이를 통해 request들을 웹 서버 대신 처리하게 된다.웹 브라우저의 설정으로 생성할 수 있으며, 보통은 ISP 측에서 구매, 설치한 뒤, browser에 단체로 설정해 놓는다.마치, client이자, web 서버처럼 진행되게 되는데, 절차를 살펴보면,      브라우저 &lt;-&gt; Web cache 간의 TCP Connection 생성        브라우저가 Web cache로 Request 요청    3-1.만약 요청한 object의 copy가 존재한다면, browser에게 response 전송    3-2. 만약 요청한 object의 copy가 없다면, origin server와 TCP connection 생성       - 이후 web cache가 origin server에 request 요청하여 object를 response로 받아옴.       - 이후, web cache는 client에게 response로 object를 보내주고, 이를 local storage에 저장.  이를 통해 응답 시간을 크게 줄일 수 있고, server와 client 사이의 대역폭 병목현상을 크게 줄일 수 있다. 뿐만 아니라 소속 ISP와 인터넷 전체의 Traffic에도 긍정적인 영향을 주어 ISP 내의 전체적인 Application 들의 성능 향상으로 이어진다.예를 들어 아래 같은 경우 delay 시간이 길어지면, 15Mbps access link 를 고비용 고성능 유선으로 업그레이드 vs cache 서버 설치로 저비용 완화를 할 수 있다.특히 최근에는 Content Ditribution Networks(CDNs)가 발달되면서 cache가 더욱 중요해졌으며, Dedicated CDN과 Shared CDN으로 나뉘어진다.The Conditional GETweb cache의 문제점 중 하나는, 저장한 object가 outdated(stale)됬을 수도 있다는 점이다.즉, cache된 뒤로, 원본 데이터가 바뀌면, 값이 업데이트 되야한다.이때 Conditional GET을 이용하면 object가 업데이트 됬는지 확인할 수 있다.먼저 web cache가 서버로 부터 object를 받고 저장할 때, response message에서 Last-Modified: 헤더의 최신 업데이트일을 함께 저장한다.그 뒤, client에서 web cache로 해당 object의 요청이 있을 때, cache는 다음과 같은 Conditional Get request message를 보낸다.GET /fruit/kiwi.gif HTTP/1.1Host: www.exotiquecuisine.comIf-modified-since: Wed, 9 Sep 2015 09:23:24위 처럼, If-modified-since: 헤더가 있는 Get 요청이, condional Get이며, 해당 헤더에 존재하는 오브젝트의 last-modified 값을 가져와 넣는다.만약 서버에서 해당 If-modified-since 날 이후로 변화가 있다면, 평범한 response message의 entitiy body 부분에 업데이트된 object를 돌려줘 갱신하게 한다.만약 변화가 없다면, 다음과 같은 304 Not Modified 돌려주며, entitiy body에는 아무것도 넣지 않는다.HTTP/1.1 304 Not ModifiedDate: Sat, 10 Oct 2015 15:39:29Server: Apache/1.3.0 (Unix)(empty entity body)위 메시지가 돌아왔다면, 여전히 최신 데이터이므로, 그냥 caching된 object를 사용하면 된다.2.2.6. HTTP/22015년에 발표되어 현재 전체 40% 웹사이트들이 지원하고 있는 protocol로, 하나의 TCP 연결로, 여러 request, response를 받기(multiplexing, 다중화: 여러 소켓으로 여러 request, response 받는 것), request 우선순위, HTTP header 압축, server push 등을 이용해 latency를 줄이는 것이다.하나의 TCP 연결로 하나의 웹페이지를 받는다는 것은, 서버측의 socket의 점유 수를 페이지당 하나로 줄이고, 이로 인해 network bandwidth를 공평하게 받을 수 있음을 의미한다.  즉 기존에는 한 웹페이지가 여러 소켓(=연결)을 사용했었다. (아래 4번째 문단 참조)하지만 HTTP/2는 HOL(Head of Line) blocking 문제를 해결하기 위해 아래의 Framing을 고안했다.HOL blocking 문제의 예시를 들자면, 하나의 커다란 용량을 가진 비디오가 웹페이지 상단에 먼저 로딩되고, 그 외에 작은 용량의 댓글, 이미지 등이 다음에 로딩되야 하는 순간에, 용량이 큰 비디오를 먼저 로딩하느라 다른 작은 용량의 댓글 이미지 등이 먼저 로딩됬으면 빨리 컨텐츠를 보여줄 수 있었음에도, 어쩔 수 없이 비디오 로딩 이후에 로딩 되는 현상(user-perceived delay)을 의미한다.HOL Blocking은 단순히 HTTP 뿐만 아니라 다른 통신 protocol (ex) switch에서의 packet outlink )에도 사용된다. 총체적으로 이야기하자면, 내가 더 빨리 갈 수 있는 데, 느린 앞 사람 때문에 기다리는 현상이다.기존의 HTTP/1.1은 이 HOL 문제를 막기 위해 각 object마다 TCP 연결을 열어 해결할 수 있었지만, 이렇게 되자, sever 측의 socket을 순간적으로 많이 사용할뿐만아니라, 한 TCP 연결당 network bandwidth를 배정하기 때문에, 부당하게도, 컨텐츠가 많을 수록 대역폭을 많이 할당받는 문제가 발생됬다.HTTTP/2 Framing위의 HOL blocking을 막기위해 HTTP/2 에서는 각 response message들을 일정 용량 크기의 frame 여러개로 나누는 Framing을 사용한다.예를 들어 다른 것보다 100배 용량이 큰 영상은 frame 200개, 나머지 작은 object들은 frame 2개로 나눈뒤, 번갈아가며 frame을 삽입하여 보낸다.비디오의 frame 1개, 작은 object1 frame 1개, 작은 object2 frame 1개 이런식으로 세트를 만든 뒤 보내면, 비디오 frame 2번째를 보내는 시점에 다른 object들은 마지막 frame을 집어넣으므로, 전송이 완료된다.이는 HTTP/2의 가장 중요한 기능으로, protocol의 sublayer 하나를 추가하여 보낼 때의 message framing과 받을 때의 message reassembling을 진행한다.보통 헤더 field는 frame 1개, entity body는 용량에 따라 여러개로 만든 뒤, binary encoding을 진행하여 보낸다.binary encoding은 해석이 쉽고, frame을 좀 줄일 수 있으며, error에도 좀더 강하다.Response Message Prioritization and Server Pushing먼저 메시지 우선순위(Message Prioritization) 기능은 request의 우선순위를 조정해 성능 향상을 노릴 수 있다.우선 순위를 request 시 설정해 보내는 것으로, response에 담길 데이터의 순위를 지정할 수 있다.1~256까지 가능하며, 높을 수록 우선이며, 높은 우선순위의 request가 요청한 frame들이 먼저 담겨져온다.두번째 기능은 server pushing으로, 하나의 요청에 여러 응답 메시지를 보낼 수 있는 기능으로, 이것을 이용하면, request가 요청한 내용을 바탕으로 필요로 할 것 같은 다른 object들 또한 함께 보내는 것이 가능해, 더 빠른 로딩이 가능하다.마지막 기능은 huffman coding을 이용한 header 압축으로, request를 연속으로 보낼 때, 이전 메시지와 겹치는 헤더는 보내지 않아 용량을 압축할 수 있다.HTTP/3QUIC는 UDP protocol을 기반으로 새로 제정되는 중인 transport protocol이다.QUIC는 기존에 HTTP가 가지고 있던 message multiplexing, per-stream flow control, low-latency connection 등의 기능을 포함한다.HTTP/3는 이 QUIC를 기반으로 만들어지고 있으며, 덕분에 더욱 심플하고 능률적인 디자인이 가능하다.2.3 Electronic Mail in the InternetE-mail은 인터넷의 태동기 때부터 생겨나, 인기가 유지되고 있는 application이다.E-mail은 현실 편지와 같이 두 사용자가 같은 시간에 활동할 필요없는 비동기 매체이며, 현실 편지보다 더욱 빠르고, 보내기 편하며, 값싸며, 첨부, 내부 사진, Hyperlink, HTML 등의 기능을 제공한다.Mail server는 e-mail system의 핵심으로, 사용자 개인 mailbox와, mail message queue, SMTP protocol을 이용한 메시지 통신하는 역할을 맡는다.mail message queue는 수신받은 상대의 mail sever의 문제로 전송에 실패했을 때, 해당 메시지를 보관하고 특정 주기마다(보통 30분) 재전송을 시도한다. 여러번 재전송이 실패하면 queue에서 message를 삭제하고 송신하려던 사용자에게 알려준다.SMTP는 인터넷 전자 메일을 위한 기본적인 application layer 프로토콜로, TCP 기반의 데이터 신뢰 보장 서비스를 송신자와 수신자 mail server 간에 제공한다.client side인 송신자측 mail server와 server side인 수신자측 mail server로 나뉘며, SMTP를 사용하기만 한다면 뱡항에 따라 서로 바뀔 수 있다.2.3.1 SMTPRFC 5321로 제정된 SMTP는 HTTP보다 오래된 Protocol로 초창기 네트워크의 제한된 성능자원 때문에 message의 header 뿐만 아니라 body 부분 또한, 7-bit ASCII로 인코딩되어있어야 한다는 제한을 가지고 있다.하지만 현대의 e-mail의 경우 image, video 등의 multimedia를 포함할 수 있으므로, binary multimedia encoding -&gt; 7-bit ASCII -&gt; binary decoding로 변환하는 과정이 필요하다.fig 2.15의 시나리오를 설명해보자면  Alice가 Bob의 email 주소(bob@someschool.edu)로 메시지를 작성하고 user agent에게 메시지 전송을 지시함.  Alice의 user aget가 메시지를 Alice mail server로 보내고, message queue에 저장됨  SMTP의 client 측인, Alice의 메일서버가 queue 내부의 message를 확인하고, SMTP Server측인 Bob의 메일버서에 TCP connection을 요청함.  SMTP Handshaking 이후, SMTP client가 Alice의 메시지를 TCP connection을 통해 보냄.  Bob의 메일서버가 해당 메시지를 받은 뒤, Bob의 개인 mailbox에 집어넣음.  Bob이 user agent를 통해 해당 메시지를 원하는 시간에 읽음.SMTP는 양 측의 서버의 거리가 얼마나 멀든, 상대 서버에 문제가 있든 상관없이, 중간 경유 mail server를 사용하지 않으며, 언제나 송수신 대상간 직접적인 TCP connection을 사용한다.S: 220 hamburger.eduC: HELO crepes.frS: 250 Hello crepes.fr, pleased to meet you C: MAIL FROM: &amp;#38;#60;alice@crepes.fr&amp;#38;#62; S: 250 alice@crepes.fr ... Sender ok C: RCPT TO: &amp;#38;#60;bob@hamburger.edu&amp;#38;#62; S: 250 bob@hamburger.edu ... Recipient ok C: DATA S: 354 Enter mail, end with &amp;#38;#60;CRLF&amp;#38;#62;.&amp;#38;#60;CRLF&amp;#38;#62; on a line by itself C: Do you like ketchup? C: How about pickles? C: . S: 250 Message accepted for delivery C: QUITS: 221 hamburger.edu closing connection“Do your like ktchup? how about pickles?” 라는 메시지를 보내는 SMTP handshaking protocol의 예시이다.client가 HELO, MAIL, RCPT, DATA, QUIT 등의 Command를 보내면, server 측에서 적절한 응답 코드와 설명을 보내준다.client의 각 메시지는 CRLF .CRLF로 끝나며 이는 carriage return과 Line feed를 의미한다.메시지 별로 handshaking을 하지 않고, 해당 메일 서버로 향하는 메시지 큐에 있는 모든 메일 보낸 뒤 TCP connection이 종료된다.(persistent connection)각각 보내는 메시지의 시작에 MAIL FROM: 을 통해 메일주소를 알려주며, 모두 보냈을 경우 QUIT를 통해 연결을 종료한다.“telnet {serverName} 25” 를 이용해서 localhost와 SMTP server와 연결이 가능하며, 위와 같은 명령어를 사용해 통신이 가능하다.앞으로 주어질 Assignment 3번을 통해 구현해 볼 것이다.2.3.2.Mail Message Formats마치 현실 우편에서 보낸이, 받는이, 날짜, 우편번호 등을 적는 것처럼, 이메일의 Message에도 header를 줘야한다.(RFC 5322),header line과 body line은 빈 줄로 구분되어있으며(정확히는 CRLF), Header에는 다음과 같은 format을 가지고 있다.DATAFrom: alice@crepes.fr To: bob@hamburger.eduSubject: Searching for the meaning of life.I love computer networks!.QUITFrom과 To는 보낸이와 받는이로 필수이며, Subject와 그 외의 추가적인 header line을 추가할 수 있다.telnet으로 확인해 볼 수 있다.extra : MIME Message Formathttps://docs.microsoft.com/ko-kr/previous-versions/office/developer/exchange-server-2010/aa494197(v=exchg.140)에서 발췌Multipurpose Internet Mail Extensions(MIME) Message Format은 문서의 컨텐츠의 내용을 표시하여 기존의 메시지 체계보다 더욱 복잡한 컨텐츠를 포함할 수 있게 해준다.이러한 문서 타입 정보를 실어나는 방법은 윈도우즈 시스템의 접미사 방법이나, 구조상 보여지는 매직 넘버를 통해 가능하나, 통용되지않거나 예외가 있는 경우가 많다.단순 메일 뿐만 아니라 HTTP Message 등에서도 많이 사용한다.MIME header를 추가하여 body 부분을 각기 다른 타입의 여러 body로 나누어 생성하여 사용하며, 다음과 같은 추가적인 정보를 제공한다.  해당 body의 content type (image, video, application etc…)  각 body content 들의 class 구분, class는 content와 달리, 목적, 용도를 의미한다. (보통 subtype을 의미, application/msword의 msword 부분)  각 body content 들의 encoding 방식, 그리고 해독한 결과의 character encoding 구분, 예를 들어 binary, ASCII 등.  각 body content 들의 배치 방식, inline or attachment.  HTML content 들의 base Uniform Resource Identifier (URI) 표기.이를 통해, header나 message를 US-ASCII 이외로 작성하거나 , text가 아닌 데이터, 여러 part로 나뉜 데이터 등을 메일에 집어넣을 수 있게 된다.MIME Message Body parts또한, 이러한 body 부분들은 계층 구조를 형성하고 관계를 형성할 수 있는데 예를 들면, 같은 메시지의 여러가지 표현들(text로, html로, audio로), 사용자가 보기엔 하나로 보이는 여러 타입으로 이루어진 자료들(예를 들면, HTML page 내부에 사진, CSS, javascript 등이 뭉쳐진 경우), 대안 자료, 첨부 자료 등, 여러 목적을 위한 그룹 생성 등이 가능하다.정확한 mail의 MIME type 결과물을 보고 싶으면 대부분의 이메일 서비스에서 original view를 지원하니, Image, 영상 등을 보내본 뒤, 확인해보자!다른 body part 자식들을 가지고 있지 않은 body part를 content body part라고 하고, 다른 자식들을 포함하는 body part들을 multipart body part라고 칭한다.이때, multipart body part는 content body part와 달리 message, text, video 같은 content를 가질 수 없으며, 오직 관계를 정의하는데 사용된다.  html에 포함되어 있는 그림의 경우, “html content body part가 Image content body part를 자식으로 가지고 있으므로, content  body part 속성도 가지고 multipart body part 속성도 가지지 않나?” 라고 생각할지도 모르겠지만, html이나 dom tree와 달리, MIME에서는 부모자식 관계로 표현하지 않고, related multipart body part로 묶어서 표현한다, (아래 예시 참조)multipart body part의 경우, 여러 body part의 상위 body part인 경우, Content-Type 헤더가 multipart/mixed로 표시되며 ,그 자식 body part는 image/xxx, text/xxx 등의 Content-Type 헤더를 가진다.또는 메시지의 대체 표현들을 위한 multipart body part도 존재하는데, 이 자신의 Content-Type 헤더는 multipart/alernative이며, 이 경우, 해당 메시지를 표현할 수 있는 여러 표현들의 값들을 자식 body part들이 Content-Type으로 가지고 있다.(같은 정보를 ASCII text, HTML, audio로 표현한 데이터들) 등,body part type은 body part가 가지고 있는 content data의 종류를 의미하는데, 이는 MIME header의 Content-Type header에 표시되어 있다.Content-Type은 {primary type}/{secondary type} 형식으로 표기하는데, 예시를 들자면, image/gif, multipart/related 같은 형식이다.최상단에는 하나의 root body part가 존재한다.Multipart/mixed header┠━━━Multipart/related header┃    ┠━━━Multipart/alternative header┃    ┃    ┠━━━Text/plain header(content)┃    ┃    ┗━━━Text/html header(content)┃    ┗━━━Image/gif header(content)    ┗━━━Application/msword header(content)예를 들어 위의 예시의 경우, msword attachment와, html 또는 plain text로 표현될 수 있는 content를 가지고 있는데, 해당 content를 render하는 데 필요한 gif 파일도 포함하고 있다고 이야기할 수 있다.Content Body Partscontent body part는 binary 값이나 html 등일 수 있는 bytes들로 이루어진 content와 content 정보를 가지고 있는 header field로 이루어져 있다.예시의 Content-Transfer-Encoding header는 body의 encoding type을 알려주며, Content-Disposition header는 content가 메시지의 첨부 파일인지, 아니면 다른 body part content와 함께 inline에 표시되는 지 등을 의미해준다.message 타입이 email 간에 ascii 코드로 변환되기에 이러한 정보들이 decoding 하고 화면에 보여주는데 필요하다.Content-Type: image/gif;   name=&amp;#34;picture_e2k7.gif&amp;#34;Content-Transfer-Encoding: base64Content-Disposition: attachment;   filename=&amp;#34;picture_e2k7.gif&amp;#34;[encoded content here]위의 예시는  content type은 GIF format의 이미지이다.  content의 binary data는 base64 알고리즘으로 encoding 되어있다.  이 body part는 message의 첨부물이다.  fiile명은 Picture_e2k7.gif 이다.를 의미한다.sample MIME MessageFrom: Some One &amp;#38;#60;someone@example.com&amp;#38;#62;MIME-Version: 1.0Content-Type: multipart/mixed;        boundary=&amp;#34;XXXXboundary text&amp;#34;This is a multipart message in MIME format.--XXXXboundary textContent-Type: text/plainthis is the body text--XXXXboundary textContent-Type: text/plain;Content-Disposition: attachment;        filename=&amp;#34;test.txt&amp;#34;this is the attachment text--XXXXboundary text--MIME-Version header는 해당 메시지가 MIME Message임을 의미한다.Content-Type: header가 multipart/mixed임을 통해, user agent에게 여러 body part로 나뉘어져 있음을 말해주며, 각 body는 boundary=”XXXXboundary text”로 나뉜다.(정확히는 boundary로 설정해준 값앞에 “–“가 추가되어야 한다.)“This is a multipart message in MIME format” 이라는 글자는 boundary 외부에 있기 때문에 MIME를 지원하는 UA에게는 보이지 않는다.boundary로 나눠진 각 content는, 각각 header가 존재하며, 만약 boundary로 나눠져 있되, header가 존재하지 않다면, 빈줄로 보인다?첫번째 body part는 plain text를 가지고 있으며, US-ASCII로 이뤄져 있으며, 볼 수 있다. Plain text는 기본 헤더값이다.두번째 body part는 첨부 파일로 표시되며, file 이름은 test.txt일 것이다.MIME Headers각 MIME header는 각 body part의 시작점에 적히며, 일부 헤더들은 message header에도 사용될 수 있다.주로 “Content-“로 시작하는 헤더들은보통 body part header로만 쓰이며, sub-header나 field, parameter들을 받는다.      MIME-Version : MIME message임을 의미하며, top-level header로, 존재하는 body part가 완전한 새로운 message 형식이 아닌 이상, 한 메시지에 하나만 존재하며, 현재 1.0 만 지원하고 있다.        Content-Type : media type, subtype 등을 표시, private content-type 또한 기술 가능하며, 이때는 X-{새로운 타입명} 형식으로 기술한다.        Content-Transfer-Encoding : 2가지 다른 의미를 가지고 있다. 기본값은 7bit, 마찬가지로 x-로 시작하는 custom 값이 존재함.          만약, base64, quoted-printable 같은 값을 가지면, encoding 형식을 의미하는 것이다.      만약, 7bit, 8bit, binary 등의 값을 가지고 있으면, encoding 값이 없으며, content의 type을 말해주는 것이다.            Content-ID headers: body part를 구분하게 해주는 세계 유일 값, body part 간의 참조를 위해 필요함.        Content-Description : 보통 비문자 정보의 추가적인 정보를 제공하기 위한 optional header        Content-Disposition : message와 body part를 어떻게 표현할 지 정보를 알려준다. 첨부 파일로 놓고 싶으면, file name parameter를 활용하면 된다.          Content-Type : MIME에서 가장 핵심적인 부분으로, 해당 content를 알맞은 form으로 보여주게 만든다.                      크게 “복합형”과 “연속형”으로 나뉜다.                    복합형                              Multipart : sub type과 추가 element를 header에 넣어 multipart message로 만들어줌                          multipart/alternative : 같은 정보가 다른 형태로 다른 body에 있을 때 사용한다.              multipart/byteranges : HTTP protocol의 일부로 정의되었다. 고유의 Content-type과 Content-range로 이루어진 2개 이상의 파트로 이루어져 있고, MIME boundary parameter를 이용해 나뉘어져 있다. binary로, 7bit로, 8bit 로도 파일을 길이가 적혀진 채로 각 파트에 나누어 보낼 수 있다.              multipart/digest : 여러개의 plain-text message들을 보내게 하기 위해, 만들어졌으며, multipart/mixed와 다른 점은, 각 body가 message/RFC 822 여야한다.              multipart/form-data : 파일 업로드 표현을 균일하게, MIME에 호환 되게끔 만듦              multipart/mixed : 각 독립적인 body parts들이 특별한 순서로 묶이게 하기 위해 사용, subtype이 호환되지 않은 경우 UA들은 기본적으로 이 subtype으로 생각한다.              multipart/parallel : 각 multipart content들이 동시에 출력되게 하기 위해 사용, 예를 들어, 사진이 load되면 동시에 소리도 나게끔 하기 위해 사용.              multipart/related : 보통 여러 형태의 데이터가 포함된 복합 문서를 표현하거나, 메시지에 포함되지 않는 컨텐츠의 링크를 제공하는데 사용한다. “start” parameter가 주어지지 않으면 첫번째 body part를 root body part를 root로 삼는다.                        ```Content-Type: Multipart/related; boundary=\"boundary-content_example\";type=Text/HTML; start=example@somplace.com;Content-Base header not allowed here;since this is a multipart MIME object–boundary-content_example  Part 1:Content-Type: Text/HTML; charset=US-ASCIIContent-ID: &#38;#60;example@somplace.com&#38;#62; Content-Location: http://www.webpage/images/the.one; This Content-Location must contain an absolute URL, ; since no base; is valid here.  –boundary-content_example  Part 2: Content-Type: Text/HTML; charset=US-ASCII Content-ID: &#38;#60;example2@somplace.com&#38;#62; Content-Location: the.one ; The Content-Base below applies to ; this relative URL Content-Base: http://www.webpage/images/  –boundary-content_example–                                     - multipart/report : 메시지 처리결과를 알리기 위해 사용하는 type, 주로 기계간 보고를 위해 사용한다.       - multipart/signed, encrypted : MIME part의 보안을 위해 사용된다.          - Message : 다른 메시지, 다른 메시지의 포인터를 포함 가능, 7bit 인코딩만 허용              - message/partial : 커다란 메시지를 쪼갠 여러 부분 메시지의 일부, id(조각들의 매치시킬 수 있는 id), number (메시지의 몇번째 조각인가?), total(전체 메시지는 몇조각인가?, 마지막 조각에만 적혀있음) 3개의 parameter를 받아 생성한다.       - message/external-body : 외부 참조 message, access-type parameter를 통해 FTP, LOCAL-ACCESS 등의 접근 방법을 설명하고, Content-ID header field에 외부 참조를 위한 ID를 제시해야 한다.      - 연속형          - Text        - text/enriched : 여러 글꼴을 지원하는 간단한 type, &lt;commandname&gt; &lt;/commandname&gt;사이의 text를 formating 할 수 있다.       - text/html, text/plain     - 그외 Audio, Video, Application(application data들, marc, octet-stream, postscript 등) ....### 2.3.3.Mail Access Protocols사용자의 host에도 mail server 역할을 할 수 있게 만들 수 있지만, 그렇게 하기 위해서는 24시간 이메일을 받을 수 있게끔 켜놔야 하므로 개인용 PC로는 어렵다.그러므로 대다수의 사용자들은 user agent를 local host에 놓고, 공용으로 사용하는 mail server를 공유하여 사용한다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018100001754.png)**대부분은 user agent들은 해당 메일을 직접 SMTP protocol로 상대방의 메일 서버로 보내는 것이 아니라 굳이 메일 서버를 거쳐서 보낸다.**그 이유는, 만약 상대방의 메일 서버가 응답하지 않은 경우, 중간에 꺼지지 않고 지속적으로 새로운 요청을 보내기 위한 능력이 필요하기 때문에, 그러한 능력을 가진 메일 서버를 이용한다.이때, 해당 서버에 HTTP 요청이나 SMTP protocol을 둘다 이용할 수 있다.그렇다면 bob은 어떻게 mail server에서 mail을 가져올 수 있을까?**SMTP는 요청을 통해 데이터를 가져오는 Pull protocol이 아니라 요청을 통해 데이터를 보내주는 Push protocol이니, SMTP protocol로는 불가능**하다.이때 **웹기반일 경우에는 HTTP Protocol을 이용해서 가져오거나, POP3 protocol, microsoft outlook같은 Internet Mail Access Protocol(IMAP) 기반 어플리케이션**을 이용할 수 있다.## 2.4.DNS-The Internet's Directory Service현실에서 사람을 구별할 때 기계는 주민등록번호를 쓰지만 친구는 이름을 쓴다.네트워크에서도 마찬가지로, 기계에서는 IP address를 사용하지만, 사람들이 웹사이트의 기억을 쉽게 하기 위해 hostname을 쓰곤 한다.### 2.4.1.Services Provided by DNS위에 설명한 대로, router는 고정된 길이의, 계층 구조 주소인 IP address로 host를 구별하지만, 인간이 기억하기 쉽도록 hostname을 써야한다.**이 둘 사이를 변환해주는 역할은 domain name system(DNS)가 맡는다.****DNS는 DNS 서버 계층에 존재하는 분산 데이터 베이스나 host가 DB에 검색할 수 있게 해주는 application layer protocol을 의미하기도 한다.**보통 UNIX 서버 머신에 Berkelty Internet Name Domain(BIND) software를 많이 사용하며, UDP 기반에 port 53을 쓴다.HTTP와 SMTP(e-mail)에서도 hostname에서 IP Address를 구하는데 사용하며, 과정은 다음과 같다.1. **유저가 브라우저에 URL을 입력하면, 브라우저가 hostname(www.someschool.edu)를 추출한다.**2. **DNS의 client side application을 실행하고 DNS Client application에게 hostname을 보낸다.**3. **DNS client가 DNS server에 해당 hostname을 query한다.**4. **hostname의 IP address를 탐색하면, DNS client로 해당 IP address를 다시 reply 한다.**5. **브라우저는 받은 IP address로 TCP 연결을 한다.**결론적으로 DNS는 TCP와 UDP 전부 사용한다.DNS로 인해 추가적인 delay가 발생할 수 있으며, 이를 막기 위해 결과값이 cache 되어있는 좀더 network 상 가까운 dns server를 만들기도 한다.이 뿐만 아니라 아래와 같은 새로운 기능을 제공하기도 한다.1. Host aliasing : 정식 hostname(canonical hostname)을 대신할 수 있는 보조 hostname을 가질 수 있게 해줍니다. 보통 헷갈리기 쉬운 이름이나, 더욱 간단한 이름을 사용하기위해 쓰며, DNS 서버에서 alias hostname을 받아 정식 이름이나 IP address를 query  해주기도 한다.2. Mail server aliasing: 위와 비슷하게 메일 서버의 hostname을 새로운 hostname으로 바꿔주거나, 또는 원래 회사 hostname을 메일 서버 aliasing으로 사용할 수 있게 해준다.3. Load distribution : 트래픽이 많은 서비스의 경우, 사용자들을 분배하기 위해 Load distribution을 사용한다. 트래픽을 분산시키기 위해, 많은 web server를 복사하게 되고 이렇게 복사된 server들은 각기 다른 IP를 가지게 되는데, hostname으로 query하면 DNS 서버는 이러한 IP address 들의 모임을 보내주되, 요청이 있을때 마다, 순서를 하나씩 rotation 하면서 돌려주며, 사용자는 이중 사용가능한 가장 앞선 IP address로 request를 보내게 되어, 사용자가 분배되는 방식이다. 웹 메일 서버에서도 DNS가 같은 방식이 쓰이기도 하며, Akamai 같은 회사에서는 CDN에도 사용한다.DNS는 복잡한 시스템이며(복잡한 시스템은 network edge에 위치하도록 되어있는 디자인 철학상), 여러 RFC에 제정되고, 개선되어 있다.### 2.4.2.Overview of How DNS WorksDNS는 사용자 입장에서는 간편하고 편리하지만 실제로는 아주 복잡하고 커다란 규모의 시스템이며, 분산 데이터베이스 구현의 좋은 예이다.만약 DNS server가 고성능으로 네트워크 상에 단 하나만 운용한다면 다음과 같은 문제점들을 가지게 된다.1. single point of faiure : 해당 DNS server가 다운되면 서비스 이용 불가2. Traffic volumn : 너무나 많은 query가 한 서버에 집중됨3. Distant centralized database: 지구상 모든 포인트 지점 정중앙에 위치할 수 없으므로, (지구 내핵 한가운데?) 어떠한 지역에서는 빠르게 response를 받지만, 어떤 지역에서는 아주 긴 delay를 겪을 것이다.4. Maintenance: 새로운 hostname이 추가되거나, 변경될 때마다, 전 세계의 모든 서비스를 끊임없이 업데이트, 추가, 삭제해야 한다.#### A Distributed, Hierachical Database![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018100207948.png)distributed 된 DNS server들은 다음과 같은 3가지 계층으로 이루어져 있으며, 모든 mapping 기록을 전부 가지고 있는 DNS server는 없다.1. **Root DNS servers : TLD server의  IP addresse를 제공한다.** IANA(Internet Assigned Numbers Authority)에 의해 편성된 12개의 단체에서 관리하는 서로 다른 13개의 root server들의 복사본들이며, 아래 지도 분포대로, 전세계에 1000개 정도 존재한다.2. **Top-level domain (TLD) servers : authoritative DNS server의 IP address를 제공한다,**  top-level domain(com, org, net, edu, or country top-level(uk,kr,jp,fr...))들을 위한 서버들로, 회사나 특정 단체가 관리 중이며, 예를 들어 .com을 관리하는 Verisign network의 [Osterweil 2012]를 통해 서버 구성을 알 수 있다..3. **Authoritative DNS servers : 공개 접근이 가능한 hostname들 가지고 있는 단체는 hostname : IP address map을 제공해야하며, 이를 제공해주는 서버를 Authoritative DNS server라고 한다**. 규모가 있는 회사의 경우 자기 자신의 DNS server를 운용하고 그렇지 않은 경우에는 관련 회사에 맡기기도 한다.유저의 이러한 계층 접근 방법은 다음과 같다.1. 유저가 DNS 서버에 원하는 hostname을 query하면, 먼저 Root server에 query한 뒤, root server는 TLD(.com, .kr, .co 등)를 보고 TLD 서버 IP를 돌려준다.2. TLD 서버 주소를 알게된 유저는 TLD 서버에 해당하는 IP address(ex) naver.com, amazon.com)를 가진 authoriative server 주소를 물어보게 되고, TLD 서버는 해당 authoriative server 주소를 response로 돌려준다.3. 유저는 다시 해당 authoriative server 주소에 full hostname(www.naver.com, www.amazon.com)를 물어보게 되고 authoriative server는 해당 웹 사이트의 IP address를 돌려준다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018100306500.png)**추가적으로 Local DNS server가 있는데, 보통 ISP의 DHCP나 LAN을 생성한 단체에서 host에 router 상 가깝게 설치한 뒤, query 결과를 캐쉬해서 보내준다.**아래 그림은 Local DNS server가 다른 DNS Server와 gaia.cs.umass.edu의 IP address를 가져오는 중에서의 동작이다.그림과 달리 실제로는 TLD DNS server와 Authoritative DNS server 사이에 Intermediate DNS server가 존재하는 경우가 많아 실제로는 총 10개의 DNS message가 보내진다.**그리고 1&lt;-&gt;8번 같이 해당 쿼리가 다른 서버의 쿼리를 유발한 경우는 recursive query라고 하고,****나머지 2~7까지 처럼 한 서버가 다른 서버와 순차적으로 쿼리하는 경우는 iterative query라고 한다.**![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018100342742.png)#### DNS Caching![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018100429942.png)DNS caching 은 DNS 전체 시스템에 있어서 아주 중요한 부분으로, 실제 전세계 1000개조차 되지 않는 root DNS server가 모든 요청의 첫번째에 있음에도 트래픽을 감당할 수 있는 이유이기도 하다.**Local DNS server에서 한번 query한 IP address map은 Local DNS server에 저장되어 동일한 또는 다른 Host가 해당 Hostname을 query 했을 시, 되돌려주는 역할을 한다.**또한, full hostname 뿐만아니라, TLD 서버 주소도 cahcing 하므로, 처음 보는 Hostname이여도 TLD 도메인 부터 query할 수 있다.보통 최장 2일 정도 보관하며, 2일마다 새로 caching한다.### 2.4.3. DNS Records and MessagesDNS server는 Resource Record(RR)을 저장하고, 이를 message에 집어넣어 작동한다.Authoriative server의 경우, DB에 저장하고, Hostname에 대한 IP address 값을 돌려주는 역할이지만,Authoriative server가 아닌 경우도, cache 서버 등에 저장하기도 한다.**Resource Record(RR)은 (Name, Value, Type, TTL), 4개 field의 tuple로 이루어져있다.****TTL의 경우, 서버에 해당 RR을 유지하는 시간**이므로 타입이 달라져도 모두 동일하므로 이번에 설명에서 생략한다.이때 **Type에 따라 Name과 Value에 들어갈 내용이 달라**지며 Type은 다음이 있다.1. Type=A : Name은 hostname, Value는 IP Address가 들어간다. 가장 기본적인 Hostname:IP address mapping.       예시 : (relay1.bar.foo.com, 145.37.93.126, A) 2. Type=NS : Name은 domain, Value는 해당 도메인의 IP address를 담고 있을 Authoritative DNS server의 host Name, 해당 서버에 정보가 없고 다른 서버에 쿼리할 필요가 있을 때 (=authoritative server가 아니고) 알려주기 위한 RR.      예시 : (foo.com, dns.foo.com, NS)3. Type=CNAME : Name은 alias name, Value는 canonical hostname, 더욱 간단한 hostname으로 표기하는 alias 값을 위한 RR.      예시 : (foo.com, relay1.bar.foo.com, CNAME)4. Type=MX : CNAME과 비슷하나, Mail Server를 위한 aliasing이며, 웹 서버와 같은 이름을 쓸 수 있게 해준다.      예시 : (foo.com, mail.bar.foo.com, MX)#### DNS MessagesDNS의 message 종류에는 query와 reply message가 존재하며, 둘다 아래와 같은 형식을 가지고 있다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018111410308.png)**첫 12 bytes는 header section으로, 숫자들로 이루어져있다.** 16bit의 Identification은 Query에서 제공되어, respond로 오는 reply message가 어느 query의 답변인지 알려준다. flags filed는 여러 종류의 flag로 이루어져 있으며 - query/reply flag는 1bit로, query(0) or reply(1)를 구분하기 위해 존재한다.- authoritative flag는 1bit로 reply message에 존재하며, 해당 query name에 대한 authoritative server 측에서 온 메시지인지 구분한다.- recursion-desired flag는 query에 존재하며, 1bit로 1로 설정시, 해당하는 record가 없으면 recursion qurey를 하게 하여, 다른 서버에서 찾게 한다.- recursion-available flag는  reply에 존재하며, reply한 DNS 서버가 recursion이 가능한지 여부를 알려준다.- 추가로 4개의 additional flags는 아래에 설명할 data section 4개에 포함된 RR의 갯수가 적혀있다., 즉 한 메시지에 여러개의 RR query와 reply를 담을 수 있다 **question section은 현재 query에 대한 정보가 담겨져있다.**- name field : query되는 hostname - type field : 위의 name이 속한 타입 (ex) Type A, Type MX, 위의 RR 참조)    **answer section은 query된 hostname에 대한 RR을 포함하고 있다.**- 위의 name field에 대한 Type, Value, TTL을 포함하고 있다.- 여러개를 포함할 수 있는데, 트래픽이 많은 웹서버가 replicated 된 경우에는 여러 IP address를 가질 수 있기 때문이다.(proxy server)**authority section은 다른 authoritative sever들의 record(rcursion query log?)를 포함하고 있다.****addtional section은 추가적으로 도움이 되는 record를 가지고 있다.**- 예를 들어, mali server aliasing을 위한 (foo.com, mail.foo.com, mx)의 경우, additional section에 mail.foo.com의 IP address를 포함할 수 있다.이러한 DNS query message를 DNS sever에 직접 보내보고 싶으면 **nslookup program, Wireshark, nslookup.io**을 이용하자.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211104102043201.png)#### Inserting Records into the DNS Database만약, 이제 막 새로 시작한 회사가 hostname을 등록하려면, DNS database에 records를 넣어야 한다. **이를 도와주는 상업 회사들을 registar이라고 부르며, 보통 유료로 여러 TLD를 사용할 수 있는 선택지를 준다.****먼저 호스트 네임과 IP address, 그리고 primary, secondary authoritative DNS server name과 해당 서버들의 IP address를 제공해야 한다.**그 뒤에는 registar 측에서 type NS, type A record를 TLD server(com이면 TLD com server 측에) 삽입하면 된다.아래는 primary authoritative server에 대한 TLD com server에 제출할 RR 예시이다.(yournewcompany.com, dns1.yournewcompany.com, NS)(dns1.yournewcompany.com, 212.212.212.1, A)추가로 해당 authoritative server 들에 자신의 hostname(웹서버, 메일서버)가 추가 되어있는지 확인하고, hostname으로 HTTP request를 보내봐서 확인하면 된다.최근에는 DNS protocol에 UPDATE option이 추가되어, database에 추가하는 것을 DNS Message로 가능하게 되었다.## 2.5.Peer-to-Peer File DistributionP2P 파일 분배 구조는 기존의 client-server 구조와 달리 여러 사용자에게 파일을 제공할 때의 서버의 부하를 없앨 수 있다.가장 유명한 P2P file distribution protocol은 BitTorrent이며, P2P에 대해 알아보자.#### Scalability of P2P ArchitecturesP2P 구조를 다음과 같이 생각해보자.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018112856340.png)서버와 peer들은 internet을 통해 연결되어 있으며, 모든 피어들이 완벽하게 파일을 받게 되는 시간을 구하려 한다.1. client-sever 구조의 경우- 서버는 파일의 복사본을 N 명의 peer에게 보내야 하며, 총 NF bits를 보내야한다. 이때, 서버의 업로드 속도를 $u_s$라고 하면, 총 $NF/u_s$만큼 걸린다.- 모든 피어들의 다운로드 속드를 $d_1,d_p,\\dots,d_N$이라고 하면, 모든 피어가 파일을 받게되는 시간은 다운로드 속도가 가장 느린 피어의 속도인 $d_{min}$에 맞추어 $F/d_{min}$만큼의 시간이 걸리게 된다.- 즉, 위 두 속도 중 더 느린쪽에 맞춰지므로$$D_{cs}\\geq\\max\\begin{Bmatrix}\\frac{NF}{u_s},\\frac{F}{d_{min}}\\end{Bmatrix}$$- 이는 client-server 구조에서의 최소 배포 시간을 의미한다.$$D_{cs}=\\max\\begin{Bmatrix}\\frac{NF}{u_s},\\frac{F}{d_{min}}\\end{Bmatrix}$$- 또한, 서버는 전송을 예정해놓을 수 있으므로, 최소 배포시간을 실제 배포시간으로 봐도 된다?  [$수정 필요$]- 해당 식에서 배포시간은 Peer 수인 N에 비례하므로 피어가 1000배 증가하면 시간 또한 100배 증가한다.2. Peer to Peer 구조의 경우- client-server 구조에 비해 속도를 구하는게 어렵다.- 먼저, 서버가 해당 파일을 peer들의 community에 올려놔야 한다. 이때 드는 시간이 $F/u_s$, 한번만 올리면, 다시 올릴 필요 없다.- client-sever 구조 때와 마찬가지로 가장 느린 peer의 파일 다운로드 속도는 $F/d_{min}$이다.- 마지막으로, 파일 업로드 속도는 각 peer들의 upload 속도와 server의 업로드 속도와 동일하며, 이는 $u_{total}=u_s+u_1+\\dots+u_N$ 로 구하며, 전체 전송되어야할 bit는 NF bits 이므로, 이때 최소 배포 시간은 $NF/(u_s+u_1+\\dots+u_N)$이다.$$D_{P2P}\\geq\\max\\begin{Bmatrix}\\frac{F}{u_s},\\frac{F}{d_{min}},\\frac{NF}{u_s+\\sum_{i=1}^{N}u_i}\\end{Bmatrix}$$- 실제로 P2P 구조에서는 각 비트별로 구분해 배포되지 않고, file의 조각 별로 배포되며, 위의 최소 속도는 실제 최소 속도와 아주 근접하므로,$$D_{P2P}=\\max\\begin{Bmatrix}\\frac{F}{u_s},\\frac{F}{d_{min}},\\frac{NF}{u_s+\\sum_{i=1}^{N}u_i}\\end{Bmatrix}$$- 실제 배포는 위와 같다고 봐도 된다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018112935978.png)- **client-Server 구조는 peer 수에 따라 선형으로 필요시간이 증가하는 것과 달리 P2P 구조에서는 self-scaling 덕분에 훨씬 적게 증가하며, 1.0 에 수렴하는 것을 알 수 있다.**#### BitTorrent BitTorrent는 인기있는 파일 배포 P2P Protocol이다. 여기에서는 **특정 파일의 배포에 참여하는 모든 peer들의 모임을 torrent**라고 표현한다. 각 피어들은 다른 피어로 부터 **256KByte 크기의 파일 조각인 chunk를 다운**받는다.첫 참여시 chunk가 존재하지 않지만, 첫번째 chunk를 다른 사용자로부터 받은 뒤, 다운받음과 동시에 다른 peer에게 배포를 시작한다. 다운로드가 끝나면 이기적이게 해당 torrent에서 나가거나, 여전히 남아 배포를 지원할 수 있다. 또한, 중도에 나갔다가 다시 들어오는 것 또한 가능하다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018113023045.png)**BitTorrent의 각각 torrent에는 Tracker라는 노드가 존재하는데, 이는 peer들이 주기적으로 tracker에게 상태를 알려, 적게는 수 명, 많게는 수천명의 peer들의 참여 상태를 알 수 있다.** 위 fig의 예시를 들자면, alice가 처음으로 torrent에 peer로 참여하게 되면, tracker node는 alice에게 랜덤으로 50명 정도의 peer들의 IP 명단을 보낸다.Alice는 해당 peer들과 TCP Connection을 수립을 시도하면, 일부 성공한 peer들을 \"이웃 peer\"라고 표현하며, 위의 그림의 경우 3명이다.(보통, 실제의 경우 더 많다.)시간이 지나면서 일부 peer들은 torrent나 alice와의 연결에서 떠날것이며, 일부는 초기 50명의 peer 명단 내외적으로 더욱 참여할 것이다.각 피어들은 파일 chunk의 일부분을 가지고 있으며, 이 일부분은 서로 모두 다를 것이다. 주기적으로 Alice는 이웃한 피어들에게 TCP Connection을 통해 가지고 있는 chunk의 목록을 요구할 것이고, 응답 받은 목록을 토대로 Alice가 가지고 있지 않은 chunk를 요구한다.이때, 2가지 중요한 결정이 있는데.1. **어떤 chunk를 먼저 요구해서 가져와야 하나?**2. **어떤 peer의 chunk 요청을 먼저 들어줘야 하나?**이다.1번째 결정은 **rarest first**, 라는 기준을 사용하는데, 이웃들 중에 가지고 있지않은 가장 희귀한 chunk를 먼저 요구하는 것이다.이를 통해, torrent 내부의 chunk들의 보유 빈도수를 최대한 고르게 만들 수 있다2번째로, 어느 peer의 chunk 요청을 들어줘야하는 문제는, 특별한 알고리즘을 사용한다.**Alice는 주기적으로 이웃들별로 자기에게 chunk를 보내는 속도를 측정한다, 그리고 매 10초마다 최고로 높은 속도의 4명을 선정해서 해당 4명이 요구하는 chunk를 보내준다.**이러한 top 4 이웃 peer를 **unchoked**라고 한다.**또한 위의 4명과 별개로, 매 30초마다 랜덤하게 이웃 peer 하나를 골라 해당 peer에게도 chunk를 보내준다.**이를 **optimistically unchoked**라고 부른다. 만약, 이 랜덤하게 chunk를 받은 peer가 측정한 top4 peer에 alice가 들게된다면, 해당 peer 또한 alice의 chunk 요구에 보탬이 되줄 것이다.만약, 두 peer가 서로 chunk를 보내주기로 결정되었다면, 한쪽의 peer가 더욱 나은 파트너를 찾을때까지 서로 chunk를 보내주게 된다.**이러한 알고리즘에 의해, 비슷한 속도로 배포하는 사람들끼리 매칭되는 경향이 생기게 된다.****optimaistically unchoked peer에 의해, 배포해줄 chunk가 없는 새로운 참여 peer 또한, chunk를 받을 기회가 생기게 된다.**위의 chunk를 받는 5명의 peer 이외를 chocked 라고 한다.이외에도 chunk를 쪼개는 pieces 알고리즘 pipelining, random first selection, endgame mode, anti-snubbing 등의 특이한 알고리즘이 Bittorrent에 적용되어 있다.이러한 인센티브 메커니즘은 tit-for-tat이라는 전략과 비슷하며, 사실 이러한 방법은 이기적인 방법으로 피해갈 수 있다.하지만 이러한 전략 때문에 BitTorrent는 크게 성공하였고, 수 많은 파일들이 공유되고 있다.Bittorrent 이외에도 **DHT(Distributed Hast Table)이라는 피어간에 db record가 분배되어있는 간단한 데이터베이스**도 존재한다.## 2.6 Video Streaming and Content Distrbution Networks동영상 스트리밍은 전체 인터넷 traffic의 80%를 차지한다. 이러한 content 중심 웹사이트에게 중요한 CDN에 대해 알아보자### 2.6.1. Intertnet Video**video는 초당 특정 프레임 수(보통 24~60 사이) 만큼의 image가 시간 순서대로의 열거된 매체**이며,image는 2차원 배열로 표현된 pixel의 모임이며,pixel은 명도와 채도가 숫자로 표기된 한 점에서의 색상을 의미한다.streaming video란, 사용자가 video 정보를 필요로 할 때, 시간대 별로 연속적으로 미리 인코딩된 비디오를 보내어 비디오 데이터를 전부 받지 않아도 볼 수 있게끔 하는 서비스이다.**보통, 비디오를 여러 화질로 미리 encoding 하여 서버에 저장한 뒤, 네트워크 상황에도 버퍼링 시간 없는 streaming과 안정적인 throughput을 위해, 화질과 초당 프레임 수를 조절해가며 streaming 한다.**최근  encoding 기술이 발달해 원하는 화질에 얼마든지 압축, encoding 할 수 있으며, video의 경우 보통 100kbps ~ 10 Mbps(4k)까지 지원한다.1시간, 2Mbps 정도의 비디오가 1GB 정도를 차지한다.### 2.6.2.HTTP Streaming and DASH**HTTP server에서 URL을 통한 GET 요청을 보내면 TCP connection이 열리고, client application의 buffer에 message를 쌓아놓게 된다.****이렇게 쌓아놓은 buffer의 bytes가 설정한 수준(predetermined threshold)를 넘기게 되면, play 가능하게 되며, application은 계속 도착하는 message를 buffer에 쌓고, frame을 decompress한 뒤, 유저의 뒷 부분 영상을 보여주게 된다.**HTTP를 이용한 streaming은 Youtube에서도 사용하고 있지만, 모든 client가 네트워크 상황에 관계없이 언제나 같은 encoding으로 받아야 한다는 단점이 있다.**Dynamic Adaptive Streaming over HTTP(DASH)는 이를 보완하기 위해 만든 HTTP-based streaming으로, video를 여러가지 버전의 bit rate로 저장해놓고, client는 매 GET 요청마다 video segment의 화질을 스트리밍 중에 동적으로 바꾸어 요청할 수 있다.** DASH Protocol은 bandwidth 상태가 좋지 않은 경우가 많고 변화무쌍한 모바일 환경 등에서 유리하다.각 버전의 video 들은 HTTP 서버에 저장되며, 이때, **manifest file을 함께 저장하는데, client가 video를 요청할 때, 가장 먼저 manifest file을 받아보게 되고, 그곳에 적혀있는 각 화질 별 URL과 byte range header에 따라 chunk별 HTTP GET request를 작성하게 된다.**그렇게 chunk를 받아본 후, bandwidth, buffer 등의 상황을 고려한 알고리즘 또는 사용자의 선택을 통해 다음 request에 요구할 bit-rate를 결정하게 된다.### 2.6.3.Content Distribution Networks용량이 큰 Video, sound, image 등을 사용자에게 보내는 일은, 사업이 글로벌화되고, 커다란 수요를 마지하게 되면서 해결하기 힘든 일이 되었다.만약 성능 좋고 거대한 용량을 가진 data center에 모든 contents를 수용하고, 사용자에게 제공한다면 세가지 문제가 생긴다.**첫번째는 사용자가 해당 data center에서 멀다면 여러 ISP를 거치면서, delay와 throughput이 엄청나게 악화된다.****단 하나의 병목 link만 존재해도, 비디오 스트림은 기나긴 버퍼링 시간과 low bit-rate에 시달려야 한다.****두번째는 요청이 많은 content의 경우 같은 내용의 content byte가 전세계 network에 중복으로 엄청나게 보내지게 되고, 이는 곧 커다란 비용과 network 혼잡을 일으키게 된다.****세번째는 만약 해당 data center에 문제가 생기면(single-point-failure), 모든 서비스가 중단된다는 점이다.**Youtube, Netflix 같은 커다란 회사들은 이 문제를 해결하기 위해 **Content Distribution Network(CDN)**을 만들었다.**CDN은 network상, 그리고 지정학 상으로 전세계로 분배된 content sever를 둔 뒤, content의 복사본들을 위치시켜 사용자에게 하여금 가까운 데이터센터에 content를 요청하게 하는 것이다.**규모가 큰 회사는 자신의 서비스만을 위한 **private CDN**을 운용하며, 아니면 다른 회사에서 운영하는 **third-party CDN**을 이용할 수 있다.보통 2가지 서버 배치 전략을 이용한다.1. **Enter Deep** : 전세계 수많은 Access ISP에 총 수천의 server cluster를 배포하여, end user에 가깝게 위치시킨다. (Akamai의 방법)   - 이를 통해 짧은 delay, 높은 throughput, 지나치는 link와 router수를 줄일 수 있지만, 대신 유지보수와 cluster 관리가 힘들다.2. **Bring Home** : 10개 정도의 커다란 서버 cluster를 IXP(Internet Exchange point)에 배치하여 content 제공(Limelight, Level-3 의 방법)   - 유지보수와 관리가 비교적 쉽지만, delay와 throughput에서 비교적 손해를 본다.모든 server cluster들은 완벽하게 content를 복제해서 저장하지 않는다. **content의 지역성을 고려해서 마치 web cache 방법처럼, 요청 받은 content가 해당 지점에 존재하지 않으면 중앙으로 부터 받아와 caching하고, 오랫동안 사용하지 않은 Content는 삭제하는 식**으로 운용한다.##### CDN Operation사용자가 DNS를 통해 content를 요청할 시, CDN은 해당 request를 가로채서, 가장 가깝고 적법한 CDN 서버는 어디인지 판별하고, 그리고 해당 CDN 서버에 redirect 해준다.이때 request를 가로채기 위해 DNS를 활용하곤 한다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018113213936.png)DNS와 CDN을 이용한 content 전달의 과정을 살펴보자면,1. 사용자가 비디오 웹페이지 접속2. 사용자가 웹페이지에서 특정 video link를 클릭하여 DNS에 요청을 보냄(ex)http://video.netcinema.com/6Y7B23V)3. local DNS 서버는 해당 비디오 웹페이지의 hostname의 \"video\" 문자를 확인하고 authoritative DNS server에 query 요청을 함, (이때 local DNS가 사용자에게 주는 reply는 아직 없음), 그럴 경우 DNS 측 authoritative sever에서 새로운 hostname을 넘겨줌 (ex)a1105.kingcdn.com)4. local DNS는 새로 받은 hostname을 이번엔 CDN 측의 authoritative server에 query하고 reply로 CDN의 content server IP address를 받음.   - 이때, Cluster selection 전략에 따라 알맞은 content server를 할당받게 된다.5. 해당 content server IP address를 Local DNS server 측에서 사용자에게 전달.6. 사용자는 content server IP를 받으면, 해당 server와 TCP connection이 생성되며, video에 대한 HTTP GET request이 가게 된다.    - DASH protocol이라면 manifest file과 URL을 받게 되고, 유저는 그 중에서 알맞은 URL을 선택하게 된다.#### Cluster Selection Strategies사용자(정확히는 사용자가 query한 LDNS 서버)에게 건네줄 적정한 content server cluster를 고르는 전략은 크게 2가지로 1. **지정학적으로 가까운 지점 추천**(geographically closest) : Quova, MaxMind와 같은 상용 지정위치 데이터베이스에 LDNS 서버의 위치를 물어봐 가장 가까운 cluster를 알려주는 방법   - 다만, 실제로 network 상의 hop 거리가 실제 거리와 비례하지 않을 수도 있어, 성능이 나쁘게 나올 수 있다.   - 또한, 네트워크 상 최적의 cluster였다고 하더라도, network traffic 상황에 따라 더욱 먼 cluster를 추천하는게 좋은 상황에도 언제나 같은 cluster를 추천하게 된다.2. **real-time mesurements** : 각 content server와 LDNS 사이의 delay와 throughput을 ping이나 DNS quey를 통해 규칙적 시기마다 조사하여, 최적의 cluster를 결정한다.   - 위의 결점을 보완할 수 있지만, 일부 LDNS는 성능과 traffic, 보안을 이유로 이러한 조사를 위한 message, query를 받아 들이지 않도록 설정하면, 결정이 불가능해진다.### 2.6.4.Case Studies: Netflix and youtubeCDN의 실제 예시를 알아보자.#### NetflixNetflix는 Amazon cloud와 사내 private CDN 인프라를 활용하고 있으며, **서비스 웹사이트와 backend DB의 경우 amazon**을 사용하고 있다.또한, Amazon cloud는 추가로 다음과 같은 업무 또한 진행하고 있는데,1. **Content injection(content 삽입?) : 스튜디오로 부터 영화를 받고, amazon cloud에 업로드**2. **Content processing(content 처리) : 영화를 PC, smartphone, TV에 적합한 여러 버전으로 바꾸고, 추가로 DASH protocol을 위한 여러 bit rate를 설정한다.**3. **Uploading versions to its CDN : 모든 버전의 영화가 생성되면, 해당 버전들을 CDN server로 옮긴다.**![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018113314519.png)Netflix는 초기에는 third-part CDN을 이용했지만, **개인 CDN 인프라를 200 군데의 IXP와 100여 군데의 직접 만든 residential ISP에 server rack을 갖춘 뒤, 옮겼다.**이때, 파트너 ISP와 상호간에 10 Gbps Ethernet prot와 100 terabyte의 저장성을 가진 sever rack을 상호 무료 설치하기도 했다.각 서버렉에는 NETFLIX의 전체 영화들의 여러 화질 버전을 저장하고 있으며, CDN 서버들은 **pull caching(캐쉬에 저장하지 않고 있다가 요청이 들어오면 저장하는 방법)을 사용하지 않고**, 대신 시간대별로 인기많은 영화를 예상해서 각 ISP와 IXP CDN 서버에 집어넣는다(**push caching** 이라고 함). netflix DASH의 byte-range header에 따른 chunk의 크기는 대략 4초정도이다.Netflix 측은 DNS redirect 방식 (2.6.3의 CDN operation) 방법을 사용하지 않고 자체 개발한 software를 Amazon cloud에서 돌려, 특정 CDN 서버를 이용하게 한다.#### Youtube구글은 세계에서 가장 큰 video 공유 사이트를 운용하고 있으며, Netflix와 비슷하게 자사의 private CDN과 수많은 IXP, ISP에 server cluster를 생성하여 운용한다.**하지만 Netflix와 달리 위에 설명한 Pull chaching 방법을 사용하고 있으며, cluster selection 방법으로 RTT를 측정하여 사용하되, load balancing 측면을 위해 DNS 측에서 더 먼거리의 cluster를 고르게 하기도 한다.** DASH 대신 HTTP를 이용해 streaming 하고 있으며, 대신 유저가 알아서 화질은 선택하게 한다.repostioning과 early termination에 의한 bandwidth와 서버 자원낭비를 피하기 위한 방법으로, HTTP byte range request를 이용하는데,video가 특정 데이터 목표량에 도달하면, 데이터 송신을 제한하고 있다.video upload에도 HTTP를 활용하고 있으며, google data center를 이용해 여러 화질 버전으로 변환한다.## 2.7.Socket Programming: Creating Network Applicationsnetwork application에는 RFC 같은 정규 문서를 따른 것과, 그러지 않은 것으로 나눌 수 있으며, RFC 등을 따른다면, TCP를 기반으로 할것이냐, UDP를 기반으로 할것이냐 또한 정해야한다.예시들은 python으로 작성된다.### 2.7.1.Socket Programming with UDP먼저 UDP를 이용한 client-server program을 만들어 보자.UDP 통신에서 packet은, 목적지 host에 도착하기 위한 destination IP address와 도착 뒤, 올바른 application에 할당되기 위한 port number, 그리고 이 둘의 source address 가 header에 추가 되어야 한다.packet에 주소를 헤더에 집어넣는 것은 코드가 아닌, operating system이 하는 역할이므로, 등장하지 않을 것이다.우리의 프로그램의 동작은1. Client가 글자를 입력받아 서버에게 넘겨줌2. 서버가 받은 글자들을 대문자로 바꿈3. 서버가 해당 문자들을 다시 클라이언트로 보냄4. 클라이언트가 해당 데이터를 받아 디스플레이에 출력아래에는 UDP와 socket을 이용한 활동을 보여준다.![](/assets/img/네트워크 정리-Chap 2-응용 계층/image-20211018114644149.png) UDPClient.py, UDPServer.py를 구현하며 코드를 설명할 것이며 포트는 12000을 사용할 것이다.#### UDPClient.py```pythonfrom socket import socket, AF_INET, SOCK_DGRAM # 프로그램을 위한 소켓 생성serverName = &amp;#39;127.0.0.1&amp;#39; # 연결할 서버의 hostname 이나 IP address, 일단은 현재 이 컴퓨터로 생성serverPort = 12000 # 연결할 서버의 포트 넘버 설정clientSocket = socket(AF_INET, SOCK_DGRAM) # client socket 생성# AF_INET : IPv4 사용, SOCK_DGRAM : UDP socket 사용# clientSocket의 port 번호는 OS에서 알아서 설정해줌.message = input(&amp;#39;Input lowercase sentence:&amp;#39;) # 메시지 입력값을 변수에 대입clientSocket.sendto(message.encode(),(serverName, serverPort)) # message.encode() : string을 byte code로# sendto() :해당 데이터를 body로 해당 서버에 보냄, 이때 source address는 자동으로 생성되므로 표기해줄 필요 없음.modifiedMessage, serverAddress = clientSocket.recvfrom(2048) # buffer size 2048# server로부터 응답을 기다리다가, 받은 메시지와 해당 서버의 주소 받기print(modifiedMessage.decode()) # message byte에서 string으로 decodeclientSocket.close() # socket 닫기, client 종료UDPServer.pyfrom socket import socket, AF_INET, SOCK_DGRAMserverPort = 12000 # 생성할 서버의 포트 설정serverSocket = socket(AF_INET, SOCK_DGRAM) # 서버 포트 생성serverSocket.bind((&amp;#39;&amp;#39;, serverPort))  # 해당 포트로 서버 생성, 앞의 &amp;#39;&amp;#39;는 hostname을 적는다.# &amp;#39;&amp;#39;는 현재 host의 요청 중 port번호가 12000이면 모두 가져옴print(&amp;#39;The server is ready to receive&amp;#39;)while True: # 무한 루프로 계속 응답을 기다림.    message, clientAddress = serverSocket.recvfrom(2048) # serverSocket에서 응답 기다리기    print(&amp;#34;message incoming&amp;#34;)    modifiedMessage = message.decode().upper() # 응답 메시지의 내용 processing    serverSocket.sendto(modifiedMessage.encode(), clientAddress) # Client 측으로 되돌리기두 파일을 실행해본 뒤 확인해보자!2.7.2.Socket Programming with TCPUDP와 달리 TCP는 연결 중심 protocol로, 데이터를 주고 받기전에 handshaking을 통해 소켓 간의 TCP 연결을 생성한다.이후에는 UDP와 달리 message에 IP address를 적어주지 않고(port는 여전히 적어줌), TCP 연결이 된 host 간에 의사소통한다.Server가 돌고 있는 중이라면, Client가 해당 sever 주소로 TCP Socket을 열고, 3-way handshaking을 실시한다.이때의 활동은 transport layer에서 진행되며, Application 단에서는 보이지 않는다.Client의 TCP 연결 요청이 server의 welcoming socket에 도착하면, server는 해당 client만을 위한 특별한 socket을 열어주며, 이를 connectionSocket이라 한다.Sever가 Client와 연결을 기다리기 위해 상시 열어놓는 Welcoming socket과 Client에게 할당된 특별한 socket인 Connection Socket이 다름에 유의하여야 한다.그렇게 생성된 TCP 연결을 통해 데이터 신뢰 전송 뿐만 아니라 상호간에 데이터를 주고 받을 수 있게 된다.TCPClient.pyfrom socket import socket, AF_INET, SOCK_STREAMserverName = &amp;#39;localhost&amp;#39;serverPort = 12000clientSocket = socket(AF_INET, SOCK_STREAM) # SOCK_STREAM: TCP socket 사용 설정, 마찬가지로 source port는 os가 자동 설정clientSocket.connect((serverName,serverPort)) # TCP connection 생성. three-way handshaking 생성sentence = &amp;#39;&amp;#39;while sentence != &amp;#34;exit&amp;#34;: # exit input을 받을때까지 TCP 연결 유지    sentence = input(&amp;#39;Input lowercase sentence:&amp;#39;) # input 받기    clientSocket.send(sentence.encode()) # UDP 때와 달리 address를 붙여주지 않고, TCP connection으로 연결     modifiedSentence = clientSocket.recv(1024)# carriage return을 받을때까지 받은 input을 server측에 보냄    print(&amp;#39;From Server:&amp;#39;, modifiedSentence.decode())# 결과값 출력clientSocket.close()# TCP 연결 종료TCPSever.pyfrom socket import socket, AF_INET, SOCK_STREAMserverPort = 12000serverSocket = socket(AF_INET, SOCK_STREAM) # TCP welcome socket 생성serverSocket.bind((&amp;#39;&amp;#39;, serverPort)) # socket address 설정serverSocket.listen(1) # TCP connection 기다리기, 최대 1개의 connection만 가능하게 설정print(&amp;#39;The server is ready to receive&amp;#39;)while True:    print(&amp;#39;waiting for connection&amp;#39;)    connectionSocket, addr = serverSocket.accept() # client가 요청시 Connection socket 생성    print(f&amp;#39;&amp;#123;addr&amp;#125; approachs your server!&amp;#39;)    while True:        sentence = connectionSocket.recv(1024).decode()        if sentence != &amp;#34;exit&amp;#34;: # exit data가 들어올 때까지 실행하기            capitalizedSentence = sentence.upper()            connectionSocket.send(capitalizedSentence.encode())            print(&amp;#34;send data back to client&amp;#34;)        else:            connectionSocket.close()# connectionSocket 종료, serverSocket은 여전히 열려있으므로, 다른 Client의 접근 가능.            print(f&amp;#34;close connection between &amp;#123;addr&amp;#125;&amp;#34;)            break2.8 Summaryaplication layer의 protocol 들인 HTTP, SMTP, DNS, CDN 등에 대해서 배웠고, P2P architecutre와 client-server architecture을 구분해보았다.또한 python socket API를 통해 가벼운 TCP, UDP 서버를 만들어 보았다."
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%203-%EC%A0%84%EB%8B%AC%20%EA%B3%84%EC%B8%B5.html": {
    title: "네트워크 정리-Chap 3-전달 계층",
    date: " Aug 21, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%203-%EC%A0%84%EB%8B%AC%20%EA%B3%84%EC%B8%B5.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 3. 전달 계층(Transport Layer)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/eTransport layer는 application layer와 network layer 사이에 있는 계층으로, 각기 다른 host에 돌고 있는 application 간의 의사소통을 맡고 있다.이번 챕터에서는 transport layer의 기본과 TCP, UDP 프로토콜들의 원리에 대해서 알아볼 것이다.핵심적인 기능인 의사소통 뿐만 아니라 혼잡을 피하기 위한 transmission rate 조절방법도 알아볼 것이다.3.1 Introduction and Transport-Layer ServicesTransport layer는 다른 호스트 내부의 application 간의 논리적 연결을 담당하며, 이 뜻은, 호스트 사이의 여러 router, swtich 등 물리적 연결 요소를 배제하고 연결하게 해준다는 의미이다.아래 fig 3.1과 같이 Transport layer는 end system에만 주로 구현되며, application단의 message를 segment라고 불리우는 packet으로 바꾸거나 segment에서 message를 추출해 의사소통하게 해준다.이때, transport layer header를 추가하거나, 데이터 크기에 따라 여러 segment로 나누고, 추가로 network layer의 datagram으로 encapsulate 되서 전달된다.보통 프로토콜로 UDP, TCP가 주로 쓰이며, 이 둘은 각각 다른 특성을 가진 서비스를 application에게 제공해준다.3.1.1 Relationship Between Transport and Network Layersnetwork layer와 transport layer의 일은 각각 현실로 비유하자면, 집(host) 간의 연결을 해주는 우체국(network layer protocol)과 받은 편지(application message)를 집안에 해당하는 사람(process)에게 나눠주는 형제(transport layer)로 비유한다.이때, 편지 나눠주는 형제들은 우체국이 어떤 경로로, 어떤 절차를 걸쳐 해당 집으로 왔는지 관심없고, 실제로 해당 일을 하는데 필요하지 않다.또한, 형제에 따라 시간과 정성을 들여 정확히 확인하여 편지가 잃어버리지 않게 하거나 잘못 전달되지 않게 하는 형제(TCP)도 있고, 그저 온 편지를 바로 전달만 해주는 형제(UDP)도 있을 것이다, 이는 흔히 프로토콜의 차이로 비교되곤 한다.또한, transport layer가 제공할 수 있는 서비스는 network layer 측에서 제공해주지 않으면 대부분 제한된다.예를 들어, transport 측에서 최대 대역폭을 정해주어도, network layer 측에서 넘겨버리면 의미가 없다.대부분이라 한 이유는, packet 암호화나 전송 신뢰성 보장(reliable transfer) 같이 network 측에서 제공하지 않아도 제공되는 서비스도 있다.3.1.2. Overview of the Transport Layer in the Internet앞서 말했듯이 Transport layer protocol은 크게 TCP(Transmission Control Protocol)와 UDP(User Datagram Protocol)로 나뉘며 둘 중에 하나를 골라 application을 만들거나 socket을 생성한다.참고로, RFC 등지에서는 TCP의 packet을 segment, UDP의 packet을 datagram이라고 표현하는 경우가 많으나, datagram은 network layer 층에서의 packet을 의미하기도 하므로, 우리는 Transport layer의 packet을 segment로 고정해 부르기로 한다.나중에 배울 Network layer의 IP는 unreliable, best-effort deliver service이는 전송 결과와 데이터 정합성을 보장하지 않음을 의미한다.이때 UDP 또한, IP 처럼 unreliable service이므로, UDP와 IP가 결합된 서비스는 데이터의 전송이 보장되지 못하지만, TCP는 보장이 된다.UDP는 multiplexing, demultiplexing 그리고, error checking 서비스 만을 제공하며, TCP는 거기에 추가로 reliable data transfer와 congestion control(혼잡 제어) 서비스를 추가로 제공한다.congestion control(혼잡 제어)는 개개인 process나 host의 이득이 아닌, 전체 network의 공평하고 효율적인 네트워크 자원 공유를 위한 것이며, 이를 위해 TCP의 경우 전송이 제한되곤 하지만, UDP는 이러한 제한으로 부터 자유롭다.우리는 제공하는 서비스들의 일반적인 버전과 TCP가 제공하는 버전을 차례대로 설명하여 비교할 것이다.예를 들어, 우리는 먼저 일반적인 reliable data transfer를 설명하고, TCP가 적용한 reliable data tranfer를 비교할 것이다.3.2 Multiplexing and Demultiplexingtransport layer multiplexing, demultiplexing은 Network layer의 host-to-host 통신을 연장하여 process-to-process 통신으로 확장해준다.이는 마치 집에 도착한 뭉탱이의 편지를 각자 적절한 사람에게 나눠주는 것과 같다.이때, transport layer가 직접 Application에게 주는 것이 아니라, socket(위의 방문)이라는 application 계층과 transport layer 계층 사이에 있는 인터페이스를 통해서 준다.각 socket은 한 host에 여러개가 존재할 수 있으므로, socket마다 고유한 식별자(Port)를 가지며, socket 또한 UDP, TCP 등의 사용하는 프로토콜이 존재한다.이때 뭉탱이(datagrams)의 편지(segment)를 나누어(segment로 변환) 편지 받는이(segment의 source port number field)를 확인한 뒤 적절한 사람(application)의 방문(socket) 앞에 놔주는 것이 demultiplexing이며, 반대로 여러 사람들이 방문(socket) 앞에 놓아둔 편지를 수거하여 받는 사람(segment의 desination port number field)을 확인한 뒤 우체통(network layer)로 보내는 것을 multiplexing이라고 한다.이때 사람의 종류(application의 protocol, FTP, TELNET, HTTP etc…)는 신경쓰지 않는다.위에서 설명한 socket의 고유한 식별자를 port라고 하며, segment에 적혀있는 송신자와 수신자의 port 번호를 각각 source port number field, destination port number field라고 한다.각 port는 16 bit, 0~65535까지의 범위를 가지며, 0~1023까지의 1024개를 well-known port numbers라고 해서, 필수적인 application들이 미리 점유하고 있으므로 사용하면 안된다.(예를 들어 HTTP : 80, FTP :21, SMTP :25) 최신 버전은 RFC 3232에서 확인.이외의 port 번호는 application 개발시 자유롭게 활용 가능하다.위에서 설명한 내용과 그림은 UDP를 기준으로 한 multiplexing/demultiplexing이며, TCP의 경우 조금 다르다.Connectionless Multiplexing and DemultiplexingclientSocket = socket(AF_INET, SOCK_DGRAM)위는 UDP socket을 생성하는 python 코드이며, 이때 socket의 port 번호는 1024~65535 중 알아서 생성된다.clientSocket.bind((&amp;#39; &amp;#39;, 19157))만약 특정 주소로, 특정 port 번호로 바꾸고 싶다면 위와 같은 bind 함수를 사용하면 된다.보통 개발시에, applicatino이 server side인 경우 특정 port를 지정하고, client side의 경우 자동으로 할당되도록 만든다.UDP socket은 desination IP address와 destination port number의 값에 따라 구별되며, 이에 따라, UDP segment가 source IP address와 source port number가 달라도 앞의 두 값이 같으면 같은 socket으로 들어가게 된다.source IP address와 source port number의 경우, 데이터를 수신받은 socket이 segement를 돌려주는 용도로 사용한다.python 코드의 경우 recvfrom() 코드를 이용해 source address를 받은 뒤, 응답을 돌려줄 수 있다, (Chap 2.7.UDPServer.py 참조)Connection-Oriented Multiplexing and DemultiplexingTCP Socket과 UDP Socket의 차이점 중 하나는 , TCP socket은 desination IP address와 destination port number, source IP address와 source port number, 총 4개의 값에 따라 구별된다는 점이다.TCP segment가 도착한다면 위의 4개 필드에 따라 적절한 TCP socket으로 보내지게 된다.(단, 처음으로 도착한 handshaking segment의 경우, source IP address와 source port number가 달라도 TCP connection을 수립하기 위해 어플리케이션 마다 할당된 동일한 TCP Welcome Socket에 들어가게 된다.)즉, TCP의 경우 desination IP address와 destination port number가 같은 TCP socket이 여러개 있을 수 있다.자세한 내용은 2.7 절의 TCPServer.py 부분을 참고하자.clientSocket = socket(AF_INET, SOCK_STREAM) # client측 TCP 소켓 생성clientSocket.connect((serverName,12000)) # 서버측과 TCP 연결connectionSocket, addr = serverSocket.accept() # 서버측에서 TCP 연결이 생성될때 까지 기다리는 함수TCP 서버는 이러한 TCP 소켓에 의한 연결을 address에 따라 구분하여 아래 그림처럼, 여러개 동시에 생성하고 유지할 수 있다.이러한 port 번호는 system 관리자가 켜져있는 application의 종류를 확인하는데도, 공격자가 보안 취약점이 있는 application을 공격하는데도 유용하다.nmap이나 port sanner같은 program으로 이러한 port들의 상태를 알 수 있고 이는 3.5.6장에 더 자세히 다룰 것이다.Web Servers and TCP실제로는, 위 그림과 달리 최신 고성능 web server들은 하나의 프로세스에 여러 thread를 생성하여 각각 연결마다 socket을 형성하므로 실제 process 마다 socket이 할당되지는 않는다.만약 HTTP 연결이 non-persistent HTTP 라면, 매 request-response 마다 새로 connection이 생성되므로 socket이 형성되고 사라질 것이다. 이는 OS의 기술 덕분에 악영향이 반감되긴 하지만, web Server에 성능상 안좋은 영향을 끼친다.3.3 Connectionless Transport: UDP만약 TCP의 여러 기능을 사용하지 않는 새로운 프로토콜을 사용하고 싶거나, 새로 프로토콜을 만들어보고 싶다면 UDP를 사용하면 된다.UDP는 앞서 설명했듯이 reliable transfer와 congestion control 기능 등이 존재하지 않고, multiplexing\\demultiplexing, 오류 검사 기능 정도만 있는 대신, 다음과 같은 장점을 가지고 있다.  혼잡 제어 무시, 절차, 비연결성 통신으로 인한 비교적 가벼운 오버헤드와 빠른 전송 속도, 정보손실이 일어나도 괜찮되, 빠른 반응이 필요한 서비스에 UDP가 많이 이용된다.  연결 설립 과정 없음(no connection estalishment) TCP의 3 handshaking 같은 과정이 없어 DNS나 QUIC 같은 속도를 중요시 여기는 프로토콜을 만드는 데 UDP가 기반이 되곤 한다.  연결 상태 없음(No connection state), TCP는 해당 연결 상태인 buffer, congestion control parameters, sequence, ack number 등을 저장해둬야한다. UDP는 해당 하지 않으므로 저장할 정보가 적어 비교적 더욱 많은 연결을 세울 수 있다.  packet header 크기가 작음. TCP 는 20 byte의 header를 가지고 UDP는 8 byte의 header를 가진다.많은 중요한 정보가 TCP로 만들어지지만, SNMP(network management 정보) 같이 네트워크가 혼잡한 상황에서도 보낼 수 있게끔 필요한 경우에도 UDP가 사용된다.많은 multimedia application들이 특유의 손실 가능한 특성과, real time성 때문에, UDP로 만들어지지만, UDP를 보안상 문제삼아 block 시켜놓은 방화벽 대비나 네트워크 성능의 증가로 TCP 또한 충분하게 사용되고 있다.또한, UDP는 네트워크 혼잡 제어를 하지 않으므로, 과용되면, 커다란 UDP 패킷 손실과 다른 TCP 연결의 제한을 가져올 수 있으며, 이를 막기 위해 UDP 뿐만 아니라 여러 Protocol들을 공통적으로 congestion control을 강제할 수 있는 방법이 연구되고 있다.물론 UDP 또한, reliable transfer 등의 서비스를 넣을 수 있는 방법이 있는데, 먼저 첫번째는 QUIC 처럼, UDP 기반의 새로운 프로토콜을 만드는 것이고, 두번째는 application layer 단에서 그러한 서비스 기능을 넣는 것이다.이러한 개발 과정은 아주 복잡하고 힘든 과정이겠지만, 데이터의 신뢰성을 보장하면서 congestion control을 피해 이기적으로 이득을 볼 수 있다.3.3.1 UDP Segment StructureRFC 768에 정해져 있는 UDP segement 구조는 아래와 같다.Application message를 payload로 가지고 있는데, 예를 들어 DNS는 query message나 response message가 될 것이며, streaming audio application에서는 audio smaple이 될 것이다.헤더는 오직 4개의 field로 이루어져 있으며, 하나에 2 byte, 총 헤더 크기 8byte(32bit)를 차지한다.  source port, dest port field는 host에서 segment가 알맞은 socket으로 들어가게 해준다.  Length field는 헤더와 데이터를 포함한 전체 UDP segement의 크기를 적어주어, 다음 메시지와의 경계를 알려준다.  Checksum field는 segment가 전송 도중에 오류가 생겨 변했는지 검사해준다. IP header에도 비슷한 역할을 하는 필드가 있다.3.3.2 UDP Checksumchecksum은 UDP segment의 어느 부분이 noise나 router의 오류 등으로 변했는지 검사할 수 있다.송신측 UDP에서 모든 16bit 단어들의 합에 자릿수를 넘어가는 값은 첫째 자리에 다시 더해주고,(wrap around) 나온 결과값의 1의 보수 값을 구한다.이렇게 구한 값을 segment header 필드에 저장하여 checksum으로 이용한다.\\((1)\\left\\{\\begin{matrix}0110011001100000\\\\0101010101010101\\\\1000111100001100\\\\\\end{matrix}\\right.\\ \\ \\ \\ \\ \\(2)\\left\\{\\begin{matrix}0110011001100000\\\\0101010101010101\\\\\\overline{1011101110110101}\\\\\\end{matrix}\\right.\\ \\ \\ \\ \\ \\(3)\\left\\{\\begin{matrix}1011101110110101\\\\1000111100001100\\\\\\overline{0100101011000010}\\\\\\end{matrix}\\right.\\)(1) 이렇게 3 단어가 포함됬다고 가정하고(2) 1번째와 2번째 단어 합산(3) 2번째와 3번째 단어 합산한 결과값이 checksum이다. (이때, 자릿수가 넘어가는 값이 첫째 자리에 더해졌다.)1의 보수는 모든 자리의 0은 1로, 1은 0으로 바꾼 것으로, 최종 결과로 1011010100111101이 checksum이 된다.receiver 측에서는 이러한 checksum과 나머지 16bit word를 더하면 자연스럽게 1111111111111111이 되며, 이는 오류가 없다는 의미이다.만약 다른 값이 나온다면 segment 어딘가에 오류가 있었다는 뜻이다.Ethernet protocol 같은 다른 layer protocol들 또한 오류 검사를 제공함에도 불구하고, 중복하여 해당 서비스를 제공하는 이유는,      모든 장비가 모든 layer를 구현하는 것도 아니며, 또 구현된 layer의 protocol이 오류 검사를 제공하지 않을 수도 있으며,          예를 들어, IP 프로토콜 또한 체크섬을 가지고 있지만, TCP는 ATM 같은 IP 이외의 네트워크 계층 위에 구현될 수 도 있다.      애시당초 IP 데이터그램 헤더의 인터넷 체크섬 필드는 IP 헤더값들의 오류만 해결해주고 데이터부분은 포함하지 않는다.            link layer 측에서 오류 검사(link-by-link error)를 완료해도 router에 저장되어 있는 중(in-memory error)에 오류가 생겨날 수 도 있다.    이러한 중복 서비스 제공은 system 설계시의 end-end principle 또한 지킨다.  end-end principle은 저레벨에서 기능을 제공하는 것이 좀 더 고레벨에서 제공할 때보다 더욱 비용이 적게 들거나 안들 수도 있다는 것이다.다만, UDP는 에러를 체크한 뒤, 잘못될 경우 수정이나 재전송 요청 없이 버려버리거나, 주의를 표하고 그대로 넘긴다.3.4 Principles of Reliable Data TransferReliable data transfer은 transport layer 뿐만 아니라 link layer와 application layer에도 구현되곤 하므로, TCP 만의 방법 뿐만 아니라 일반적인 방법을 배워보자.신뢰할 수 있는 channel이 존재한다면, 전송되는 데이터는 bit가 바뀌거나 잃어버리거나, 순서가 바뀌는 경우가 없으며, 이는 신뢰적 데이터 전송 프로토콜(reliable data transfer protocol)인 TCP가 application에 지원하는 기능이다.문제는 RDT(Reliable Data Transfer)를 구현해야하는 layer보다 저레벨의 layer가 비신뢰적 데이터 전송 기반이면 힘들어지며, 심지어 그러한 layer들이 구현된 router와 link-layer 장비를 네트워크 상 여러번 거쳐야 한다면 더욱 힘들다. (ex) TCP layer 밑의 IP layer는 데이터 전송을 신뢰할 수 없다.)그러한 신뢰할 수 없는 channel 들을 상단의 그림처럼 하나의 Unreliable channel로 놓고 우리는 그 양끝단 송신측과 수신측에 적용될 신뢰 데이터 전송 프로토콜을 생각해봐야 한다.이때 그림처럼 통신은 일방향 통신(unidirectional data transfer)라고 가정하고,송신 Application측이 데이터를 reliable data transfer protocol(RDT)로 보낼 때의 기능을 rdt_send(),수신측 RDT가 Unreliable data transfer(UDT)로 부터 데이터를 주고 받을 때의 기능을 rdt_rcv(),송신측 RDT측이 UDT측과 데이터를 주고 받는 부분을 udt_send(),마지막으로 수신측 RDT가 Application에게 데이터를 주는 부분을 deliver_data()라고 가정하겠다.3.4.1. Building a Reliable Data Transfer ProtocolReliable Data Transfer over a Perfectly Reliable Channel: rdt1.0먼저 UDT측의 데이터 전송이 완전히 신뢰할 수 있는 경우를  FSM(finite-state machine)을 통해서 살펴보고, 이를 rdt 1.0 이라고 하겠다.파란 화살표는 transition 화살표로, 한 상태에서 다른 상태로 이동함을 의미한다. (현재는 상태가 하나밖에 없으므로 자기자신으로만 이동한다.)파란화살표 옆의 수평선의 윗 부분은 해당 transition을 일으키는 이벤트를 의미한다.수평선 아랫부분은 이벤트가 일어났을 때 생기는 행동들을 의미한다.$\\Lambda$(람다)가 있는 경우에는 아무런 활동이나 이벤트가 일어나지 않았음을 의미한다.점선 화살표는 FSM의 초기 상태를 의미한다.먼저 송신측 상위 layer에서 데이터를 받아 데이터가 들어있는 패킷을 만들고(packet=make_pkt(data)), 해당 패킷을 UDT 채널로 보내(udt_send(packet))는 rdt_send(data)가 실행된다.  이는 실제로는 상위 layer에서 실행하는 함수 일 것이다.그 다음 수신측 RDT가 UDT로부터 해당 packet을 받고 data를 추출해내서(extract(packet, data)) 상위 layer로 보내(deliver_data(data))는 rdt_rcv(packet)이 실행된다.간단한 예제이며, data나 packet의 단위가 변하거나, packet이 흐름이 수신측에서 송신측으로 역전되는 경우가 없다.또한, 통신간에 문제가 없으므로, 수신측의 특별한 재요청이 없고, 송신과 수신의 처리속도가 같다고 가정하여, 어느 한쪽이 기다릴 필요도 없다.Reliable Data Transfer over a Channel with Bit Errors: rdt2.0이번에는 전송, 전파, buffer 간에 bit error가 생길 수 있다고 가정하되, packet의 순서는 바뀌지 않는다고 가정해보자.사람들은 대화할 때, 잘듣지못한 부분은 다시 말해달라고 요청하고, 잘들었을 경우 맞장구 쳐준다.이러한 부분을 통신에 활용하면, 통신 시, positive acknowledgment(OK), negative acknowledgment(다시 말해 줘)가 필요하다는 것을 알 수 있다.이러한 재전송 기반 프로토콜을 ARQ(Automatic Repeat reQuest) protocol 이라고 한다.이러한 ARQ의 필수적인 3가지 기능은 다음과 같다.  에러 탐지 : UDP의 checksum과 같이, 에러를 탐지 및 수정 하기 위해 사용된다. 보통 추가적인 bit를 할당해 에러를 체크한다.  수신자 피드백 : 1 bit(ack or nak)짜리 packet을 송신자 측으로 보내 받은 message의 상태를 보고, 이를 통해 수신자측이 메시지를 잘 받았나 확인 가능  재전송 : 위의 피드백을 받은 송신자가 packet을 재전송 여부를 파악함.위의 fig 3.10은 rdt2.0 재전송을 보여주고 있다.송신 측에 상태가 하나늘어 2개가 되었는데,      상위 layer로 부터 데이터를 기다리는 상태와        packet을 보낸 뒤 수신 측으로부터 피드백을 기다리는 상태  이며, rdt_send(data) 이벤트가 생기면 1에서 2로 transition되며, 이때, 데이터와 checksum이 존재하는 패킷을 형성하고 udt_send(sndpkt)로 UDT로 packet을 보낸다.이후 b로 전환 된 뒤, 송신자는 ACK나 NAK packet을 기다리게 되고,  ACK가 오게되면, sender는 packet을 제대로 보낸 것으로 확인하고 1 상태로 다시 전환된다.  NAK가 오게되면, udt_send(sndpkt)가 다시 실행되고 2 상태로 다시 전환된다.          주의할 점이, 2로 전환되면 1로 전활될 때까지(ack를 받을때까지) 상위 layer에서 데이터를 줘도 받지 못한다. 이러한 protocol을 stop-and-wait protocol이라 한다.      수신측은 rdt2.0에도 여전히 상태가 1개이며, 수신자는 packet을 받고, 상태를 확인하고(rdt_rcv(rcvpkt) &amp;&amp; corrupt(rcvpkt)) ACK나 NAK를 돌려준다.rdt2.0의 단점은 ACK, NACK packet 또한 손실이나 변질될 수 있다는 점이며, 이를 막기위해  다시 ACK, NACK를 보내라고 요청 : 하지만 이 방법은 그렇게 다시 보내라는 요청 또한 망가질 수 도 있으며, 그런 요청 또한 다시 보내다 보면, 어떤 요청을 다시 보내야 될지 모른다.  ACK, NACK에 checksum을 아주 많이 넣어주어 오류 검사 뿐만 아니라 오류 수정도 가능하게 한다 : 하지만 packet이 아예 손실되는 경우는 방지할 수 없다.  망가진 ACK, NACK가 오거나 아예 안오기 시작할 때 무조건 다시 data packet 보내기 : 그렇게 다시 온 data packet이 이전 packet의 중복인지, 아니면 새로운 packet인지 수신자 측에서 알 수 없다.위와 같은 단점들이 있고, 이러한 단점을 해결하기 위한 방법으로 패킷에 sequence number를 추가해줄 수 있다.만약, sender가 이전에 보냈던 packet을 다시 보냈다면, sequence number가 이전에 받았던 packet과 같거나 작을 것이고,sender가 새로운 packet을 보냈다면, sequence number가 새로운 진행된 sequence number를 가지고 있을 것이다.stop-and-wait 방식인 현재는 아직 ACK와 NACK에 sequence number가 필요없다(마지막에 보낸 packet의 ACK일테니까!)다음은 sequence number 기능을 첨가한 rdt2.1 버전의 figure다.rdt2.1에서는 sequence number가 추가되어 sender, receiver 전부 상태가 2배로 늘어나고 복잡해졌다.여기서 0번 packet은 보내야될 packet, 1번 packet은 앞으로 보낼 packet을 의미한다.rdt 2.2에서는 rdt2.1과 달리 NACK packet가 존재하지 않는다.대신에 이때는 수신자측이 제대로 받았다는 의미로 ACK packet을 보내고, packet을 다시 보내라는 의미로 마지막에 제대로 받은 packet에 대한 ACK를 다시 보낼 것이다.만약, 서로 다른 packet 둘을 보낸 송신자측이 똑같은 sequence number의 ACK packet(duplicated ACKs)을 둘 받았다면, 중복된 ACK packet의 대상 packet은 제대로 보내졌고 나머지 하나는 아니라는 것을 알 수 있다.즉, rdt2.2 부터는 ACK packet에도 sequence number가 추가되며, 이는 해당 ACK의 대상이 되는 packet의 Sequence number이다.Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0rdt3.0에서는 packet의 완전한 손실 또한 추가로 고려한다.해결해야할 문제는  packet 손실을 어떻게 알아챌 것인가?  packet 손실을 어떻게 해결할 것인가?이다.2번째 문제는 우리가 이미 도입한 기능들(checksum, sequence numbers, ACK packets, retransmissions) 등으로 해결 가능 하지만1번째 문제는 Countdown timer를 만들어, data packet이나 ack packet이 보내질 때, 시간을 설정하고, 일정 시간까지 response가 돌아오지 않으면, 재전송하는 방식으로 해결한다.이때의 응답 한계 시간을 설정하는 것이 중요하며, RTT와 기타 buffer 시간을 고려해서 설정해야 하는데, 너무 길면 에러 수정까지 오래 걸려 설정하기 힘들다.rdt3.0은 packet의 sequence number가 0 아니면 1 두가지로 구분되기 때문에 alternating-bit protoocol 이라고도 한다.3.4.2 Pipelined Reliable Data Transfer Protocols위의 rdt3.0은 기능적으로 문제없지만, stop-and-wait 방식 특유의 패킷을 하나씩 보내기에는 너무 느리다는 성능적 문제가 있다.예를 들어 미국 서부와 동부 사이의 RTT를 30ms, 8000bit짜리 packet을 처리하는데 8 micro초가 걸린다면\\[d_{trans}=\\frac{L}{R}=\\frac{8000\\ bits}{10^9\\ bits/sec}=8\\ microseconds\\]ACK packet이 다시도착하는 시간과 동시에 다음 packet을 보내는데 걸리는 시간은 30.008ms 이후가 되고,이때 실제로 sender가 기다리는 시간과 일하는 시간의 비율은 다음과 같다.\\(U_{sender}=\\frac{L/R}{RTT+L/R}=\\frac{.008}{30.008}=0.00027\\)sender 측이 일하는 시간보다 대부분의 시간을 Ack packet을 기다리는데 쓰게 된다.심지어 이 시간은 호스트 사이의 네트워크에 속한 장비들의 processing delay와 queueing delay를 제외한 값이므로, 실제 1Gbps link에서 옮길 수 있는 data는 200kbps도 채 안될것이다.이러한 비효율을 막기 위해 Ack packet이 올때까지 기다리지 않고 특정 갯수의 packet들을 한꺼번에 보낸뒤 한꺼번에 받아내는 Pipelining(fig 3.18)이 가능하다Pipelining은 다음과 같은 변화를 가져온다.  ack Packet이 아직 오지 않았더라도 packet을 생성하면서 sequence number를 줄 수 있어야한다.  아직 Ack Packet이 오지않은 packet들은 재전송해야할 수 있으므로, 버리지않고 Buffer에 저장해두어야 한다.  준비해야하는 sequence number 범위나 buffering 방법들이 데이터 전송 결과(실패, 손실, 딜레이 등)에 따라 바뀌어야 한다.          이 방법의 기본적인 접근 방법은 Go-Back-N과 selective repeat가 존재한다.      3.4.3 Go-Back-N (GBN)Go-Back-N(GBN) protocol은 sender 측이 acknowledgment를 기다리지 않고도 최대 지정한 N개의 packet을 보낼 수 있게 해준다.위의 그림은 GBN에서의 seq num의 범위에 대한 그림이다.base는 아직 확인되지않은 가장 오래된 packet의 seq num이고, nextseqnum은 아직 사용하지 않은 가장 작은 seqnum이다.범위 [0, base-1]까지는 이미 확인이 끝난 보낸 packet들이고, [base, nextseqnum-1]은 전송했지만 아직 ack되지 않은 packet들이다.[nextseqnum, base+N-1]은 data가  상위 layer에서 현재로 도착 즉시 사용할 수 있는 seq number이다. 마지막으로 base +N 부터 끝까지 pipelining 과정 중에 아직 ack되지 않은 packet이 ack 되기 전까지 사용 불가능한 seq num이다.전송과 ack 도착이 진행되면서 window size N은 크기는 그대로 인채 점점 뒤로 옮겨가게 된다.이러한 GBN protocol을 sliding-window protocol 이라고도 한다.패킷을 무제한으로 보낼 수 없고 윈도우 사이즈 N으로 제한을 둔 이유는 네트워크의 혼잡을 제어하기 위해서이다.seq num header가 packet에서 k bit 크기를 차지한다면, 최대 0~$2^k-1$까지의 seq num을 가질 수 있고 이는 많은 문제를 야기시킨다.아래의 fig 3.20과 fig 3.21은 rdt 3.0의 동작에 대한 FSM이다.GBN sender에게는 3개의 일이 생겨나는데  상위 계층에서의 발동, upper layer에서 data를 보내면 먼저, window을 살펴보고, window에 남은 seq num이 있다면, packet을 생성한 뒤, 보내고, 만약 더이상 남은 seq num이 없다면, ack packet이 돌아와 window에 자리가 남을 때까지, buffer에 data를 저장해 두거나, upper layer와 연동(세마포어, flag)하여 자리가 남을 때만 data를 보내도록 설정한다.  Ack packet 받아들이기,  cumulative acknowledgment: seq N 번째 ACK는 이전 packet들의 ACK를 대신 한다. 만약 N 번째 packet 이전의seq  N-m 번째 패킷이 전송 실패였다면, seq N-(m-1)번 ACK까지만 왔을 것이므로,  seq N번째 ACK가 왔다는 건, 그 이전의 packet은 제대로 받았다는 의미이다.  타임아웃 이벤트,Go-Back-N 프로토콜에서는 여러 packet을 보내지만 Timer는 가장 오래된 unacked packet 하나에만 설정된다. 만약 Timeout이 된다면, sender 측은 모든 unacked packet들을 버리고 다시 재전송한다, 만약 가장 오래된 unacked packet이 ack된다면, 새롭게 찾은 가장 오래된 unacked packet에 Timer를 처음부터 다시 설정한다. 만약 더이상 unacked된 packet이 없다면 타이머는 멈춘다.또한 GBN에서는 수신자 측에서 seq 순서가 이상하게 먼저 도착하거나 늦게 도착한 packet들은 정상적인 packet이여도 버리는데, 이를 통해 수신 측은 순서에 어긋난 패킷은 다 버리고, 다음 seq num(expectedseqnum)만 기억하면 되는 등, buffer 관리를 단순하게 할 수 있고, 송신측은 window의 범위와 nextseqnum등의 변수를 관리해야 한다.아래 그림처럼, window size에 따라 seq num 증가와 그에 따른 sender 측의 송신이 제한되며, 만약 이전 packet이 성공적으로 송신되었다면, 그때부터 window 범위를 높은 번호로 옮기면서 seq num을 늘릴 수 있다.3.4.4. Selective Repeat (SR)위에서 설명했던 Go-Back-N에서 순서에 어긋난 packet은 해당 packet seq num 이상의 packet들은 정상 전송하여도 전부 버리는 것을 막기 위해 Selective Repeat (SR) 방법을 이용할 수 있다.해당 TCP receiver가 Selectective Acknowledgment를 사용할 수 있으면 TCP option header 마지막 자리쯤 8bit로 sack permitted (0100 xxxx) 표시가 되있음SR 수신자는 마치 송신자처럼 직접 기록하는 윈도우를 주시하며, 기존에 버리던 순서가 어긋난 packet들을 전부 buffer에 저장해두고, 순서에 맞지않은 packet들이 끼워질때까지 기다렸다가 한꺼번에 상위 계층로 보낸다.  하지만 이때, 수신자와 송신자의 윈도우 범우 크기는 같지만, 가르키고 있는 범위는 다를 수 있다. fig 3.23 참조title: Fig3.24.SR 송신자 이벤트와 활동      상위 계층에서 데이터 수신, GBN에서의 활동과 동일함.        타임 아웃, GBN과 다르게 이번에는 각 packet 별로 논리적으로 생성된 timer가 실행되며, 타임아웃된 해당 packet만 재전송된다.        ACK 수신, ACK packet을 받으면 해당 ACK packet이 알려준 seq number의 packet만 성공한 것으로 윈도우에 기록, 만약 send_base(윈도우의 가장 처음에 있는 packet)이 ack되었다면, 윈도우 범위를 다음 ack안된 packet으로 옮기고, 새로 생긴 window 범위 내에서 아직 안보낸 packet을 전송한다.  title: Fig.3.25.SR 수신자 이벤트와 활동  윈도우 범위 [rcv_base, rcv_base+N-1](즉, 윈도우 범위 전체)에 존재하는 seq num를 가진 packet을 받고 각각마다 ACK 되돌려주기.만약 해당 packet이 순서에 맞지않는다면 Ack는 보내되, 상위 계층으로 보내지 않고 buffer해둔다.만약 해당 packet의 seq num이 rcv_base(즉, 받아야했던 순서의 packet)이라면 해당 seq num에 연속된 숫자를 가진 buffer된 packet들을 순차적으로 상위 계층에 보내고, receiver window를 보낸 만큼(=buffer 되지않은 seq num까지) 윈도우를 옮긴다.\t- 예를 들어, packet 2가 아직 도착하지 않았고 3,4,5가 이미 도착해 buffer 된 상태였다면, 2가 도착하면, 2,3,4,5는 상위 계층으로, window는 seq num 6을 시작 지점으로 바꿀 것이다.  [rcv_base-N, rcv_base-1] 범위(=윈도우 사이즈 만큼 앞으로 이동한 범위의 이미 ack했던 seq num)의 packet이 오면, 이전에 ACK를 보냈어도 ACK를 또 다시 보내줘야한다.          왜냐하면, 송신자의 윈도우 입장에서는 해당 ack packet이 손실, 변형 등의 이유로 받지 못해, Timeout되어 다시 보낸 것일 수도 있으므로, 다시 확인시켜야한다.        그 외의 packet은 그냥 무시한다.하지만 이때, window size의 크기 제한으로 인해 문제가 생길 수 있다.예를 들어 아래 fig3.27의 시나리오를 살펴보면,(a)      sender는 0, 1, 2 번째 패킷을 보냈고 잘 도착했지만, ACK가 손실되어 ack를 하지 못했다.        하지만 receiver는 이미 0, 1, 2 패킷을 받고 윈도우 범위를 3,0,1로 옮겼다.          여기서는 header의 sequence number bit의 한계로 0~3까지의 수가 한계이므로, 넘어가면 0으로 돌아오게 된다.(일순한다 표현하겠다.)            하지만 sender의 0번은 ack를 받지 않았으므로 timeout되면 다시 seq num 0의 packet을 보내게된다.        receivier는 이렇게 새롭게 도착한 0번 seq num packet을 새로 윈도우를 옮겨서 생긴 0번 packet이라고 착각한다.  즉, sender의 0번 packet과 receiver의 0번 packet이 달라지는 문제가 생긴다.이러한 문제를 해결하기 위해 window size 크기와 sequence number 범위를 다르게 하는 방법을 사용할 수 있다.window size 크기를 sequence number 범위의 절반 이하로 설정하면 SR에서 이 문제를 해결할 수 있다.수신자가 현재 rcv base에서 윈도우 사이즈 크기 이전 범위 미만의 packet은 무시하도록 설정해놨기 때문에 해당 문제를 막을 수 있다.(fig 3.25 3번)예를 들어 위의 (a) 시나리오의 윈도우 사이즈를 3, seq number 범위를 6으로 늘릴 경우, recevier가 필요한 새로운 packet은 4,5,6이고, 받은 packet은 0번일테니, 그저 ack를 다시 한번 보내주면 된다.아래는 지금까지 배운 RDT의 요약이다.            메커니즘      사용                  Checksum      전송된 packet의 오류 검사에 활용              Timer      Time out하여 packet을 재전송하는데 사용, ACK packet, 또는 데이터 packet가 UDT 내부에서 사라졌을 수 있으므로, 다시 보내 확인한다.              Sequence number      packet에 순차적인 번호를 부여하는데 사용, 이 순차적 번호를 이용해 중간에 없어진 번호는 손실된 packet이라고 추측하고, 중복된 packet은 버리게 할 수 있다.              Acknowledgement      sender에게 packet이 잘 도착했다고 응답하는데 사용, ACK packet에는 응답 하려는 seq number가 적혀있으며, 프로토콜에 따라 cumulative ACK, individual ACK일 수 있다.              Negative acknowledgement      sender에게 packet이 도착하지 않았다고 응답하는데 사용, 보통 도착해야했던 seq number를 적어놓는다.              Window, pipelining      여러 packet을 ack가 확인되지 않아도 동시에 보내게 하여 하나 하나 보내는 stop-and-wait 방식에 비해 성능을 늘릴 수 있다. window size는 sender와 receiver의 역량, buffer 사이즈, 네트워크 혼잡 상황 등에 따라 정한다.      이러한 packet reorder 문제는 모든 packet이 순차적으로 도착하는 하나의 물리적 도선의 연결에서는 일어날 수 없다.하지만 실제로는 packet들이 여러 host간의 연결과 buffering을 거쳐 도착하므로, 실제로 일어날 수 있다.예를 들어 sender 측에서 3번 packet(3-가)을 보내고 ack (3-A)를 기다리는데, 타임 아웃 때까지 도착하지 않아, 3번 packet(3-가-dup)을 결국 재전송하였고, 결국 ack(3-B)를 받아냈다고 치자.그후 10분 뒤, 다시 sequence number가 일순하여 3번 packet(3-나)을 보냈는데, 이후, 손실된 줄 알았던 ack(3-A)가 10분만에 도착하면, 3-나 packet은 receiver가 ack를 보내지 않아도 ack되게 된다.실무에서는 일순 전에 해당 seq num의 관련 packet(3-가, 3-가-dup, 3-A, 3-B)가 네트워크에 존재하지 않는다고 확실할 때, 일순된 seq num을 사용(3-나)할 수 있도록 제한하여 해결한다.바로, packet을 보낼 때, packet TTL(time to live, packet의 최대 수명 시간)을 정해서 보낸 뒤, 이를 넘은 packet은 버려지도록 설정하는 것이다.예시를 들면 3-가  packet를 보낸지 TTL 시간만큼 시간이 지난다면, 3-나 packet를 보내도 3-가 관련 packet은 전부 없어져있을 것이다.TCP의 고성능 네트워크에서는 TTL을 3분으로 제한하였고, 이 방법이 packet reordering 문제를 해결한다고 [sunshine 1978]에서 증명하였다.3.5 Connection-Oriented Transport :TCP연결 중심, 신뢰가능 데이터, transport layer 프로토콜인 TCP에 대해 알아보자 (RFC 793, RFC 1122, RFC 2018, RFC 5681)3.5.1 The TCP ConnectionTCP의 연결은 circuit switching의 TDM이나 FDM을 의미하는 것이 아니라, 두 host 간에 연결 상태를 저장하는 것을 의미한다.TCP는 end-to-end 통신이므로, host 사이의 network에는 아무런 상태가 저장되지 않는다.TCP는 양측이 서로 동시에 통신 가능한 full-duplex service이며, 1 대 1 대화인 point-to-point이다,  한명이 여러명에게 송신하는 multicasting은 불가능하다먼저, client process 측에서 server process 측으로 TCP 연결을 요청하면서 시작되며 코드는 다음과 같다.clientSocket.connect((serverName, serverPort))이때, client 측에서 한번, server 측에서 한번, 마지막으로 client 측에서 한번 더 총 3번의 특별한 segment를 보내게 된다.처음 두번은 payload가 없으며, 3번째에는 data를 포함하고 있다.이 과정을 three-way handshake라고 한다.TCP 연결이 생성되면 두 연결 사이에는 버퍼가 형성되며, 원하는 시간에 버퍼에 application측의 data를 저장하거나 application 측에 전달하면서 상위 layer와 통신한다.      버퍼는 sender와 receiver 양측에 전부 존재하며, congestion control에 활용되기도 한다.        이때 통신하는 packet을 TCP segment라고 하며, IP datagram에 encapsulate 된다.  이때 최대 segment 크기 제한값을 MSS(Maximum Segment Size)라고 부르며, 이 값은 host 간의 Link-layer 계층을 지나칠 수 있는 한계 크기(Maximum Transmission Unit, MTU)가 TCP/IP header(보통 40 bytes) + segment 크기에 맞게 정한다.      Ethernet과 PPP의 MTU는 1500bytes 지만 TCP와 IP 헤더크기가 20+20bytes 이므로 보통 1460bytes로 MSS를 잡는다.        네트워크상 지나치는 모든 Link 계층에서의 MTU 값은 path MTU라고 하며, 이를 기반으로 하는 MSS도 있다.        MSS는 segment에 들어갈 수 있는 application layer data의  최대 크기 값를 의미하지, header가 포함된 TCP segment의 최대 크기를 의미하지 않는다.  3.5.2 TCP Segment StructureTCP segment의 크기가 정해져있으므로 MSS 이상의 대용량 데이터는 여러 segment로 나뉘어 전송하게 된다.하지만 언제나 MSS 크기인 것은 아니고 필요에 따라  (Telnet, ssh)1 byte 까지 줄어들때도 있다.            field name      using      size(bit) total 20 bytes                  source and destination port numbers      multiplexing/demultiplexing      16, 16              Internet checksum      error check      16              sequence number      reliable data transfer service      32              acknowledgement number      reliable data transfer service      32              receive window      flow control, recceiver 측이 받을 수 있는 bytes 수      16              header length      32bit word 기준 TCP header의 길이, 보통 20 bytes      4              options      길이나 존재 여부가 선택적, sender-receiver 간 MSS 조율, window size 변경      optional, variable              flag-ACK      acknowledgement의 값이 정상적인가?      1              flag-RST      connection 수립 시 사용      1              flag-SYN      connection 수립 시 사용      1              flag-FIN      connection 종료 시 사용      1              flag-CWR      혼잡 상황 알림      1              flag-ECE      혼잡 상황 알림      1              flag-PSH      receiver에게 이 segment data를 바로 upper layer로 보내게 함. (실무에서 사용 안함)      1              flag-URG      sender 측 application layer에서 urgent segment로 지정했음을 의미. (실무에서 사용 안함)      1              Unused      사용 안하는 4 bit 값, 나중에 버전이 올라가면 추가하기 위한 빈 공간      4              Urgent data pointer      urgent data 값의 마지막 byte의 pointer 주소값. (실무에서 사용 안함)      16              data      payload, Application layer에서 보내준 데이터값      MSS      Sequence Numbers and Acknowledgment NumbersSequence Numbers과 Acknowledgment Numbers field는 TCP header 중 데이터 신뢰 전송(Reliable Data Transfer) 구현을 위한 가장 중요한 부분이다.TCP는 byte 번호를 기준으로  segment를 구별하지, 일정한 segment별로 나누어 구분하지 않는다.이해를 쉽게하자면,  segment의 sequence 번호는 data의 첫번째 byte-stream 의 번호이다.아래 fig3.30 예시를 들자면, 최대 segment size(MSS)가 1000byte이고, 500000byte의 데이터가 총 500개의 segment로 나뉘어 전송되어야 하는 상황이라면, 첫번째 segment는 sequenc num이 0, 두번째 segemet의 seq num은 1000, 그다음은 2000이 되는 식이다.Acknowledgement number의 경우, 연결상대 host에게 다음에 올것으로 예상되는 byte의 번호를 넣어 ack한다.위의 예시를 다시 들자면, bytes 0~999까지인 첫번째 segment를 받은 상태에서 돌려줘야할 ack segement의 acknowledgment number는 1000이 될 것이다.2번째 segment를 받았다면 다음은 2000을 ack num으로 써야한다.참고로 TCP는 가장 큰 acknowledgment number에 해당하는 ack segment가 오면 그 이하의 bytes 들은 자동으로 ack한걸로 치는 위에서 언급했던 cumulative acknowledgment를 이용한다.만약, acknowledment에 맞지않는 out-of-order(순서에 맞지 않은) bytes를 가진 segment가 먼저 온다면, TCP를 구현하는 개발자 입장에서 2가지를 중 하나의 방법을 고를 수 있게 되어있다.  맞지 않는 순서의 segment는 그냥 버린다          구현이 쉽고 이해가 쉬운 방법        맞지 않는 순서의 segment도 buffer에 포함하고 있다가, 원하던 segment가 도착하면 함께 처리한다.          구현이 어렵지만 성능상 효율적인 방법      또한, sequence number의 시작 번호를 0으로 가정하고 설명했지만, 실제로는 이전 TCP 연결이나 통신의 seq number 오류를 피하기 위해 랜덤으로 정해진다.  이때 sequence number는 양방향 통신 특성상 각각 host에 하나씩 있다.Telnet: A Case Study for Sequence and Acknowledgment NumbersTelnet(RFC854)는 원격 로그인에 사용되는 TCP 사용하는 application layer protocol이다.민감한 정보를 포함하여, data가 암호화되지 않고, eavesdropping attack 같은 공격에도 취약하여 최근에는 SSH protocol에 밀리는 추세이다.Telnet client가 화면상에 글자(character)를 입력하면 매 글자마다 server측에 전달되어 입력되고, 또 그 출력물을 client에게 돌려주는 형식이다.fig 3.31의 예시를 보자면 먼저 글자 C를 입력하면, 랜덤으로 정한 seq 대로 seq와 ack가 정해진 segment가 전달되고, 이 출력물을 되돌려주는 겸 ack를 진행하기 위해 서버측에서 data c와 seq, ack를 포함한 채로 segment를 돌려준다. 이런식으로 ack 정보가 data를 포함한 segment에 겸사겸사 얹혀 가는 것을 piggybacked이라고 한다.그 다음, 세번째로 client 측에서 server측에서 보내준 데이터를 ack하기 위해 다시 segment를 보낸다. 이때 모든 segment들은 sequence number를 포함해야하기 때문에 data payload가 없어도 seq number가 1 증가한채로 전달된다.3.5.3 Round-Trip Time Estimation and TimeoutTCP는 rdt 통신을 위하여 timer를 쓴다.timer의 설정값은 최소 rtt보다 더 커야하지만, 처음 rtt는 어떻게 설정하는가? 모든 segment마다 timer를 설정해야하나? RTT 보다 얼마나 더 커야하는가? 등의 의문점이 있다.이러한 것을 이야기 해볼것이며, 자세한 내용은  [Jacobson 1988], [RFC 6298]에서 볼 수 있다.Estimating the Round-Trip Time먼저 타이머 설정을 위한 RTT는 sampleRTT들에 관한 식을 통해 구하는데, sampleRTT는 전체 segment의 평균값으로 구하지 않고, 대부분은 segment을 이미 한번 보냈지만 현재  ack되지 않은 segment의 RTT를 하나 sampling 해서 구한다. 이때, 한번 재전송한 segment는 보내지 않고, 오직 한번만 보낸 경우에만 샘플링 대상이 된다.(왜냐하면, 재전송된 segement에 대한 ack가 아니라 첫번째 전송했엇던 segment의 늦게 도착한 ack으로 잘못 측정될 수도 있으므로)이렇게 구한 sampleRTT에서 TCP에서 사용하는 EstimatedRTT를 구하며, 이러한 EstimatedRTT 매번 다음과 같은 식으로 갱신된다.\\[EstimatedRTT = (1-\\alpha)\\cdot EstimatedRTT+\\alpha\\cdot SampleRTT\\\\recommended\\ \\alpha = 0.125\\]\\[EsimatedRTT = 0.875\\cdot EstimatedRTT+0.125\\cdot SampleRTT\\]평균 값으로 구하지 않는 이유는, 최신의 네트워크 상태를 반영한 최신 sampleRTT에 좀더 가중치를 주기 위해서이다. 이렇게 최신 가중치를 주는 평균 방법을 exponential weighted moving average(EWMA)라고 한다.이전에 측정했던 SampleRTT의 영향은 매 갱신시마다 줄어들게 된다.아래 figure는 Estimated RTT의 예시이다.DevRTT는 EstimatedRTT와 SampleRTT를 이용한 추가적인 RTT로, 네트워크 상황을 가늠할때 쓴다. 주로 네트워크 상황이 안정적일 때는 값이 작고, 네트워크 상황이 들죽날죽일 때는 커진다.\\[DevRTT = (1-\\beta)\\cdot DevRTT+\\beta \\cdot | SampleRTT -EstimatedRTT|\\]Principles in practice일부 버전의 TCP에서는 3개 이상의 중복된 ACK NUM를 받으면 해당 segment seq num 다음의 segment를 손실로 치고 다시 보내는 NAK 메커니즘을 이용해서 타이머보다 빠르게 재전송을 판단한다.또한, 전송한 segment 중 아직 ack 되지 않은 segment를 통해 flow-control과 congestion control이 이루어진다.Setting and Managing the Retransmission Timeout IntervalTimeout Interval은 EstimatedRTT 보다는 충분히 커야 하므로 다음과 같은 식으로 구한다.\\[TimeoutInterval = EstimatedRTT + 4\\cdot DevRTT\\]EstimatedRTT 기준으로, 네트워크 상황이 들죽날죽이면 더욱 여유를 주고, 네트워크 상황이 안정적이면 마진을 적게 주기 위해 DevRTT를 이용한다.최초 샘플링이 되지 않았을 때의 initial TimeoutInterval은 1초로 추천된다(RFC6298)또한, timeout이 생겨나면 EstimatedRTT가 측정될 때까지 TimeoutInterval을 2배로 늘렸다가, 나중에 제대로 측정된 값을 쓴다.3.5.4 Reliable Data TransferTCP는 unreliable한 서비스인 IP와 그 이하의 layer 위에 쌓아올린 reliable data tranfer protocol로, buffer 내부의 전송받은 byte stream이 정상적인 순서의 손실되지 않아야한다.TCP는 timeout을 측정할 때 여러 timer를 이용하지않고, 오직 하나의 재전송 타이머만 사용하며 이를 single-timer recommendation 이라고 한다.우리는 TCP sender가 timeout을 이용해 segment의 손실을 방지하는 것, 그리고 나중에 추가로 중복 ack segment로 방지하는 법을 배울 것이다.아래의 코드 fig3.33은 TCP sender의 동작을 pseudocode로 나타낸 것이다. 동작은 크게 application으로 부터 데이터 받아오기, 타이머 timeout, ACK 받아내기 로 나뉜다.title: Fig.3.33.Simplified/* TCP는 혼잡과 흐름 제어에 영향을 받지 않고, MSS 보다 작은 데이터를 전송해야 하며, 단방향 통신만 한다는 가정 */NextSeqNum=InitialSeqNumberSendBase=InitialSeqNumberloop (forever) {    switch(event)        event: 1. application layer에서 데이터 받음            NextSeqNum의 seq num을 가진 TCP segment 생성            if (타이머가 설정되어있지 않으면)                타이머 시작            segment IP layer로 넘김            NextSeqNum=NextSeqNum+length(data)            break;        event: 2. 타임 아웃 in TimeoutInterval             아직 ack 되지않고 가장 작은 seq num을 가진 segment 재전송            타이머 재시작            break;        event: 3. y값을 가진 Ack 도착            if (y &gt; SendBase) {                SendBase=y                if (아직 ack 되지 않은 segment가 있다면){                    타이머 시작                }            }            break;} /* end of loop forever */A Few Interesting Scenarios우리가 만든 간단한 TCP 모델은 미묘함이 있다.우리가 만든 모델의 동작을 알 수 있는 간단한 시나리오 몇개를 보자.먼저 아래 그림에 묘사된 시나리오 1이다.중간에 ACK segment가 유실되면서 Timeout 시간이 지나면서 같은 data의 segment를 한번 더 보내는 상황이다.같은 내용을 받은 Host B는 해당 segment를 버린다.아래 그림은 시나리오 2번째이다.2개의 세그먼트를 보냈지만 첫번째 segment의 timeout 시간 이후에 ack가 늦게 도착하여 첫번째 segment를 다시 보내는 상황이다.마지막으로 아래는 세번째 시나리오이다.첫번째 segment의 ack가 손실되어 도착하지 않았지만 두번째 Ack가 도착하여 cumulative ack 특성을 가진 TCP는 굳이 첫번재 segment의 ack를 다시 보내달라 하지 않는다.Doubling the Timeout Interval널리 구현되어 있는 실제 TCP에서의 동작을 알아보자우리는 TimeInterval을 EstimatedRTT와 DevRTT로 구한다고 했었지만실제로는 Timeout이 발생할 때마다, 다음 unacked segment 부터는 TimeInterval을 2배로 적용시키고, Timeout 이벤트 이외의 이벤트가 발생하면,(pseudo code에서 1,3번) EstimatedRTT와 DevRTT로 TimeInterval을 설정하는 방법을 쓴다.예를 들어, 초기 TimeInterval이 1초였고, Timeout이 발생했다면, 해당 segment를 재전송하고, TimeInterval을 2초로, 만약 또 다시 Timeout이 발생했다면 4초로 2배씩 늘리는 것이다.이를 통해 기초적인 혼잡 제어(congestion control)이 가능한데, 자주 Timeout이 발생한다는 것은 네트워크 상황이 좋지 않다는 의미이며, 점점 TimeInterval을 늘려 재전송하는 segment 빈도를 줄여 네트워크 상황이 개선될 수 있기 때문이다.Fast RetransmitTimeout이 발생되면 재전송하는 현 시스템에는 기나긴 TimeInterval 시간을 기다려야 재전송이 되므로, end-to-end delay가 커진다는 단점이 있다.이를 duplicate ACK(중복 ACK)로 sender 측에서 더 빠른 시간에 segment의 손실을 알아챌 수 있다.duplicate ACK는 이미 Ack가 끝난 segment의 ACK segment가 다시 온 것을 의미한다.아래 표는 segment 도착 이벤트 시 receiver의 활동이다.            이벤트      TCP 수신측 활동                  수신측에 순서에 맞는 segment가 예상된 seq num을 가지고 도착, 이전의 segment 들은 이미 ACK 된 상태      지연된 ACK. 500msec 동안 다음 순서의 segment를 기다리고, 오지 않으면 이전 segment까지의 ACK 전송              수신측에 순서에 맞는 segment가 예상된 seq num을 가지고 도착, 송신측에서 몇몇 segment들이 ACK를 기다리는 상태      즉시 최신의 cumulative ack 전송하여 순서에 맞는 모든 segment를 ack하게 함              수신측에 예상보다 높은 순서의 seq num을 가진 segment가 순서에 맞지 않게 빨리 도착, buffer에 들어가면서 중간에 window에 갭이 생김 (= 중간 segment가 손실 됬다는 의미)      이전에 보냈던 seq num을 가진 중복 ack를 보냄. 예를 들어 3번 segment까지 오고, 이후, 8번 segment가 왔다면 3번에 대한 ack를 보냄(갭의 처음 바로 앞 번호)              위의 이벤트에 의해 window에 갭이 있던 상태에서 갭을 전부 혹은 일부 채워줄 수 있는 segment 도착      즉시 가장 첫 갭의 마지막 세그먼트에 대한 ack를 하나 보냄.      duplicate ack(중복 ack)는 총 3개의 중복 ack, 즉 총 4개의 같은 ACK가 돌아 왔다면, 그 경우 Timer가 남은 경우에도 fast retransmit을 실시하게 된다.굳이 1~2개의 중복 ack가 아니라 3개의 중복 ack인 이유는 그 이전 중복 ack는 segment의 유실이 아닌 그저 늦게 도착하는 것일 수 도 있기 때문에 3번까지 더 기다려 주는 것이다.아래는 fast transmit의 예시이다.title: fig.3.33event: 3. y ack num을 가진 ack 받음    if (y &gt; sendBase) {        sendBase = y         if (아직 ack 안된 segment가 있다면){            타이머 설정                  }     }     else{ /* y값이 중복인 ack를 받을 경우 */         y의 중복 ack 갯수를 하나 늘림         if (중복된 ack 갯수가 3개 이상이 됬을때){             /*TCP fast retransmit*/             y번 segment 재전송         }          }break;ACK 수신 시의 event(3번 이벤트)을 위의 코드로 바꿔야 한다.이러한 TCP 재전송 메커니즘의 효과는 30년의 세월 동안 현실에서 증명되었다.Go-Back-N or Selective Repeat?TCP는 순수하게 GBN 방식도, 순수하게 SR 방식도 아니며 둘을 절충한 selective acknowledgment 방식이다.TCP는 cumulative ack를 사용하여 segment 일일이 ACK를 확인하는 방식은 GBN과 비슷하지만, ACK 되지않아 손실된 걸로 판단된(타이머로 인한 것이든, 중복 ACK로 인한 것이든) segmentn 만 재전송 요청한다는 점은 n 부터 전송한 마지막 segment인 N까지를 전부 재전송하는 방법인 GBN과 다른 SR 방법과 비슷하다.  심지어 만약 제한 시간 이내에 ack n+1 이상을 받게되면 그마저 재전송하지 않는다.3.5.5 Flow ControlTCP에서 flow-control service란, application이 사정상 data를 늦게 가져가 receiver buffer가 가득차는 것을 막기 위해 sender 측에 전송량을 낮추도록 하는 것이며,congestion-control은 중간 네트워크 상황이 나빠지지 않도록, sender 측에 전송량을 낮추도록 하는 것이다.이 둘은 비슷하지만, 다른 목적을 가지고 있고, 사람들이 많이 혼동하고 혼용하기도 한다.이번에는 flow-control에 대해서 알아보기 위해 TCP service가 순서에 맞지 않게 도착한 segment는 전부 버린다고 가정한다.TCP에서 송신자는 receive window 라는 변수를 가지고 있어 이를 통해 전송량을 조절한다. 또한 TCP는 양방향 통신 (full-duplex)이므로 양 측에 각각 상대를 위한 receive window를 가지고 있다.Host A가 TCP를 통해 대용량 파일을 Host B에 보낸다고 가정하자.Host B는 receiver buffer overflow를 방지하기 위해 버퍼 크기를 RcvBuffer로 정하고, 두 가지 변수를 정한다.  LastByteRead: application이 buffer에서 data를 빼간 bytes의 크기  LastByteRcvd: sender가 buffer에 data를 전송한 bytes의 크기overflow가 일어나지 않으려면 다음과 같이 위 두 변수의 차가 buffer 크기를 넘으면 안된다.$LastByteRcvd - LastByteRead \\leq RcvBuffer$receive window, 변수명 rwnd는 buffer에서 남은 공간을 의미한다.$rwnd = RcvBuffer - [LastByteRcvd - LastByteRead]$rwnd 값은 시간에 따라 동적으로 바뀌며, 아래는 그 rwnd를 그림으로 표현한 것이다.이렇게 recevier는 sender에게 보내는 segement들(주로 ack) header에 rwnd 값을 넣어 보내어 flow-control이 이루어진다.sender는 LastByteSent와 LastByteAcked라는 두 변수를 이용해 현재 보냈지만 acked 되지 않은 segment의 bytes양(LastByteSent - LastByteAcked)을 체크한 뒤 이 값이 rwnd 값보다 작게 유지된다면, buffer overflowing이 일어나지 않는다고 확신한다.$LastByteSent - LastByteAcked\\leq rwnd$하지만 문제가 하나 있는데, rwnd가 0이 되는 순간과 receiver가 더이상 ack를 보낼 segment도 없게되는 순간이 겹칠 경우, sender 측에서는 rwnd가 0이라 더 이상  segment를 보낼 수 없고, receiver 측은 sender가 segment를 보내지 않아, ack를 보낼게 없어 더이상 rwnd가 자리가 생겨도 알려줄 수 없는 상황이 생긴다.이때를 방지하기위해 rwnd가 0이라고 보고 받는 순간부터 sender 측은 1 byte 짜리 더미 데이터가 포함된 segment를 주기적으로 보내고, receiver 측은 이에 ack를 보내면서 rwnd를 주기적으로 sender 측에 보고한다.UDP의 경우 이러한 flow control이 없어, socket에 한계인 buffer에 멈춤 없이 제공되며, process가 빨리 처리하지 않으면 overflow가 일어난다.3.5.6 TCP Connection ManagementTCP 연결(Threeway handshake)의 방법을 아는 것은 SYN food attack 같은 취약점 공격 관련 보안 방지 및 connection delay 인지 등의 이유로 중요하다.      먼저 client TCP 측에서 server측에 특별한 segment를 보낸다. 이 segment는 SYN header가 1로 설정되어 있고, 추가로, client 측에서 랜덤하게 설정한 sequence number(client-isn)가 sequence number field에 적혀있는 data가 없는 segment (SYN segment)이다.          이미 종료된 이전 TCP 연결 등에서 늦게 도착하는 segment 등의 영향으로, sequence number가 겹치는 것을 막기 위해 랜덤으로 시작 seq num(ISN)을 정한다.      초기설정 sequence number를 정하는 방법은 보안적인 측면에서 고려해서 고르기도 한다.[CERT 2001-09; RFC 4987]            SYN segment가 서버측에 도착하게 되면 서버측은 해당 연결을 위한 buffer와 variable을 할당하고, connection-granted segment(SYNACK segment)를 준비한다. 이 SYNACK segment를 만드는 과정은 먼저          segment를 생성한 뒤, syn header 값을 1로 만든다.      SYN segment의 segment header에서 client_isn 값을 가져와 client_isn + 1값을 acknowledment field에 넣어주고,      서버측의 시작 sequence number(=server_isn)을 설정해준 뒤, server_isn 값을 sequence number field에 넣고 client 측에 재전송한다.            Client 측에서 SYNACK segment를 받고 나서, 마찬가지로 buffer와 variable을 설정한다. 이후 client host가 새로운 segment를 만드는데,          SYNACK segmen에 포함되있던 server_sin+1 값을 acknowledgment field에 넣고,      SYN header 값은 0으로 넣고      payload에 server에 주려던 data를 넣고 sequence number를 client_isn+ data bytes만큼 증가시킨 뒤,  서버측에 다시 전달해 Threeway handshake을 완성한다.      이후로는 SYN bit가 0으로 설정된 segment로 서로간에 통신한다이 과정을 통해 서로 통신 준비가 완료됬음을 확인할 수 있으며 3번의 통신이 이루어지는 위 과정을 Three-way handshake라고 부른다.  어째서 세번인가?, 이 과정은 서로에게 seq number 설정값과 buffer 준비를 하고, 됬냐고 물어보는 과정, 쉽게 말해 서로에게 통신 준비가 완료됬냐고 물어보는 과정이다.          즉, A와 B가 서로, “나 이렇게 통신 준비 완료했어”, 그리고 “알았어 확인했어” 두 메시지를 주고 받는 것이다.      2명이 2개의 메시지, 총 4개의 메시지를 주고받아야 한다.      이때 B의 “나 이렇게 통신 준비 완료했어(SYN segment)”와 “알았어 확인했어(SYNACK segment)”는 굳이 두 메시지로 나누지 않고 하나로 보내도 된다.      그래서 A의 SYN, B의 SYN과 SYNACK, 마지막으로 A의 SYNACK, 3개의 메시지가 필요하므로 3번 진행한다.      위 그림은 TCP의 three-way hand shake를 그림으로 표현한 것이다.만약, Connection을 종료하고 싶다면, 아래의 3.40 처럼 four-hand shake로 끝내게 되는데,  client 측에서 연결 종료를 신청, FIN header가 1인 특별한 segment 서버측에 전송  Server측에서 위의 segment에 대한 ack segment 전송, 이후 Interval Time 만큼 시간을 기다렸다가 server 측에서 종료  Server측에서 다시 1번에서 보냈던 FIN segment 전송  client 측에서 위의 segment에 대한 ack segment 전송, 이후 Interval Time 만큼 시간을 기다렸다가 client 측에서 종료으로 양측 간의 연결에 대한 모든 자원을 해제하게 된다.TCP 연결 중에는 각 호스트가 TCP states(TCP 상태)를 기반으로 행동하며, 아래는 client TCP 측에서의 TCP 상태 그림이다.            상태명      설명                  CLOSED      아직 아무 TCP 연결도 되어있지 않은 상태, server에 SYN segment를 보내 SYN_SENT 상태로 바뀐다.              SYN_SENT      SYN segment를 보낸 뒤 server로부터 SYNACK segment를 기다리는 상태, SYNACK segment가 도착하면 ESTABLISHED 상태로 바뀐다              ESTABLISHED      TCP client와 TCP server가 서로 data segment를 주고 받을 수 있는 단계, Client가 FIN segment를 보내면 FIN_WAIT_1 상태로 바뀐다.              FIN_WAIT_1      client가 FIN segment를 보낸 뒤, server측으로 부터 ACK segment를 기다리는 상태, 서버측으로 부터 ack segment를 받으면 FIN_WAIT_2 상태로 바뀐다.              FIN_WAIT_2      client가 ACK segment를 받고, server 측의 FIN segment를 기다리는 상태, 서버측으로부터 FIN segment를 받으면 TIME_WAIT 상태로 바뀐다.              TIME_WAIT      TIME_WAIT 상태에서는 서버측에 ACK segment를 보내고 구현에 따라 30초~2분 정도 기다렸다가 CLOSED 상태로 바뀐다. CLOSED 상태로 바뀌면 할당했던 buffer, variable 등의 자원을 풀어버리고 새로운 TCP 연결을 할 수 있다. 이때 잠시 시간을 기다리는 이유는 혹시 보낸 ACK segment가 유실되면, Server 측에서 다시 FIN segment를 보낼 것이며, ACK segment를 재전송 해주기 위해서이다.      아래는 TCP 연결의 server side 측이다.TCP 연결이 불가능한 port로 보낸 TCP 관련 segment들의 응답으로 RST segment를 받게 된다.RST segment, 또는 Reset segment는 RST header bit가 1인 TCP segment로 “해당 socket은 TCP 통신 불가능합니다”라는 의미를 가지고 있다.만약 UDP socket이 아닌 socket으로 UDP segment를 보내면, UDP는 특별한 ICMP datagram을 응답으로 받는다.Nmap port Scanning Tool을 통해 TCP, UDP 연결 등을 확인해 볼 수 있다.host를 정하고 랜덤한 port, 예를 들어 6789 port에 보내보면 다음과 같은 3가지 반응 중 하나가 올 것 이다.  TCP SYNACK segment 반환, 해당 host의 해당 port가 TCP 연결을 지원 중 이라는 의미이다. nmap 상에서 “open”이 반환.  TCP RST segment 반환, 해당 host의 해당 port가 application이 돌아가고 있지 않은 빈자리란 의미이다. 이 정보를 통해 공격자들이 해당 port에 firewall이 없어 보안적으로 취약함을 알 수 있다.  아무것도 반환되지 않음. firewall이나 기타 이유로 SYN segment가 막혔음을 의미한다.THE SYN FLOOD ATTACK이전에 TCP 연결은 보안에 취약하다는 이야기를 한 적이 있다.그중에 DOS 공격의 일종인 SYN FLOOD ATTACK은 TCP 연결 수립 과정의 맹점에 의해 생긴 보안 취약점이다.주 방법은 공격자가 해당 서버에 짧은 시간에(TCP는 연결이 수립되지 않은면 ACK segment를 1분 정도 기다린다. ) 끊임 없이 SYN segment를 보내게 되면, 서버는 끊임없이 buffer와 variable을 할당하고, 수많은 SYNACK segment를 보내고 ACK를 기다리게 되는데, 공격자는 ACK를 보내 연결을 수립해주지 않는다.그 결과, 공격자는 별다른 자원이 필요없지만, 서버측에서는 결국에 자원의 한계에 도달해 정상적인 사용자들은 TCP 연결이 불가능하게 된다.이를 해결하기 위해 SYN cookie(RFC 4987)이 여러 OS에 구현되었다.  Server 측에서 SYN segment를 받으면 먼저 buffer와 variable을 할당하지 않고, SYN segment의 Source/Destination IP address/ port number와 server만의 sercret number, 총 5개의 값의 hash function의 결과 값을 initial sequence number를 만든다. 이를 cookie라고 한다. 이 값을 SYNACK segment에 넣어 client 측에 보낸다. 이때 서버측은 cookie 값이나 SYN의 상태 정보를 저장하지 않는다.  정상적인 사용자라면 client 측에서 서버로 ACK segment를 돌려보낸다. 이때, ACK segment의 acknowledgement number - 1의 값이 이전에 보냈던 SYNACK segment의 cookie(=initial seqeunce number) 값이므로, ACK segment의 Source/Destination IP address/ port number와 server만의 sercret number로 다시 hash function을 돌린 뒤, 이 값이 ACK segment의 acknowledgment number - 1인지 비교하여, 해당 client가 이전에 통신한 사람이 맞는지 확인하고, 그 다음 서버의 자원을 할당하여 TCP 연결을 해준다.  이때, 만약 client측에서 ack가 돌아오지 않는다면, 잠시 뒤, TCP 연결을 종료하고, 이때 이전에 서버 자원 할당이 되지 않았으므로, SYN FLOOD ATTACK의 피해는 없게 된다.3.6 Principles of Congestion Controlcongestion control의 방법과, congestion의 원인, 증상 등을 알아보자.3.6.1 The Causes and the Costs of Congestion3가지 상황의 네트워크 혼잡 시나리오를 원인과 비용을 기준으로 알아보자Scenario 1: Two Senders, a Router with Infinite Buffers아래 그림처럼 HOST A, B가 각각 $\\lambda_{in}\\ bytes/sec$ 속도로 공통된 Router를 거쳐 데이터를 전송하고 있으며, 이때 router의 buffer는 무한대이고, outgoin link의 전송 속도는 최고 R 이라고 가정한다.에러 조정이나 혼잡 제어, 흐름 제어 기능이 전혀 없고, 추가적인 header overhead 또한 존재하지 않은 이상적인 상황이라고 가정한다면,fig.3.44와 같은 그래프를 볼 수 있다.왼쪽은 각 연결 별 throuhput 그래프이고, 오른쪽은 각 연결별 delay 그래프이다.throuhput은 각 연결별 전송속도가 R/2에 가까울 수록 효율이 좋다. 두 호스트가 합쳐서 라우터의 최대 속도인 R을 100% 활용하기 때문이다.하지만 delay 관점에서는 R/2에 가까워질 수록 패킷의 평균 delay가 무한대로 치솟는다. buffer가 무한대이므로 packet이 버려지지 않고, 계속 packet이 queue에 들어가는 속도가 나가는 속도보다 많아지면서 delay가 점점 증가하기 때문이다.물론 무한대로 전송할 경우에 해당하는 delay이지만, 이상적인 상황임에도 불구하고 상당한 비효율을 보임을 알 수 있다.Scenario 2: Two Senders and a Router with Finite buffers이번에는 위와 같은 상황이나 두가지 가정을 바꾸었다.  router의 buffer 크기가 한정되있다.buffer 크기를 넘은 packet은 queuing되지 않고 버려지게 될 것이다.  각 연결은 reliable data transfer로, packet이 손실되면 재전송을 시도할 것이다.이때, 기존의 data transfer 속도와 재전송 packet의 전송 속도를 포함해서 $\\lambda^{‘}_{in}$ bytes/sec 라고 하자.아래 그림의 (a)는 버퍼가 비어있을 당시만 재전송이 이루어질 때,(b)는 packet이 손실됬다고 확실히 생각되는 것만 재전송이 이루어질 때,(c)는 packet이 delay된것도 재전송이 이루어질 때의 그래프이다.각각 최고속도가 R/3, R/4로 줄어드는 것을 알 수 있다.왜냐하면, 손실된 packet 재전송 속도에도 속도를 할당 또는 손실되지 않고 제대로 간 packet도 손실됬다고 생각하고 전송 속도를 할당했기 때문이다.Scenario 3 : Four Senders, Routers with Finite Buffers, and Multihop Paths먼소린지 잘 모르겠음 ㅡㅡ4개의 host가 다른 host와 2 hop 만큼 떨어져 통신하는 예제이다.아래의 예제처럼 $\\lambda_{in}$는 data throughput이고, $\\lambda_{in}^{‘}$는 재전송 throughput을 포함한 throughput이다.Host  A-C 연결은 Host D-B 연결과 router R1을 공유하고, Host B-D 연결과 router R2를 공유한다.$\\lambda_{in}$ 가 충분히 작아 buffer overflow가 가끔씩 일어나며, throughput은 비슷한 부하를 제공한다.$\\lambda_{in}$가 조금 늘어나면, 그에 따라 throughput은 조금 늘어나고 여전히 buffer overflow는 가끔씩 일어난다.즉 $\\lambda_{in}$가 늘어나면, 그에 따라 $\\lambda_{out}$도 늘어난다.반대로 $\\lambda_{in}$가 아주 큰 경우를 가정해 보면, R1과 R2를 거친 A-C 트래픽은 크기와 상관 없이 R1과 R2 사이의 최대 트래픽인 R이 최대치이다.만약  B-D 연결의 $\\lambda_{in}^{‘}$가 아주 크다면 B-D 연결의 R2에서의 지분이 A-C에서보다 커지게된다.왜냐하면, B-D 연결 지분이 A-C 연결보다 더욱 많은 부하를 주며 더 많은 지분을 차지하게 되기 때문이다.B-D 연결 지분이 계속 커지다보면 A-C연결의 지분은 0에 가까워질 것이다.이러한 가정에 의한 결과 그래프는 아래와 같다.위와 같이 부하가 한계에 다다르지 않아도 throughput이 크게 줄어드는 이유는 한 router가 처리한 packet이 다음 router에서 버려지면 이전 router가 한 노력이 버려지는 꼴이기 때문이다.이를 막기 위해 각 router는 packet 처리 우선순위를 두어, 여러 router를 지나왔던 packet에 우선순위를 주어 처리하는 방법을 고려할 수 있다.여기서 우리는 네트워크의 혼잡에 의해 packet이 버려지면, 해당 packet을 처리하는데 사용한 이전 router들의 자원이 낭비된다는 것을 알 수 있었다.3.6.2 Approaches to Congestion Control실무에서 혼잡 제어를 위한 2가지 방법과 자세한 네트워크 구조와 혼잡 제어 protocol에 대해서 알아보자.우리는 혼잡 제어 방법을 network layer에서 transport layer에 도움을 주냐 아니냐로 구분할 수 있다.  End-to-End 혼잡 제어 : network layer에서 transport layer에 도움을 주지 않는 방법으로, TCP가 IP의 도움을 받지않게 할 수 있도록 선택한 방법이다. 주로 TCP segment의 손실(타임 아웃이던, dupliacated ack이던)을 네트워크 혼잡의 전조로 보고, window size를 줄여서 해결하며, 추가로 최근에는 segement의 RTT delay를 이용하는 방법도 있으며, 알아볼 것이다.  Network 지원 혼잡 제어: router 측에서 network 혼잡 상태를 sender와 receiver에게 제공하는 방법이다. 단순히 하나의 bit로 제공하는 방법(IBM SNA, DEC DECnet, ATM network)도 있고, 좀더 복잡한 방법도 있다. ATM Avilable Bite Rate(ABR)의 혼잡 제어 방법의 경우, router 측에서 sender에게 outgoing link의 최대 sending rate를 알려주기도 한다. 대부분의 TCP/IP 구조에서 사용하진 않지만, 일부 서비스는 구현에 따라 사용하기도 한다.          Network에서 혼잡 상태를 Host에게 알려주는 방법도 2가지가 있다. (fig.3.49)                  router 측에서 sender 측에게 직접적으로 packet을 보내 알려주는 방법(Direct network feedback): 이때 보내는 packet을 choke packet이라고 한다.          rotuer가 sender가 보내는 packet의 field를 수정한 뒤, 이 수정된 packet을 받은 receiver측에서 sender측에게 혼잡 상황을 알리도록 하는 방법(Network feedback via receiver): 좀더 자주 사용되는 방법이며, delay가 RTT 시간 만큼 걸린다.                    3.7 TCP Congestion ControlTCP는 위에서 설명한 IP 측의 도움을 받지 않는 End-to-End 제어를 이용하는 방식이 일반적이지만, 최근에는 2번째의 IP측의 도움을 받는 방법도 사용한다.먼저 classic한 혼잡 제어 방법 그리고, 최근의 혼잡 제어 방법을 알아보고, 마지막으로 혼잡한 상황의 network 의 자원을 공평하게 나누는 것에 대한 transport 층의 어려움을 알아보자.3.7.1 Classic TCP Congestion ControlTCP는 각 sender 측이 네트워크 혼잡 정도에 따라 전송량을 줄이게끔 하는 방법을 사용하고 있다.여기서 세가지 궁금증이 생기는데,  TCP sender 측은 어떻게 전송량을 줄이는가?  TCP sender 측은 어떻게 receiver 측과 사이의 network 혼잡을 알 수 있는가?  TCP sender 측은 network 혼잡에 따라 얼마나 전송량을 줄여야 하는가?(또는 이를 위한 알고리즘은 무엇인가?)이에 대해 알아보자.우리는 이전에 TCP 연결을 위해 receive, send buffer, LasyByteRead, rwnd 같은 여러가지 connection state가 필요하다고 배웠다.여기에 추가로 TCP 혼잡 제어를 위해 congestion window(cwnd)를 이용한다.cwnd는 TCP sender에게 traffic의 한계를 제한하는데, 구체적으로 sender측에서 ack되지 않은 데이터의 양을 cwnd와 rwnd 사이 값으로 제한한다.$LastByteSent-LastByteAcked\\leq \\min{cwnd,\\ rwnd}$먼저, 흐름 제어를 제외하고 혼잡 제어 측면만 보기 위해 몇가지 가정을 하자면,TCP receiver buffer는 충분히 커서 receive-window의 제한은 무시할수 있으며, sender 측의 ack 되지 않은 data의 양은 cwnd에 의해 제한되며, sender 측은 언제나 receiver 측에 보낼 데이터가 있다고 가정한다.loss와 packet transmission delay(router가 packet을 적절한 outer link에 적재하는 시간)가 무시할 만큼 적을 때, 매 통신의 왕복시 마다, sender는 갱신된 cwnd bytes 만큼만 보낼 수 있다. 즉, $cwnd/RTT\\ bytes/sec$만 보낼 수 있는 것이다.먼저 congestion이 일어나지 않은 이상적인 상황을 생각해본다면, TCP 연결은 ack segment가 도착할때마다, congestion window size(cwnd)의 크기를 늘린다. 만약 어떤 이유로 ack segment가 도착할 때까지의 delay가 발생한다면, 해당 delay 만큼, congestion window size가 증가의 속도가 늦춰질 것이다.TCP는 이렇게 ack segment가 도착할 때마다 congestion window size를 늘리는 행동을 발현(clock, triagger)하며 이를 self-clocking라고 한다.그렇다면 도착할 때 마다 어떤 속도로 올려야 할까? 너무 빠르면 congestion을 유발할 것이고, 너무 느리면 비효율적일것이다.이런 것을 정하기 위해 TCP는 이러한 기본 가이드라인을 따른다.  segment가 손실됨은 혼잡함을 의미하므로, segment가 손실되면 전송 속도를 낮춰야한다.  acknowledged segment가 도착함은 성공적으로 통신이 됬음을 의미하며, ack가 도착하면 전송 속도를 늘려야한다.  대역폭 측정(Bandwidth probing), 각각의 ACK와 loss는 네트워크 혼잡의 신호로 사용된다. 보통은 loss가 생기기 전까지 천천히 올리다가, loss가 생기면 전송속도를 늦추는 방식을 사용한다. 이러한 과정은 네트워크 내의 TCP간에 비동기적으로 실시된다.이러한 가이드라인으로 TCP congestion-control algotrithm이 탄생하였다. [Jacobson 1988, RFC 5681]이 알고리즘은 크게 (1) slow start, (2) congestion avoidance, (3) fast recovery 세 부분으로 나뉘어져 있다.특히, (1), (2)는 TCP에 필수적으로 사용되며, (3)은 추천되는 부분이다.Slow StartTCP 연결 시작시, cwnd의 값은 1 MSS라는 작은 값으로 시작한다. MSS가 500bytes, RTT가 200msec라면, 초기 sending rate는 20kbps이다.이후, slow-start 상태가 되어, 보낸 segment에 첫 acknowledgement가 돌아올 때마다 1 MSS 씩 늘리게 된다.아래 그림의 예시를 보면 첫 ack가 도착하면 2개의 segment를 보내고, 그 뒤에 2개가 도착하면 4개를 보내는 것을 알 수 있다.이를 통해 slow-start 상태에는 매번 손실이 없다는 가정 하에, 2배씩 전송량을 늘리는 것을 알 수 있다.이러한 slow start 상태를 끝내는 이유는 총 3가지가 있다.  timeout에 의한 loss event 발생 : 즉시 ssthresh(slow start threshold)라는 값을 현재 cwnd/2로 설정하고, cwnd 값을 1 MSS로 바꾸고 다시 slow start 시작  증가하던 cwnd값이 ssthresh 이상의 값의 됨 : 2배씩 늘리던 slow start 상태에서 벗어나 아래 설명할 congestion avoidance 상태로 바뀐다.  Three duplicate ack에 의한 loss event 발생 : TCP의 fast retransmission 활동 후 아래에 설명할 fast recovery 상태로 전환Congestion Avoidancecongestion avoidance 시기에 들어가면 cwnd는 2배가 아니라 1 MSS 씩 상승한다.구체적으로 예시를 들자면, 만약, MSS가 1460bytes, cwnd 14600bytes 였다면, 이전에 10개의 segment를 보냈다는 의미이며, ack segment가 하나 도착 할때마다 1/10 MSS, 즉 146 byte씩 cwnd를 증가시킨다.이러한 선형 cwnd 증가인 Congestion Avoidance 상태는 이후 loss를 만나면 slow start 때와 같이 ssthresh에 현재 cwnd/2값을 저장하고 1MSS로 바꾸고 slow start 상태로 바뀐다.만약 3 duplicated ack로 인해 loss를 측정했다면, ssthresh를 cwnd의 절반으로 기록하고, cwnd를 1 MSS가 아닌 절반+3MSS(duplicated ack로 인해 상태 변화 됬음을 알리기 위해 더함) 으로 바꾼 뒤, 아래의 fast-recovery 상태로 바뀐다.아래는 TCP congestion control의 FSM이다.Fast Recoveryfast recovery에서는 fast-recovery에 들어가게 유발한 동일한 segment에 대한 duplicated ack 하나당 cwnd 값이 1 MSS 씩 상승한다.예를 들어, cwnd가 12이고, duplicated ack가 3개 들어왔다면, 12/2 + 3 = 9로 cwnd가 시작된다.이때 cwnd값만 그러하고, ssthresh값은 여전히 이전 값 그대로이다.(cwnd의 절반 값으로 fast recovery를 시작했었다.)만약, Fast Recovery 상태에서 fast retransmission으로 손실된 걸로 판단한 segment에 대한 Ack가 들어온다면 cwnd 값을 ssthresh 값으로 바꾼 뒤, congestion avoidance 상태로 들어간다.만약, Fast Recovery 상태에서 Timeout이 일어난다면, 평소와 같이 ssthresh는 cwnd/2로, cwnd는 1 MSS로 바꾼 뒤 slow-start 상태로 변경된다.fast recovery는 TCP 구현에 있어 추천되지만, 필수는 아니며, 초기버전 TCP인 TCP Tahoe의 경우, Timeout, Duplicated ACK 둘다 관계없이 slow-start 상태로 들어가지만, 좀더 최신 버전인 TCP Reno의 경우 fast recovery가 구현되어있다.아래 Fig.3.52를 보면 TCP Reno와 TCP Taho를 비교할 수 있다.(8라운드에 duplicated ack가 3개 들어왔다는 가정이며, 검정색 Reno의 9라운드 이전 그래프는 Tahoe와 같은 그래프이다.)TCP Splitting을 이용한 cloud service 성능 향상많은 서비스들이 클라우드의 서버를 이용해 사용자들에게 서비스를 제공한다.양질의 서비스를 제공하기 위해 빠른 응답이 필요한데, 이때, 해당 서버와 사용자간의 거리가 멀면 화면을 보여주는게 너무 느려지곤 한다.예를 들어, 검색 결과 정보의 크기가 보통 TCP 연결의 window 사이즈의 3배 만하다고 가정하면, 사용자는 서버측에 정보를 얻기 위해 TCP 연결 1번 + 3번 segment 왕복에 의해 총 4RTT 만큼의 시간을 기다려야 한다.이는, 서버가 가까울 대는 상관없지만 멀면 아주 긴 시간을 기다려야 할 것이다.이를 막기 위해 1. Frontend server를 사용자에게 가까이 둘것, 2. TCP splitting을 이용할 것 을 통해 해결할 수 있다.TCP splitting은 client는 가까운 Front-end 서버에 요청을 하고, Front-end 서버는 영구 지속 TCP 연결을 backend 서버와 연결하면서, 높은 MSS의 연결망을 이용해 backend와 연결하면 Window size를 크게 유지할 수 있다.기존에 4RTT가 걸리던 검색 결과가, $4RTT_{frontend}+1RTT_{backend}+processing\\ time$ 만큼 걸리게 된다.유저와 frontend가 충분히 가까우므로 앞의 부분을 0으로 무시하고, processing time 또한 생략하면 1RTT 만큼의 시간이 걸리게 된다.이렇게 TCP 연결 하나 짜리를 여러개로 나누는 것을 TCP spliiting이라고 한다.이러한 TCP splitting은 안정적이고 성능좋은 연결이 추가되고, 기존의 client-frontend 구간은 짧아져 segment 손실이 적어 retransmission rate로 낮아지는 효과가 있다.현재 여러 CDN과 cloud service가 제공하고 있다.TCP Congestioini Control: Retrospective시작시 slow-start 상태로 시작 하지 않고, cwnd가 선형으로 1MSS 씩 증가하며, triple duplicate ack로 인한 loss로 cwnd가 절반으로 줄어든다고 가정하면, 아래와 같이 톱니 모양의 그래프가 나오는데 이러한 이유로 additive-increase, multiplicative decrease(AIMD) 형태의 혼잡제어라고 불리운다.선형으로 증가하며 가능한 최대의 window 크기를 찾다, 혼잡이 야기될 것 같으면 절반으로 줄어들며 최적의 대역폭을 찾는 모양새이다. 이러한 AIMD 알고리즘은 이후, 수많은 공학적 시야와 실험을 통해서 꽤나 효과적임이 검증되었다.[Kelly 1998, Srikant 2012].TCP Cubic앞선 TCP Reno의 AIMD의 cwnd를 절반으로 줄이는 방법, 심지어 아예 1MSS 줄이는 초기버전 TCP Tahoe 등이 너무 조심스럽게 접근하는 것은 아닌가 하는 생각이 들 수 있다. 이전에 혼잡하지 않은 경계선까지는 빠르게 접근한 뒤, 이후로 천천히 증가하는 편이 효율적여 보인다.이러한 생각으로 만들어진 것이 TCP CUBIC인데, TCP Reno와의 차이점은 congestion avoidance 상태가 다음과 같이 다른다.  $W_{max}$가 혼잡이 관측되는 최대 cwnd라고 하고, K를 TCP CUBIC의 window 크기가 loss 관측없이 $W_{max}$에 도달하는 시간이라고 하자. K는 여러 변수의 변화에 따라 바뀔 수 있으며, 짧을 수록 좋다.  CUBIC 에서는 cwnd의 증가속도를 현재 시간 t와 K 사이의 거리의 세제곱에 비례하여 설정한다. 즉, $W_{max}$에 멀수록 급격하게 증가하며, 가까울수록 천천히 증가하여 $W_{max}$에 가까울 수록 조심해진다  만약 loss가 관측되지 않은 채로 시간 t가 K 보다 커지게 되면, CUBIC은 cwnd를 급격하게 증가시켜 새로운 congestion 한계를 탐색하게 된다.위 fig.3.54를 보면, Reno와 CUBIC를 비교할 수 있으며, Reno의 경우가 좀더 효율적임을 알 수 있다.현재 실무에서는 TCP CUBIC을 Reno보다 좀더 선호한다.Macroscopic Description of TCP Reno ThroughputTCP Reno에서의 평균적인 전송량(average throughput)을 알아보자.먼저 timeout으로 인한 slow-start 상태는 전체 상태 중에 아주 짧은 시간만 지속되므로 무시한다.window size를 w bytes, 이라 할때 TCP의 전송속도는 w/RTT이며, 매 RTT마다 w를 1 MSS씩 상승시켜 loss를 찾게 된다.W를 loss가 생긴 뒤의 w 값이고 거의 일정하다고 가정할 때, TCP의 transmission rate는 W/(2RTT) ~ W/RTT 사이이다.loss event가 발생하면 시간이 W/RTT로 줄어들며, 매 RTT 마다 1MSS/RTT 만큼 증가하며 이를 계속 반복한다.이를 통해 다음과 같은 식을 알 수 있다.$average\\ throughput\\ of\\ a\\ connection = \\frac{0.75\\cdot W}{RTT}$이를 통해서 loss rate와 가능한 bandwidth에 관한 식도 도출할 수 있다.3.7.2 Network-Assisted Explicit Congestion Notification and Delayed-based Congestion Control최근에는 IP와 TCP 기능이 확장되면서 network가 TCP sender과 receiver에게 직접적으로 혼잡 상태를 신호할 수 있게 되었고, 추가로 packet delay를 이용해 혼잡 제어가 가능한 방법도 제안되었다.Explicit Congestion NotificationExplicit Congestion Notrification(ECN)[RFC 3168]은 TCP, IP가 함께 연관된 인터넷에서 사용되는 네트워크 지원 혼잡 제어의 한 형태이다.network layer의 packet인 IP datagrame의 header에 Type of Service field에 2bit(0~3, 총 4가지 표현)을 통해 혼잡 관련 정보를 알릴 수 있다.ECN bit의 첫번째는 해당 router가 혼잡을 겪고 있는가에 대한 표시로 사용된다. 이를 표시하여 recevier 측에 혼잡을 알리고, sending host에게도 알리도록 만든다. 이러한 혼잡 상황은 RFC 문서에 정의되지 않았으므로 router 별로 서비스 제공자가 판단하여 설정해줘야 하며, 보통 loss가 발생하기 직전을 혼잡 상황으로 놓는다.ECN bit의 2번재 bit는 router에게 sender-receiver host가 ECN을 지원함을 알리는 bit 이다.위의 Fig.3.55에서 예시를 보자면, TCP의 receiver 측에서 ECN bit가 혼잡으로 설정되어 있는 datagram을 받으면, receiver 측은 sender측에게 TCP header의 ECE(Explicit Congestion Notification Echo) bit를 ACK segment에 넣어 알려준다. sender 측은 이를 확인하고, cwnd를 절반으로 줄이고, 다음에 보낼 예정인 TCP segment header의 CWR(Congestion Window Reduced, CWR 줄였음) bit를 1로 넣는다.TCP를 계승한 다른 transport-layer 프로토콜들 또한 ECN을 활용한다. Datagram Congestion Control Protocol(DCCP)의 경우, ECN을 활용해 적은 오버헤드, 혼잡 제어, UDP 같은 비신뢰성 데이터 통신을 제공한다. DCTCP(Data Center TCP)와 DCQCN(Data Center Quantized Congestion Notification) 또한 ECN을 이용하며, 최근에는 많은 서버들이 ECN을 지원한다.Delay-based Congestion Controlpacket loss가 일어나기 전에 혼잡을 탐색할 수 있는 또 다른 방법은 packet의 delay를 이용하는 것이다.TCP Vegas에서 sender 측은 모든 acknowledged된 segment의 RTT를 측정한다. 이때 최소 RTT(=가장 빨리 돌아온 segment의 RTT)를 $RTT_{min}$라고 하자.이는 네트워크 경로가 혼잡하지 않고, queuing delay가 가장 적었음을 의미한다. 이때의 congestion window size를 cwnd라고 할 때, throughput은 cwnd/$RTT_{min}$이라고 하며, 아마 다른 RTT에 비해 가장 큰 값일 것이다.만약 현재의 측정되고 있는 RTT가  $RTT_{min}$에 가까워진다면, 네트워크의 상황이 좋다는 의미이므로 cwnd를 늘려도 되며, 반대로 점점 멀어진다면 혼잡해가고 있다는 의미이므로 cwnd를 줄이게된다.자세한 사항은 [Brakmo 1995]에서 알 수 있다.TCP Vegas는 “keep the pip just full, but no fuller” 원칙에 따라, 최대한 자원을 활용하되 bottlenet link의 역량을 넘어서지 않게끔 설계되었다.BBR 혼잡 제어 protocol[Cardwell 2017]은 TCP Vegas에서 아이디어를 얻어 BBR을 지원하지 않는 TCP sender들과도 공평하게 경쟁할 수 있게 끔 설계되었다.Google은 사적인 B4 network에서 CUBIC을 BBR로 대체하였다. 이외에도 TIMELY, Compound TCP(CTCP), FAST 등의 dealy based TCP congestion control protocol이 존재한다.3.7.3 FairnessK 개의 TCP 연결이 최대 전송속도가 R인 link를 공유한다면, 가장 이상적인 각각의 전송속도는 R/K일 것이다. 하지만 우리가 사용하는 TCP의 AIMD 알고리즘이 정말 그러한 공평함을 제공할 것인가?[Chiu 1989]이를 알아보기 위해 가정을 하나 해보자.아래 그림 fig3.56처럼 두개의 같은 MSS와 RTT를 가진 TCP 연결이 하나의 link를 공유하며 이때의 병목 router의 최대 전송속도는 R이다. 다른 TCP, UDP 연결 등은 존재하지 않으며, slow-start state는 무시하고 언제나 CA mode(AIMD)로 동작한다고 하자.이들의 이상적인 전송속도는 R/2일 것이다.두 TCP 연결의 throughput에 대한 그래프로 표현하자면 아래 fig.3.57이며, 이상적인 상황은 최대한 자원을 활용하면서(검은색), 동일한 자원이 분배되는(파란색) 정 가운데 교차점 부분이다.만약 불공평한 지점 A에서 시작한다면 두 TCP 연결은 서로 서서히 같은 비율로 (1MSS) cwnd를 올릴 것이고, 결국 최대 활용 가능 지점(검은색 선)을 넘어 B로 이동할 것이다.B로 이동한다면, 두 TCP 연결은 loss를 관측할 것이고, 이에 따라 둘다 절반으로 cwnd를 줄여 C지점으로 이동할 것이다. 이때 중요한 점은 더욱 큰 cwnd(=더욱 빠른 전송속도)를 가지던 TCP 연결이 절대값으로 더욱 크게 전송속도가 깎인다는 점이다.C지점에서도 두 TCP 연결은 A지점같이 행동하며, cwnd가 줄어든 비율은 다르지만, 오르는 cwnd량(1MSS)은 같으므로, 이를 무한히 반복하다보면 결국에 공평한 지점에 도달하게 된다.            time round (loss ocurred over 15)(add 1MSS/sec)      1      2(loss)      3      4      5(loss)      6      7      8      9      10(loss) fair point      11      12      …                  TCP 1 (start cwnd 9)(unit: MSS)      9+1      (11+1)/2 = 6      6+1      7+1      (8+1)/2 = 4      4+1      5+1      6+1      7+1      (8+1)/2 = 4      4+1      5+1      …              TCP 2 (start cwnd 4)(unit: MSS)      4+1      (6+1)/2 = 3      3+1      4+1      (5+1)/2 = 3      3+1      4+1      5+1      6+1      (7+1)/2 = 4      4+1      5+1      …      이는 그래프 어느 지점에 시작해도 같은 결과가 나온다!하지만 이러한 결과는 이상적인 상황에서만 그러하며, 실제로는 application의 병렬 TCP 연결 요청, UDP packet, RTT의 차이(작은 RTT일 수록 빠르게 대역폭을 선점하여 전송 속도를 더욱 크게 가져가는 경향[Lakshman 1997]) 공평하게 공유하지 않는다.Fairness and UDP혼잡 제어에서 벗어난 일정한 전송속도가 필요하고, packet loss를 피하기 위해 전송속도를 제한하는 것보다 차라리 packet이 loss 되도록 하는 것이 좋다고 생각하는 많은 multimedia application 들은 TCP 대신 UDP를 사용한다.이로 인해 TCP 혼잡제어가 힘들어지고, 심지어 UDP가 많아지면 TCP는 완전히 통신 불가할 수도 있다.이러한 UDP의 불공정성을 막기 위해 여러가지 연구가 이루어지고 있다.[Floyd 1999, Floyd 2000, Kohler 2006, RFC 4340]Fairness and Parallel TCP Connections만약 UDP의 불공정성을 막는다고 해도 불공정성이 완전 해결되지 않는데, TCP 베이스 application 들이 TCP 연결 분배기(TCP Connection splitter)를 이용해다중 병렬 연결(multiple parallel connection)을 이용하면 연결 수에 비례하여 한 호스트가 더욱 많은 비율의 대역폭을 가질 수 있다.예를 들어 10개의 Host가 각각 1개의 TCP 연결을 가지는 대역 R의 link의 경우 R/10 씩 대역폭이 할당되지만, 만약 11번째 사람이 10개의 TCP 병렬 요청을 하면 해당 사람이 전체의 R/2의 대역폭을 가지고 나머지 10사람이 R/10으로 줄어들게 된다.이러한 다중 병렬 연결은 주로 웹 브라우저가 여러 object들을 동시에 가져올 때 많이 사용한다.3.8 Evolution of Transport-Layer FunctionalityUDP와 TCP가 대세이고, 이 둘에 대해서만 알아봤지만, 네트워크의 발달에 따라 이 둘에 맞지 않은 서비스를 제공하기 위해 새로운 protocol, 개량된 protocol 또한 개발되었다.현재 웹에서 가장 인기 있는 TCP는 CUBIC과 CTCP이지만, 무선 환경을 위한 TCP, 고성능 네트워크을 위한 TCP, Data center를 위한 TCP 등 여러가지가 있다.심지어 packet acknowledgement 방법이나 3 way handshake 방법이 다른 TCP 등 우리가 공부해왔던 “TCP”의 특징이 전혀 다른 TCP도 많으며, 사실, TCP라고 이름 붙인 protocol들의 공통점은 같은 TCP segment format을 공유한다는 점과, 그들간에 공평하게 경쟁한다는 점뿐이다.QUIC: Quick UDP Internet Connections만약, TCP의 잡다한 기능이 필요없거나 조금 다른 기능이 필요하지만, UDP 처럼 아무런 기능이 없으면 안되는 새로운 protocol로 application service를 만들고 싶다면 QUIC(Quick UDP Internet Connection)[Langley 2017, QUIC 2020]이 제격이다.QUIC는 UDP를 기반으로 만들어졌으며, 간편화된(slimed) HTTP/2, secure HTTP를 위해 고안된 더욱 성능이 좋고 application layer protocol(transport가 아님을 주의!)이다.이미 구글 등 여러 웹 서비스에서 사용중이며, 데이터 신뢰성 전송, 혼잡 제어, 연결 관리 등의 기능을 가지고 있다.아래는 현재 TCP 기반 웹과 QUIC 기반과 간단화된 HTTP/2를 이용한 HTTP/3 버전의 웹의 구조 비교이다.HTTP/3에서도 사용 예정인 QUIC의 주요 기능은 다음과 같다.  연결 중심, 보안성          TCP와 마찬가지로 QUIC는 통신 이전에 호스트간의 연결과 handshake가 필요하다. 이때 이 연결을 구별하는 수단은 source와 destination connection ID로 구별하며, 모든 packet들은 암호화되며, handshake 과정에 인증 과정과 암호 과정이 있다.      또한 이전에 사용한 TCP, TLS connection 과정중에 많은 RTT가 필요했던 TCP, TLS 구조보다 더욱 빠른 연결을 자랑한다.        데이터 흐름(Streams)          QUIC는 한 연결 안에 application 레벨의 multiplxed(다중화)될 수 있는 데이터 흐름을 제공한다. 이러한 데이터 흐름은 연결이 생성된 후 빠르게 추가할 수 있으며, 각 데이터 흐름은 양 QUIC Host간의 데이터 신뢰(reliable), in-order(패킷 순서가 맞는), bi-directional(양방향 통신)을 제공한다.      Web으로 따지면 각 object(사진, 영상, 글, 등)마다 데이터 흐름을 생성할 수 있으며 각 Connection이 connection ID로 구분 되듯이, 각 stream은 stream ID로 구분할 수 있고, 이 두 ID가 QUIC packet header에 적히게 된다. 크기가 작다면 UDP 기반의 segment에 여러 데이터 흐름의 데이터가 적재될 수도 있다.      이는 4G/5G 망에 자주 사용되는 Stream Control Transmission Protocol(SCTP)에서 이 먼저 고안한 방법이다.        데이터 신뢰, TCP-friendly, 혼잡제어 데이터 전송          아래 fig.3.59와 같이 QUIC는 각 데이터 흐름마다 데이터 신뢰성을 보장한다.      fig.3.59(a)의 HTTP 같은 경우, 하나의 TCP connectino에서 여러 request가 보내지면서, 신뢰성과 동시에 순서의 정합성을 보장해야했다. 그래서 request packet 하나가 손실되면, 해당 손실된 packet이 다시 재전송될 때까지 이후 순서의 packet들은 기다려야 했다(HOL blocking problem).      하지만 QUIC는 각 stream마다 순서의 정합성을 보장하므로, 여러 stream을 열어놓고 하나의 stream이 손실되면 다른 stream은 미리 적재되고 application 측에 전달될 수 있다.      QUIC는 TCP와 비슷한 acknowledgment 메커니즘으로 데이터 신뢰성을 보장한다.      QUIC의 혼잡제어는 TCP NewReno를 기반으로 조금의 변경을 가해서 만들었으므로, loss detection가 congestion control algorithm이 TCP와 비슷하다.      QUIC의 주목할 점은 application layer임에도 reliable data transfer와 congestion controll을 제공한다는 점이다. QUIC의 개발자는 이러한 변화가 TCP, UDP 보다 더욱 빠르게 application data를 업데이트 할 수 있다고 한다.3.9 Summary"
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%204-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EA%B3%84%EC%B8%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%B8%A1%EB%A9%B4.html": {
    title: "네트워크 정리-Chap 4-네트워크 계층-데이터 측면",
    date: " Aug 21, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%204-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EA%B3%84%EC%B8%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%B8%A1%EB%A9%B4.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 4. 네트워크 계층: 데이터 측면(Network Layer: Data plane)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/e우리는 이전 챕터에서 네트워크 계층의 서비스를 몰라도 이전 계층 서비스들의 동작들을 배울 수 있었다.이제 네트워크 계층가 진행하는 호스트간 통신에 대하여 배워보자.네트워크 계층은 앞서 배운 계층들과 다르게 네트워크 장비와 호스트에 모두 구현되어 있으며, 그에 따라 복잡하고 흥미로운 주제이기도 하다.복잡한 네트워크 계층을 데이터 측면과 컨트롤 측면으로 나누어 두 챕터 동안 배울 예정이다.데이터 측면에서는 라우터에서 하는 일인, 데이터그램이 어떻게 라우터에서 올바른 다른 라우터 링크로 포워딩되는가에 대해서 알아볼 예정이다.5장에서는 컨트롤 측면에서는 네트워크 계층이 어떻게 데이터그램의 목적 호스트에 도착할 수 있게, 올바른 라우터 경로를 형성하는가에 대해서 알아볼 것이다.라우팅 알고리즘 뿐만 아니라, OSPF, BGP 등의 라우팅 프로토콜도 알아볼 것이다.보통 이 두 측면은 라우터 하나에 함께 구현되어 있지만, 소프트웨어 정의 네트워크(SDN, Software-defiend Network)에서는 각자 컨트롤러(controller)에 따로 구현한다.이렇게 역할에 따라 두 측면으로 나누는 것이 이해에 도움이 될 것 이다.4.1 네트워크 계층 개요(Overview of Network Layer)아래 fig4.1과 같이 두 호스트 H1과 호스트 H2가 라우터 R1과 라우터 R2를 포함한 경로를 통해 통신하는 과정을 살펴보자.H1은 H2에게 데이터를 보내려하면, 트랜스포트 계층의 세그먼트가 네트워크 계층에서 데이터그램으로 캡슐화되면서 가까운 라우터인 R1으로 보내진다.이윽고 R2를 거쳐 H2에 도달한다.각 라우터의 네트워크 계층의 데이터 측면의 역할은 데이터그램들을 입력 링크에서 알맞은 출력 링크로 포워드하는 것이다.네트워크 계층의 컨트롤 측면의 역할은 데이터 측면의 행동 같은 라우터 별 포워딩들을 알맞게 편성하여 궁극적으로 호스트 간의 통신하는 것이다.그림에서 라우터들이 네트워크 계층 위로 없는 것을 주목하자, 라우터는 어플리케이션을 돌리지 않으며, 전달 계층도 필요하지 않다.4.1.1 포워딩과 라우팅: 데이터와 컨트롤 측면 (Forwarding and Routing: The Data and Control Planes)결국, 네트워크 계층의 궁극적인 목적은 송신 호스트에서 수신 호스트로 패킷을 이동시키는 것이다. 이를 위한 두가지 중요한 기능은      데이터 측면의 포워딩(Forwarding)          패킷이 라우터의 입력 링크에 도착하면 라우터는 해당 패킷을 목적 호스트에 도착하는 길에서 다음 라우터에 해당하는 출력 링크에 보내야 한다.      악의적인 호스트에게서 온 패킷이나 접근 불가한 호스트에게 가는 패킷을 block시키거나, 패킷을 복사하여 여러 링크에 동시에 뿌리는 일도 맡는다.      주로 하드웨어에서 실행되며, 수 ns(나노세컨드) 정도로 짧게 소요됨.      자동차 여행 중에 도로의 분기점에서 올바른 도로로 선택해서 가는 것과 같다.            컨트롤 측면의 라우팅(Routing)          네트워크 계층에서는 수신 호스트에 도착하기 위해 패킷이 흘러갈 일종의 길이나 경로를 설정해야한다.      이러한 경로를 만드는 알고리즘을 라우팅 알고리즘(routing algorithm)이라고 하며, 이는 컨트롤 측면에서 정의된다.      주로 소프트웨어에서 실행되며, 1초 정도로 길게 소요됨      자동차 여행 출발 전, 플로리다에서 뉴욕까지 가는 전체 경로를 계획하는 것과 같다.      각 라우터에는 포워딩 테이블이 존재하는데, 라우터는 패킷이 들어오면 해당 패킷의 헤더의 한개 이상의 필드를 살펴보고 필드의 값으로 포워딩 테이블을 색인하여 패킷을 알맞은 링크로 나아가게 한다.포워딩 테이블에 담긴 값은 해당 라우터의 외부 출력 링크(outgoing link)의 인터페이스(일종의 정보 교환 수단)이다.컨트롤 측면: 전통적인 접근방법 (Control Plane: The Traditional Approach)그렇다면 처음에 라우터는 어떻게 포워딩 테이블을 얻을 수 있는가?아래 그림에서 보듯, 전통적인 방법에서는 라우팅 알고리즘이 라우터의 포워딩 테이블을 결정한다.컨트롤 측면에 속한 라우팅 알고리즘이 각 라우터에서 실행되며, 한 라우터의 라우팅 알고리즘이 다른 라우터의 라우팅 알고리즘과 통신하며, 데이터 측면의 포워딩 테이블의 값을 결정한다.이때의 통신은 라우팅 프로토콜을 지키는 라우팅 메시지를 주고 받음으로써 실시된다.컨트롤 측면: SDN의 접근방법(Control Plane: The SDN Approach)위에서 우리는 컨트롤 측면의 기능인 라우팅 알고리즘이 데이터 측면의 기능인 포워딩 테이블을 정해주는 것을 보았고, 이는 오랫동안 라우터 생산자들이 사용했던 방법이다.데이터 측면의 포워딩 테이블을 결정해주는 컨트롤 측면의 또 다른 기능은 없을까?아래 그림이 설명하는 그러한 방법은 물리적으로 분리되었는 원격 컨트롤러(Remote Controller)가 각 라우터가 사용할 포워딩 테이블을 계산하고 분배하는 방법이다.위의 전통적인 방법과 다른 점은, 컨트롤 측면의 기능이 같은 라우터에 존재하는게 아니라 물리적으로 떨어진 다른 시스템이 진행한다는 점이다.이때 라우터는 오직 데이터 측면의 포워딩만 진행하며, 컨트롤 측면은 다른 시스템이 대신 해준다고 볼 수 있다.이러한 원격 컨트롤러는 높은 신뢰성과 보안을 자랑하는 원격 데이터 센터 내부에 ISP가 구현하며, 이때 원격 컨트롤러는 각 라우터와 forwarding table이 포함되어 있는 메시지를 교환하여 통신한다.이 방법은 소프트웨어 정의 네트워킹(SDN, software-defined networking)의 핵심이며, 스프트웨어 정의 네트워크는 해당 원격 컨트롤러를 소프트웨어로 구현하기 때문에 이렇게 불리운다.오픈소스로 많이 나와있기 때문에 코드를 살필 수 있다.4.1.2.네트워크 서비스 모델(Network Service Model)네트워크 측면에 더 알아보기 전에 네트워크 서비스 모델에 대해 알아보자.전달 계층에서 네트워크 서비스에 데이터를 줄 때, 어떠한 서비스(데이터 신뢰 통신? 혼잡 제어?)를 받을 수 있을까?전달 계층에 제공해줄 서비스를 결정하는 것이 바로 네트워크 서비스 모델이다.네트워크 서비스 모델에 따라 패킷의 통신의 성격이 달라지게 된다.예를 들어 네트워크 계층이 줄 수 있는 서비스의 예시로,  보장된 전달(Guaranteed delivery) : 해당 패킷이 확실히 목표 호스트에 도달할 수 있음을 보장하는 서비스  제한된 지연 내로 보장된 전달(Guaranteed delivery with bounded delay) : 위의 서비스에 추가로 호스트간 최소 딜레이를 보장한다.  순차 패킷 전달 (In-order packet delivery) : 패킷의 순서가 뒤섞이지 않고 원래 순서대로 통신하는 것을 보장한다.  최소 대역폭 보장(Guaranteed minimal bandwidth) : 특정 bit rate 이상의 전송 속도 보장  보안(Security) : 네트워크 계층에서 모든 데이터그램을 암호화하고 목적 호스트에서 복호화하여 보안성 보장.등 등..인터넷 구조의 네트워크 계층이 제공하는 서비스는  최선 서비스(best-effort service) 하나이다.최선 서비스(best-effort service)에서는 통신은 하되 결과의 모든 걸 보장하지 않는다.즉, 지연될 수도, 패킷 내용이 바뀔 수도, 없어질 수도, 순서가 바뀔 수도 있다는 것이다.ATM 네트워크 구조의 네트워크 계층이 최소 딜레이, 순서와 최소 대역폭을 보장하며, 인터넷 구조를 개선하기 위한 Intserv 구조[RFC 1633]에서도 최소 딜레이와 무 혼잡 통신을 보장하지만, 사용되지 않고 있다.대신, 대역폭 확보(Bandwidth provisioning)과 대역폭 적응형 어플리케이션 프로토콜(bandwidth-adaptive application-level protocol, 예를 들어 DASH 등) 등의 활약으로 고성능 멀티미디어 어플리케이션(유튜브, 넷플릭스, 스카이프…)을 돌릴 수 있게 되었다.챕터 4의 개요 (An Overview of Chapter 4)이제 네트워크 계층의 데이터 측면에 대해 알아볼 것이다.포워딩(forwarding)과 스위칭(switching)은 사람들이 많이 의미를 혼용해서 사용하고 여기서도 그럴 것이지만,라우터(router)는 네트워크 계층에서의 데이터그램의 헤더 필드를 보고 스위칭하는 스위치 장비로,링크 계층 스위치(link-layer switch)는 링크 계층에서의 패킷의 헤더 필드를 보고 스위칭하는 스위치 장비로 구별해서 부를 것이다.4.2 라우터의 내부(What’s Inside a Router)라우터에 입력 링크에서 입력된 패킷을 알맞은 출력 링크로 전달하는 역할을 하는 포워딩 기능에 대해 알아보자아래는 일반적인 라우터 구조를 표현한 것이다.그림에서 총 4개의 부분을 구별할 수 있다.      입력 포트들(Input ports)    여기에서의 포트는 앞서 배웠던 소켓과 어플리케이션의 포트를 의미하지 않으며, 포트는 라우터의 물리적인 입출력 인터페이스(꽂는 구멍)를 의미한다.          10여개의 10 Gbps 포트를 가진 기업용 라우터에서부터 800개의 100Gbps 이더넷 포트를 가진 엣지 라우터까지 다양하다.        라우터의 물리적 링크(그림에서 왼쪽, 또는 오른쪽 끝에 있는 파란색 박스)를 종료하는 물리 계층 기능 담당    반대쪽에 들어오는(incoming) 링크의 링크 계층(파란 포트 사각형 안에 하얀 박스들)와 상호운용(interop)하는데 필요한 기능 담당    가장 중요한 기능으로, 룩업(look up) 기능 담당(위 입력 포트 박스의 오른쪽 끝, 포워딩 테이블을 참조하여 스위치 패브릭을 통해 적절한 출력 포트에 배정)    라우팅 프로토콜 정보가 담긴 컨트롤 패킷 같은 경우 입력 포트에서 라우팅 프로세서로 보낸다.        스위칭 패브릭 또는 스위칭 매트릭스(Switch fabric)    입력 포트와 출력 포트를 연결하는 역할, 일종의 라우터 내부의 네트워크, 입출력 포트들과의 가상의 회선이 마치 직물과도 같다고 해서 이렇게 불리운다.        출력 포트들(Output ports)    스위칭 패브릭에게 받은 패킷을 저장해두고 링크 계층, 물리 계층 기능을 통해 나가는(outgoing) 링크로 보낸다., 양방향 링크일 경우 같은 라인 카드의 입력 포트와 쌍을 이룸?        라우팅 프로세서(Routing processor)    과거에는 라우팅 테이블과 부착된 링크 상태 정보, 포워딩 테이블을 유지 관리하는 컨트롤 측면 기능을 수행    SDN 라우터에서는 라우팅 프로세서가 원격 컨트롤러와 통신하여 포워딩 테이블 항목을 받아와 라우터 입력 포트에 설치 하는 역할을 함.    나중에 배울 네트워크 관리 기능도 수행  위 4개 부분은 거의 모든 라우터에서 하드웨어로 구현되었으며,  여러 포트의 데이터그램 처리 파이프라인으로 데이터그램을 처리가능해 소프트웨어 구현에 비해 더 빠를 수 있었다.이러한 데이터 측면 동작이 ns 수준의 빠른 처리가 가능하다면, 라우터의 컨트롤 측면(라우팅 프로토콜 실행, 링크에 대한 응답, 원격 컨트롤러와의 통신, 관리 기능 등)은 ms~s 수준으로, 보통 라우팅 프로세서(CPU) 내에서 소프트웨어로 처리된다.또한, 자동차가 로터리에서 잠시 고민할 때 처럼, 패킷들도 라우팅될 때 처리가 필요한데, 그때 필요한 정보는 다음과 같다.      목적지 기반 포워딩(Destination-based forwarding)    자동차가 출발 하자마자 로터리 한번만 지나면 목적지에 도착하는 상황이라면, 로터리 안내원은 어느 도로 출구가 목적지인지 구별해야 한다.(패킷의 목적지 호스트 주소)        일반화된 포워딩(Generalized forwarding)    자동차의 도로 출구는 또한, 자동차의 정보(패킷의 헤더)(번호판, 자동차 모델, 생상년도)에 따라 차별화할 수 있을 것이다.    예를 들어, 일부 특별 회원에게는 속도가 빠른 도로출구로 보내주고, 아닌 회원에게는 느린 회선을 줄 수 있다.  자동차가 출구를 결정하고 떠날 때, 같은 결정을 한 여러 자동차에 의해 밀릴 수도 있다.이러한 비유를 들자면 진입 도로와 스테이션은 입력 포트이며, 로터리는 스위칭 패브릭, 로터리 출구는 출력 포트라고 생각하면 되며, 자동차 러쉬아워 처럼 병목현상을 겪을 수도 있으며, 안내원이 일처리속도가 느리면 대기열 등이 생기거나, 처음보는 차는 차별화 할 수 없는 등의 문제가 생긴다.이후에 라우터 기능에 대해 알아볼 때 이해하기 쉽도록, 일단 패킷의 헤더는 보지않고 도착 주소만 보고 판단하다고 가정할 것이다.4.2.1.입력 포트 처리와 목적지 기반 포워딩(Input Port Processing and Destination-Based Forwarding)fig.4.5는 입력 패킷 처리에 대한 그림이다.입력 포트는 앞서 설명한 기능과 라우터 기능의 핵심인 룩업 기능을 포워딩 테이블을 통해 실행해 패킷을 스위칭 패브릭을 통해 보내며, 포워딩 테이블은 라우팅 테이블에 의해 다른 라우터의 프로세서와 상호작용을 통해 가져오거나, SDN의 경우 원격 컨트롤러에 의해 처리된다.포워딩 테이블은 라우팅 프로세서에서 복사되어 라인 카드로 버스(일종의 데이터 통로, fig4.4의 점선, PCI bus 등이 존재)를 통해 전달되어, 각 라인 카드마다 입력 포트에 의해 개별 처리되므로 라우팅 프로세서를 이용할 필요없어, 프로세서 병목 현상이 일어나지 않는다.패킷이 스위칭되어 도착할 출력 포트를 패킷의 목적지 주소를 통해 구해야하는 상황에서 32비트 아이피 주소를 이용해 포워딩 테이블과 대조하려면 브루트 포스를 이용할 경우 도합 $2^{32}$개의 주소를 대입해야하며, 이는 불가능하다.다음과 같이 라우터에 0부터 3까지 4개의 링크가 있고 링크 인터페이스는 다음과 같다고 생각 하자.            Destination Address Range      Link Interface                  11001000 00010111 00010000 00000000부터 11001000 00010111 00010111 11111111까지      0              11001000 00010111 00011000 00000000부터  11001000 00010111 00011000 11111111까지      1              11001000 00010111 00011001 00000000부터 11001000 00010111 00011111 11111111까지      2              나머지      3      이 예시에서는 확실히 $2^{32}$를 전부 볼 필요는 없어보인다.극단적으로 아이피 범위에서 겹치는 접두어(prefix)를 이용하면 4개의 항목으로 줄일 수 있는데,            Prefix      Link Interface                  11001000 00010111 000110      0              11001000 00010111 00011000      1              11001000 00010111 00011      2              Otherwise      3      이런 식으로 라우터는 포워딩 테이블의 항목들의 접두어(Prefix)를 패킷의 목적지 주소와 대조하여 일치하는 링크 인터페이스로 보낸다.만약 11001000 00010111 00011000 110101110 처럼 여러 접두어와 일치하는 경우에는 가장 길게 일치된 링크 인터페이스로 보내며, 위의 경우 1번이다.이러한 방법을 최장 접두어 일치(Longest prefix matching)이라고 하며, 접두어가 가장 길게 일치하는 테이블의 항목으로 패킷을 보내게 된다.자세한 사항은 4.3절에서 추가설명할 것이다.포워드 테이블의 존재 덕분에 룩업 기능은 하드웨어적으로 간단하게 최장 접두어 일치만 찾으면 되며, ns 수준의 빠른 성능을 자랑하게 되었다.또한 하드웨어뿐만 아니라 선형 탐색보다 빠른 룩업 알고리즘들을 이용해 더욱 빨라졌는데[Gupta 2001, Ruiz-Sanchez 2001],  DRAM과 DRAM cache로 SRAM을 칩셋 내부에 포함하여 빠른 메모리 접근 시간을 달성하거나, 이진 내용 주소화 기억장치(TCAM, Ternary Content Addressable Memories)를 이용해 메모리에 IP address를 투영하여 상수 시간 내에 값을 구할 수도 있다.이렇게 출력포트가 정해진 패킷은 스위칭 패브릭에 들어가기 전에 큐에 들어가고, 스케듈링에 따라 처리된다. 이러한 패킷의 블록킹, 큐잉, 스케듈링은 나중에 볼것이다.이러한 룩업 이외에도 입력 포트는 물리, 링크 계층 처리, 패킷의 버전 번호, 체크섬(checksum), TTL(Time-to-live) 등이 확인되고 체크섬과 TTL은 덮어쓰기, 네트워크 관리를 위한 카운터(주로 전달받은 아이피 데이터그램 수) 변경 등의 일을한다.또한, 입력 포트의 룩업과 같은 “일치 후 행동(match plus action)”은 라우터 이외에도 링크 계층 스위치, 방화벽 등, 네트워크 주소 번역기(NAT, network address translator) 같은 여러 네트워크 개념에서 볼 수 있다.4.2.2 스위칭(Switching)스위칭 패브릭(switching fabric)은 실제로 패킷이 입력 포트에서 출력 포트로 스위칭(= 포워딩)되기 때문에 아주 중요하다, 이러한 동작의 원리는 다음과 같은 방법들이 존재한다.      메모리를 통한 스위칭(Switching via memory)    가장 초기의 간단한 방법은 라우팅 프로세서(=CPU)에 의해 통제되는 것이다. 입력과 출력 포트는 기존의 입출력 장치에서 기존의 운영체제에 의해 처리되는데, 라우팅 프로세서에 의해 입력된 패킷이 인터럽트(Interrupt)되어, 프로세서 메모리로 복제된 뒤, 패킷 헤더에서 목적지 주소를 가져와 포워딩 테이블을 참고해 적절한 출력 포트의 버퍼로 패킷을 복사 붙여넣기 한다.    이 시나리오에서는 메모리 대역폭을 B라고 놓으면, 메모리에 패킷을 읽고, 쓰면서 최대 패킷 처리속도(throughput)가 B/2로 줄어들며, 설령 목적지가 다르더라도 공유 시스템 버스 상에서 하나씩 메모리에서 읽고 쓸 수 있으므로 동시에 포워딩 될 수 없다.    최근에도 메모리를 통해 처리하는 라우터와 다른 점은 룩업 기능과 적절한 메모리 지점에 패킷을 적재하는 기능을 입력 라인 카드에서 실행한다는 점이다.    각각의 라인 카드에서 병렬적으로 스위칭되는 모습이 마치 공유 메모리 다중처리장치(multiprocessor)로 처리하는 것과 같다.        버스를 이용한 스위칭(Switching via a bus)    이 방법은 라우팅 프로세서를 거치지 않고 공유 버스(share bus)를 이용해 직접적으로 입력 포트에서 출력 포트로 전달된다.    **패킷이 입력포트에 도착하면 버스를 통해서 어느 출력 포트로 가야하는 지에 대한 정보를 스위치 내부 레이블이 헤더에 적히게 된다. **    **버스를 지나간 이후, 모든 출력포트는 각각 모든 패킷의 복사본을 받지만, 그 중 패킷의 레이블을 살펴 출력 포트가 일치하는 것만 취하고 나머지는 버리게 된다. **    이 레이블은 출력 포트에서 제거되며, 오직 버스를 넘어 스위칭되는데만 사용되며, 만약 여러 입력포트에서 여러 패킷이 도착하면 버스 이용을 기다리기 위해 큐잉된다.    모든 패킷이 하나의 버스를 지나야 하므로, 마치 로터리에 차량이 한대씩만 들어갈 수 있는 것과 같으므로, 최대 스위칭 스피드가 버스 처리 속도에 제한되어 있어 한계가 있지만, 소규모 네트워크나 기업 망등에는 사용할 만하다.        상호연결 네트워크를 이용한 스위칭(Switching via an interconnection network)    하나의 공유 버스의 대역폭 한계를 극복하는 방법 중 하나로 다중처리장치(multiprocessor) 컴퓨터 구조에서 사용하던 상호연결 프로세서의 정교한 상호연결 네트워크를 이용하는 방법이 있다.    십자 스위치(crossbar switch)는 fig.4.6의 Interconnection network 그림처럼 입력 포트 수의 버스, 출력 포트 수만큼의 버스를 각각 수직 버스, 수평 버스로 놓고 이를 교차시켜 $N^2$개의 교차지점을 만들고, 각 교차지점을 스위치 패브릭 컨트롤러에 의해 여닫을 수 있게 한다. 패킷이 한 입력 포트에서 출발하면 스위치 패브릭 컨트롤러가 버스의 교차지점을 조작해 특정 출력 버스로 향하게 만든다.    이전 두 방법과 다르게 동시에 여러 패킷을 처리가 가능한 비블로킹(non-blocking) 방식이지만, 만약 서로 다른 입력포트에서 온 두 패킷이 같은 방향으로 향한다면 이 역시 처리가 순서대로 처리될 때 까지 기다려야 한다.    이를 극복하기 위해 다중층 스위칭 패브릭(multi-stage switching fabric)을 이용해 층의 갯수 만큼 같은 출력 포트를 향하는 패킷들을 동시 처리할 수 있다.    또한, 입력포트에서 패킷을 k개로 나눈 뒤, 이를 다중 스위칭 패브릭에서 동시에 빠르게 전달하고, 출력 포트에서 k개의 조각을 1개의 패킷으로 재조립하여 빠르게 처리할 수도 있다.  4.2.3.출력 포트 처리(Output Port Processing)출력 포트 처리에서는 패킷을 받고 메모리에 적재한 뒤, 출력 링크에 전송한다.출력 포트 처리는 패킷 선택(=스케듈링)과 디큐잉(de-dqueueing), 필요한 링크 계층과 물리 계층의 전송 기능을 포함한다.4.2.4.큐잉이 일어나는 부분(Where Does Queuing Occur?)입력 포트와 출력 포트의 기능을 살펴보았다면, 양 측에서 모두 패킷의 큐잉이 일어난다는 것을 알 수 있다.마치, 로터리의 진입 부분과, 출구 도로에서 자동차가 막히듯이, 스위칭 패브릭의 처리속도, 라인 속도 등에 따라 양측 버퍼에 큐잉이 일어날 수 있으며, 이러한 큐잉이 아주 커지면 결국 메모리의 한계에 도달해 패킷 손실(packet loss)일어난다.우리가 앞선 챕터들에서 배웠던 패킷의 손실이 일어나는 곳이 바로 이 두 곳이다.각 라인별 처리 속도를 $R_{line}$, 패브릭 스위치의 속도가 $R_{switch}$, 입출력 포트 쌍이 총 N개가 있으며, 입력 포트마다 동일한 크기의 모두 서로 다른 출력포트가 목적지인 packet이 동시에 하나씩 도착한다고 가정하면, $R_{line}$*N보다 $R_{switch}$가 크거나 같으면 각 패킷이 모두 적시에 처리되어 각 큐잉이 발생하지 않는다.만약 $R_{switch}$가 충분히 크지 않다면 큐잉이 발생하며, 각 패킷은 자기 차례가 올때까지 기다려야 한다.입력 큐잉(Input Queuing)위의 가정을 조금 바꾼다면, $R_{switch}$ 가 충분히 빠르더라도 큐잉이 발생할 수도 있는데, 각 패킷이 갈 출력포트가 겹치는 순간, 패킷은 큐잉이 생겨 해당 출력포트가 처리가 끝날때 까지 기다려야 한다.이는 큰 문제가 되는데 아래 Fig.4.8과 같이, (위에서 부터)1번 입력 포트와 3번 입력 포트의 첫번째 패킷이 동일한 출력 포트를 목적지로 가지게 되어, 3번째 입력포트에 큐잉이 발생하게 되고, 3번째 입력포트의 2번째 패킷은 목적지인 2번째 출력 포트가 아무도 사용하지 않고 있음에도 불구하고 자신의 앞에 있는 패킷이 처리될때 까지 접근할 수 없고 기다려야 한다.이러한 경우를 HOL 블록킹(HOL Blocking, head-of-the-line blocking)이라고 부르며, 엄청난 비효율과 이에 의한 패킷 손실을 초래하게 된다[Karol 1987].HOL 블록킹의 해결방안이 몇 개 제안 되기도 했다[McKeown 1997].출력 큐잉(Output Queuing)$R_{switch}$이 무한히 크고 HOL 블록킹이 발생하지 않고, 출력 큐잉이 발생할 수 있다.출력 포트가 나가는 링크로 보내는 전송 시간이 추가로 필요하기 때문에 전송이 완료될 때까지 출력 포트에 도착한 패킷들이 큐잉되며, 이때도 처리속도가 충분하지 않으면 버퍼 오버플로우로 인한 패킷 손실이 발생할 수 있다.버퍼 크기가 충분하지 않고, 패킷의 처리속도 보다 도착 속도가 빠르다면 여러가지 방안을 고민해야 하는데, 그중 하나가 버퍼가 가득찬 이후 도착하는 패킷은 버리는 꼬리 버리기(drop-tail) 정책이 있고, 버퍼 내부의 큐잉된 패킷을 몇개 버려 이후 오는 패킷을 받아들이는 정책 또한 있다.버퍼가 가득차지 않았음에도 미리 패킷을 버리거나 패킷의 헤더의 Explicit congestion Notification bit를 표시하여 수신자 측에 알리는 정책 또한 존재하는데 이러한 정책을 능동 큐 관리 알고리즘(active queue management, AQM)이라 한다.가장 널리 알려진 AQM 알고리즘은 임의 조기 발견(RED, Random Early Detection)이며, PIE(Proportional Integral controller enhanced)나 CoDel 또한 존재한다.이러한 패킷의 스위칭, 큐잉, 전송 등은 패킷 스케듈러에 의해 실행되며 나중에 다시 다룰 것이다.적정 버퍼링 찾기(How Much Buffering Is “Enough”)?앞서 큐잉은 대량의 패킷이 라우터의 입출력 포트에 도착하거나 패킷의 도착량이 패킷 포워딩 처리량 보다 순간적으로 넘어갔을 때 등장한다고 이야기 했다.이러한 큐잉이 지속되면 버퍼 크기는 점점 커지다가 가득차게 되고 결국 패킷이 손실된다.그렇다면 적절한 버퍼링은 얼마나 되야할까? 이 답은 생각보다 복잡하고 네트워크 여러 분야가 관련된다.오랫동안 버퍼 사이즈의 정답은 $B=RTT\\cdot C$로, 버퍼링 사이즈 B는 평균 왕복 시간(RTT)에 링크 전송 허용량(capacity)를 곱한 것이였다.예를 들어, 10Gbps 링크에 250msec RTT라면 2.5Gbits의 버퍼 사이즈가 적정량이었다.이는 큐잉과 TCP 연결의 상관관계에 따라 나온 값이었다.최근에는 여러 실험과 연구 끝에 $B=RTT\\cdot C/\\sqrt{N}$이며, N은 링크 내부의 독립된 TCP 연결 수이다.보통 버퍼 사이즈가 크면 클수록, 많은 양의 패킷 도착량을 받아들일 수 있고, 패킷 손실이 줄어들어 좋다고 생각할 수도 있지만.버퍼 사이즈가 크면 손실되는 패킷이 줄어들면서, 혼잡 제어를 위해 표시된 패킷의 딜레이가 커지고, RTT가 증가한다.예를 들어, 게임이나 실시간 영상회의 같은 경우, 버퍼가 10배 늘어날 경우 종단 간의 지연 시간은 10배가 늘어난다.늘어난 RTT에 의해 TCP의 반응성과 혼잡 상황 파악 등이 늦어 지기도 한다.즉 버퍼 크기는 소금처럼 적절한 양이 좋다.위의 Fig.4.10은 TCP 세그먼트를 게임 서버에 보내는 홈 라우터의 예제이다.패킷 전송 딜레이(packet transmission delay, 라우터에 나가 링크에 적재되는 딜레이)가 20ms, RTT가 200ms, queueing delay는 무시할 정도로 작고, 0초에 25개의 packet이 한꺼번에 queue에 들어왔다고 가정하자.그럴 경우 fig.4.10.b 와 같은 그래프가 나타나는데, 매 20ms에 1개씩 패킷이 처리되며, 큐의 길이가 점점 줄어들어 200ms에 5개가 남는다.200ms 시점에서 RTT가 200ms이므로, 가장 처음에 처리했던 1번째 패킷의 ACK 가 도착함과 동시에 21번째 패킷이 처리되어 나가는 링크로 나가게 된다.ACK가 도착했으므로 TCP 송신자는 다음 세그먼트 번호에 속한 패킷을 보내려할 것이다 (TCP의 self-clocking 참조).ACK가 하나 도착할 때 마다 새로운 패킷을 하나 보내게 되므로, 큐의 길이는 5로 유지가 될것이다.즉, 언제나 새로 보내는 패킷은 큐의 가장 마지막 5번째에 들어가게 될 것이며, 게임, 멀티미디어 응용 프로그램 같이 패킷을 지속적으로 보내는 한, 사용자는 언제나 같은 시간의 딜레이에 시달리게 된다.버퍼의 크기가 클수록 큐의 길이가 길어지므로 지연시간은 더욱 컷을 것이다.위 시나리오처럼 일정 길이로 유지되는 버퍼링에 의한 지속적이고 일정한 딜레이를 버퍼블로트(bufferbloat)이라고 하며, 단순히 처리율(throughput) 뿐만 아니라 최소 딜레이(minimal delay) 중요하다는 것을 알 수 있다. (시나리오 상에서 처리율은 일정하고 이상적인 상태를 유지했다.)케이블 네트워크를 위한 DOCSIS 3.1 기준에서는 처리율 성능을 유지하며 이러한 버퍼 블로트와 싸우기 위해 자세한 AQM 메커니즘을 추가하였다.4.2.5 패킷 스케듈링 (Packet Scheduling)라우터가 큐된 패킷들을 어떤 순서로 처리하는지 알아보자.흔히 알고있는 먼저 온 패킷이 먼저 처리되는 선입선출(FCFS(first-come-first-served), 또는 FIFO(first-in-first-out))부터,우선등급과 계급을 정하여 차별적으로 처리 서비스를 제공하는 방법,패킷을 종류별로 나눈 뒤, 종류별로 순차적으로 처리되는 라운드로빈(round-robin) 등이 있다.선입선출(FIFO) (First-in-First-Out (FIFO))아래 fig.4.11은 선입선출 구조를 나타낸 그림이다.버퍼가 넘치게 되면, 추가로 들어오는 패킷을 버리거나, 또는 큐 내부의 패킷을 버려 공간을 만드는 정책 등도 고려해야하지만, 이번에는 버퍼가 넘치는 경우가 없다고 가정하고, 오직 패킷이 올바르게 처리될 시에만 큐에서 제거된다고 가정하자.선입 선출(FIFO, First-in-First-out) 구조는 출력 링크 도착시간 순으로 패킷의 링크 전송을 처리하는 것이다.우리가 일상 생활에서 줄을 서는 방법과 같으며, 익숙한 방법이다.아래 fig.4.12는 FIFO 큐의 동작 예시이다.파란색 상자로 표시된 패킷들에 적혀있는 번호는 도착 순서이며, Packet in service는 큐에서 패킷들이 보낸 시간이다.패킷 전송 처리 시간이 3 단위시간 만큼이라고 가정하면, 선입선출 구조에서는 도착한 순서와 같게 패킷이 떠나며, 패킷이 없을 때, 5번 패킷이 도착할 때 까지 대기 상태를 유지하는 것도 볼 수 있다.우선순위 큐잉(Priority Queuing)우선순위 큐잉에서는 도착한 패킷들이 특정 기준에 따라 우선순위 등급이 나뉘어지고, 각자 별도로 존재하는 큐에서 처리되게 된다.실무에서는 우선순위 큐잉을 통해 네트워크 관리자가 큐를 조작하여 네트워크 정보를 담은 패킷을 우선처리 할 수 있다.이메일처럼, 실시간 처리가 필요없는 패킷은 우선순위가 낮게, VOIP 같은 실시간 패킷은 우선순위를 높게 잡을 수도 있을 것이다.보통은 각 우선순위 종류 마다 별도의 큐를 가지고 있으며, 기다리는 패킷이 존재하는 높은 우선순위의 큐 먼저 처리하는 방식이다.같은 우선순위의 큐 내의 패킷들은 보통 선입선출로 처리된다.위 fig.4.14는 패킷의 색깔로 우선순위를 구별한다.도착 순서와 관계없이 우선순위가 높은 파란색 색깔의 패킷들이 우선 처리된 후, 하얀 패킷들이 처리되는 모습이다.단, 4번과 2번 패킷처럼, 우선순위가 낮은 패킷이 전송되는 중간에 우선순위가 높은 패킷이 도착해도, 중간에 전송을 취소하고 우선순위가 높은 패킷을 먼저 전송해주진 않는데, 이를 비선제 우선순위 큐잉(non-preemptive priority queuing)라고 한다.라운드 로빈과 가중 공정 큐잉(WFQ) (Round Robin and Weighted Fair Queuing(WFQ))라운드 로빈 큐잉 정책에서는 패킷들은 우선순위 큐잉때 처럼 여러 분류로 나뉘지만, 우선순위 때와 달리 서비스를 다르게 제공하지 않고 클래스 별로 순차적으로 제공한다.작업 보존 큐잉 (work-conserving queuing)의 경우 대기하고 있는 패킷이 있을 경우, 링크가 대기상태에 있지 않고 계속 패킷을 전송하게 하는데, 만약 특정 분류에 더이상 전송 대기 중인 패킷이 없다면 라운드 로빈 순서로 다음 분류의 패킷들을 전송한다.위 fig.4.15는 2개의 분류를 가진 라운드 로빈의 예시이다.마찬가지로, 작업 보존 큐잉의 성격을 띄고 있어, 2번 패킷을 보낸 뒤, 다음은 라운드로빈에 의해 흰색 패킷의 차례지만, 흰색 패킷의 큐가 비어있어 바로 같은 파란색인 4번 패킷이 전송되었다.일반화되고 널리 사용되고 있는 라운드 로빈 정책의 일종으로 가중 공정 큐잉(WFQ, weighted fair qureuing)이 있다.위 4.16은 WFQ를 나타낸 것으로, 기본적으로 라운드 로빈처럼 각자 분류 별로 큐를 생성한 뒤, 각 분류에 맞는 패킷 간에 선입선출로 전송된다.역시나 작업 보존 큐잉의 성격을 띄고 있어 비어있는 분류의 큐는 대기하지 않고 다음 분류의 큐로 넘어간다.WFQ가 기존의 라운드 로빈과 다른 점은, 각 분류마다 가중치를 설정하여, 해당 가중치의 비율 만큼 서비스를 배정한다는 점이다.예를 들어 $w_1$의 차례가 됬다면, $w_1/\\sum_{i=1}^{3}{w_i}$ 만큼의 패킷이 처리된 후, $w_2$의 차례가 되는 식이다.위와 같이 처리하면 가중치 비율 만큼 처리율(throughput), 또는 대역폭이 형성되어, 이를 통해 분류별로 서비스를 차별화할 수 있다.다만, 실제 패킷은 소수점으로 나누어 전송할 수 있지 않고, 이산적인(discrete) 처리가 필요하며, 패킷 중간에 전송이 취소되지 않고, 끝까지 전송된 후, 다음으로 넘어가며, 이를 packetization 문제라고 한다..망 중립성 (Net Neutrality)앞서 설명했던 패킷 스케듈링에 의힌 패킷 차별화를 통해 SNMP 네트워크 관리 데이터그램의 포트 번호(161)에 우선순위를 주어 네트워크 혼잡을 막는 등에 사용할 수 있지만, ISP의 의도에 따라 특정 고객에게 높은 대역폭과 처리율을 주거나, 특정 회사의 영업 방해 등을 이룩할 수도 있다.이러한 ISP의 행동은 국가 마다 다른 법률에 적법성이 달려있지만, 대표적인 예로 미국의 정책인 망중립성(Net Neutrality)에 대해 알아보자.망 중립성은 사실 정확한 정의는 없지만, 미국 연방 통신 위원회가 2015년 3월에 열린 열린 인터넷의 보호 및 촉진을 위한 질서[FCC 2015] 에서 다음과 같은 규칙을 제공하였다.  차단 금지 (No blocking) : 인터넷 제공업에 종사하는 자는 합리적인 네트워크 관리에 준해 합법적인 컨텐츠, 응용 프로그램, 서비스, 무해한 장비를 차단하면 안된다.  조정 금지 (No Throttling) : 인터넷 제공업에 종사하는 자는 합리적인 네트워크 관리에 준해 합법적인 컨텐츠, 응용 프로그램, 서비스, 무해한 장비를 저해하거나 조정하면 안된다.  유료 순위 금지 (No Paid Prioritization) : 인터넷 제공업에 종사하는 자는 유료 순위(Paid Prioritization)를 제공하면 안된다. 유료 순위란, 트래픽 형성, 우선순위 지정, 리소스 예약 등의 모든 기술을 포함하여 직간접적으로 인터넷 제공자가 특정 트래픽을 다른 트래픽에 비해 선호되도록 네트워크를 관리하는 것을 의미한다.실제로 이렇나 규칙을 어겨 발각된 ISP도 존재하고, 망 중립성은 고객의 편의 제공과 인터넷의 혁신에 주제를 맞춰 맹렬히 토의되고 있는 주제이다.최근에는 미국 연방 통신 위원회는 이러한 인터넷 관리에 대한 강제적인 규칙이 아니라 ISP의 투명성에 초점을 맞춰 규칙이 대체되기도 했다.[FCC 2017]4.3 인터넷 프로토콜(IP) : IPv4, 어드레싱, IPv6, 등등 (The Internet Protocol (IP): IPv4, Addressing, IPv6, and More)인터넷 프로토콜 (IP), IPv4, IPv6, 아이피 어드레싱에 대해 알아보자.4.3.1.IPv4 데이터그램 형식 (IPv4 Datagram Format)네트워크 계층에서는 패킷을 데이터그램이라고 부른다. 먼저 IPv4 데이터그램의 구조와 의미에 대해 알아보자.총 옵션 제외 20 byte의 헤더를 가지고 있고, TCP header의 20 bytes와 합치면 총 40 bytes의 헤더가 응용 계층 메시지에 존재한다.fig.4.17은 IPv4의 데이터그램 형식이다.            필드 이름      설명      크기(bit)                  Version number (버전명)      데이터그램의 아이피 프로토콜 버전, 버전에 따라 라우터가 데이터그램을 비트들을 해석하는 방법이 달라짐. (IPv4 vs IPv6)      4              Header length (헤더 길이)      IPv4는 추가적인 옵션을 헤더에 넣을 수 있으므로 패킷 내의 페이로드 부분이 시작하는 위치를 바이트 기준으로 적는다. 옵션이 없는 기준에서는 20byte      4              Type of service (TOS, 서비스 종류)      아이피 데이터그램의 종류를 구분하기 위한 부분, 라우터의 정의에 따라 달라지지만, 보통 실시간 데이터그램(게임 등) vs 비실시간 데이터그램(FTP 등)을 구분하거나, ECN(Explicit Congestion Notification)을 위해 네트워크 상황을 알리는 비트로 사용      8              Datagram length (데이터그램 길이)      데이터가 포함된 아이피 데이터 그램의 바이트 기준의 총 길이, 16 비트이므로 이론상 $2^{16}$ 바이트까지 커질 수 있다.(보통은 이더넷 프레임 최대 크기를 통과하기 위해 1500바이트 이하이다.)      16              Identifier (식별자), flags (부호들), fragmentation offset (조각 오프셋)      IPv4는 데이터그램의 조각화를 지원하므로 여러 조각으로 나뉜 데이터그램을 합치기 위한 헤더 필드, 조각난 데이터그램은 목적 호스트에 도착하면 이를 통해 재조립된다. IPv6는 더이상 지원하지 않는다.      16, 3,13              Time-to-live (TTL)      데이터그램이 장기간 라우팅 루프 등의 이유로 영원히 네트워크 상을 떠도는 것을 방지하기 위한 필드, 라우터를 지날 때마다 1씩 줄어들고 0이 되면 라우터가 데이터그램을 버린다.      8              Protocol (프로토콜)      데이터그램이 목적지 도착 후, 어떤 전달 계층 프로토콜로 사용된 데이터인지 알려주는 필드, TCP의 경우 6, UDP의 경우 17 이외는 [IANA Protocol Numbers 2016] 참조. 이는 전달 계층 세그먼트의 포트 번호처럼 다른 계층간의 데이터 변환을 위해서다.      8              Header checksum (헤더 체크섬)      라우터가 비트 에러를 탐지하기 위한 필드, 페이로드 제외 헤더만 대상이라는 점을 제외하고 전달 계층의 체크섬과 같은 방식이다.(원리, 중복 구현 이유는 챕터 3 체크섬 참조), 에러가 발견되면 보통 데이터그램을 버린다. 라우터를 지날 때마다 TTL이 바뀌므로 라우터가 새로 바꿔넣어야 한다. [RFC 1071]에 빠른 인터넷 체크섬 알고리즘이 있다.      16              Source and Destination IP addresses (주소)      데이터 그램 생성시, 출발지 주소와 목적지 주소를 적어넣는다. 목적지 주소는 DNS 룩업을 통해서 구하기도 한다. 자세한 내용은 이후 설명      32,32              Options (옵션)      아이피 헤더에 추가적인 정보를 추가할 수 있다. 데이터그램 크기가 증가할 뿐만 아니라, 라우터에서 해당 옵션을 지원하지 않을 수도 있고, 자원을 잡아먹는 옵션 필드 처리 과정을 추가시키기 때문에 오버헤드를 유발한다는 이유로 거의 사용되지 않는다. 옵션은 IPv6부터 지원하지않는다.      가변길이              Data(payload) (데이터)      전달 계층 세그먼트가 캡슐화(encapsulated)되어있는 부분, TCP, UDP, ICMP 등이 존재할 수 있다.             4.2.3. IPv4 어드레싱 (IPv4 Addressing)아이피에 대해 알아보기 전에 호스트와 라우터, 인터페이스에 대한 이야기를 해보자.호스트는 보통 하나의 연결링크를 가지고 있으며, 이를 통해 인터넷과 통신이 가능하다. 이러한 통신 주체(호스트, 라우터 등)와 물리적 연결(케이블, 와이파이) 간의 경계를 인터페이스라고 한다.라우터 또한, 패킷을 주고 받을 수 있으므로 인터페이스를 가지고 있으며, 심지어 하나만 가지고 있는 호스트와 달리 여러 들어오는 링크에서 온 패킷을 여러 나가는 링크로 보내줘야 하므로 각 입출력 링크 별로 하나씩 여러개의 인터페이스를 가지있다.누군가가 그러한 라우터나 호스트와 연결을 하고 싶으면, 그들의 인터페이스를 통해서 하는 것이므로, 그들의 인터페이스의 고유한 주소를 알아야하는데, 이를 아이 주소(IP address)라고 한다. 즉, 아이피 주소는 엄밀히 말하면 인터페이스의 주소이지, 호스트, 라우터의 주소가 아니다.각 아이피 주소는 32 비트(4 바이트) 길이로, 총 $2^{32}$개의 주소를 표현할 수 있으며, 바이트 별로 점으로 구분되는 10진수 정수 표현(dotted-decimal notation)으로 표현한다.(11000001 00100000 11011000 00001001) == (193.32.216.9)NAT(4.3.3 참고)내부의 인터페이스를 제외한 모든 인터페이스는 전체 인터넷 망에서 유일한 값을 가지고 있어야 하며, 마음대로 고를 수 없다.하지만, 아이피 주소의 일부분은 인터페이스가 연결된 서브넷(subnet)에 의해 결정되기도 하는데 예시를 들자면, 아래 fig.4.18은 아이피 어드레스와 인터페이스의 예시로, 3개의 인터페이스를 가진 하나의 라우터가 일곱개의 호스트를 연결하고 있고, 라우터와 연결된 호스트들의 인터페이스의 아이피 주소가 라우터의 인터페이스에 따라 24 비트 자리까지 동일함을 알 수 있다.하나의 라우터 인터페이스에 여러개의 호스트가 연결될 수 있는 이유는 나중에 설명할 이더넷 LAN(Ethernet LAN)의 이더넷 스위치나 무선 접근 지점(wireless access point)에 의한 것이다.이런식으로 여러 호스트 인터페이스가 하나의 라우터 인터페이스와 연결을 이룬 것을 서브넷(subnet)[RFC 950] 또는 아이피 네트워크라고 한다.아이피 어드레싱은 이러한 주소를 (223.1.1.0/24) 같은 식으로 표현해 할당하는데 이를 서브넷 마스크(subnet mask)라고 한다.여기서 모두 동일한 값을 가지는 앞의 24bit는 \"x.x.x.x/24\" 같은 식으로 표현하고 이를 서브넷 주소(subnet address)라고 하며, 서브넷에 추가로 참가할 인터페이스는 해당 어드레스 형식을 맞춰야 한다.  현재 위의 그림의 서브넷 예시에서 223.1.1.0/24 서브넷에 속한 아이피는 223.1.1.1~223.1.1.3과 라우터 인터페이스인 223.1.1.4까지 총 4개이다.  또한 총 3개의 아이피 서브넷이 존재하며, 각각 223.1.3.0/24, 223.1.2.0/24가 추가로 존재한다.  여기서 서브넷 마스크에서의 0은 실제 주소가 0임을 표현한 것이 아닌 소속한 아이피 주소들 별로 구분되는 부분이다.위에서 설명한 아이피에서의 서브넷 정의는 실제로 이더넷 영역에서는 엄격하게 지켜지지 않는데, 아래와 같은 예시를 볼 수 있다.여기서 서브넷은 우리가 생각하는  223.1.1.0/24          호스트 인터페이스 223.1.1.1      호스트 인터페이스 223.1.1.4      라우터 인터페이스 223.1.1.3        223.1.2.0/24          호스트 인터페이스 223.1.2.1      호스트 인터페이스 223.1.2.2      라우터 인터페이스 223.1.2.6        223.1.3.0/24          호스트 인터페이스 223.1.3.1      호스트 인터페이스 223.1.3.2      라우터 인터페이스 223.1.3.27      뿐만 아니라  223.1.7.0/24          라우터 인터페이스 223.1.7.0      라우터 인터페이스 223.1.7.1        223.1.8.0/24          라우터 인터페이스 223.1.8.0      라우터 인터페이스 223.1.8.1        223.1.9.0/24          라우터 인터페이스 223.1.9.1      라우터 인터페이스 223.1.9.2      또한 추가로 존재한다.그러므로 서브넷은 인터페이스 하나를 통해 분리되어 있는 인터페이스들의 네트워크를 서브넷이라고 부른다.이더넷 세그먼트와 점간 연결을 구축한 회사나 대학 같은 단체들은 이러한 서브넷을 여럿 가지고 있으며, 보통 같은 단체의 서브넷들 간은 유사한 서브넷 주소를 가진다. 그 이유를 알아보기 전에 먼저 CIDR에 대해서 알아보자면,인터넷 주소 할당 전략을 Classless Interdomain Routing(클래스 없는 도메인간 라우팅, CIDR, 사이더)이라고 부르며, CIDR은 서브넷 어드레싱의 개념 정립하였는데,a.b.c.d/x서브넷 마스크와 똑같이 생겼으므로 잘 구분해야한다.서브넷 어드레싱에 의해 단체가 할당받는 아이피 주소는 두개의 부분으로 나뉘는데, 뒤의 “/x”는 첫번째 부분과 두번째 부분의 경계 위치를 알려준다.  /x 같이 표현 하는 방식을 CIDR notation 이라고 한다.첫번째 비트부터 x번째 비트 부분을 접두어(prefix) 또는 네트워크 접두어(network prefix)라고 부르며, 한 단체는 보통 연속된 하나의 블록 주소를 할당 받으며, 그 부분이 공통 접두어(common prefix)의 범위이다.단체에 속한 아이피 장비들의 개개의 아이피 주소는 이러한 공통 접두어를 공유하며, 나중에 알아볼 인터넷의 BGP 프로토콜에서는 단체에 속하지 않은 외부의 라우터들이 이러한 x개의 공통 접두어만 인식하여 단체를 구분한다.  즉 라우터들이 데이터그램을 보고 해당 주소가 특정 단체에 소속된 장비의 주소라면, 해당 단체의 공통 접두어만 보고 해당 단체로 라우팅한다.  전체 아이피 주소 중 일부 접두어만 참고하면 되므로, 이로 인해 포워딩 테이블의 크기가 줄어들게 된다.나머지 x+1 비트 부터 마지막 32 비트까지는 해당 단체 내부에서 추가로 기기들이 구분될 수 있는 주소로, 단체 내부에 속한 라우터들은 이 부분만 보고 패킷을 포워딩 한다.이 두번째 부분은 앞서 설명했던 추가적인 서브넷 구조의 서브넷 주소를 가질 수도 있는데 예를 들어, 단체의 공동 접두어인 a.b.c.d/21은 첫 21 비트의 경우 CIDRized(사이더화 됨)이라고 하여 단체 내부의 모든 아이피 주소가 공유하며, 나머지 32-21개의 비트는 단체 내부의 서브넷으로도, 또는 전부 개인으로도 사용될 수 있다.CIDR이 생기기 전의 아이피 주소는 8, 16, 24 비트의 고정된 길이만큼의 서브넷 주소로 네트워크를 구분했으며 이러한 네트워크 어드레싱 구조를 네트워크 클래스(classful addressing)라고 불럿다.이 각각의 8, 16, 24 bit의 서브넷 마스크를 가진 서브넷 주소를 클래스 A, B, C로 나누었는데, 이러한 경직된 구조는 단체 네트워크와 소규모 서브넷이 마구 생겨나던 시절에 문제가 되었다.예를 들어, 클래스 C의 경우 /24 서브넷의 경우 전체에 $2^8$-2=254개의 호스트만 가질 수 있었고 이는 너무 작은 수이고, 그다음 클래스인 클래스 B의 경우 /16 서브넷이 $2^{16}$-2=65634개로 너무 많았다.  2를 뺀 이유는 아래 네트워크 클래스 참조또한, 클래스 B의 경우 전체 가능한 경우의 수 65634개 중 겨우 2000개 정도가 할당될 수 있게 했는데, 이는 너무 적었다.참고로 255.255.255.255는 브로드캐스트 주소(broadcast address)라고 하며, 브로드캐스트 주소를 목적지 아이피 주소로 데이터그램을 생성해 보내면 라우터는 보통 자신이 속한 서브넷에 한해 모든 연결된 호스트에게 메시지를 복사해서 보내게 된다.  아주 가끔 설정에 따라 이웃 서브넷에게도 보내는 경우가 있다.네트워크 클래스(Network class)https://en.wikipedia.org/wiki/Classful_network 발췌네트워크 클래스(Network class), 클래스식 네트워크(classfull network) 또는 IP 주소 클래스는 CIDR 이전에 사용하던 네트워크 어드레싱 구조 중 하나이다.IPv4 공간을 마스크(위의 서브넷 마스크와 같은 방식)와 범위, 규모에 따라 5개로 나누며,A,B,C 클래스는 일반 사용자의 양방향 1:1 통신 네트워크를 정의하기 위한 클래스로, 네트워크 크기로 구분되며, 국가, 회사, 개인 들에게 할애 되어 사용되었다.D 클래스는 다대다 네트워크를 위한 클래스이고, E 클래스는 실험용 및 미래를 위한 예비 클래스이다.지금은 더이상 사용하지 않지만, 여전히 소프트웨어, 하드웨어 설정 등에 남아있고, 특히 서브넷 마스크의 기본 값으로 많이 사용한다.            클래스      접두어 비트      네트워크 번호 마스크      서브넷 마스크      범위      클래스 당 할당 네트워크 수      네트워크당 할당 주소 수                  Class A      0      /8      255.0.0.0      0.0.0.0 ~ 127.255.255.255      128($2^7$)      16777216($2^{24}$)              Class B      10      /16      255.255.0.0      128.0.0.0 ~ 191.255.255.255      16384($2^{16}$)      65536($2^{16}$)              Class C      110      /24      255.255.255.0      192.0.0.0 ~ 223.255.255.255      2097152($2^{21}$)      256($2^8$)              Class D (multicast)      1110      없음      없음      224.0.0.0 ~ 239.255.255.255      없음      없음              Class E (reserved)      1111      없음      없음      240.0.0.0 ~ 255.255.255.255      없음      없음      실제 사용 가능한 네트워크 내의 주소 수는 2개를 빼줘야 하는데, 1개는 브로드캐스트 주소, 나머지 1개는 네트워크의 주소로 사용되기 때문이다.브로드캐스트(boradcast) 주소는 위의 IPv4 어드레싱 마지막 부분에 설명되어 있다.네트워크 주소는 해당 서브네트워크를 지칭하는 주소를 의미한다.몇 몇 주소들은 특별한 용도를 위해 예약되어 있어 사용 불가하다.| 주소                              | 해당 사이더         | 목적                                                                                                  | RFC                                             | 클래스 | 전체 주소 개수    ||:——————————-:|:————–:|:—————————————————————————————————:|:———————————————–:|:—:|:———–:|| `  0.0.0.0 - 0.255.255.255     | 0.0.0.0/8      | Zero 주소                                                                                             | [RFC 1700](https://tools.ietf.org/html/rfc1700){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | A   | 16,777,216  ||   10.0.0.0 - 10.255.255.255   | 10.0.0.0/8     | [사설망](https://ko.wikipedia.org/wiki/사설망)                                                            | [RFC 1918](https://tools.ietf.org/html/rfc1918){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | A   | 16,777,216  ||  127.0.0.0 - 127.255.255.255  | 127.0.0.0/8    | 로컬호스트 Loopback 주소                                                                                   | [RFC 1700](https://tools.ietf.org/html/rfc1700){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | A   | 16,777,216  || 169.254.0.0 - 169.254.255.255 | 169.254.0.0/16 | [Zeroconf](https://ko.wikipedia.org/wiki/Zeroconf){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink}                                                  | [RFC 3330](https://tools.ietf.org/html/rfc3330){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | B   | 65,536      ||  172.16.0.0 - 172.31.255.255  | 172.16.0.0/12  | [사설망](https://ko.wikipedia.org/wiki/사설망)                                                            | [RFC 1918](https://tools.ietf.org/html/rfc1918){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | B   | 1,048,576   ||  192.0.2.0 - 192.0.2.255      | 192.0.2.0/24   | 문서와 예제                                                                                              | [RFC 3330](https://tools.ietf.org/html/rfc3330){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | C   | 256         || 192.88.99.0 - 192.88.99.255   | 192.88.99.0/24 | [IPv6](https://ko.wikipedia.org/wiki/IPv6){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink}에서 [IPv4](https://ko.wikipedia.org/wiki/IPv4){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink}로의 애니캐스트 릴레이 | [RFC 3068](https://tools.ietf.org/html/rfc3068){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | C   | 256         || 192.168.0.0 - 192.168.255.255 | 192.168.0.0/16 | [사설망](https://ko.wikipedia.org/wiki/사설망)                                                            | [RFC 1918](https://tools.ietf.org/html/rfc1918){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | C   | 65,536      ||  198.18.0.0 - 198.19.255.255  | 198.18.0.0/15  | 네트워크 장치 벤치마크                                                                                        | [RFC 2544](https://tools.ietf.org/html/rfc2544){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | C   | 131,072     ||  224.0.0.0 - 239.255.255.255  | 224.0.0.0/4    | [멀티캐스트](https://ko.wikipedia.org/wiki/멀티캐스트)                                                        | [RFC 3171](https://tools.ietf.org/html/rfc3171){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} | D   | 268,435,456 ||  240.0.0.0 - 255.255.255.255`  | 240.0.0.0/4    | 예약됨                                                                                                 | RFC 1700 | E   | 268,435,456 |표 출처 https://ko.wikipedia.org/wiki/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%ED%81%B4%EB%9E%98%EC%8A%A4_결국 아이피 주소와 네트워크의 고갈과 스케일 가능성 등의 이유로 사장되었고 CIDR로 바뀌게 되었다.주소 집계(또는 경로 집계, 경로 요약)(address aggregation(or route aggregation, route summarization))CIDR화(CIDRized) 주소가 라우팅에 미친 긍정적인 영향을 예시로 알아보자fig.4.21의 경우, Fly-By-Night-ISP에서 도합 8개의 조직(organization) 네트워크를 가지고 있으며, 인터넷 망에 자신의 서브넷 주소(200.23.16.0/20)를 등록하였다.해당 ISP 외부에 속해있는 라우터들은 이제  Fly-By-Night-ISP의 서브넷 주소만 일치한다면, 어느 단체(organization)의 어떤 호스트인지 관계없이 그 뒤의 비트를 확인하지 않고 ISP 측 네트워크 방향의 링크로 패킷을 넘긴다.이런식으로 하나의 접두어(prefix) 주소로 여러 네트워크를 통신 가능하도록 등록(advertise)하는 것을 주소 집계(또는 경로 집계, 경로 요약)(address aggregation(or route aggregation, route summarization))이라고 한다.주소 집계는 블록 째로 ISP에 추가된 조직 네트워크와 잘어울린다.하지만 만약 서브넷의 주소와 ISP 네트워크의 주소가 비슷하지 않은 경우, 예를 들어 fig.4.22처럼 조직 1(200.23.18.0/23)을 ISPs-R-US ISP측으로 이동시키려면 어떻게 해야하는가?첫번째 방법으로 조직 1의 모든 장치들의 아이피 주소를 ISPs-R-Us ISP의 서브넷 마스크에 맞게 바꾸는 방법이 있다. 하지만 이 방법은 아주 힘들고 번거로운 일이며, 다른 조직의 추가, 삭제가 있을 때마다 할 수 있을 만한 일이 아니다.두번째 방법은 조직 1의 서브넷 주소를 그대로 둔 채, ISPs-R-Us ISP 측에서 새로 추가된 해당 서브넷 주소도 함께 등록(advertise)하는 것이다.아래 fig.4.22는 ISPs-R-Us가 새로 추가된 조직 1의 서브넷 마스크 또한 함께 인터넷에 등록하는 모습이다.외부 라우터들은 언제나 최장 접두어 일치(Longest Prefix Match)를 통해 라우팅하므로, 조직 1의 서브넷 마스크는 23자리까지이므로 서브넷 주소가 비슷한 Fly-By-Night-ISP 측이 아니라 ISPs-R-Us측으로 패킷을 보낼 것이다.조직을 위한 주소 블록 확보하기 (Obtaining a Block of Addresses)만약 단체를 위한 서브넷을 조직하기 위해서는 ISP와 연락하여 ISP가 가지고 있는 아이피 주소 범위에서 일부를 할당받을 수 있다.예를 들어, ISP가 8개의 조직을 연속적으로 할당해준다면 아래와 같이 가능하다.ISP를 통하지 않고 아이피 주소를 할당 받는 방법은 ICANN(Internet Corporation for Assigned Names and Numbers)의 허락을 받는 방법밖에 없다.ICANN은 아이피 주소, DNS 루트 서버, 도메인 이름과 분쟁에 대한 일을 처리하는 비영리 단체이다.ICANN은 지역 인터넷 회사에게 아이피를 할당하고 지역 인터넷 회사들은 해당 할당받은 주소들을 관리한다.호스트 주소 확보 : 동적 호스트 설정 프로토콜(DHCP)(Obtaining a Host Address: The Dynamic Host Configuration Protocol)조직을 위한 주소 블록을 확보했다면, 호스트나 라우터 인터페이스에 아이피 주소를 할당할 수 있는데, 이는 네트워크 관리 도구로 직접 할당하는 것도 가능하지만, 보통은 동적 호스트 설정 프로토콜(Dynamic Host Configuration Protocol, DHCP)을 이용한다.DHCP는 주소를 자동으로 설정해주며, 해당 호스트에게 연결시마다 같은 아이피 주소를 할당해 줄 수도 있고, 아니면 연결시에만 잠시 유지되는 임시 아이피(temporary IP address)를 할당할 수도 있다. 추가로 DHCP는 서브넷 마스크, 첫 홉 라우터(first hop router, 기본 게이트웨이(default gateway)라고 불리운다), 지역 DNS 서버의 주소 등의 추가 정보를 제공해준다.이러한 자동 아이피 설정 기능 덕에 플러그 앤 플레이(plug-and-play), 제로컨프(zeroconf) 프로토콜이라고도 불리운다.주로 주거지 인터넷 접근 네트워크나, 기업 네트워크, 무선 네트워크 같이 호스트가 자주 들어왔다 나가는 환경에 사용되며, 동적 아이피를 제한된 기간만큼으로 자주 할당해준다.DHCP는 클라이언트-서버 구조 프로토콜이며, 클라이언트는 새로 네트워크에 참여하기 위해 네트워크 설정 정보를 요구하는 호스트이고, DHCP 서버가 서버 역할을 한다.보통은 서브넷 마다 DHCP 서버가 있지만, 서브넷 내부에 DHCP가 없다면, DHCP 서버의 주소를 알고, 알려주는 역할을 하는 DHCP 릴레이 에이전트(DHCP relay agent)를 담당하는 라우터가 필요하다.아래 fig.4.23 그림에서 DHCP 서버는 223.1.2/24 서브넷에만 DHCP 서버가 있으며, 중앙의 라우터는 다른 DHCP가 없는 서브넷을 위한 릴레이 에이전트 역할을 하고 있다.위의 fig.4.23의 구조에서 새로운 호스트가 도착하면 DHCP는 fig.4.24의 4단계 과정을 진행한다. yiaddr은 새로운 호스트에게 할당된 아이피 주소를 의미한다.      DHCP 서버 발견 (DHCP discover)    호스트가 통신할 DHCP를 찾는다, 호스트는 DHCP의 주소를 모르기 때문에 DHCP 발견 메시지(DHCP discover message)를 이용한다.    DHCP 발견 메시지는 포트번호 67번을 사용하는 UDP 패킷으로, 헤더의 목적지(destination) 아이피 주소를 255.255.255.255로, 자신을 의미하는 발신지(source) 아이피 주소를 0.0.0.0으로 설정하여 서브넷에 속한 모든 노드(호스트, 라우터, 스위치, 등등)들에게 메시지를 아이피 데이터그램에 캡슐화하여 보낸다.    목적지 주소가 255.255.255.255:67라면 라우터는 해당 패킷을 복사해 모든 노드에게 보내주며, 발신지 주소가 0.0.0.0:68인 이유는 이제 막 들어온 호스트의 경우 해당 서브넷에 아이피 주소가 할당되지 않았기 때문이다.        DHCP 서버 제공 (DHCP offers)    DHCP 서버가 DHCP 발견 메시지를 받게되면, DHCP 제공 메시지(DHCP offer message)를 255.255.255.255 번호로 브로드캐스트하여 보내게 된다.    클라이언트는 받은 여러개(DHCP 서버가 하나가 아닐 수도 있다.)의 DHCP 제공 메시지 중에 가장 나은 제안을 받아들인다.    각 DHCP 제공 메시지에는 어떠한 DHCP 발견 메시지의 요청인지 표해주는 트랜잭션 아이디(transaction ID)가 포함되있고, 아이피 주소, 네트워크 마스크, 아이피 대여 시간(보통 몇 시간~ 하루) 등이 제안되어있다.        DHCP 서버 요청 (DHCP request)    클라이언트(호스트)가 DHCP 제공 메시지를 받아들이고 싶다면, DHCP 요청 메시지(DHCP request message)로 해당 설정 파라미터들을 포함해 서버에 보내야한다.    아직 제안에 대한 확인이 오지않았으므로, 여전히 브로드캐스하며, 발신자 주소는 여전히 0.0.0.0:68 이다.    이때 트랜잭션 아이디가 새로 갱신되어, 그 뒤로 오는 ACK 메시지가 이전 메시지와 헷갈리지 않도록 한다.        DHCP ACK (DHCP ACK)    DHCP 서버가 위의 DHCP 요청 메시지를 받았다면, 같은 파라미터를 집어넣고 DHCP ACK 메시지(DHCP ACK message)를 돌려줘야한다.    사용자가 ACK 메시지를 받은 뒤 부터 아이피 주소를 사용 가능하므로 여전히 브로드캐스트로 보내야한다.  클라이언트가 DHCP ACK 메시지를 받은 뒤, 클라이언트는 할당받은 아이피 주소를 주어진 시간동안 사용 가능하다.만약 해당 기간을 연장하고 싶으면, 위의 과정과 조금 다른 방법으로 연장 가능하다.(아마 discover과 offer 과정이 필요 없지 않을까?)다만, 위 과정은 아쉬운 점이 하나 있는데, 사용자가 무선 인터넷 환경 등에서 이동을 하며 기존의 서브넷에서 나가 새로운 서브넷의 범위에 들면, 사용 중이었던 응용프로그램들의 아이피 주소가 바뀌므로 TCP 연결이 유지될 수 없다는 점이다.이를 극복하는 방법은 챕터 7에서 배운다.추가적인 DHCP에 대한 내용은 [Droms 2002], [dhc 2020] 등에서 알 수 있고, DHCP의 오픈소스 구현은 Internet Systems Consortium[ISC 2020]에서 볼 수 있다.4.3.3.네트워크 주소 번역 (Network Address Translation (NAT))작은 규모의 사무소나 집의(Small office, Home Office, SOHO) 서브넷이 계속 증가하는 추세이나, 이러한 작은 규모에서는 네트워크를 관리하기도 힘들고, ISP 측에서는 LAN으로 설정된 여러 장비(스마트폰, 인쇄기, TV, 프린터 등) 갯수 만큼 연속된 아이피 주소 공간을 무한히 할당해주는 것도 한계가 있다.이를 해결해주는 방법이 네트워크 주소 번역(Network Address Translation, NAT)이다.아래 fig.4.25는 NAT가 설정된 라우터의 모습이다. NAT 설정 라우터(NAT-enabled router)는 오른쪽 부분의 홈 네트워크(10.0.0.0/24)의 일부를 구성하고 있다.아이피 주소 10.0.0.0/8은 사설망(private network)나 개인 주소 영역(realm with private addresses)을 위해 예약된 주소로, 이외의 용도로 해당 네트워크를 사용할 수 없다.(앞서 배웠던 네트워크 클래스 부분 참조)개인 주소 영역은 소속 장치들의 아이피 주소가 해당 사설망내에서만 의미있음을 의미하는데, 이 세상에는 수많은 10.0.0.0/24 아이피를 사용하는 SOHO 네트워크가 존재할 것이므로, 개인 주소 영역의 아이피로 통신을 하면, 중복되는 같은 아이피가 수없이 많을 것이기 때문이다.그러므로, 사설망 내의 아이피 주소를 가진 장비가 설정되었다면, 외부 인터넷과 통신하기 위해 NAT를 사용한다.먼저, 홈 네트워크를 대표하는 NAT 설정 라우터의 아이피 주소를 첫 설정 하기 위해, ISP 측의 DHCP로 부터 가져오고, NAT 설정 라우터는 그 뒤로, DHCP 서버 행세를 하면서 홈 네트워크 내부의 디바이스에게 사설망의 아이피 주소(10.0.0.0/8)를 할당해준다.NAT 설정 라우터(NAT-enabled router)는 외부 인터넷 입장에서 라우터가 아닌 하나의 아이피 주소를 가진 호스트로 행세하며, 패킷을 주고 받는다.아래 예시에서는 홈 네트워크로 보내는 모든 패킷의 목적지 주소와 홈 네트워크에서 내보내는 모든 패킷의 발신지 주소가 NAT 설정 라우터의 아이피 주소인 138.76.29.7로 되있음을 알 수 있다.그렇다면 NAT 설정 라우터에게 오는 모든 WAN 측의 패킷은 같은 목적지 주소(NAT 설정 라우터의 아이피 주소)를 가지는데, 이를 어떻게 서로 구별하고, 각기 다른 홈 네트워크 내의 장치에게 돌려줄 수 있을까?바로, NAT 번역 테이블(NAT translation table)을 이용해서 라우터가 가진 포트 번호를 각기 다른 홈 네트워크 장치에게 할당하여 구분해준다.위 fig.4.25의 경우, 1번(10.0.0.1)의 경우, 포트번호를 3345의 응용 프로그램으로 외부의 웹서버(128.119.40.186:80)의 패킷을 보내주면, NAT 설정 라우터는 해당 디바이스의 아이피 주소와 포트번호(10.0.0.1:3345)를 NAT 번역 테이블에 자신의 아이피 주소와 새로 지정한 포트번호(138.76.29.7:5001)로 지정하여 이를 발신지 주소로 바꾸어 보낸다.반대로 해당 장치를 위한 패킷 받을 시에는 자신의 아이피 주소와 지정했던 포트번호(138.76.29.7:5001)를 가진 패킷의 목적지 주소를 NAT 번역 테이블을 참고해 홈 네트워크 내의 주소(10.0.0.1:3345)로 고치고 보내준다.즉, 홈 네트워크 장치의 응용 프로그램 하나 하나를 NAT 설정 라우터의 응용 프로그램으로 행세하게끔 하는 것이다. 참고로 포트번호 필드는 16비트 길이이므로 60000여개가 한계이다.NAT로 인해 생기는 문제들을 생각해보자.먼저, 홈 네트워크 내부의 장치가 well known 포트 번호를 가진 어플리케이션을 돌리려면 어떻게 해야할까?예를 들어 홈 네트워크 장치에서 웹 서버(포트번호 80)를 돌리면, NAT 설정 라우터에서 임의의 포트번호로 바꾸어 버리면, 외부에서는 웹 서버로 인식되지 않을 것이다. NAT 설정 라우터의 웹서버 포트 번호 80을 할당할 경우, 이외의 다른 홈 네트워크 장치가 똑같이 웹 서버를 돌리면 중복되는 문제가 될것이다.이를 해결하기 위해 NAT 트래버설(NAT traverasal) 장비를 이용한다[RFC 5390, 5128, Ford 2005].두번째로 네트워크의 설계 철학에 반하는 이유인데, 라우터는 네트워크 레이어의 장비로써, 오직 데이터그램에만 관여해야 한다.하지만 NAT 설정 라우터는 중간자 행세를 하며 패킷의 헤더 정보(목적지 주소)를 조작하고, 심지어 다른 계층인 전달 계층 헤더인 포트번호까지 조작한다. 자세한 내용은 4.5절의 미들박스(middlebox)에서 알아보자.데이터 그램 조사: 방화벽과 침입 감지 시스템(INSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION SYSTEMS)우리가 네트워크 관리자가 됬을 경우, 외부의 공격자에게로 부터 다음과 같은 공격을 받을 수 있다.공격자가 우리 네트워크의 아이피 주소 공간에 대한 정보를 확보한 뒤, 해당 아이피 주소 공간에 데이터그램 패킷을 보내 핑 스윕(ping sweep), 포트 스캔(port scan) 등을 통해 보안상 취약한 호스트, TCP/UDP 포트, 주소 등을 찾아내어, 변형된 패킷으로 취약점을 공격하거나 악성 소프트웨어(malware) 등을 심을 수도 있다.이를 막기 위해서 두가지 보안 방법이 있다.첫번째는 방화벽(Firewall)이다.방화벽은 데이터그램과 세그먼트 헤더 필드를 확인하고, 의심스러운 패킷은 접근을 제어할 수 있다.예를 들어 ICMP 에코 요청 패킷(ICMP echo request packets)을 막으면 기본적인 방법의 포트 스캔은 막을 수 있다. 의심스러운 발신지 주소의 패킷을 막거나, TCP 연결을 확인해 허락된 연결만 할 수 있도록 만들 수 있다.(Blacklist, Whitelist)두번째는 침입 감지 시스템, IDS(Intrusion Detection System)이다.IDS는 네트워크 경계에서 심층 패킷 조사(Deep packet inspection)을 통해, 패킷의 헤더 필드 뿐만 아니라 페이로드와 응용 계층 페이로드를 살펴보고 시그니처(일종의 해쉬)를 생성한다.해당 시그니처를, 공격자들이 지금까지 시도해온 악성 패킷의 시그니처들을 저장해 놓은 데이터베이스와 대조하여, 만약 일치하는 악성 패킷이 감지되면 시스템에 주의를 표하거나 패킷을 차단한다.데이터베이스는 새로운 공격자나 공격 패킷이 발생하면 업데이트한다.IPS(Instrusion Prevention System)은 단순 주의 뿐만 아니라 추가로 패킷을 차단한다는 점이 다르다.물론 이 두가지 보안만으로 완벽하게 방어해낼 수 없다, 예를 들어 새로운 공격 패킷을 이용하는 등의 방법이 있다.하지만 네트워크를 공격자를 방어하는데 도움이 될 것이다.보안에 대한 자세한 내용은 8장에 다룰 예정이다.4.3.4.IPv6IPv4의 빠른 고갈에 대처하기 위해 IPv6를 만들었다.IPv6는 IPv4의 고갈과 기타 다른 문제점을 해결하기위해 만들어 졌고, 중간에 IPv5로 ST-2 프로토콜이 있었으나, 사장되었다.IPv6 데이터그램 형식(IPv6 Datagram Format)아래 fig.4.26은 IPv6 데이터그램의 형식이다.이 가장 크게 변한 부분은      주소 수용량 확장(Expanded addressing capabilities)    IPv6에서는 기존의 IPv4의 아이피 주소 크기를 32비트에서 128비트로 늘렸으며, 할당될 수 있는 주소는 지구 상의 모든 모래알 보다 많다.    또한 일대일 통신인 유니캐스트(unicast), 일대다 통신인 멀티캐스트(multicast), 이외에 일대 가장 적합한 일 통신인 애니케스트(anycast)통신이 생겼으며, 이를 통해 예를 들면 사용자와 서버 클러스터들 중 가장 딜레이가 적은 서버와 통신 등을구축 할 수 있다.        간결한 40바이트 헤더(Streamlined 40byte header)    많은 양의 IPv4 헤더 필드가 없어지거나 선택할 수 있게 되었고, 언제나 고정적으로 40byte인 헤더 길이로 헤더 길이 탐색을 할 필요가 없어 오버헤드가 줄어들었고, 새로운 옵션 인코딩으로 옵션 처리가 자유로워 졌다.        흐름 레이블링(Flow labeling)    IPv6 개발자가 흐름을 애매모호하게 정의했기 때문에 덕분에 흐름 레이블을 여러가지 용도로 사용할 수 있다.    예를 들면, 비디오나 오디오 스트리밍을 흐름으로 레이블하고, 이메일과 FTP 등을 비흐름으로 레이블하여 처리율에 차이를 둘 수 있고, 일부 우선순위가 높은 사용자를 지정해줄 수도 있다.  다음은 IPv4에 비해 간결화된 IPv6의 헤더 필드들이다.            필드 이름      설명      크기(bit)                  버전(Version)      IP 버전 번호, IPv6는 6번이다.      4              트래픽 클래스(Traffic class)      IPv4의 TOS 필드, 흐름의 우선순위 등을 적는 곳, 특정 패킷 흐름에 우선순위를 주는 방법 등으로 사용      8              흐름 레이블(Flow label)      위에서 말한 패킷 흐름에서 흐름을 구분하기 위해 사용함, 즉, 이곳은 흐름의 식별자를 적는 곳      20              페이로드 길이(Payload length)      비부호(unsigned, 양수만 존재) 정수로 데이터그램의 페이로드 크기를 적는다.      16              다음 헤더(Next header)      현재 데이터그램이 어떤 전달 계층 프로토콜을 이용하는가 적는다. IPv4와 같은 값을 사용함.      8              홉 한계(Hop limit)      현재 데이터그램이 몇번의 라우터를 거칠 수 있는지 적는다. 라우터를 지날때 마다, 이 필드의 값을 1씩 줄이고 0이 되는 순간 라우터가 패킷을 폐기한다.      8              발신지, 목적지 주소(Source and Destination addresses)      각각 128 비트의 주소 정보      128, 128              데이터(data)      데이터그램의 데이터가 들어가는 부분, 목적지 도착시 이 부분만 다음 계층에 전달된다. 보통 전달 계층의 세그먼트가 담긴다.             IPv4 데이터그램과 비교해서 다음과 같은 일부 필드가 없어진 것을 확인할 수 있다.      조각/조립(Fragmentation/reassembly)    IPv6는 데이터그램을 경로 중간의 라우터가 조각내고 다시 조립하는 것을 허락하지 않으며, 오직 발신지와 목적지에서만 가능하다.    대신, IPv6 데이터그램의 크기가 너무 크면, 라우터는 해당 패킷을 버리고 송신자 측으로 ICMP 에러 메시지 “패킷 사이즈가 너무 큽니다.” 를 보내며, 송신자 측은 이를 받으면 데이터그램을 여러개로 쪼개서 다시 보낸다.    위 과정을 통해 중간에 라우터들이 반복적으로 쪼개고 조립하는 과정을 없애서 성능을 높일 수 있다.        헤더 체크섬 (Header checksum)    IPv6 개발자들은 상위계층인 전달 계층에서 오류 확인을 하므로 중복하여 체크할 필요없다고 느꼈을 뿐만 아니라, 매 라우터 마다 TTL, 또는 홉 한계(Hop limit)값이 바뀌므로 해당 체크섬 기능을 위해 매 라우터 마다 바뀐 헤더의 체크섬값을 재계산하는 것이 너무 성능이 나빠진다고 생각했다.        옵션(options)    IPv6부터는 옵션을 제공하지 않아 언제나 헤더 크기가 40byte 로 고정되어 있다. 대신 next header 필드의 값으로 확장 헤더를 사용하고 있는지 표시할 수 있어서 이를 통해 확장 헤드의 추가 헤더를 이용할 수 있다.  IPv4에서 IPv6로의 전환 (Transition from IPv4 to IPv6)IPv4 환경의 인터넷을 IPv6 시스템으로 바꾸려면 어떻게 해야할까? 과거에는 특정 기점으로 이전의 방법은 대체되었음을 알리고 새로운 방법을 쓰게 하였다. 과거 NCP가 TCP 프로토콜로 바뀌었을 때 그러했지만, 네트워크 규모가 작았을 때 가능한 일이였고, 현재로서는 불가능하고 비효율적인 방법이다.현재 IPv4에서 IPv6 시스템으로 바꾸는 데 가장 많이 사용되고 있는 방법은 터널링(tunneling)[RFC 4213]이다.fig.4.27에서 처럼 IPv6 라우터 사이의 IPv4만 지원하는 라우터가 존재할 수 있는 중간 경로를 터널(Tunnel)이라고 칭한다.      터널의 시작점(즉, 데이터그램의 보내는 서버 방향에 가까운 쪽)의 IPv6 라우터 B는 받은 IPv6 데이터그램을 IPv4 데이터그램의 payload에 집어넣어 새로운 패킷을 생성한다. 이때 IPv4의 헤더는 IPv6에서 값을 가져온다.        그렇게 IPv6 데이터그램을 품은 IPv4 데이터그램은 터널을 지나면서 IPv4 라우터나 IPv6 라우터(보통 헤더를 확인해 IPv4도 지원하게 만듦)를 만나도 안전히 다음 라우터로 이동할 수 있다.        터널을 통과하면, 앞으로 IPv6 라우터 밖에 존재하지 않으므로, 터널을 끝점(즉, 데이터그램의 받는 서버 방향에 가까운 쪽)에 존재하는 첫번째 IPv6가 해당 IPv4 내부의 페이로드를 확인하고, 만약 IPv6 데이터그램이 통채로 들어있으면 페이로드를 패킷으로 목적지 서버까지 보낸다.  이렇게 IPv6는 IPv4만 지원하는 라우터들을 지나고서도 안전하게 목적지 라우터에 도착할 수 있다.이러한 방법 덕분에 IPv4에서 IPv6로 천천히 시스템이 바뀌어 나가고 있다.이러한 낮은 계층의 변화는 마치 건물의 기반부터 바꾸는게 어려운 것처럼, 갑자기 바꾸기 쉽지 않다. 하지만 그에 반해 웹, 게임, 소셜 미디어 같은 응용 계층의 프로토콜. 응용 프로그램은 시대가 바뀌면서 빠르게 바뀌고 추가된다. 위에 영향을 줄 다른 계층이 없기때문이다.4.4.일반화된 포워딩과 SDN (Generalized Forwarding and SDN)우리는 라우터에서의 포워딩이 match-plus-action으로, 목적지 주소를 살피고(match), 해당 출력 포트로 스위칭 패브릭이 패킷을 보내는(action) 과정이라고 말했다.좀더 일반화된 match-plus-action을 살펴보자면, 먼저, match의 경우 여러 종류의, 여러 계층에서의, 여러 프로토콜에서의, 헤더 필드를 읽어야 한다.또한, action의 경우, 단순히 패킷을 포워딩하는 것 뿐만 아니라, 여러 출력 포트로 포워딩, 부하 밸런싱(load balancing), 패킷 헤더 고쳐쓰기(NAT), 패킷 필터링(방화벽), 패킷을 특정 서버에 보내 추가적인 처리를 거친 후 포워딩 하기(DPI) 등이 존재한다.일반화된 match-plus-action에서는 링크 계층의 스위치나 네트워크 계층의 라우터를 합쳐서 SDN에서 사용하는 용어인 패킷 스위치로 부르겠다.아래 fig.4.28을 보면 이전 라우팅에 관한 그림과 달리 일반화된 포워딩에서는 원격 컨트롤러가 포워딩 테이블(정확히는 같은 역할을 하는 플로우 테이블)을 계산, 설치, 갱신 해주며, 포워딩 테이블 대신 지역 플로우 테이블(Local flow table) 이라는 것을 이용한다는 것을 알 수 있다.일반화된 포워딩에서는 오픈플로우(OpenFlow)[McKeown 2009, ONF 2020, Casado 2014, Tourrilhes 2014]를 이용할 것이다.오픈플로우는 match-plus-action 포워딩과 컨트롤러, SDN에 혁신적인 영향을 끼친 개념이다.오픈플로우 1.0을 통해 SDN의 개념과 기능에 대해서 알아볼 것이고, 최신 버전 오픈플로우는 [ONF 2020]에서 볼 수있다.오픈 플로우에서 사용하는 match-plus-action 포워드 테이블 또는 플로우 테이블(flow table)의 항목(entry)들은 다음과 같은 정보를 포함하고 있다.      입력 패킷의 헤더와 비교할 헤더 필드 값들의 집합    하드웨어 기반 방법인 TCAM을 이용하면 길이 수십만 수준의 플로우테이블도 검사할 수 있다.    플로우테이블에 일치하는 결과가 없는 패킷의 경우 버려지거나 원격 컨트롤러로 보내져 추가 처리를 받게 된다.    실제로는 성능상의 이유로 여러 플로우테이블을 유지하지만 우리는 일단 하나로 생각하고 학습할것이다.        플로우 테이블에 일치한 패킷들로 갱신되는 카운터의 집합    플로우 테이블의 항목에 일치한 패킷의 수와 마지막으로 업데이트한 시점을 갱신하며 기록한다.        플로우 테이블에 일치되면 취할 활동들의 집합    패킷을 출력 포트로 포워딩하거나, 버리거나, 복사하여 여러 출력포트로 보내거나 헤더 필드를 기록하는 등의 활동이 있다.  플로우테이블은 엄밀히 말하면 패킷 별로 할 행동이 적혀있는 API로, 여러 패킷 스위치에 존재하는 테이블들을 수정하여 행동들을 행하게할 수 있다.4.4.1.일치(Match)아래 그림 fig.4.29는 오픈플로우 1.0에서 match-plus-action 규칙에 의해 일치할 수 있는 11개의 패킷 헤더 필드와 들어온 포트 아이디 정보를 표현한 것이다.  총, 12개지만 최근 버전 오픈플로우에는 41개까지 늘어난다.우리 모두 패킷은 여러 계층의 패킷이 상위 계층의 패킷을 페이로드에 캡슐화하여 만들어진 것임을 알고 있으며, 아래 그림을 보면 3개의 계층 패킷의 헤더 요소를 참조하여 비교한다는 것을 알 수 있다.아직 링크 계층에 대해서 자세히 배우지 못했지만, 대략적으로 각 요소를 설명해보겠다.      Src/Dst MAC    링크 계층의 송수신 이더넷 주소, 오픈플로우는 네트워크 계층 장비인 라우터이외에  링크 계층 장비인 스위치에서도 프레임을 포워딩하는데 쓸 수 있다.        Eth Type    이더넷 종류 필드는 어떤 종류의 상위계층 패킷으로 역다중화(demultiplexed)될 것인가? 보통은 아이피 프로토콜        VLAN    가상 로컬 지역 네트워크(virtual local area network)에 대한 필드, 챕터 6 참조        Ingress Port    입장(Ingress) 포트는 패킷 스위치의 패킷이 들어온 입력 포트를 의미한다.        IP Src, IP Dst, IP Proto, IP TOS    아이피 데이터그램 형식에서 배웠던 헤더 필드 그대로이다.        TCP/UDP Src/Dst Port    전달 계층 패킷 헤더의 필드  오픈플로우의 플로우테이블의 각 항목(entry)에는 와일드카드(wildcard)가 존재할 수 있는데, 예를 들어 아이피 주소 정보가 128.119.*.* 로 되어있다면, 이는 앞의 128.119 부분이 일치하는 모든 아이피 주소를 일치시키라는 의미이다. 또한 여러 항목과 일치할 때를 대비하여 관련 우선순위에 대한 정보도 있다.굳이 패킷 헤더의 모든 필드를 비교하지 않고 일부만 하는 이유는 기능성과 복잡도를 저울질 하여 최소한의 비교가 가능하게 끔 만든 것이다. (추상화에 필요한 요소 최소화)4.4.2.활동(Action)각 플로우 테이블의 항목은 일치했을 때 취해야할 행동(Actioin)이 0개 이상 존재하며, 여러개가 존재할 시, 주어진 순서대로 처리한다.활동의 예시로는      포워딩 (Forwarding)    들어온 패킷을 입력 포트를 제외한 특정 물리적 출력 포트나, 여러 포트, 또는 모든 포트에게 보낼 수 있다. 추가로 패킷은 캡슐화되거나 원격 컨트롤러에게 보내져 추가 처리를 받고 돌아오거나, 플로우 테이블가 업데이트 되는 등의 활동을 할 수 있다.        버림 (Dropping)    플로우 테이블 항목에 아무런 활동이 적혀있지 않으면 패킷은 버려진다.        필드 수정 (Modify-field)    아이피 프로토콜 필드(IP Protocol field)를 제외한 10개의 패킷 헤더 필드의 값을 수정한 뒤 포워딩 될 수 있다.          입장 포트(Ingress Port)는 패킷 헤더 필드 값이 아니다.      4.4.3.매치-플러스-액션 활동의 오픈플로우 예제(OpenFlow Examples of Match-plus-action in Action)아래 fig.4.30과 같은 네트워크 예시에서 구현할 네트워크 전역 행동(network-wide behaviors)과 이러한 행동을 구현할 플로우 테이블 항목 s1, s2, s3에 대해 고려해보자.첫 예제: 단순 포워딩(A First Example: Simple Forwarding)먼저 호스트 h5, h6가 라우터 s3, s1, s2 순서로 거쳐 호스트 h4나 h3에 패킷을 전송하는 행동을 구현해보자.아래는 위 행동을 하기 위한 라우터별 플로우 테이블 예시이다.s1 라우터의 경우 호스트 h5 또는 h6의 패킷이 1번 포트를 통해 들어올 것이므로, 활동(Action)에 4번 포트로 보내 s2 라우터로 보낸다.s3 라우터의 경우, 입장 포트가 1번 또는 2번이 될 수 있으므로, 입장 포트(Ingress Port)는 일치(Match) 항목에 적지 않고, 주소로 판별하여 3번 포트를 통해 s1 라우터로 보낸다.s2 라우터의 경우, 입장한 패킷의 주소를 보고 해당 호스트가 존재하는 출력포트로 보낸다.두번째 예제: 부하 밸런싱(A Second Example: Load Balancing)이번에는 호스트 3이 10.1.*.*로 보낸 데이터그램은 라우터 s2와 s1 사이의 링크를 이용해 이동하고, 호스트 4가 10.1.*.*로 보낸 데이터그램은 라우터 s2와 s3 사이의 링크와, 라우터 s3와 s1 사이의 링크를 지나 이동하는 시나리오를 생각해보자.같은 목적지 주소를 가지되, 입장 포트에 따라 다른 라우터로 보내게 만들면 된다.세번째 예제: 방화벽 차단(A Third Example: Firewalling)s2 라우터가 s3 라우터와 연결되어 있는 호스트의 패킷만 받아들이게 하고 싶다면,아이피 발신자 주소가 10.3.*.*인 패킷만 3번 포트, 4번 포트로 나가게 될 것이며, s2의 플로우테이블에 위 두 항목 이외의 항목이 없다면 이외의 패킷은 행동이 적혀있지 않으므로, 버려지게 된다.우리가 살펴본 시나리오는 상당히 제한되고 기본적인 부분이며, 좀더 복잡하고 논리적으로 분리된 가상 네트워크 등 또한 만들 수 있다.플로우 테이블 내의 활동과 일치보다 더욱 복잡한 프로그래밍을 P4(Programming Protocol-independent Pacekt Processors)[P4 2020] 프로그램 언어를 통해 가능하다.4.5 미들 박스(Middleboxes)RFC3234에 적혀있는 미들박스의 정의는 다음과 같다.발신지와 목적지 사이 데이터 경로에서 아이피 라우터가 실행하는 기본적인 기능 이외의 추가적인 기능을 실행하는 중간 박스우리는 앞서 웹캐쉬, TCP 연결 분배기, NAT, 방화벽과 IDS, 부하 밸런싱과 match-plus-action을 실행하는 최신 라우터들 같은 미들박스들을 많이 만나보았다.무선 환경, 유선환경 따지지 않고 미들박스가 실행하는 서비스의 종류는 크게 세가지로 나뉜다.      NAT 번역(NAT Translation)    4.3.4에서 배웠던 것처럼, 주로 사설망 주소 생성, 데이터그램 헤더 주소와 포트 번호 수정 등을 함.        보안 서비스(Security Services)    방화벽은 트래픽을 헤더 필드 기준으로 막거나 DPI(deep packet inspection) 같은 추가적인 처리 이후 리다이렉트한다. IDS는 이전에 방화벽과 IDS 절 참조.        성능 향상(Performance Enhancement)    패킷 압축, 컨텐츠 캐싱, 부하 밸런싱 등을 통해 서버의 부하를 줄이고 요청에 빠르게 대답할 수 있음.  이러한 미들박스의 확산은 성능 상의 이점을 주지만 별도로 관리자에게 장비를 장비, 관리 및 업그레이드 하는데 시간과 비용을 쏟게 만든다.이런 서비스를 쉽게 제공하기 위해 개별 라우터가 아닌 중앙 서버 등에서 미들박스를 대체할 소프트웨어 스택으로 이루어진 전용 소프트웨어가 포함되어 있는 상용 하드웨어(SDN의 접근 방법과 유사) 등을 연구 중인데 이러한 방법을 네트워크 기능 가상화(NFV, Network function virtualization)라고 한다.또 다른 방법은 이런 미들박스의 기능들을 클라우드를 이용해 처리하는 것이다.사실 이전까지의 인터넷 구조에서는 네트워크 엣지에서 실행되는 호스트 측에서 주로 활동하며 전달/응용 계층과 네트워크 코어에서 실행되는 라우터, 스위치 측에서 주로 활동하는 나머지 계층이 확연히 구분되어왔지만, 이러한 미들박스들은 이러한 원리를 어기고 있다.예를 들어, 방화벽이 계층과 관계없이 헤더들을 확인하고, NAT가 라우터와 호스트 사이에서 멋대로 패킷을 수정하거나, 이메일 보안 게이트웨이(e-mail security gateway)가 이메일 송수신자 사이를 가로막는 등의 일이다.하지만 미들박스는 어떻게든 필요성에 의해 존재하고 점점 늘어나고 있다.인터넷의 구조적 기반(ARCHITECTURAL PRINCIPLES OF THE INTERNET)[RFC 1958]에 의하면 인터넷의 구조적 기반은 다음과 같다.“많은 인터넷 커뮤니티 구성원들이 주장하길, 적어도 25년 역사동안 인터넷은 구조는 없고 오직 관습만 존재했다고 한다. 하지만 커뮤니티들은 목적은 연결성이고, 도구는 인터넷 프로토콜이며, 지식은 네트워크에 숨겨져 있기보단 종단간 연결이라고 믿는다”즉 연결을 하기위해 아이피 프로토콜을 이용하였고, 지식(또는 복잡성)은 네트워크 코어보다는 네트워크 엣지에 있어야 한다는 것이다.아이피 모래 시계 (The IP HOURGLASS)다른 계층은 다양한 프로토콜이 사용되고 있음과 달리 네트워크 계층은 아이피 프로토콜이 거의 주가 되어 사용되고 있다.이러한 인터넷 구조를 아이피 모래 시계 (The IP HOURGLASS) 또는 얇은 허리(narrow waist)라고 한다.아이피 프토토콜의 간단함과 통일성 덕분에 인터넷이 성황될 수 있었고 미들박스가 등장했다.종단 인자 (THE END-TO-END ARGUMENT)앞서 언급했던 End-to-End는 네트워크에서의 기능들의 위치를 의미한다. 최근 대부분의 기능들은 네트워크 엣지에 위치한다."
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%205-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EA%B3%84%EC%B8%B5-%EC%BB%A8%ED%8A%B8%EB%A1%A4%20%EC%B8%A1%EB%A9%B4.html": {
    title: "네트워크 정리-Chap 5-네트워크 계층-컨트롤 측면",
    date: " Aug 22, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%205-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EA%B3%84%EC%B8%B5-%EC%BB%A8%ED%8A%B8%EB%A1%A4%20%EC%B8%A1%EB%A9%B4.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 5. 네트워크 계층: 컨트롤 측면(Network Layer: Control plane)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/e컨트록 측면에서는 최소 비용 경로를 구하는 두가지 알고리즘 OSPF, BGP를 배울 것이다.OSPF는 단일 ISP 네트워크 내부에서, BGP의 경우 모든 인터넷 내의 네트워크를 연결하기 위한 라우팅 알고리즘이다.보통 컨트롤 측면의 라우팅 알고리즘은 라우터 내에서 데이터 측면의 포워딩 함수와 함께 구현됬지만, 이전 장에서 배운 소프트웨어 정의 네트워크(SDN, software-defined networking) 이후로, 컨트롤 측면의 컨트롤러와 데이터 측면의 포워딩 함수를 물리적으로 구별하게 되었다.또한 ICMP(Internet Contro노드는l Message Protocol)과 SNMP(Simple Network Management Protocol)에 대해서도 배울 것이다.5.1 소개(Introduction)우리는 이전 장의 데이터 측면에서 포워딩 테이블(forwarding table)과 플로우 테이블(flow table)을 이용해 라우터에서의 포워딩을 포함해 다른 패킷의 활동을 실행하는 것에 대해서 배웠다.이번 장에서는 어떻게 그러한 포워딩 테이블과 플로우 테이블이 계산되고, 유지되고, 설치되는지 배워볼 것이며, 크게 두가지가 있다.      라우터별 조정 (Per-router control)    아래의 fig.5.1의 그림은 각 라우터 마다 라우팅 알고리즘이 돌아가고 있음을 표현하였다. 포워딩과 라우팅 기능이 둘다 라우터에서 존재하며, 각 라우터는 다른 라우터의 라우팅 구성품과 통신하여 포워딩 테이블을 만들 수 있는 라우팅 구성품이 들어가 있다. 우리가 배울 OSPF와 BGP 프로토콜의 기반 방법이며, 지난 몇 십년간 사용된 방법이다.            논리적 중앙화된 조정 (Logically centralized control)    아래 fig.5.2는 논리적 중앙화된 컨트롤러가 각 라우터가 사용할 포워딩 테이블을 계산하고 분배하는 그림을 표현했다.    이전에 배웠듯이 일반화된(generalized) match-plus-action 추상화는 전통적인 아이피 포워딩 뿐만 아니라 미들박스를 이용해 부하 밸런싱, 방화벽, NAT 같은 다른 기능을 제공할 수 있다고 배웠으며, 이 방법에 속한다.    여기서는 CA(Control agent)가 미리 정의된 프로토콜로 원격의 컨트롤러와 통신하며 라우터 내부의 플로우 테이블을 설정하고 관리한다. 보통 CA는 컨트롤러와 통신하는 정도의 최소한의 기능을 가지며, 다른 라우터와 통신하고 포워딩 테이블을 계산하는 등의 일을 하지 않는다. 이는 위에서 설명한 라우터별 조정과의 가장 큰 차이이다.  논리적 중앙화된 컨트롤은 라우팅 컨트롤 서비스가 마치 하나의 중앙화된 서비스 지점으로 표현되어 다른 라우터들이 해당 단일 지점에 접근해서 서비스를 이용하여 붙여진 이름이다.  실제로 원격 컨트롤러는 단일 지점 실패 방지와 부하 밸런싱, 스케일링의 이유로 여러 서버에 구현될 것이다.우리가 배울 SDN(software-defined network)이 논리적 중앙화의 개념을 받아들여 구글 사설망, 마이크로소프트 연구용 망, 여러 ISP 등에서 SDN이 널리 퍼지고 있다.5.2 라우팅 알고리즘 (Routing Algorithms)라우팅 알고리즘의 목표는 송수신자 사이의 좋은 경로를 찾는 것으로, 네트워크에서 아주 중요한 개념이다. 여기서 좋은 경로는 주로 비용을 의미하지만, 실제 세계에서는 단순히 비용 뿐만아니라, 특정 라우터의 경로 설정 정책(특정 포트에서 온 패킷은 접근 불가, 패킷 우선 순위 등), 라우팅 방식(라우터별 접근 방법인가, 논리적 중앙화된 접근 방법인가?) 등을 살펴보고, 이를 통해 최적의 경로를 찾아야 한다.이러한 라우팅 문제를 표현하기 위해 그래프(graph)를 이용한다. 그래프 $G=(N,E)$는 N개의 노드와 E 개의 노드 간의 경로, 엣지의 모임으로 된 그래프를 의미한다.여기서 노드는 라우터를, 엣지는 라우터 간의 물리적 링크를 의미한다.위에서 설명한 네트워크의 연결에 대한 알고리즘인 BGP의 경우, 조금 다르게 노드는 ISP 등의 네트워크, 엣지는 이 네트워크 간의 연결성(connectivity, 정확히는 피어링(peering))로 표현된다.fig.5.3에서의 엣지 위의 숫자는 비용을 의미하며, 보통 물리적 링크의 길이에 의한 성능 저하를 의미하지만, 실제로는 이외에도 링크의 성능, 금전적인 비용 등을 고려해야한다. 하지만 우리는 비용의 의미를 크게 관심 두지 않고, 값을 이용할 것이다.또한 표현 적으로 노드 x와 노드 y 사이의 엣지(링크)를 (x,y)로 놓고, 이 엣지의 비용을 c(x,y)라고 표현할 것이며, 만약 x와 y 간에 연결이 없다면 c(x,y) = $\\infty$이다. 방향성이 없는 그래프로 가정할 것이므로 (x,y) = (y,x)이고, 비용도 동일하다. x와 y를 연결하는 엣지가 존재할 때,  x와 y는 서로의 이웃 노드라고 표현한다.그리고 우리의 목적은 시작 노드부터 끝 노드까지 비용의 합이 최소가 지나갈 엣지의 집합과 노드의 순서를 찾는 것이며, 이를 최소 비용 경로(least-cost path)라고 부르자.또한, 모든 엣지의 비용이 같을 때는, 이는 지나가는 노드가 가장 적은 최단 경로(shortest path)이기도 할 것이다.라우팅 알고리즘은 중앙화(centralized)와 분산화(decentralized)된 알고리즘으로 나누며 이 둘을 나누는 방법은 크게 세가지다.먼저, 첫번째는      중앙화된 라우팅 알고리즘(centralized routing algorithm)은 노드간의 최단거리를 구하기 위해서 가능한 모든 경로의 경우의 수를 따져서 그 중 최솟값을 취한다. 즉, 모든 노드의 연결상태와 엣지들의 비용을 알고 있어야하며, 네트워크 상태가 바뀌지 않는 이상 한번만 계산해서 저장하며, 논리적 중앙화된 컨트롤러가 해당 결과를 가지고 있거나, 아니면 복사해서 각 라우터에 뿌려준다.    이러한 전역 상태 정보가 필요한 알고리즘을 연결 상태(link-state, LS) 알고리즘이라고 한다.        분산화된 라우팅 알고리즘(decentralized routing algorithm)의 경우, 최소 비용 경로를 구할 시, 라우터들이 서로 반복적이고(iterative) 분산된 방법으로 구한다. 각 노드는 자기 자신과 연결된 엣지 비용 정보만 알고 있고, 이 정보들을 이웃 노드 간에 서로 정보 교환하며 자기 자신과 나머지 노드간의 예상 최소 경로와 비용의 벡터(vector)를 저장해 놓는다. 이러한 전역 정보가 필요없고, 주변 노드와 반복적인 정보 교환을 통해 정답을 만드는 알고리즘을 거리 벡터(distance-vector, DV) 알고리즘이라고 한다. 보통 라우터별(per-router) 조정처럼 노드 간 메시지 교환이 활발한 구조에서 사용한다.  라우팅 알고리즘을 구분하는 두번째 방법은 정적(static)과 동적(dynamic)으로 나뉜다.정적 라우팅 알고리즘(static routing algorithms)은 경로의 정보가 아주 드물게 바뀌는 경우이며, 동적 라우팅 알고리즘(Dynamic routing algorithm)은 경로의 정보의 트래픽 부하 상태나 배치가 계속 바뀌는 경우를 의미하며, 주기적으로나 변화가 감지될 때마다 재계산한다.  동적 라우팅 알고리즘(Dynamic routing algorithm)은 그래서 네트워크의 변화, 라우팅 루프(routing loop), 루트 진동(route oscillation) 등에 취약하다.마지막 세번째 방법은 부하 민감(load-sensitive)과 부하 둔감(load-insensitive)으로, 부하 민감 알고리즘(load-sensitive algorithm)은 링크의 비용이 현재 네트워크 혼잡 상황을 반영해 자주 바뀌며, 혼잡한 회선을 우회하는 등의 경향이 있다. 과거 ARPAnet 등, 초기의 네트워크에 자주 사용되었지만 여러 문제가 있어 사용하지 않고 최신의 인터넷 라우팅 알고리즘들(RIP, OSPF, BGP 등..)은 부하 둔감 알고리즘(load-insensitive algorithm) 이며, 현재 네트워크 혼잡도에 민감하게 반응하지 않는다.5.2.1.연결 상태(LS) 알고리즘 (The Link-State (LS) Routing Algorithm)LS 알고리즘은 전역 네트워크 상태를 알아야 하며, OSPF 라우팅 알고리즘 같은 실무에서는, 연결 상태 전파 알고리즘(link-state broadcast algorithm)을 통해 각 노드가 다른 모든 노드들에게 연결 상태(link-state) 패킷을 전파(broadcast)하여 네트워크 전역 정보를 얻고, 자신과 다른 노드간의 최소 비용 경로를 계산한다. 이때 연결 상태 패킷에는 노드의 식별자와 주변 이웃 노드와 연결된 링크의 비용이 포함되어 있다.다익스트라 알고리즘(Dijkstra’s algorithm)은 프림 알고리즘(Prim algorithm)와 연관 알고리즘으로, 연결 상태 알고리즘 중 하나이며, 시작지점으로 놓을 시작 노드를 선정하고 다른 노드간의 거리를 무한대로 산정한(초기화(initialization) 과정)한 뒤, 전체 노드의 갯수를 k개로 생각한다면, 반복적으로 k번의 노드간의 최소비용을 갱신(반복(loop) 과정)하여 시작노드와 나머지 모든 노드들 간의 최소 비용 경로를 구할 수 있다.시작 노드 u에서 출발하는 연결 상태(LS) 알고리즘(Link-State (LS) Algorithm for Source Node u)시작 노드 u에서 각 노드간의 최소 거리 비용을 구하는 다익스트라 알고리즘의 pseudo code를 살펴보자.u, v: 각각 시작 지점 노드와 현재 최소 거리 비용을 구하고 싶은 대상 노드D(v) : 현재까지 계산된 시작 지점 u 부터 v 노드까지의 최소 비용 경로의 비용p(v) : 최소 비용 경로에서 도착 노드(v)의 이전 노드, 즉 도착 노드의 이웃 노드N : 모든 노드들의 집합N`: 현재 u와의 최소 비용 경로가 계산된 노드들의 집합 /* 초기화(Initialization) 구간*/ N` = &amp;#123;u&amp;#125; for 모든 노드 중에 노드 v     if v 노드가 u 노드의 이웃이라면         then D(v) = c(u, v)     else D(v) = 무한대/* 반복 구간 */N == N`일 때까지 반복:     노드 집합 N`에 존재하지 않는 노드들 중 D(w)가 가장 작은 w 선택        w 노드를 N` 집합에 추가        w의 이웃 노드이며 N`에 존재하지 않는 노드들 v들의 D(v)들을 갱신            D(v) = min(D(v), D(w) + c(w, v)) /*이전 최소 비용이거나 갱신된 최소 비용 중 작은 것으로 변경*/    아래 table 5.1은 반복 구간이 한번 반복될 때마다 step을 증가시켰을 때의 변수 값들이다.  초기화 단계에서 시작 지점 u와 직접 연결된 노드의 최소 거리 비용을 비용만큼으로 설정하고, 그렇지 않은 노드는 비용을 무한대로 설정한다.  첫번째 반복에서 최소 거리 비용이 제일 작고 ${N}’$에 포함되어 있지 않은 노드인 x를 선정해, ${N}’$집합에 포함시킨다.  이웃 노드 v들을 노드 x를 거쳐 지나가는 비용과 현재 설정되어있는 비용을 비교하여 작은 것으로 바꿔준다.  더이상 ${N}’$에 포함되지 않은 노드가 없어질 때까지 2,3을 반복한다.게산의 결과로 시작 노드 u로 부터 각각 노드 v까지의 경로 중 바로 이전 노드인 p(v)를 알 수 있으며, 이전 노드 p(v)를 통해 이전 노드의 이전 노드 p(p(v))도 알 수 있으므로, 이를 시작 노드 u가 나올때 까지 반복하여 최소 비용 경로를 구할 수 있다.  p(v)는 v의 이웃 노드들 w 중 D(w)가 가장 작은 노드이다.라우터의 포워딩 테이블의 경우 위 방법을 이용해 경로를 구한뒤, 해당 경로를 기반으로 u의 이웃한 노드의 링크(최소 비용 경로에서 자신의 다음 순서에 있는 노드 방향)로 포워딩 해주면 된다(fig.5.4 참조).해당 알고리즘의 시간 복잡도는 노드의 갯수를 n으로 놓고 O($n^2$)이며, 만약 힙 자료구조를 이용해서 반복 구조의 최소 비용 노드를 찾는다면 O(nlogn)까지 줄일 수 있다.fig 5.5를 참조하기 전에 먼저 이전 가정과 달리, c(u,v)와 c(v,u)는 같지 않은걸로 가정할 것이다.먼저 각 노드는 패킷을 전송해 링크에 부하와 트래픽을 유발하며, 라우터 z, x는 각각 1만큼, 라우터 y는 e 만큼 유발한다고 가정하자.라우터 z,x,y 전부 라우터 w와 통신하려고 한다. 그럴 경우 시작 상태는 fig.5.5(a)처럼 될 것이다. 라우터 y가 패킷을 w로 보내며 링크 (y,x)를 택해 c(y,x)는 e, c(x,w)는 라우터 x의 트래픽에 추가로 라우터 y의 트래픽 e를 감당해야 하므로 c(x,w) = 1+e가 된 모습이다.이후, LS 알고리즘이 실행되면, 이에 따라 fig.5.5(a)의 상황에서 비용을 판단하게 되어, 라우터 y의 시점에서는 시계 방향의 경로를 택하게 되고(1 vs 1+ 2e),  라우터 x 또한 시계 방향의 긴 길을 택하게 된다.(1 vs 1 + e), 라우터 z는 그대로 일것이다.위의 라우팅 알고리즘에 따른 결과는 fig.5.5(b)처럼 된다. 이번에는 모든 라우터가 비용이 0인 반시계 방향으로 라우팅할 것이다. 그에 따른 비용 변화는 fig.5.5(c)처럼 될 것이다.fig.5.5(c)에서 새로 LS 알고리즘으로 계산한 비용에 의해, 전부 시계방향으로 다시 바뀌게 되고 fig.5.5(d)처럼 되는데, 자세히 그림을 살피면 앞으로는 fig.5.5(c)와 fig.5.5(d)가 무한히 반복됨을 알 수 있다.이러한 현상을 경로 진동(route oscillation)이라고 하며, LS 알고리즘 뿐만 아니라 지연이나 혼잡을 측정해 정보로 사용하는 알고리즘은 모두 일어나게 된다.해결방법으로 첫번째는 트래픽 상황을 알고리즘에서 고려하지 않는 것이지만, 혼잡한 네트워크를 우회하는 것이 최소 비용 경로의 목적이므로 모순되는 방법이다.두번째는 LS 알고리즘을 모든 라우터에서 동시에 계산하지 않게 하는 것이다. 일정 주기로 LS 알고리즘을 실행하되, 다른 시간에 시작하게 만드는게 이상적이게 보이겠지만, 실제 시뮬레이션 결과, LS 알고리즘을 돌리기 위한 라우터 간의 통신 패킷이 패킷의 지연 등으로 어느 순간부터 주기적으로 변해버려 소용이 없으며, 실제로는 각 라우터에서 랜덤한 시간에 링크 정보를 주고받게 하여 해결한다.5.2.2. 거리 벡터(DV) 라우팅 알고리즘(The Distance-Vector (DV) Routing Algorithm)LS 알고리즘과 달리 DV 알고리즘은 반복적(iterative)이고, 비동기적(asynchronous)이며, 분배(distributed)된 알고리즘이다.분배적 특성은 각 노드가 직접적으로 연결된 이웃 노드와 정보를 주고 받아, 연산 결과를 다시 이웃들에게 분배하기 때문에 붙여졌으며,반복적 특성은 이웃 간에 더이상 정보교환이 없어져 노드들이 스스로 알고리즘을 멈출 때까지 과정이 반복되기 때문에 붙여졌고,비동기적 특성은 각자 노드들의 과정이 다른 노드의 과정의 진행에 영향을 받지않기 때문에 붙여졌다.$d_x(y)$를 노드 x에서 노드 y로 가는 최소 비용이라고 한다면, 이를 구하는 벨만 포드(Bellman-Ford) 방정식은 다음과 같다.$d_x(y)=\\min_v{c(x,v)+d_v(y)},    (5.1)$이는 직관적으로 최소 거리 비용이 중간 경유지 노드 v를 지나갈 때, 가장 비용이 적게 드는 중간 경유지를 골라야 최소 거리 비용이 나온다는 의미이다.벨만 포드 방정식을 통해, 라우터의 경로 상에 위치한 중간 경유 노드로의 포워딩 개념과 DV 알고리즘에서의 라우터의 이웃간 통신의 개념이 생겨났다.DV 알고리즘의 기본적인 골자는 다음과 같다.각 노드들은 DV 알고리즘에서 다음과 같은 정보를 유지해야 한다.  자신으로부터 자신을 제외한 다른 모든 노드들까지의 예상 최소 거리 비용 벡터  각 이웃들에 대해서 이웃으로부터 해당 이웃을 제외한 다른 모든 노드들까지의 예상 최소 거리 비용 벡터  이웃 노드과 연결된 링크의 비용(최소 거리 비용과 다르다)한 노드가 자신에 대한 최소 거리 비용 벡터를 새로 갱신하면 이를 주변 이웃 노드들에게 전달하고, 전달 받은 이웃 노드는 아래와 같이 자신에 대한 최소 거리 비용 벡터를 갱신한다.$D_x(y)=\\min_v{c(x,v)+D_v(y)}\\ for\\ each\\ node\\ y\\ in\\ N$이러한 갱신은 이웃들에게 비동기적으로 반복적으로 일어나게 되고, 그 결과 실제 최소 경로 비용에 가까워지게 된다.거리 벡터(DV) 라우팅 알고리즘 (Distance-Vector (DV) Algorithm)DV 알고리즘은 LS 알고리즘과 달리 전체 네트워크의 정보가 필요 없고, 이웃 노드로 가는 링크 비용과 이웃 노드가 주는 정보만 있으면 된다.DV 알고리즘이 사용되는 라우틴 프로토콜은 RIP, BGP, ISO, IDRP, Novell IPX, 초기의 ARPAnet 등이 있다.초기화(Initialization):    for 모든 목적지 노드 y in N        D_x(y) = c(x,y)/* 만약 y가 x의 이웃이 아니면 비용은 무한대*/    for 노드 w in x의 이웃 노드        for 노드 y in N            D_w(y)= ? (빈칸)    for 노드 w in x의 이웃 노드        거리 벡터 D_x = [D_x(y): y in N]를 w에게 전송loop(무한 반복)    if (이웃으로부터 거리 벡터 수신 또는 내 이웃과의 링크 비용이 변함)        for 목적지 노드 y in N            D_x(y) = min_v(c(x,v) + D_v(y)) /* 여기서 최소값을 이룩하는 v가 v*, 즉 최소 거리로 y를 가기위해 라우팅되야 할 이웃 노드이다. */        if D_x = [D_x(y): y in N]가 하나라도 변경됨            for 노드 w in x의 이웃 노드                거리 벡터 D_x = [D_x(y): y in N]를 w에게 전송라우터가 DV 알고리즘을 돌리는 이유는 포워딩 테이블을 생성하기 위해이므로, 중요한 것은 특정 노드 y까지의 최소 비용이 아니라 특정 노드 y까지 최소 비용으로 가기 위해 첫 발을 내디뎌야 할 x의 이웃 노드(v*)이다.아래 fig.5.6은 DV 알고리즘에 의한 라우팅 테이블 갱신의 시뮬레이션이다.원래는 비동기적으로 시행되지만, 편의를 위해 동기적으로 시행한다.그래프의 행은 각 라우터의 라우팅 테이블이고, 열은 포워드 테이블의 시간별 변화이다.가장 왼쪽 열은 초기상태의 라우팅 테이블로, 이웃 노드의 정보가 아직 없어 무한대로 표시되어있다. 이후 화살표 방향의 라우터에게 자신의 정보를 나눠준다.중앙 열은 이웃 노드에게 받은 정보를 토대로 자신의 이웃과의 거리 벡터를 재조정한다. 이후 x와 z의 라우팅 테이블은 변화가 생겼으므로, 자신의 정보를 보내주고, y의 라우팅 테이블은 그대로 이므로 정보를 전파하지 않는다.  이때, 라우팅 테이블의 변화는 아래와 같이 생긴다.\\[D_x(x)=0\\\\D_x(y)=\\min\\{c(x,y)+D_y(y),\\ c(x,z)+D_z(y)\\} = \\min\\{2+0,\\ 7+1\\}=2\\\\D_x(z)=\\min\\{c(x,y)+D_y(z),\\ c(x,z)+D_z(z)\\} = \\min\\{2+1,\\ 7+0\\}=3\\]마지막 열은 모두들 새로운 정보를 받았지만 이로 인해 생기는 라우팅 테이블 변화가 없으므로, 아무런 변화나 정보 전파가 이루어 지지 않는다. 앞으로 링크의 비용이 바뀌는 등의 변화가 생기기 전까지는 DV 알고리즘이 실행되지 않을 것이다.거리 벡터(DV) 알고리즘 : 연결 비용 변경과 연결 실패 (Distance-Vector Algorithm: Link-Cost Changes and Link Failure)연결 비용 변경으로 인한 라우팅 테이블 갱신의 두가지 예제를 살펴보겠다. 이번 경우는 중요한 라우터 y의 라우팅 테이블과 라우터 z의 라우팅 테이블만 살펴보겠다.먼저 fig.5.7(a)의 경우 c(x,y)가 4에서 1로 변경되었다.  y가 링크 비용의 변경을 탐지하고 자신의 라우팅 테이블값(c(x,y)는 4-&gt;1)을 바꾸고 주변 라우터에게 전파한다.  z가 y의 정보를 받고 라우팅 테이블 값을 바꾼다.(c(z,x)는 5-&gt;2)또 이를 주변 라우터에게 전파한다.  y는 z의 정보를 받고 DV 알고리즘을 실행하나 기존과 다를게 없으므로 전파를 멈춘다.이는 이상적인 상황으로 2번의 전파만으로 최적의 경로를 찾을 수 있게 되었다.fig.5.7(b)의 경우 c(x,y)가 4에서 60으로 변경되었다.      초기 변경 되기 전에 y의 라우팅 테이블에는 $D_y(x)=4,\\ D_z(y)=1$로 되있고,    z의 라우팅 테이블에는 $D_z(y)=1,\\ D_z(x)=5$로 되있을 것이다.    여기서 y가 c(x,y)가 4에서 60으로 증가한 것을 감지하면, 다음과 같은 식으로 $D_y(x)$를 바꿀 것이다.\\[D_y(x)= \\min\\{c(y,x)+D_x(x),c(y,z)+D_z(x)\\}=\\min\\{60+0,1+5\\}=6\\]    하지만 알다시피 위의 식의 $D_z(x)$가 5인 것은 이전 네트워크가 변경되기 전의 정보이고, 실제로는 해당 하는 길은 60+1만큼의 비용이 들어야한다. 하지만, 이를 이해하지 못하는 라우터 y는 잘못 갱신한 라우터 테이블 정보를 라우터 z를 포함한 주변 라우터들에게 보낼 것이다.        라우터 z는 해당 정보를 받고 다음과 같은 식으로 $D_z(x)$를 바꿀 것이다.\\[D_z(x)= \\min\\{c(z,x)+D_x(x),c(z,y)+D_y(x)\\}=\\min\\{50+0,1+6\\}=7\\]    이 또한 잘못된 값이며, 위에서 잘못 계산된 $D_y(x)$로 인한 오류이다. 오류가 있는 라우팅 테이블은 이 정보를 다시 주변 노드에게 전파하게 된다.        $D_y(x)$는 같은 방법으로, 이번에는 $D_y(x)=8$로 바꾸게 되며,  결국 y와 z 두 라우터는 잘못된 비용과 최소 경로를 가지고, 반복적으로 서로 계속 잘못된 정보를 갱신하고 전파하는 라우팅 루프(routing loop) 현상에 의해 count-to-infinity 문제에 빠지게 된다.          이 갱신은 잘못된 최소 경로 비용이 반복마다 1씩 증가하다 60의 값을 넘기면 잘못이 고쳐지고 올바른 비용과 올바른 경로를 선정하게 된다.      b의 시나리오의 경우 이때, 총 44번의 계산과 메시지 교환이 있게 되며, 만약 갱신된 링크 비용이 60이 아니라 9999면 끔찍할 정도로 긴 시간 동안 잘못된 계산을 이어가게 되고 이는 크나큰 성능 저하로 이어진다.  같은 증상이 서로 다른 두 라우터가 서로에게 메시지를 동시에 보내서 생기기도 한다.거리 벡터(DV) 라우팅 알고리즘 : 포이즌드 리버스 추가 (Distance-vector Algorithm: Adding Poisoned Reverse)위 fig.5.7(b) 시나리오에서 설명한 라우팅 루프 문제는 포이즌드 리버스(Poisoned Reverse)로 해결할 수 있다.포이즌드 리버스는 특정 노드가 라우팅 정보를 전파할 때, 특정 목적지 노드까지의 경로에 포함된 이웃 노드에게는 해당 목적지 노드까지의 최소 경로 비용을 무한대 바꿔 표기해 전파하는 방법으로, 이를 통해 라우팅 루프 문제를 해결할 수 있다.쉽게 말해, 라우터 z가 라우터 x로 가기위한 경로에 라우터 y가 존재한다면, 라우터 y에게 정보를 전파해줄 때, 라우터 y 한정으로 $D_z(x)=\\infty$로 바꿔 표기해서 보내주는 것이다.위의 fig.5.7.(b) 시나리오에서 예시를 들어보자.      라우터 y가 c(x,y) 비용의 변화를 감지하고 다음과 같은 식으로 라우팅 테이블을 변경하고 주변 노드에게 정보를 전파한다.\\[D_y(x)= \\min\\{c(y,x)+D_x(x),c(y,z)+D_z(x)\\}=\\min\\{60+0,1+5\\}=6\\]    하지만 이때, 라우터y에서 라우터 x로 가는 경로에 라우터 z가 있는 경우가 최소 비용이므로, 라우터 z에게 $D_y(x)=\\infty$로 바꾸어 전파해준다.        라우터 z는 해당 정보를 가지고 다음과 같이 라우팅 테이블을 변경한다.\\[D_z(x)= \\min\\{c(z,x)+D_x(x),c(z,y)+D_y(x)\\}=\\min\\{50+0,1+\\infty\\}=50\\]    라우터 z는 무한대인 라우터 y의 경로 대신, 비용 50짜리 라우터 x의 경로를 최소 비용 경로로 택했으며, 이곳에서는 라우터 y가 포함되지 않으므로 정상적으로 전파한다.        라우터 y는 해당 정보를 가지고 다음과 같이 라우팅 테이블을 변경한다.\\[D_y(x)= \\min\\{c(y,x)+D_x(x),c(y,z)+D_z(x)\\}=\\min\\{60+0,1+50\\}=51\\]    이제 양측다 정상적인 경로와 비용을 계산하게 된다!  하지만 포이즌드 리버스로도 여러 노드가 연관되어 있는 라우팅 루프는 풀 수 없다.포이즌드 리버스 추가로 막기위해      Triggered Update : 네트워크에 변경사항이 있으면 지체 없이 바로 상태를 갱신하도록 함.        Hold Down : 회선이 먹통이 될 시, 라우팅 테이블을 바로 갱신하지 않고, 전체 네트워크 경로 정보가 올 때까지 기다림          1, 3으로도 못막는 루프를 막을 수 있다            Split Horizon: 이웃 노드로부터 받은 새로운 라우팅 정보가 자신의 라우팅 정보의 갱신을 초래하여 이를 전파해야할 때, 이전에 새로운 라우팅 정보를 줬던 이웃에게는 전파하지 않는다.          보통 포이즌드 리버스를 함께 사용하여 효율을 극대화함      LS와 DV 라우팅 알고리즘 비교(A Comparison of LS and DV Routing Algorithms)앞서 설명한 두 알고리즘은 서로 상호보완적인 접근 방법을 가지고 있다.DV는 주로 이웃들과 통신하여 모든 노드에 대한 예상 최소 거리 비용 주고 받고, LS는 전역 네트워크 정보를 활용해 경로를 구하며, 전파(boradcast)를 이용하여 이웃과의 비용을 모든 노드에게 알려준다.DV와 LS의 주요 차이점을 알아보자, N은 노드들의 집합(routers), E는 엣지들의 집합(link)  메시지 복잡도(Message complexity)                                                      LS에서는 각 노드가 모든 노드의 비용을 알고 있어야 하므로 $O(              N              \\              E              )$만큼의 메시지를 보내야하며, 링크 비용이 바뀌면, 새로운 메시지가 각 노드마다 보내져야 한다.                                          DV에서는 매 연산마다 이웃 노드간 메시지 교환이 필요하다. 링크 비용이 변경되면 메시지가 전파되기 시작하지만, 링크와 관계가 된 노드로 가는 최소 비용이 바뀔 때만 전파가 되므로 제한된다. 이외에도 여러 요인에 따라 알고리즘의 복잡도가 달라진다.        수렴 속도(Speed of convergence)          네트워크의 변화로 부터 올바른 최소 비용 경로를 계산하는데 걸리는 시간                                                  LS는 $O(              N              ^2)$ 시간복잡도에 $O(              N              \\              E              )$ 만큼의 메시지가 필요하며, DV는 수렴 과정이 느리고(약 3분, LS의 3배정도 느림) count-to-infinity 문제가 생길 수 있다.                                            강건성(Robustness)          라우터가 실패, 오작동, 공격 당했을 때의 피해      LS는 잘못된 링크 비용이나 오류인 정보를 주변 이웃 노드 한정으로만 전파한다. LS에서 한 노드의 포워딩 테이블 연산은 다른 노드의 연산과 별개로 진행되므로, 시스템을 오류 등에서 강건하게 만들어 준다.      잘못된 최소 비용 경로나 오류가 이웃간 전파의 반복을 통해 모든 노드에게 전염될 수 있다. DV는 노드의 계산 결과를 이웃에게 전달하고, 해당 이웃은 그에 대한 반응으로 마찬가지로 계산결과를 하여 전달하므로 순식간에 오류가 퍼진다.      둘다 일장일단이 있는 알고리즘이며, 인터넷에 다양한 곳에 쓰이는 중이다.5.3 인터넷에서의 AS 내부 라우팅 : OSPF (Intra-AS Routing in the Internet: OSPF)우리가 앞서 가정했엇던 네트워크와 라우터의 행동들은 너무 간단하게 생각한 건데, 그 이유는  규모(Scale), 최근의 인터넷은 수많은 라우터로 이루어져 있으며, 통신, 계산, 라우팅 정보 저장 등에 필요한 오버헤드가 불가능할 정도로 크다.  관리상의 조절(Administrative autonomoy), ISP들은 자신의 거대한 네트워크를 원하는 대로 설정, 조정하고, 외부의 인터넷과 연결됨과 동시에 외부로 부터 정보를 숨기고 싶어한다.이 둘의 문제를 해결하기위해 라우터들의 자동화 시스템(autonomouse systems, ASs)을 만들게 되는데, 각 자동화 시스템은 같은 관리 하에 놓이게 한다.ISP는 자신의 네트워크를 거대한 하나의 AS에 놓기도하고, 여러개의 AS로 쪼개어 관리하기도 하며, 이러한 AS에는 마치 아이피 주소처럼 전세계에서 유일한 식별자값인 ASN(autonomouse system number)를 ICANN 지역 레지스트리에게서 부여받는다.같은 AS 하에서는 같은 라우팅 알고리즘과 라우팅 정보를 공유하며, AS 내부에서 실행되는 라우팅 알고리즘을 AS 내부 라우팅 알고리즘(intra-autonomous system routing protocol)이라고 한다.열린 최단 경로 우선 (Open Shortest Path First (OSPF))열린 최단 경로 우선(OSPF, Open Shortest Path First), 그리고 이와 비슷한 IS-IS은 널리 사용되는 AS 내부 라우팅 알고리즘(intra-autonomous system routing protocol)이다.OSPF의 OPEN은 라우팅 프로토콜이 대중에게 공개되어 있기 때문이며 최신버전인 버전 2는 [RFC 2328]에서 볼 수 있다.  반대로 Cisco의 EIGRP 프로토콜은 20년 동안 Cisco 사의 사유 프토콜이었다가 최근에 풀렸다.OSPF는 LS(link-state) 프로토콜로, 연결 상태(link-state) 정보와 다익스트라 최소 비용 경로 알고리즘을 사용한다.OSPF 하의 라우터들은 각각 AS 네트워크 전체에 대한 그래프와 자기자신을 루트로 하는 각각 서브넷에 대한 최단 거리 트리를 가지고 있으며, 모든 링크 비용들은 네트워크 관리자(network administrator)에게 의해 관리된다.네트워크 관리자는 자유롭게 링크 비용을 산정할 수 있는데, 모든 링크를 1로 설정하여 최소합(minimum-hop) 라우팅을 하거나, 각 링크의 대역폭의 역수를 가중치로 두어 트래픽에 따라 라우팅을 할 수 있다.  이후, 네트워크 관리자의 의도대로 설정한 비용에 의거해 다익스트라 알고리즘으로 경로를 구한다. 이를 역이용해, 네트워크 링크 사용률 관리 등을 위해 특정 경로로 라우팅을 강제하기 위해 일부러 링크의 비용을 바꾸는 경우도 있다.OSPF에서는 라우터가 라우팅 정보를 이웃 뿐만 아니라 AS 내의 전체 라우터에게 전파하며, 링크의 상태가 바뀌거나, 바뀌지 않았어도 주기적으로(최소 30분 이상) 링크 상태를 체크하고 전파한다. 이는 연결 상태(link-state) 알고리즘을 강건하게 해준다.OSPF 전파(broadcast)는 아이피 주소에 따라 상위 계층 프로토콜인 OSPF(헤더 필드 값 89번)에 의해 OSPF 메시지를 주고 받으며 이루어지며 따라서 메시지 신뢰성 보장 전송(reliable message transfer)과 연결 상태 전파(link-state boradcast) 같은 기능이 구현되어있어야 한다.또한 OSPF 프로토콜은 라우터 간에 HELLO 메시지를 보내게 하여 링크의 사용 가능 상태를 확인하고, 라우터들에게 주변 라우터의 네트워크 전역 연결 상태 정보에 접근할 수 있게 해준다.다음은 OSPF의 장점이다.      보안(Security)    OSPF의 라우터들은 정보를 공유할 때 인증을 이용해 공격자들의 잘못된 라우터 정보 입력을 막을 수 있다. 설정에 따라 보안 설정을 안하거나, 간단, MD5로 설정할 수 있다.    간단 버전은 단순히 각 라우터 간에 비밀번호를 공유하며, OSPF 패킷을 보내면 암호가 plaintext 형태로 적혀있으므로 전혀 안전하지 않다.    MD5 버전은 나중에 8장에서 배울 방법으로, 라우터 전역에 공유된 비밀 키를 이용해 OSPF 패킷의 데이터를 해쉬화하고 해쉬 결과값를 패킷에 포함해 보낸다. 이후 도착한 라우터에서는 이를 공유한 비밀키로 패킷의 데이터를 해쉬화 하고 이 해쉬값을 패킷의 해쉬값과 비교하여 인증한다. 시퀀스 번호(sequence number)를 사용해 리플레이 공격(replay attack)을 방지한다.        다중 동일 비용 경로(Multiple same-cost paths)    동일한 비용의 최소 비용 경로가 여럿이 있다면 부하 방지를 위해 여러 경로를 번갈아가며 사용한다.        유니캐스트, 멀티 캐스트 라우팅 통합 지원(Integrated support for unicast and multicast routing)    멀티 캐스트 OSPF (Multicast OSPF, MOSPF)는 OSPF의 확장형으로, 멀티캐스트 라우팅을 제공하며, OSPF 연결상태 전파 메커니즘을 이용해 기존의 OSPF 정보 데이터베이스에 새로운 종류의 연결 상태를 추가하고 전파한다.        단일 AS 내의 계층 지원(Support for hierarchy within a single AS)    AS 내부를 또 각각의 지역(area)으로 나누어 라우터간의 정보전파나 라우팅 알고리즘을 구별할 수 있다.    지역의 가장자리 바깥 라우터들이 다른 지역의 패킷을 라우팅하는데 이러한 지역 가장자리 라우터들과 가장자리가 아니어도 추가로 포함된 라우터들의 지역을 백본 네트워크로 설정하여 지역 간의 트래픽을 전달하는 역할을 한다.    먼저 지역 내부의 패킷은 외부 지역과 통신하기 위해 지역 가장자리 라우터에게 보내지고(지역 내부 라우팅), 이후 백본을 거쳐 목적지 지역에 도착한다.    OSPF는 복잡한 프로토콜이므로 더욱 많이 알고 싶다면 [Huitema 1998; Moy 1998; RFC 2328] 참조  RIP(Routing Information Protocol)DV 방식을 이용하는 AS 내부 라우팅 프로토콜(=게이트웨이 내부 프로토콜, IGP, Interior Gateway Protocol &lt;=&gt; EGP), UDP를 이용해서 정보교환라우터를 얼마나 넘어가느냐 (=홉, HOP)으로 거리를 측정하며, 최대 15개가 넘어가는 홉은 측정하지 못하므로 대규모 네트워크에 부적합하다.매 주기 마다(기본 30초) RIPv1은 브로드캐스트(broadcast), RIPv2부터는 멀티캐스트(Multicast)로 주변 네트워크에 UDP 패킷을 보내 라우터의 정보를 갱신한다.하지만 이 주기 안에 갱신되지 않은 정보로 인해 라우팅 루프(routing loop)에 빠질 수 있다.5.4 ISP 간의 라우팅 : BGP (Routing  Among the ISPs: BGP)다른 AS를 포함하고 있는 여러 ISP가 서로 통신하기 위해서는 AS 간 라우팅 프로토콜(inter-autonomous system routing protocol)이 필요하다.AS 간 라우팅 프로토콜(inter-autonomous system routing protocol)를 통해 서로 다른 AS가 통신하기 위해서는 같은 프로토콜을 공유해야 하는데 그것이 바로 BGP(Border Gateway Protocol)이며, 전체 인터넷에서 BGP만 사용한다.BGP는 ISP 간 통신을 위해 아주 중요하며, distance-vector 라우팅과 비슷하게 분산적(decentralized), 비동기적(asynchronous) 프로토콜이다.5.4.1.BGP의 역할 (The Role of BGP)AS 내부의 라우터 간의 통신은 AS 내부 라우팅 프로토콜(intra-AS routing protocol)을 통해 이루어지며, AS 외부와의 통신은 BGP를 이용하게 된다.BGP는 특정 아이피 주소가 아닌 CIDR화된 접두어(CIDRized prefixes)를 이용해 서브넷이나 서브넷의 모임을 주소로 지정한다.  예시로 BGP의 목적지 주소가 138.16.68/22이면, 1024개의 아이피 주소가 포함되있는 주소이며, 이에 따라 라우터의 포워딩 테이블의 항목은 (CIDR화 접두어 주소, 라우터의 특정 인터페이스 번호) 형태로 표시된다.BGP는 다음과 같은 것을 라우터들에게 제공한다.      인접한 AS간의 접두어 주소 도달가능성 정보 확보(Obtain prefix reachability information from neighboring ASs)    BGP는 각 서브넷에게 자신의 존재를 인터넷 상에 전파할 수 있도록 하여 접두어 주소를 알려주는 역할을 한다.        접두어 주소로의 최선의 경로 결정(Determine the “best” routes to the prefixes)    라우터는 특정 접두어 주소로 접근하는 여러 경로를 알 수 있다. 최선의 루트를 결정하기 위해서, 라우터는 BGP 경로 선택 절차(BGP route-selection procedure)를 실행할 수 있으며, 최선의 경로는 정책과 도달가능성 정보에 따라 결정된다.  5.4.2.BGP 경로 정보 전파(Advertising BGP Route Information)아래 fig.5.8과 같이 AS1, AS2, AS3로 이루어진 네트워크를 가정하자, AS3는 접두어 주소가 x인 서브넷을 포함하고 있다.모든 라우터들은 관문 라우터(gateway router) 아니면 내부 라우터(internal router)이다. 관문 라우터는 AS의 가장자리에 위치해 다른 AS와 연결되어 있는 라우터이며, 내부 라우터는 오직 같은 AS의 라우터와만 연결되어 있는 라우터이다. 아래와 같은 경우, 1c, 2c ,3a가 관문 라우터, 나머지는 경계 라우터이다.이제 접두어 주소 x의 도달가능성 정보를 모든 라우터에게 전파하는 과정을 살펴보자.먼저, AS3가 AS2에게 BGP 메시지로 x가 AS3 내부에 존재한다고 알리는 메시지 “AS3 x”를 보내면, AS2는 AS1에게 BGP 메시지 “AS2 AS3 x”를 보내 자신, AS2와 연결된 AS3에 x가 존재함을 전파한다.이렇게 모든 AS 들은 x의 존재 뿐만 아니라 도달하기 위한 AS들의 경로 또한 알게 된다.정확히는 AS 간의 통신이 아니라 라우터 간의 BGP 메시지 교환을 통해 이루어지며, 통신을 위해 포트 179번을 이용하는 반영구적 TCP 연결을 이용한다.이때, TCP 연결로 BGP 메시지가 주고 받아진다면, BGP connection이라고 부르며, 같은 AS 소속 간의 통신은 내부 BGP(Internal BGP, iBGP) 연결, 다른 AS 소속 간의 통신은 외부 BGP(external BGP, eBGP) 연결이라고 한다.아래 fig.5.9는 그러한 BGP 연결의 예시이며, 보통 AS 간에 직접적으로 연결된 하나의 라우터마다 하나의 eBGP 연결이 수립되어 있으며 예시로는 1c-2a, 2c-3a 연결이다.iBGP 연결의 경우 같은 AS 소속 라우터간의 연결로 표시되 있지만, iBGP 연결이 오직 직접적으로 물리 도선으로 연결되어야만 수립되는 것은 아니다.도달가능성 정보를 전파하기 위해서는 이러한 eBGP와 iBGP가 활용된다.예를 들어, AS3 x 메시지는 3a에 의해 2c에 전파되고, 2c라우터는 2a 라우터를 포함해  2b, 2d 라우터에게도 해당 AS3 x 메세지를 전파해 AS2 전체에게 알리게 된다.이후 2a는 이 메시지를 AS2 AS3 x로 바꾸어 1c에게 전달하고, 1c는 해당 메시지를 AS1 라우터 전체에게 퍼트려 모든 라우터는 x의 존재와 도달하기 위한 경로를 알게 된다.물론 실제로는 x로 도달하는 경로가 하나가 아니라 여러 경로가 존재할 수 있으며, 이는 fig.5.10에 예시로 나와있다.이 경우, 두가지 경로의 전파가 모두 일어나 라우터들은 x로 향하는 경로 정보 또는 도달가능성 정보를 두개 유지하게 된다.5.4.3.최적 경로 결정 (Determining the Best Routes)그렇다면 여러 경로 중 하나를 고를 땐 어떻게 고를까?라우터가 BGP 연결을 통해 접두어 주소를 전파할 때, 접두어 주소 뿐만아니라 BGP 속성(BGTP attributes)를 추가로 포함해서 보낸다.BGP 속성의 예시로 route는 접두어 주소를 의미하며, AS-PATH는 전파하면서 지나온 AS들을 의미하며, 뒤의 메시지의 예시에서 AS2 AS3 x 에서 “AS2 AS3” 부분이다.  이를 통해 단순히 경로 뿐만 아니라 전파의 루프 또한 감지할 수 있다. 만약 해당 AS-PATH에 자신이 속한 AS가 있다면, 라우터는 해당 메시지를 전파하는 것을 거부한다.NEXT-HOP 속성은 AS 간 프로토콜과 AS 내부 프로토콜의 연결점을 알려주는 속성으로, AS-PATH의 가장 첫번째 AS의 관문 라우터의 인터페이스 아이피 주소를 가지고 있다.예시를 들자면  아래 fig.5.10에서 AS1에 전파된 메시지의 AS-PATH가 AS2 AS3 x인 경우, NEXT-HOP은 2a 라우터가 1c와 연결되어있는 인터페이스의 아이피 주소이다.AS2가 제일 처음에 나온 AS이고, AS2와 연결하기 위해 2a 라우터와 연결되어 있기 때문이다.여기서 주의할 점은 NEXT_HOP은 자신이 속한 AS에 속해있지 않은 라우터의 인터페이스라는 점이다.정리하자면 AS1의 각 라우터가 x로 가는 경로 정보는 다음과 같이 두개가 저장되어 있다.2a 라우터의 AS1가 연결된 인터페이스 아이피 주소; AS2 AS3; x3d 라우터의 AS1가 연결된 인터페이스 아이피 주소; AS3; x현재는 NEXT-HOP, AS-PATH, 목적지 접두어 주소 3개로 이루어졌다고 가정했지만, 실제의 BGP 루트 정보는 더 많은 속성을 포함하고 있다.뜨거운 감자 라우팅(Hot Potato Routing)이제 좀더 자세한 BGP 라우팅 알고리즘에 대해서 알아볼 것이며, 그중 간단한 라우팅 알고리즘으로 뜨거운 감자 라우팅(Hot Potato Routing)이 있다.뜨거운 감자 라우팅에서는 여러 경로 중에 NEXT-HOP으로 가는 경로의 비용이 가장 적은 경로가 선택된다.라우터가 AS 외부의 접두어 주소를 포워딩 테이블에 넣기위해, AS 간 프로토콜(BGP)과 AS 내부 프로토콜(OSPF)가 둘다 사용된다.아래 fig.5.11은 뜨거운 감자 라우팅을 사용한 포워딩 테이블 생성의 요약이다.            1      2      3      4                  BGP를 이용해 서브넷 x가 여러 관문 라우터를 통해 도달할 수 있음을 알게됨      OSPF를 이용해 얻은 라우팅 정보로 각 관문 라우터까지 도달하는 최소 비용 경로의 비용을 계산함.      뜨거운 감자 라우팅 : 가장 작은 비용이 드는 관문 라우터 선택      최소 비용 관문 라우터의 인터페이스 I를 포워딩 테이블에 (x, I) 형태로 집어넣음      뜨거운 감자 라우팅의 아이디어는 최대한 패킷을 저비용으로 다른 AS에게 넘기는 것이다.뜨거운 감자의 명칭도, 손이 뜨거우니 최대한 빨리 뜨거운 감자(패킷)를 다른 사람(AS)에게 넘기자는 것이다.그리하여 뜨거운 감자 라우팅은 이기적인 알고리즘으로 불리운다.문제는, 그렇게 넘긴 AS가 x로 향하는 AS 루트 중에 최적의 루트가 아닐 수도 있다는 점이다.당장에 fig.5.10의 예시를 보자면 1b 입장에서 뜨거운 감자 라우팅 상으로 2a가 3d보다 가까우므로 AS2로 패킷을 보내겠지만, 실제로는 AS 내부 상에서는 조금 멀더라도 3d로 보내는 것이 패킷이 빨리 도착한다.또한 주의할 점은 뜨거운 감자 라우팅의 결과 값은 같은 AS 라우터여도 서로 다를 수 있다는 점이다.1b와 달리 1a 라우터는 3d 라우터가 더욱 가까우므로 패킷을 AS3로 보내게 될것이다.경로 선택 알고리즘 (Route-Selection Algorithm)실제 BGP는 뜨거운 감자 라우팅보다 더 복잡하지만 이를 일부에서 포함하고 있다.BGP의 경로 선택 알고리즘(Route-Selection Algorithm)은 각 라우터들이 알고 있는 해당 접두어 주소로 가는 경로 정보들이다.하나만 있다면 그것을 선택하겠지만 여러 개 있다면 다음과 같은 소거법으로 경로가 하나만 남을 때까지 누락한다.      BGP 경로 정보에는 추가로 지역적 선호(local preference) 속성이 있어, 가장 선호하는 경로를 고른다.    라우터가 설정하거나 다른 라우터가 설정한 값으로 같은 AS 소속 라우터들이 공유하는 값이며, AS의 네트워크 관리자가 정한 정책에 따라 정해진다.        만약, 가장 높은 지역적 선호 속성값을 가진 경로가 여럿 있다면, AS-PATH 속성이 가장 짧은 것(AS를 가장 적게 지나는 경로)를 고른다.    BGP 이때 DV 알고리즘을 이용해 경로를 계산하며, 라우터 홉보다는 AS 홉을 기준으로 구한다.        AS-PATH 속성의 길이가 같은 경로가 여럿 있다면, 뜨거운 감자 라우팅을 이용한다. 위에서 설명한대로 NEXT_HOP까지의 비용이 가장 적은 경로를 구한다.        여전히 경로가 둘 이상 남았다면, 라우터는 BGP 식별자를 사용하여 고른다.  3번의 뜨거운 감자 라우팅을 이용하기 전에 2번의 AS-PATH 속성의 길이를 고려하니 이제 더이상 이기적인 알고리즘이 아니다.BGP 알고리즘은 AS 간 알고리즘의 사실상의 기준이 됬으며, http://www.routeviews.org 에서 tier-1 ISP의 BGP routing table을 볼 수 있다.5.4.4.아이피-애니케스트 (IP-Anycast)BGP는 추가로 DNS 등에서 사용되는 아이피 애니케스트를 구현하는데 사용된다[RFC 1546, RFC 7094]우리는 이전에 CDN의 멀티미디어 컨텐츠, DNS의 레코드 등을 배울때, 지정학적으로 떨어져있는 사용자들에게 양질의 서비스를 제공하기 위해 여러 서버 클러스터를 지정학적으로 퍼뜨리고, 가장 응답이 빠른 서버를 연결해주는 방식을 사용했었다.이 가장 가까운, 즉 가장 비용이 적게드는 서버를 고를 때 BGP의 경로 선택 알고리즘을 이용할 수 있다.fig.5.12의 CDN의 예시를 보자면, CDN 측에서 여러 서버의 아이피 주소를 같은 걸로 하여 ISP에 연결하면, 각 라우터들은 BGP 경로 정보 전파를 통해 제각기 같은 아이피 주소를 가진 여러개의 루트 정보가 생기게 되고, BGP 경로 선택 알고리즘에 의해 가장 최적의 서버의 주소를 포워드 테이블에 추가하게 된다.하지만 실제로 CDN들은 아이피 애니캐스트로 구현하지 않는데, BGP 라우팅으로 인해 TCP 연결 중에 패킷들이 여러 서버에 나뉘어 보내질 수 있기 때문이다. 실제로는 2장에서 설명했었던 지정학적 위치 데이터베이스나 주기적인 성능 측정으로 서버를 지정해준다.하지만 BGP를 이용한 아이피 애니캐스트 방법은 루트 DNS 서버 쿼리에는 자주 사용된다.루트 DNS 서버가 13개 있다고 알려진 것은 아이피 주소가 13개라는 의미이며, 실제로 돌아가는 루트 DNS 서버는 백여개가 세계에 퍼져있으며, 같은 아이피 주소의 DNS 서버를 BGP 라우팅 알고리즘으로 쿼리해준다.5.4.5.라우팅 정책 (Routing Policy)라우팅 정책을 이용해 AS 라우팅 알고리즘을 조정할 수 있다. 예를 들어 BGP 경로 정보의 local-preference 속성을 이용할 수 있다.fig.5.13의 예를 들자면 A, B, C, X, Y는 각각 한 ISP에 속한 AS들이다. 다만 ABC는 W, X, Y를 연결해주는 백본 제공자 네트워크(backbone provider network)이며, XYZ는 소비자들이 사용하는 고객 접근 ISP(customer access ISP)이다. 특히 X는 여러 백본 제공자 네트워크에게 연결된 멀티홈드 접근 ISP(multi-homed access ISP)이다.W, X, Y에 들어오는/나가는 패킷은 각각 해당 AS 내부 라우터만 목적지 아이피 주소/ 발신지 아이피 주소야 한다.즉, 그 어떤 패킷도 W, X, Y 네트워크를 경유를 목적으로 들어갈 수 없다.이러한 경로를 강제하기 위해 BGP 경로 정보(BGP route)를 조정하게 끔 라우팅 정책(Routing Policy)을 수립할 수 있다.예를 들어 XCY 경로의 BGP 경로 정보를 가지고 있음에도 B에게 알리지 않게끔 정책을 세우면, B는 Y로 패킷을 보내는데 해당 경로를 사용할 수 없다.이를 통해 고객/제공자 네트워크를 구현할 수 있다추가적인 예로 BAW 루트 정보를 C에게 알리지 않아 C가 굳이 패킷을 W에게 보낼 때 CAW처럼 직접적인 효율적인 경로를 내버려두고, CBAW처럼 비효율적인 경로를 선택하지 않게 한다.ABC 같은 백본 네트워크에는 패킷이 경유할 이유만 있지 패킷이 도착할 이유는 많지 않기 때문에, 목적지나 출발지가 ABC인 패킷을 막을 수도 있을 것이다.실제로 ISP 간의 공개적이거나, 비밀적이거나, 암묵적인 피어링 동의를 통해 저런 비효율을 막는다.BGP에 대한 추가 자료는 [Stewart 1999; Huston 2019a; Labovitz 1997; Halabi 2000; Huitema 1998; Gao 2001; Feamster 2004; Caesar 2005b; Li 2007]에서 볼 수 있다.왜 AS 간 프로토콜과 AS 내부 프로토콜이 차이를 보이는가?(WHY ARE THERE DIFFERENT INTER-AS AND INTRA-AS ROUTING PROTOCOLS?)이유는 두 프로토콜 간의 목적이 차이를 보이기 때문이다.  정책(policy) : AS 간 프로토콜은 AS 간의 정책의 차이가 존재하지만, AS 내부 프로토콜은 보통 하나의 관리적 조정 하에 라우팅 된다.  규모(scale) : 라우팅 알고리즘과 자료 구조의 대규모 네트워크에서의 라우팅을 다루고 규모를 바꾸는 능력이 AS 간 라우팅에서 중요하다. 하지만 AS 내부 라우팅에서는 규모의 변경은 큰 일이 아니다. 만약 해당 AS가 너무 커져 성능이 저하된다면 AS를 여러 지역(area)로 나누어 관리하거나 AS를 여러개로 쪼갤 수 있다.  성능(performance) : 앞서 말했던 정책 덕분에, AS 간 프로토콜은 가끔 성능 보다는 정책을 우선시 해야되는 경우가 있다. 예를 들어 조금 더 많은 hop을 지나가도, 계약되지 않은 ISP는 피해간다던지, 등이다. 하지만 AS 내부 프로토콜은 대부분 그러한 정책에서 자유로우므로, 라우팅의 성능에 좀더 집중할 수 있다.5.4.6.정리하기 : 인터넷 존재감 확보 (Putting the Pieces Together: Obtaining Internet Presence)당신이 여러개의 서버로 이루어진 작은 서버와 공개 웹 서버, 사원들이 이용할 메일 서버와 DNS 서버를 가지고 있다고 하자.가장 먼저 해야할 일은 인터넷과 연결성을 확보하는 것이다. 지역 ISP와 연락하여 계약하면, 당신의 관문 라우터(gateway router)와 ISP의 라우터를 연결하기 위해 DSL 연결로 전화선을 통해 ISP의 라우터와 연결되거나 챕터 1에서 묘사된 다양한 방법으로 연결해 줄 것이다.물리적 연결이 끝난 이후, ISP는 당신에게 아이피 주소 범위(예를 들어 a/24, 최대 256개 아이피 주소)를 줄 것이며 해당 아이피 주소들을 웹서버, 메일서버, DNS 서버, 관문 라우터, 기타 인터넷 장비등에 할당해야 한다.이후 추가로 인터넷 레지스타(registar)에게 연락해 당신의 회사의 도메인명을 확보하고, 레지스타에게 DNS 서버의 아이피 주소를 알려줘서 DNS 시스템에서 존재성을 확보해야한다. 당신 회사의 도메인명과 DNS 서버 주소는 레지스타에 의해 top-level-domain(.com 등)에 추가될 것이며, 이후로는 사람들이 해당 도메인명으로 접근할 수 있게 된다.이제 당신은 웹서버의 호스트명과 아이피 주소를 매핑하여 당신의 DNS 서버의 항목으로 추가할 것이다. 이를 통해 도메인명으로 찾아온 사람들이 DNS 시스템에 도메인명에 매핑된 아이피 주소를 요구할 것이고, DNS 시스템은 당신의 DNS 서버에 물어봐 웹서버의 아이피 주소를 알려할 것이며, 당신은 방금 추가한 DNS 서버에 매핑된 아이피 주소를 돌려줄 수 있다. 그리하여 사람들이 웹서버에 도달해 TCP 연결을 할 수 있게 되고, 메일 서버 같은 기타 공개 서버에 대해서도 같은 일을 해야한다.하지만, 그렇게 하기 위해서는 인터넷 AS 내부의 라우터들이 당신의 웹서버로 가는 경로를 알고있어야지 데이터그램을 적절한 출력 포트로 포워딩해줄 수 있고, 그러기 위해서는 당신의 a/24 주소와 BGP 알고리즘을 이용해야 한다.당신이 계약한 지역 ISP회사는 당신의 접두어 주소(아이피 주소 범위)를  BGP를 이용해 전파할 것이다.이제 전세계의 라우터가 당신의 서비스로 가는 길을 알게 되고, 사람들이 당신의 웹 서비스와 메일 서버에 접근할 수 있게 된다.5.5 SDN 컨트롤 측면 (The SDN Control Plane)SDN을 지원하는 장비들과 서비스를 설정하고, 관리하며, 패킷 포워딩을 조정하는 네트워크 전역 논리인 SDN 컨트롤 측면에 대해서 배울 것이다.또한 SDN의 용어대로 여러 계층 패킷의 헤더를 참고해 포워딩하는 장비를 라우터 대신 패킷 스위치로 바꿔 부를 것이다.SDN 구조의 특징은 크게 4가지가 있다.      흐름 기반 포워딩 (Flow-based forwarding)    SDN 제어 스위치는 전달 계층, 네트워크 계층, 링크 계층의 헤더 필드들을 활용해 포워딩할 수 있다.    우리는 이전에 오픈플로우 1.0 추상화에서 11개의 헤더필드를 이용해 포워딩하는 것을 배웠고, 이는 기존의 각자의 계층 헤더만으로 포워딩하던 방법과의 주요한 차이이며,    이러한 오픈플로우의 포워딩 규칙인 항목을 스위치에서 계산, 관리, 설치하는 하는 것이 SDN의 일이다.        데이터 측면과 컨트롤 측면의 분리(Separation of data plane and control plane)    데이터 측면은 match-plus-action을 행하는 스위치들로 이루어져 비교적 간단하고 빠른 장비들이다.    컨트롤 측면은 스위치의 플로우 테이블을 설치 관리하는 서버와 소프트웨어로 이루어졌다.        네트워크 제어 기능(Network control functions)    SDN의 컨트롤 측면은 소프트웨어로 정의되며, 기존의 라우터와 달리, 컨트롤러가 네트워크 스위치와 물리적으로 분리되어 있는 원격 컨트롤러이다.    컨트롤 측면은 SDN 컨트롤러(또는 네트워크 운영체제, network operating system)와 네트워크 제어 응용 프로그램(network control application)으로 이루어져있으며,    컨트롤러는 정확한 네트워크 상태 정보를 유지하고, 네트워크 제어 응용 프로그램에게 상태 정보를 주어 네트워크 장비를 모니터, 프로그래밍, 조종하게 해준다.    fig.5.14에 그림에서는 하나의 단일 컨트롤러로 보이지만 실제로는 여러 서버에 분산되어 구현되며 논리적으로만 중앙화되어 있다.        프로그래밍 가능 네트워크(A programmable network)    네트워크는 상기한 네트워크 제어 응용 프로그램에 의해 프로그램이 될 수 있다.    이러한 응용 프로그램은 SDN 컨트롤 측면에서 뇌로 작용하며 SDN 컨트롤러의 API를 이용해 네트워크 장비를 모니터링하고 조종한다.    예를 들어 SDN 컨트롤러가 준 노드 상태와 연결 상태 정보를 이용해 다익스트라 알고리즘을 돌린다.    이외에도 패킷의 차단이나 서버의 부하 밸런싱 등을 수행한다.  SDN이 여러 네트워크의 기능을 나누는 특성은 오픈소스 시스템 확립과 많은 회사들의 참여를 끌어 모았고 혁신을 가능케 하였다.5.5.1 SDN 컨트롤 측면 : SDN 컨트롤러와 SDN 네트워크 컨트롤 응용 프로그램(The SDN Control Plane: SDN Controller and SDN Network-control Applications)컨트롤 측면이 제공해야할 기능들을 알아볾으로써 SDN 컨트롤 측면에 대해 알아보자.SDN 컨트롤 측면은 크게 SDN 컨트롤러와 SDN 네트워크 제어 응용 프로그램 부분으로 나뉜다.SDN 컨트롤러는 SDN 초기부터 고안되었으며 fig.5.15는 SDN을 크게 3가지로 나누어 설명하는 그림이다.      통신 계층 : SDN 컨트롤러와 제어되는 네트워크 장비 간의 통신(A communication layer: communicatin between the SDN controller and controlled network devices)    SDN 컨트롤러는 SDN 제어 스위치, 호스트 같은 네트워크 장비들의 운용을 원격으로 조종하므로, 컨트롤러와 장비 간의 정보를 교활할 프로토콜이 필요하다.    추가로 네트워크 장비가 탐지하게 되는 링크의 상태나 새로운 장치의 추가 같은 지역적 이벤트 또한 컨트롤러 측에 보고되어 최신 네트워크 상태를 유지해야되며, 이러한 프로토콜은 컨트롤러 구조 상 가장 아래쪽에 위치하기 때문에 장비와 컨트롤러의 통신을 남방 경계 인터페이스(Southbound interface, Southbound API)라고도 부른다.    여러 프로토콜 중 대부분의 SDN 네트워크 장치에서 사용하는 오픈플로우에 대해서 배워볼 것이다.        네트워크 전역 상태 관리 계층(A network-wide state-management layer)    SDN 컨트롤 측면에서 하는 플로우 테이블 갱신이나 부하 밸런싱, 방화벽 같은 중요한 일들은 네트워크 장비들이 측정하는 최신 네트워크 정보가 필요하다.    각 스위치의 플로우 테이블에는 네트워크 제어 응용 프로그램들이 유용하게 쓸 수 있는 카운터 값들이 갱신되고 있으며, 이를 그들에게 전달해 줘야한다.    플로우 테이블은 각 스위치에 전달될 뿐만 아니라 컨트롤 측면에서도 저장되어야 하며, 앞에서 설명한 이러한 정보들이 모두 SDN 컨트롤러가 유지해야할 네트워크 전역 정보 중 일부이다.        네트워크 제어 응용 프로그램 계층의 인터페이스(The interface to the network-control application layer)    컨트롤러와 장비가 통신 계층의 남쪽 경계 인터페이스를 통해 정보를 주고 받음으로써, 상위의 네트워크 전역 상태 관리 계층이 네트워크 제어 응용 프로그램들이 네트워크의 상태와 플로우 테이블을를 읽고 쓰게 해준다.    응용 프로그램들은 상태 변경 이벤트 알림에 반응하여 대응할 수 있어야 한다. 그러기 위해서는 컨트롤러 장비 간 뿐만 아니라 컨트롤러 응용 프로그램 간의 인터페이스가 필요하고 네트워크 응용 프로그램 계층에 존재하며 북방 경계 인터페이스 (Northbound interface, Northbound API)한다.  SDN 컨트롤러가 논리적으로 중앙화됬다는 의미는 네트워크 장치나 응용 프로그램 시점에서 SDN 컨트롤러는 하나의 일체화된 서비스로 보이지만 실제로 컨트롤러의 서비스와 데이터들은 서버 장애 방지, 고가용성, 성능상의 이유로 분산된 서버에 구현된다.이러한 분산 처리 시스템의 경우, 데이터의 일관성, 동시성, 이벤트의 순서 등을 고려해야하며, OpenDaylight나 ONOS 같은 최신형 컨트롤러들은 이러한 분산처리  컨트롤러의 고가용성과 확장 가능성을 염두에 두고 설계되었다.위의 그림 fig.5.15는 NOX 컨트롤러에서 제안된 구조로 최신의 OpenDaylight와 ONOS 컨트롤러도 차용하고 있다.5.5.2.오픈플로우 프로토콜(OpenFlow Protocol)오픈플로우 프로토콜은 SDN 컨트롤러와 오픈플로우 API가 구현되어 있는 SDN 제어 스위치, 기타 장비 등에서 사용하는 프로토콜로, TCP 프로토콜 위에 6653번 포트를 사용한다.컨트롤러 측에서 제어되고 있는 스위치 측으로 보내는 중요한 메시지들은 다음과 같다.      설정(Configuration)    이 메시지는 컨트롤러가 스위치의 설정 파라미터 값을 설정하고 요청할 수 있다.        수정 상태(Modify-State)    이 메시지는 컨트롤러가 스위치의 플로우 테이블의 항목을 추가, 삭제, 변경하거나 스위치 포트 속성을 설정한다.        읽기 상태(Read-State)    이 메시지는 컨트롤러가 스위치의 플로우 테이블이나 포트로 부터 통계값이나 카운터 값을 가져온다.        패킷 전송(Send-Packet)    이 메시지는 컨트롤러가 보내고 싶은 패킷을 보내고 싶은 스위치의 포트로 보내도록 한다.    메시지의 페이로드로 보내고 싶은 패킷이 포함되어 있다.  스위치 측에서 컨트롤러 측으로 보내는 중요한 메시지들은 다음과 같다.      흐름 제거(Flow-Removed)    컨트롤러에게 플로우 테이블 항목 하나가 지워졌음을 알리는 메시지, 주로 시간 초과나 수정 상태(Modify-State) 메시지를 받은 뒤에 돌려준다.        포트 상태(Port-status)    스위치가 컨트롤러 측에게 포트의 상태가 변경됬음을 알리는 메시지        패킷 입력(Packet-in)    스위치의 포트를 통해 도착한 패킷이 플로우 테이블의 어떤 항목에도 맞지 않는다면 컨트롤러 보내 추가적인 처리를 받는다.    설정에 따라 일치된 패킷도 컨트롤러로 보내져 추가적인 처리를 받을 수 있다. 패킷 입력 메시지를 통해 해당 패킷을 페이로드에 넣어 보낼 수 있다.  추가적인 오픈플로우 메시지는 [OpenFlow 2009, ONF 2020]에서 볼 수 있다.5.5.3.데이터와 컨트롤 측면 상호작용: 예제(Data and Conrol Plane Interaction: An Example)아래 그림 fig.5.16은 SDN의 다익스트라 연결 상태 라우팅 시나리오이다.이는 기존의 다익스트라 알고리즘이 모든 라우터에서 실행되고, 연결 상태가 갱신되어 모든 라우터 간에 공유하던 라우터별 조정 시나리오와 두가지가 다르다.  SDN에서는 다익스트라 알고리즘이 패킷 스위치가 아닌 별도의 응용 프로그램에서 실행된다.  패킷 스위치는 연결 상태 갱신 정보를 다른 라우터 간에 공유하지 않고 SDN 컨트롤러에게만 보낸다.s1과 s2 스위치 사이의 연결이 먹통이 되었고, 이로 인해 s1과 s2, s4에는 포워딩 규칙에 영향이 갔고, s3에는 가지 않았으며, 통신계층 프로토콜로 오픈플로우가 사용되었으며 연결 상태 라우팅을 사용했다고 가정하자.  스위치 s1이 s2와의 연결이 실패하자, SDN 컨트롤러에게 연결 상태 변경을 오픈 플로우 포트 상태(port-status) 메시지로 통보한다.  SDN 컨트롤러가 연결 상태 변경에 관한 메시지를 받고 연결 상태 데이터베이스를 갱신한다.  다익스트라 연결 상태 라우팅이 구현되어 있는 네트워크 제어 어플리케이션이 사전에 연결 상태가 바뀌면 알림을 받도록 되어 있었고, 연결 상태가 바뀌자 연결 상태 변경의 통지를 받는다.  연결 상태 라우팅 응용 프로그램이 연결 상태 매니저와 상호작용하여 최신 연결 상태를 가져오고, 추가로 상태 관리(state-management) 계층의 다른 요소들과 상담한뒤, 새로운 최소 비용 경로를 계산한다.  연결 상태 라우팅 응용 프로그램이 다음에 플로우 테이블 매니저와 상호작용하여 새로운 최소 비용 경로에 기반해 플로우 테이블을 갱신하도록 한다.  플로우 테이블 매니저가 오픈 플로우 프로토콜을 통해 영향을 받은 스위치들의 플로우테이블을 바꾼다. - s1(s4를 경유해 s2에 패킷 전송), s2(s4를 통해 s1의 패킷 받음), s4(s1의 패킷을 s2에게 전달해줘야 함)SDN 컨트롤 측면에서 어떻게 네트워크 계층 라우팅을 라우터별 제어 방법을 대신하여 구현할 수 있는지 알아보았다.SDN 컨트롤의 장점 중 하나로, 어떻게 최소 비용 경로 뿐만 아니라 다른 기준을 쉽게 적용하여 경로를 구할 수 있었다.기존의 방법으로는 그러한 기준을 바꾸기 위해 라우터의 소프트웨어를 일일이 바꿔줘야 했지만, SDN에서는 중앙에서 해당 비용 경로를 구하는 소프트웨어나 소프트웨어의 설정값을 바꿔주면 손쉽게 변경이 가능하다.5.5.4 SDN: 과거와 미래(SDN: Past and Future)SDN은 최근의 개념이지만, SDN의 데이터 측면과 컨트롤 측면을 나누는 아이디어는 [Feamster 2004, Lakshman 2004, RFC 3746] 등 꽤나 예전부터 등장했으며, ATM 네트워크에서의 예시[van der Merwe 1998, Black 1995]등이 있었다.에단 프로젝트[Casado 2007]은 중앙 라우팅 관리와 포워딩, match-plus-action 플로우 테이블을 이용한 흐름기반 이더넷 스위치의 개념을 만들었으며 이 실험은 오픈플로우에 큰 영향을 주었다.상당히 많은 연구 노력이 SDN 구조와 가능성 향상을 목표로 하였고, SDN 혁신으로 컨트롤 측면과 데이터 측면, 전부 일체형 서비스로 제공하는 네트워크 스위치들을 간단한 상용 하드웨어와 복잡한 소프트웨어로 바꿔나가고 있다.SDN의 일반화는 이전에 배웠던 네트워크 기능 가상화(NFV, network functions virtualization)을 통해 복잡한 미들박스들을 간단한 상용 서버와 스위치, 저장소로 대체하려 하는 시도에 영향을 주었고, 최근에는 SDN의 개념을 AS 내부에서 뿐만 아니라 AS 간에서도 사용해보려고 연구 중이다[Gupta 2014]SDN 컨트롤러 예시 (SDN CONTROLLER CASE STUDIES: THE OPENDAYLIGHT AND ONOS CONTROLLERS)과거에는 오직 하나의 SDN 프로토콜 (오픈플로우)와 하나의 SDN 컨트롤러(NOX)만 존재하였지만 최근에는 기업의 맞춤형 컨트롤러 부터 오픈소스까지 다양화되었다.리눅스 그룹과 함께 오픈소스로 제작된 사업가능 컨트롤러로 OpenDaylight와 ONOS에 대해 알아보자.OpenDaylight 컨트롤러 (THE OpenDaylight Controller)아래 fig.5.17은 OpenDaylight(ODL) 컨트롤러 기반의 간단화된 모습이다.ODL의 기본 네트워크 기능(Basic Network Functions)은 컨트롤러의 심장부 중간층에 존재하며 네트워크 전역 상태 관리 능력(network-wide state management capability)에 관계있다.서비스 추상화 계층(SAL, Service Abstraction Layer)는 컨트롤의 중추 심경으로, 남방 경계 쪽에 가깝게 위치해있으며, 컨트롤러 요소와 응용 프로그램이 서로의 서비스를 이용하고 설정과 운영 데이터를 공유하고 서로가 생성하는 이벤트를 구독할 수 있다.또한 SAL에서는 ODL 컨트롤러와 네트워크 장비들 사이를 연결하는 프로토콜과 통신할 수 있는 정형화된 추상화 인터페이스를 제공한다.ODL 컨트롤러와 네트워크 장비들 사이를 연결하는 프로토콜의 예시로, 오픈 플로우, NETCONF(Network Configuration), SNMP(Simple Network Management Protocol), OVSDB(vSwitch Database Management Protocol, 데이터센터 스위치에 사용) 등이 존재한다.네트워크 통합 및 응용 프로그램(Network Orchestrations and Applications)은 데이터 측면 포워딩과 방화벽, 부하 밸런싱 같은 서비스들이 어떻게 네트워크 장치에 동작할 지 결정한다.응용 프로그램이 컨트롤러 서비스, 컨트롤러 서비스를 통한 네트워크 장치 등과 서로 운영할 수 있게 만드는 두가지 방법이 있는데,첫번째는 API 기반 방법(AD-SAL)으로, 응용 프로그램이 HTTP 기반의 REST API를 이용해 컨트롤러 모듈과 통신하는 방법이고, ODL 컨트롤러 초기에 사용된 방법이다.두번째는 ODL이 네트워크 설정 및 관리에 널리 사용되면서 바뀐 방법으로 모델 기반 방식(MDL-SAL)이다.YANG 데이터 모델링 언어[RFC 6020]이 장치, 프로토콜, 네트워크 설정과 운영 상태 데이터 등의 모델을 정의하고, NETCONF 프로토콜을 통해 이런 데이터들에 조작하여 네트워크 장치들이 설정, 관리되어진다.ONOS 컨트롤러 (The ONOS Controller)위의 fig.5.18은 ONOS 컨트롤러의 간소화된 그림이다.표준 컨트롤러 구조와 비슷하게 3가지 층으로 이루어져있다.      북방 경계 추상화와 프로토콜(Northbound abstractions and protocols)    ONOS의 프레임워크는 응용 프로그램에게 TCP 연결 설정 또는 금지 같은 고차원 서비스를 작동의 이해없이 요구할 수 있게 해준다.    상태 정보가 북방 경계 API를 통해 네트워크 제어 응용 프로그램에게 동기(쿼리), 또는 비동기적(콜백, 이벤트)으로 제공된다.        분산 코어(Distributed core)    네트워크의 상태는 ONOS의 분산 코어에 의해 유지관리되는데, ONOS는 서로 연결된 여러 서버에 ONOS 소프트웨어가 여럿 복사되어 배포되어 서비스를 제공한다.    서비스 인스턴스 간의 복제와 조화를 통해 성능과 서버 장애 문제 없이 위로는 응용 프로그램, 아래로는 네트워크 장치에게 서비스 제공이 가능하다        남방 경계 추상화와 프로토콜(Southbound abstractions and protocols)    ONOS의 남방 경계 추상화는 호스트, 네트워크 장치, 프로토콜들의 세세한 원리를 가려 사용자가 구애받지 않게 해주며, 이 때문에 ONOS의 남방 경계 인터페이스는 논리적으로 다른 컨트롤러 구조에 비해 좀더 많은 역할을 수행한다.  5.6 ICMP: 인터넷 컨트롤 메세지 프로토콜 (ICMP: The Internet Control Message Protocol)인터넷 컨트롤 메세지 프로토콜 (ICMP: The Internet Control Message Protocol)[RFC 792]는 호스트와 라우터가 사용하여 네트워크 계층 정보를 서로 공유하게 해준다.주로 에러 보고에 사용되며, 예를 들면 HTTP로 요청한 아이피 주소가 존재하지 않다면 에러 메시지로 “Destination network unreachable”이 오는데, 이는 라우터 측에서 해당 아이피로 접근하지 못하니 ICMP 메시지로 에러 메시지를 생성해서 호스트로 되돌려보낸 것이다.ICMP는 구조적으로 아이피보다 상위 계층인 전달 계층 프로토콜로, TCP와 UDP 세그먼트처럼 아이피 데이터그램의 페이로드에 담겨진다. 아이피 데이터그램 upper-layer 헤더 필드 번호는 1이다.ICMP 메시지에는 code 필드와 type 필드가 존재하며, 메시지에 오류가 일어난 아이피 데이터그램의 헤더와 첫 8바이트를 포함하고 있어, 수신자에게 어떤 데이터그램이 문제였는지 알 수 있게 한다.아래 fig.5.19는 ICMP 메시지의 종류이며, ICMP가 에러 메시지로만 쓰이지 않음을 알 수 있다.ICMP type 8 code 0를 호스트에게 보내 핑을 구현할 수 있고, 호스트는 핑 메시지를 받고 ICMP type 0 code 0로 응답한다. 대부분은 이러한 ICMP 메시지 생성은 운영체제에서 구현되어 있으며, 클라이언트가 요청하여 만들어준다. 즉, ICMP를 생성하는 것은 프로세서, 응용 프로그램이 아니다. [Stevens 1990]에 핑 클라이언트 프로그램의 코드가 나와있다.ICMP type 4 code 0는 근원 해소(source quench) 메시지로 TCP 혼잡 제어를 위한 필드인 Explicit Congestion Notification bit 헤더에 밀려 자주 사용되지 않지만 라우터가 해당 메시지를 호스트에게 보내 전송량을 줄이게 만드는 메시지가 원래 의도였다.Traceroute 프로그램은 ICMP의 type 11 code 0의 TTL expired 메시지를 이용한다. 프로그램은 TTL이 1이고 존재하지 않을법한 UDP 포트 번호가 적힌 UDP 세그먼트를 만들어 목적지를 향해 보낸다. 이때 라우터에서 TTL이 다하면 라우터는 해당 패킷을 버리고 ICMP type 11 code 0 메시지를 돌려보낸다.  정확히는, 더 많은 정보를 위해 같은 UDP 세그먼트를 3개씩 보내어 3개의 결과 값을 보게된다.송신자는 이 ICMP 메시지 내부의 라우터명과 아이피 주소, 라우터의 순서 번호와 측정한 RTT를 보여주고, TTL을 1 증가시킨 UDP 세그먼트를 다시 보내는 과정을 목적지에 도착할 때까지 반복한다.TTL이 충분히 커져, 목적지에 도착하게 되면 목적지 호스트는 해당 UDP 세그먼트의 포트 번호를 확인하게 되고 존재하지 않는 포트임을 알아 차리고 ICMP type 3 code 3,”port unreachable” 메시지를 돌려주게 되고 , 해당 메시지를 돌려받은 송신자는 목적지에 도착했음 알 수 있다.traceroute 프로그램을 구현하려면 운영체제에게 ICMP 메시지를 생성시키고, 보내며, 운영체제로 부터 응답 받은 메시지를 받을 수 있어야 한다.최신버전 ICMP인 ICMPv6는 [RFC 4443]에서 확인할 수 있으며, 기존의 type과 code뿐만 아니라 “Packet Too Big”이나 “unrecognized IPv6 options” 같은 IPv6를 위한 새로운 에러용 type과 code가 추가 되었다.5.7 네트워크 관리와 SNMP, NETCONF/YANG (Network Management and SNMP, NETCONF/YANG)네트워크를 관리하는 것은 힘들고 복잡한 일이며, 이를 돕기위한 도구와 기술들이 많이 나와 있으며, 공부해볼 것이다.네트워크 관리의 정의는 [Saydam 1996]에 다음과 같이 나와있다.네트워크 관리는 네트워크와 기타 요소를 실시간으로, 운영가능한 성능 하, QOS(Quality of Service) 요구사항에 맞추어, 합리적인 비용으로 모니터링, 테스트, 통계, 설정, 분석, 평가, 조절하기 위한 하드웨어, 소프트웨어, 인적 요소를 배포, 집약, 통합하는 것을 포함한다.네트워크 관리자의 의사결정 과정, 결함 발견, 이상현상 감지, 서비스 레벨 동의(Service Level Agreements, SLA)에 맞는네트워크 디자인 및 엔지니어링 등의 다른 부분은 배우지 않을 것이다.자세한 네트워크 관리자에 대한 사항은 [Subramanian 2000; Schonwalder 2010; Claise 2019]와 웹사이트를 참조 바란다.5.7.1. 네트워크 관리 프레임워크(The Network Management Framework)아래 fig.5.20은 네트워크 관리 프레임워크의 주요 요소를 보여주고 있다.      관리 서버(Managing server)    관리 서버(managing server)는 네트워크 관리자(network manager)가 네트워크 운영 센터(network operations center, NOC)의 중앙 네트워크 관리 스테이션(cneralized network management station)에서 반복적으로 작동시키고 있는 응용 프로그램이다.    관리 서버는 네트워크 관리 활동의 중심으로, 네트워크 관리 정보와 명령을 수집, 처리, 분석, 발송한다.    네트워크 장비들을 설정, 모니터링, 조종하는 활동이 시작되는 곳이기도 하다.    한 네트워크가 여러 개의 관리서버를 가질 수 있다.        관리되는 장치(Managed device)    관리되는 장치는 관리되고 있는 네트워크(managed network)의 소프트웨어를 포함해서 네트워크 장비의 일부이다.    호스트, 라우터, 스위치, 미들박스, 모뎀, 온도계 등 네트워크가 연결되어있는 장비를 의미한다.    장비는 또 추가적으로 관리할 수 있는 요소(호스트나 라우터가 가지고 있는 네트워크 인터페이스 등,)나 그러한 하드웨어와 소프트웨어(OSPF 같은 프로토콜)의 설정 변수를 가질 수 있다.        데이터(Data)    각 관리되는 장치는 장치와 관련된 상태(state)라는 데이터를 가지고 있으며, 데이터에는 여러 종류가 있다.    설정 데이터(Configuration data)는 네트워크 관리자가 장치에 설정하는 정보로, 아이피 어드레스, 인터페이스 제한 속도 등이 있다.    운영 데이터(Operational data)는 네트워크를 운영하면서 장치들이 획득하는 정보로, OSPF 등에서의 주변 이웃 라우터 정보 등이 있다.    장치 통계(Device statistics)는 장치가 기록하는 상태 지시자와 횟수 정보로, 차단한 패킷의 수, 장치 쿨링 팬 속도 등이 있다.    장치 관리자는 원격으로 데이터를 읽거나 데이터를 써서 장치를 통제할 수 있으며, 관리 서버에서는 이러한 정보들을 장치와 네트워크 전역에서 가져와 복사본을 저장해 둔다.        네트워크 관리 에이전트(Network management agent)    네트워크 관리 에이전트는 관리 서버와 통신하는 관리되는 장치에 돌아가고 있는 소프트웨어로, 관리 서버의 명령에 따라 지역적인 활동을 취한다.        네트워크 관리 프로토콜(Network management protocol)    관리 서버와 관리되는 장치 사이에서 작동하며, 관리서버에게 관리 장치의 상태를 읽을 수 있게 하거나 에이전트를 통해 활동하게 한다.    에이전트는 네트워크 관리 프로토콜을 통해 요소 장애나 성능 미달 등의 소식을 관리서버에게 알려줄 수 있다.    중간 매개체 역할을 하지 직접 네트워크를 관리하지 않는다.  현업에서는 네트워크 운영자가 네트워크를 관리할 수 있게 해주는 세개의 요소가 있다.      명령어 인터페이스 (Command Line Interface, CLI)    네트워크 관리자는 직접 장치에 포함된 CLI를 이용하거나 Telnet, SSH 프로토콜로 원격에서 명령어나 스크립트를 이용해 장치의 콘솔에 접근할 수 있다.    CLI 명령어는 회사나 장치마다 다를 수 있고, 자동화 및 규모 증강이 힘들며, 오류가 일어날 수 있으므로 소규모 네트워크나 개인 장치 등에만 활용된다.    최근 홈 네트워크 장비의 경우 HTTP를 통해 장치를 제어할 수 있는 환경을 제공한다.        SNMP/MIB(Simple Network Management Protocol/Management Information Base)    네트워크 운영자는 관리 정보 베이스(MIB,Management Information Base) 객체가 가지고 있는 데이터를 간단 네트워크 관리 프로토콜(SNMP, Simple Network Management Protocol) 통해 쿼리, 설정 할 수 있다.    일부 MIB는 장비나 회사에 따라 다르지만 많은 MIB(라우터가 차단한 패킷의 수, 호스트가 받은 UDP 패킷 수 등)들이 공통적으로 구현되어있다.    주로 MIB로 정보를 쿼리하거나 장치 상태를 모니터링 하는데 사용되고, CLI를 통해 장치 정보를 설정하거나 제어한다.    두 방법 전부 장치를 하나 하나 접근하는 방법이므로, 규모가 큰 방법에 좋은 방법이 아니다.        NETCONF/YANG    정확성 제한 같은 설정 관리 측이 강조되어 세세하면서도 여러 장치를 동시에 관리할 수 있게 해주어 좀더 추상화되고 네트워크 전역의 전체론적인 네트워크를 가능하게 해준다.    YANG[RFC 6020]은 설정과 운용 데이터의 모델을 설정하는 데 사용되는 데이터 모델링 언어이며, NETCONF 프로토콜[RFC 6241]은 원격장치 간/에/로 부터 YANG에 호환되는 활동과 데이터를 주고 받는데 사용된다.  5.7.2.간단 네트워크 관리 프로토콜(SNMP, Simple Network Management Protocol)와 관리 정보 베이스(MIB) (The Simple Network Management Protocol(SNMP) and the Management Information Base (MIB, Management Information Base))간단 네트워크 관리 프로토콜 버전 3(SNMPv3, Simple Network Management Protocol v3)[RFC 3410]는 관리 서버와 관리 서버의 지시대로 행동하는 에이전트 사이에 네트워크 관리 정보 메시지와 제어 메시지를 전하는 응용 계층 프로토콜이다.SNMP의 가장 흔한 사용법은 SNMP와 에이전트 간의 요청-응답(request-response) 모드로, 요청을 받으면, 요청 대로 행동한 뒤, 응답 메시지를 돌려 보낸다.보통은 관리되는 장치 관련 MIB 객체를 쿼리하거나 수정하는 요구가 많다.두번째 사용법은 에이전트가 링크 인터페이스의 장애 등의 예외적인 상황에서 관리 서버에게 요청하지 않았어도 보고를 위한 메시지를 보낸다. 이를 Trap 메시지라고 한다.MIB 객체는 SMI(Structure of Management Information, 관리 정보 구조)라는 의미불명의 이름을 가진 데이터 묘사 언어(data description language)로 표현된다.형식 정의 언어는 주로 네트워크 관리 데이터의 의미와 구문이 애매모호하지 않게 잘정의되었는 지 확인하는데 사용됩니다.MIB 모듈은 연관된 MIB들의 모임으로, 현재 MIB는 400개 이상의 연관된 RFC 문서를 가지고 회사마다 다른 MIB 모듈을 가지고 있다.SNMPv3는 PDU(protocol data units라고 불리우는 7종류의 메시지를 정의하였고, 아래 Table.5.2와 Fig.5.21에서 형식과 예시를 볼 수 있다.      GetRequest, GetNextRequest, GetBulkRequest PDU    관리서버가 에이전트에게 관리되는 장치로 부터 하나 이상의 MIB 오브젝트 인스턴스를 요청한다. 요구된 MIB 오브젝트는 PDU의 변수 공간의 일부에 자신의 값을 채워넣고 돌려준다.    GetRequest : 임의의 MIB 값들의 집합을 요구    GetNextRequest: 여러 GetNextRequest를 보내서 MIB 객체 표나 리스트로 만들어 받음    GetBulkRequest: 한꺼번에 대량의 데이터를 받을 수 있음        SetRequest PDU    관리 서버가 하나 이상의 장치의 MIB 객체의 값을 설정하도록 에이전트에게 요청, 에이전트는 “noError” 에러 상태를 가진 Response PDU를 응답 메시지로 보내 확인시켜준다.    SNMPv1 시절에는 보안상의 이유로 거의 안쓰였다.        InformRequest PDU    관리 서버가 다른 관리서버에게 해당 관리서버가 닿을 수 없는 MIB 정보를 보내주는데 사용        Response PDU    관리되는 장치가 관리 서버에게 응답 메시지로 보내 요청 받은 정보를 돌려주는데 사용        Trap 메시지    관리 서버에게 필요한 알림 이벤트가 발생할 시 관리서버에게 알리기 위해 비동기적으로 생성되는 메시지, RFC 3418에 여러 기본 trap type이 정의되어있다. 예를 들어, 링크의 작동 여부, 인증 실패, 이웃 장치의 손실 등이다.    Trap 메시지를 받은 관리서버는 이에 응답 메시지를 보내지 않아도 된다.  SNMP PDU는 다양한 전달 프로토콜을 통해 전달될 수 있지만, UDP 다이어그램을 권장하며, 가장 많이 사용된다.하지만 UDP는 reliable data transfer를 지원하지 않으므로, PDU에 Request Id 헤더 필드를 이용한다.관리 서버는 이 PDU에 해당 필드에 요청 번호를 부여하여, 에이전트가 응답 메시지에 기입하도록 한다.이를 통해 응답 메시지가 오지 않은 PDU를 알 수 있고, SNMP는 재전송이 의무가 아니므로 일정 시간이 기다린 뒤, 재전송하거나 다른 행동을 취한다.SNMP는 SNMPv1부터 SNMPv3까지 있으며 최신 버전은 보안과 관리 능력이 좋아졌다.관리 정보 베이스(MIB) (The Management Information Base (MIB))우리는 SNMP/MIB에서 관리되는 장치들의 운영 상태, 설정 데이터 등이 MIB의 형태로 장치에 저장된다고 배웠다.MIB 오브젝트는 라우터에서 폐기된 IP 데이터그램의 숫자, DNS 서버의 버전 묘사 정보 등이 될 수 있다.관련된 MIB 객체들은 MIB 모듈로 모이게 되고, 용도별, 장비별, 회사별 등으로 400여개의 MIB 모듈이 IETC RFC에 정의되어있다.아래는 IP와 ICMP를 구현을 관리하기 위한 MIB 객체의 설명 예시이다.ipSystemStatsinDelivers 객체 타입    구문 Counter32    최대 접근 권한 읽기만 가능    상태 사용 가능    묘사        &amp;#34;ICMP와 IP 사용자 프로토콜에 성공적으로 전달된 전체 데이터그램 숫자.        인터페이스 통계에서는 앞서 말한 데이터그램들이 전달된 인터페이스의 카운터는 증가한다. 이 인터페이스는 일부 데이터그램의 입력 인터페이스와 다를         수 있다.         카운터의 기록의 불연속성은, 관리 시스템의 재시작 또는 ipSystemStatsDiscontinuityTime의 값이 지시한 시간대로 인해 일어날 수 있다.         &amp;#34;:: = &amp;#123; ipSystemStatsEntry 18 &amp;#125;5.7.3.네트워크 설정 프로토콜(NETCONF)과 YANG(The Network Configuration Protocol (NETCONF) and YANG)NETCONF 프로토콜은 관리 서버와 관리 네트워크 장비 사이에서 사용되는 프로토콜로,  관리 장치의 설정 데이터를 설정, 수정, 쿼리해주는 메시지          관리 서버가 장치의 설정하는 XML 문서 메시지를 통해 능동적으로 관리 장치 조종 가능        관리 장치의 운영 데이터와 통계를 쿼리해주는 메시지  관리 장치가 생성하는 알림을 구독하는 메시지를 제공한다.NETCONF은 원격 프로시저 호출(remote procedure, RPC) 패러다임을 사용하는 데, 이는 프로토콜 메시지가 XML로 인코딩 되고, 관리 서버와 관리되는 장치 간에 TLS (Transport Layer Security) 프로토콜 같은 보안이 보장된 연결 중심 세션을 이용하는 것을 의미한다.위의 Figure 5.22는 NETCONF의 예시이다.  관리 서버가 클라이언트 역할,  관리되는 장치가 서버 역할로 보안 연결을 수립한다.  연결이 수립되면 서로  메시지를 주고 받고 자신의 역량(capabilities, NETCONF 기능 중 하나[RFC 6241])을 선언한다.  RPC의 형태로 서버와 장치 간에 와 를 사용해 상호작용한다.          이러한 메시지들은 설정, 운영, 통계, 데이터 등을 읽고 쓰고, 수정하게 해주며, 장치의 알림을 구독하게 해준다        장치는 서버 측에 해당함에도,  메시지를 통해 일어난 이벤트를 서버측에 전달할 수 있다.  세션은  message를 보냄으로 종료된다.       : 장치의 해당 설정을 전부 또는 일부를 가져옴, 장치는 여러 설정을 가지고 있으며 장치가 현재 실행되고 있는가를 의미하는 running/ 설정은 언제나 가지고 있다.         : 설정 상태와 운용 상태 데이터를 전부 또는 일부 가져옴         : 관리되는 장치로부터 주어진 설정 값으로 설정을 전부 또는 일부 바꿈, 예를 들어 running/ 값을 off로 바꾸면 장치가 꺼질 것이다. 받아들일 수 있는 명령이라면 장치는  요소가 포함되어 있는  메시지를 돌려줄 것이며, 아니면  메시지가 응답으로 간다. 에러가 난 상황에서는 명령을 실행하기 이전의 상태로 롤백된다.        ,  : 관리 서버가 관리되는 장치들의 전체 설정 데이터 저장소 시스템을 잠그거나 해제할 수 있다. lock은 NETCONF, SNMP, CLI 등으로 상호작용 하는 동안, 다른 곳의 변경으로 인한 충돌을 방지하기 위해 짧게 유지한다.        ,  : 대상 장치가 특수한 상황이 발생할 때 서버측에 변화 정보를 보내주는 비동기 이벤트 을 보내주는 이벤트 알림 구독(event notification subscription)을 설정한다. 알림 구독은 해제할 수 있다.    Table 5.3은 NETCONF 동작 중 중요한 몇몇 예시이다. 추가적인 NETCONF는 [RFC 6241, RFC 5277, Claise 2019; Schonwalder 2010] 참조. 메시지 처럼 SNMP에서도 볼 수 있었던 설정 상태와 운영 상태 데이터를 읽는 메시지도 있지만, , , , 은 **NETCONF의 디바이스 설정에 대한 특화**를 볼 수 있다.또한, 마치 **관계형 데이터베이스의 트랜잭션처럼, 복잡한 네트워크 관리 트랜잭션을 생성해서, 여러 디바이스 그룹에 다수의 명령을 순서에 맞게 적용하고, 원자성(=제시된 명령어를 모두 지키거나 문제가 생길시 중간에 멈추지 않고 하나도 안지켜져야 함)을 지키며, 원할 시에 적용 이전 상태로 되돌릴 수 있다.**이러한 **다중 장치 트랜잭션을 통한 디바이스가 아닌 네트워크 중심적인 행동은 NETCONF의 핵심 철학**이다.```xml&#38;#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#38;#62;&#38;#60;rpc message-id=&#34;101&#34; xmlns=&#34;urn:ietf:params:xml:ns:netconf:base:1.0&#34;&#38;#62;    &#38;#60;get/&#38;#62;&#38;#60;/rpc&#38;#62;```첫번째 예시는 관리 서버가 관리되는 장치에게 NETCONF  명령어를 보내 모든 장치 설정과 운영 데이터를 요구한다. 이를 통해 장치의 설정값을 알 수 있다.이러한 XML 형식은 SNMP PDU와 비교하여 마치 HTTP나 HTML처럼 사람의 이해할 수 있게 되어있다.RPC 메시지의 ID는 101, 하나의 NETCONF  명령어가 포함되어 있고, 아래와 같은 응답 메시지와 설정 데이터가 포함되어 장치로 부터 돌아온다. ```xml&#38;#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#38;#62;&#38;#60;rpc-reply message-id=&#34;101&#34; xmlns=&#34;urn:ietf:params:xml:ns:netconf:base:1.0&#34;&#38;#62;    &#38;#60;!-- . . . all configuration data returned... --&#38;#62;    . . .&#38;#60;/rpc-reply&#38;#62;```아래의 두번째 예시는 장치의 Ethrnet0/0라는 인터페이스의 MTU(Maximum Transmission Unit) 설정값을 1500으로 바꾸는 설정 바꿈 메시지이다.```xml&#38;#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#38;#62; &#38;#60;rpc message-id=&#34;101&#34; xmlns=&#34;urn:ietf:params:xml:ns:netconf:base:1.0&#34;&#38;#62;    &#38;#60;edit-config&#38;#62;         &#38;#60;target&#38;#62;             &#38;#60;running/&#38;#62;        &#38;#60;/target&#38;#62;         &#38;#60;config&#38;#62;            &#38;#60;top xmlns=&#34;http://example.com/schema/1.2/config&#34;&#38;#62;                &#38;#60;interface&#38;#62;                    &#38;#60;name&#38;#62;Ethernet0/0&#38;#60;/name&#38;#62;                    &#38;#60;mtu&#38;#62;1500&#38;#60;/mtu&#38;#62;                &#38;#60;/interface&#38;#62;             &#38;#60;/top&#38;#62;        &#38;#60;/config&#38;#62;    &#38;#60;/edit-config&#38;#62;&#38;#60;/rpc&#38;#62;```NETCONF의  명렁어가 하나 포함되어 있으며, 설정이 정상적으로 바뀌었으면 아래와 같은 OK 응답 메시지가 XML 형태로 돌아온다.```xml&#38;#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#38;#62;&#38;#60;rpc-reply message-id=&#34;101&#34; xmlns=&#34;urn:ietf:params:xml:ns:netconf:base:1.0&#34;&#38;#62;    &#38;#60;ok/&#38;#62;&#38;#60;/rpc-reply&#38;#62;```### 양 (YANG)**YANG은 NETCONF에서 사용하는 네트워크 관리 데이터의 구조와 구문, 의미를 정확히 묘사할 수 있는 데이터 모델링 언어**이다.마치 SNMP가 MIB를 묘사하기 위해 SMI를 쓰는 것처럼, **모든 YANG 정의는 모듈에 담겨져 있고, 장치와 장치의 역량(capabilities)을 묘사하는 XML 문서를 YANG 모듈을 통해 생성**한다.YANG은 SMI처럼 **내장된 데이터 타입을 지원하여 데이터 모델러가 NETCONF 설정 값에 들어갈 수 있는 값으로 제한**할 수 있으며, 이를 통해 설정 정보의 정합성과 일관성을 유지할 수 있다. YANG은 또한 NETCONF 알림을 지정하는데도 사용한다.더 자세한 정보는 [Claise 2019]에서 확인하자."
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%206-%EC%97%B0%EA%B2%B0%20%EA%B3%84%EC%B8%B5%EA%B3%BC%20LAN.html": {
    title: "네트워크 정리-Chap 6-연결 계층과 LAN",
    date: " Aug 22, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%206-%EC%97%B0%EA%B2%B0%20%EA%B3%84%EC%B8%B5%EA%B3%BC%20LAN.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 6. 연결 계층과 LAN(Link Layer and LAN)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/e이전 챕터에서는 두 호스트 간의 연결에 대해서 배웠다. 이번에는 각 링크에서의 통신에 대해서 배워보자.연결 계층에는 근본적으로 다른 두가지 연결 계층 채널이 있다.      브로드캐스트 채널(broadcast channel)    여러 호스트를 무선 LAN, 인공위성, HFC(hybrid fiber-coaxial cable, 복합 동축 섬유 회선) 등을 이용해 연결한 네트워크.    동일한 브로드캐스트 통신 채널에서 연결되려면, 매체 연결 프로토콜(medium acess protocol)로 프레임 전송을 조정 해야한다.    주로 중앙 컨트롤러나, 개개의 호스트들이  조정한다.        지점간 통신 연결(point-to-point communication link)    보통 두 라우터나 장치 같이 두 객체간을 연결하는 방법.    전화선부터 광섬유까지 다양한 곳에서 프레임을 전송하는데 사용하는 프로토콜인 PPP(Point-to-Point Protocol)가 있다.  이외에도 에러 감지와 수정, 이더넷, LAN, virtual LAN, 데이터 센터 네트워크 등에 대해 알아보자.6.1 연결 계층 소개 (Introduction to the Link Layer)앞으로 링크 계층 프로토콜로 돌아가는 장치들을 노드(node)라고 칭할 것이며, 호스트, 라우터, 스위치, 와이파이 접근 지점 등이 그 예이다.링크는 멀리 떨어진 호스트간의 연결(connection)과도 구분하기위해 통신 경로를 통해 인접 노드를 연결하는 통신 채널을 링크(link)라고 칭할 것이다.아래 fig 6.1의 화살표를 보면 무선 통신 호스트와 서버 호스트의 통신(호스트, WIFI 접근 지점, 이더넷 링크, 스위치 링크, 라우터 링크를 통함)을 볼 수 있다. 전송 노드는 데이터그램을 캡슐화하여 보내는데, 이를 연결 계층 프레임(link-layer frame)이라고 한다.연결 계층과 네트워크 계층을 비유해보자면, 여행객은 데이터그램, 여행객이 타는 교통 수단의 종류(기차, 배, 비행기)은 링크의 연결 계층 프로토콜, 각 교통수단의 출발지에서 도착지까지 연결은 링크, 이러한 여행객의 경로를 짠 여행 가이드는 라우팅 프로토콜이라 볼 수 있다.6.1.1 연결 계층에 의해 제공되는 서비스들 (The Services Provided by the Link Layer)연결 계층 프로토콜들의 공통의 기본 서비스는 데이터그램을 단일 통신 링크를 통해 한 노드에서 인접 노드로 보내는 것이며, 상세한 추가 서비스는 프로토콜마다 다르며 다음과 같은 것이 있다..  프레임화(Framing) : 모든 연결 계층 프로토콜은 링크로 보내기 전에 각 네트워크 계층 데이터그램을 연결 계층 프레임으로 캡슐화한다. 여느 패킷들과 마찬가지로 헤더 필드와 페이로드 필드로 되어 있으며, 프로토콜 마다 프레임의 구조가 다르다.  링크 접근(Link access) : 매체 접근 제어(medium access control, MAC) 프로토콜은 프레임이 링크를 통해 전송되는 규칙을 명시하는데, 간단하게 링크가 사용 가능할 때, 송신자가 전송할 수 있게 해준다. 여러 노드가 동시에 하나의 전파(broadcast) 링크를 공유하려 하면 다중 접근 문제(multiple access problem)이라는 문제가 생기며, MAC 프로토콜은 이러한 많은 노드의 전송을 관리한다.  신뢰성 전달(Reliable devlivery): 노드 간의 전송에 오류가 없음을 보장한다. 전달 계층과 유사하게 재전송과 ACK를 통해 구현되며, 오류가 잦은 무선 연결은 이를 중시하며, 오류가 적은 유선 연결의 경우 일부 프로토콜은 데이터 신뢰성 전달 서비스를 제공하지 않기도 한다.  에러 감지 및 수정(Error detection and correction) : 수신자측의 연결 계층 하드웨어는 데이터의 비트 값이 반전되면 이를 감지할 수 있다. 이러한 비트 에러는 물리적으로, 신호 감쇠나 전자기적 노이즈로 인해 발생할 수 있으며, 이를 감지하고 수정하기 위해 에러 감지 비트가 프레임에 포함되어 있고 수신자 노드 측에서 에러를 확인한다. 전달 계층, 연결 계층의 오류 감지인 인터넷 체크섬(checksum)보다 더욱 복잡하고 하드웨어로 구현되어 있으며, 감지 뿐만 아니라  오류를 수정할 수도 있다.6.1.2 연결 계층은 어디에 구현되는가? (Where Is the Link Layer Implemented?)연결 계층의 경우 주로 네트워크 어답터(network adapter) 또는 네트워크 인터페이스 컨트롤러(NIC, network interface controller)라는 하드웨어 칩에 주로 구현되며, 예시로 이더넷의 경우 주로 메인보드 칩셋이나 저가형 이더넷 칩에 구현된다.구동의 경우, 송신자 측은 컨트롤러가 상위 계층 프로토콜로 인해 호스트 메모리에 적재된 데이터그램을 가져와 연결 계층 프레임으로 만들고 통신 링크로 전송한다. 수신자 측은 전체 프레임을 받은 뒤 네트워크 계층 데이터그램을 추출하고, 에러 감지를 한다면, 헤더의 error-detection 비트를 확인하여 확인한다.아래 fig 6.2는 전형적인 호스트 구조에서  보여준다.대부분의 연결 계층 부분은 칩셋 속의 하드웨어로, 일부분은 호스트의 CPU 안의 소프트웨어로 구현되는데, 소프트웨어 부분은 컨트롤러 가동, 연결 계층 주소 정보 취합 같은 좀더 상위 단계의 기능을 구현한다.수신자 측의 연결 계층 소프트웨어은 프레임이 도착하면 하드웨어 컨트롤러의 방해에 반응하여, 에러 상태 확인 및 네트워크 계층으로의 전달 등을 진행한다.이처럼 하드웨어와 소프트웨어가 섞여있는 구조는 연결 계층이 프로토콜 스택에서 하드웨어와 소프트웨어가 만나는 부분임을 의미한다.6.2 에러 감지와 수정 기술(Error-Detection and Correction Techniques)비트 단계 에러 감지 및 수정(bit-level error detection and correction)은 연결 계층 프레임의 비트들의 오류를 감지, 수정하는 서비스이며, 여기서 간단한 원리를 살펴보자.아래 fig 6.3은 에러 감지 및 수정 시나리오를 나타낸 그림이다.송신자측 노드는 헤더가 포함된 데이터그램 D를 에러로 부터 보호하기위해 에러 감지 및 수정 비트들(EDC, Error-detection and correction bits)를 데이터그램에 추가하고 이를 전송한다.도착한 데이터 D와 EDC는 수신자측 노드에 의해 에러를 감지하게 되는데, 간단한 방법으로는 일부 탐지되지 않은 비트 에러(undetected bit errors)가 존재할 수 있다.탐지되지 않은 비트 에러를 최소화하는 방법일 수록 더욱 많은 계산과 복잡함, 오버헤드가 요구된다.에러를 줄이기 위한 방법들을 살펴보자.6.2.1 동등성 검사 (Parity Checks)가장 간단한 에러 감지 형태는 단일 Parity 비트(Parity bit)이며, 짝수 동등성과 홀수 동등성이 있다.짝수 동등성 비트는 d개의 데이터 비트와 Parity 비트 자신의 1의 갯수의 합이 짝수개가 되도록 표기하며,홀수 동등성 비트는 d개의 데이터 비트와 Parity 비트 자신의 1의 갯수의 합이 홀수개가 되도록 표기한다.즉, d개의 데이터 비트의 1의 갯수가 홀수개라면 짝수 동등성 비트는 1, 홀수 동등성 비트는 0으로 표기되며, 반대로 짝수개라면 값이 반전된다.수신 측에서도 간단하게, Parity 비트를 포함, 1의 갯수를 센 뒤, Parity 비트 설정에 따라 아래와 같은 표의 원리로 에러를 감지한다. (직접 예시를 생각하며 이해해 보자.)            짝수 동등성에서의 1의 갯수      짝수 Parity 비트가 1인 경우      짝수 Parity 비트가 0인 경우                  짝수      에러 없음 혹은 짝수 개의 에러 존재      홀수 개의 에러 존재              홀수      홀수 개의 에러 존재      에러 없음 혹은 짝수 개의 에러 존재                  홀수 동등성에서의 1의 갯수      홀수 Parity 비트가 1인 경우      홀수 Parity 비트가 0인 경우                  짝수      홀수 개의 에러 존재      에러 없음 혹은 짝수 개의 에러 존재              홀수      에러 없음 혹은 짝수 개의 에러 존재      홀수 개의 에러 존재      어느 설정이던지, 홀수 개의 에러는 감지 가능하고 짝수 개의 에러는 감지할 수 없다. 즉, 전체 에러의 절반은 감지할 수 없다.비트 에러는 여러 비트에서 에러가 일어나는 버스트 에러(burst error)가 자주 일어나므로, 1 비트 에러만 감지하는 이 경우는 실용성이 없다.  에러 비트가 연속 해야만 버스트 에러 인것이  아님을 주의!  예를 들어 111111 비트가 001101 비트로 오류가 났을 시, 0번~1번 비트만 버스트에러인 것이 아니라, 0~4번까지가 버스트 에러 지점이다.  즉 에러 비트 사이의 올바른 비트도 포함해서, 비트가 처음 에러난 구간부터 마지막 비트 에러 구간까지가 버스트 에러이다.fig 6.5는 1비트 동등성 에러 체크의 이차원 일반화 모습이다.위처럼 데이터를 (i,j)의 2차원 배열로 바꾼 뒤, 각 열과 행에 해당하는 총 i + j + 1((i,j), 최우측아래 줄에도 있어야 함)개의 Parity 비트를 이용해 단일 비트 에러에 대해 탐지 및 수정이 가능하며, 어떠한 조합에서의 2개의 비트 에러는 탐지만 가능하다.  3개 이상 부터는 모든 에러의 탐지를 보장하지 않는다.수신자 측에서 에러를 탐지 및 수정할 수 있는 능력을 포워드 에러 수정(FEC, Forward Error Correction)이라고 하며, 오디오 관련해서 사용된다.주로 FEC 단일로, 또는 ARQ(Automatic Repeat reQuest, 재전송 기반 프로토콜, 3장 참조)와 함께 사용하여 에러 수정으로 ARQ의 재전송 횟수를 줄여주는 역할을  하며, 이는 대역폭이 좁거나 아주 먼 거리 통신에 의해 전파(propagation) 속도가 늦어진 통신이나, 실시간 통신에 효과적이다.FEC 에러 제어에 대한 내용은 [Biersack 1992; Nonnenmacher 1998; Byers 1998; Shacham 1990]6.2.2 체크섬 방법들 (Checksumming methods)인터넷 체크섬(Internet checksum)은 이전에 우리가 배웠던 방법으로, 데이터 바이트들을 16 비트 정수로 생각하고 모두 더해서 구한다. 이렇게 구한 값의 1의 보수를 헤더에 추가하면, 수신자 측에서 모든 바이트들의 합이 16비트 자리 전부 1인 값이 나오지 않으면 오류로 생각한다. 자세한 구현은 [RFC 1071]에 나와있고, TCP와 UDP 처럼, 전체 모든 바이트를 대상으로 하는 체크섬, IP 처럼 헤더만 대상으로 하는 체크섬, XPS 처럼 헤더와 전체 패킷에 대한 체크섬이 따로 나와 있는 경우도 있다.체크섬은 오버헤드가 비교적 적지만 에러 방지 효과가 순환 중복 검사 (CRC, cyclic redundancy check)에 비해 약하다. 하지만 하드웨어를 사용할 수 있는 연결 계층에 비해 소프트웨어에서 구현되는 전달 계층의 특성상, 오버헤드가 큰 검사를 사용하면 너무 느려지기 때문에 간단하고 빠른 체크섬을 사용하는 것이다.가중 체크섬, CRC 등의 최적화되고 빠른 소프트웨어적 구현 기술은 [Feldmeier 1995]에서 확인 가능하다.6.2.3 순환 중복 검사 (Cyclic Redundancy Check (CRC))CRC 코드는 널리 사용되고 있는 에러 검사 방법으로, CRC codes는 비트를 0 또는 1의 계수를 가진 다항식으로 보면, 마치 다항식처럼 보이므로 다항식 코드(polynomial codes)라고도 알려져있다.CRC 코드의 원리는 Figure 6.6과 같다.d 비트 길이의 데이터 D와 송수신자 양 측이 사전에 합의한 임의로 정한 r값에 대해 r+1 길이의 비트들인 생성자(generator) G가 있다.G의 가장 중요한 비트는 G의 제일 좌측으로, 무조건 1이다.  G의 길이가 CRC 표준으로 정해져있기 때문에, 지켜져야한다.  가장 왼쪽 첫번째 비트가 1이 아니면 길이가 달라진다. 예를 들어 1014는 4자리 수지만 0121는 3자리 수이다.송신자는 기존 데이터 D에 추가로 r 만큼의 데이터 R을 더해 총 d+r 길이의 새로운 비트 패턴을 만드는 데, 이때의 r 비트는 d+r 비트 패턴이 G의 모듈로-2(modulo-2) 연산상, 나머지 없이 나누어 떨어지는 비트 패턴이 되도록 설정한다.이후 수신자 측에서 받은 d+r 데이터를 G로 나누어 나누어 떨어진다면, 오류가 없고, 나머지가 생긴다면 오류가 생긴 것이다.이때 모든 CRC 계산은 모듈로-2 연산에 덧셈과 뺄셈에 의한 받아올림(carry)과 받아내림(borrow)이 발생하지 않는다.즉, 1+1이 된다면 10이 아니라 받아올림이 사라져 0이며, 10-1은 1이 아니라 받아내림이 사라져 11이다.이는 덧셈과 뺄샘이 둘다 결과가 같고, 마치 비트 간의 XOR 연산을 한 것과 같은 결과를 낸다.1011 XOR 0101 = 11101001 XOR 1101 = 01001011 - 0101 = 11101001 - 1101 = 0100곱셈과 나눗셈의 경우, 받아올림(carry)과 받아내림(borrow)이 없는 베이스-2 연산(base-2 arithmetic)과 같으며, 마찬가지로 XOR 연산과 결과가 같다.  참고한 전공서적에서는 Base와 modulo를 동일하게 적어놓은것 같다…베이스-n 연산(base-n arithmetic): 0부터 n-1까지를 밑수로 하는 밑(base)이 10인 산술 연산, base-10은 우리가 흔히 쓰는 10진수  밑(base)은 수학 연산에 기초가 되는 수를 의미하며, 베이스-10은 우리가 사용하는 0~9를 밑수로 사용하는 밑(base)이 10인 10진수를 의미  덧셈의 경우, n진수 덧셈을 하면 된다. (base-7, 5 + 4 = 12)  곱셈의 경우, n진수 곱셈을 하면 된다. (base-7, 3 * 5 = 21)모듈로-n 연산(modulo-n arithmetic): 0부터 n-1까지, n개의 수를 이용한 산술 연산, 모듈로-2의 경우, 0과 1만으로 산술한다.      즉, 베이스 연산과 달리 모듈로 연산은 언제나 한 자릿 수의 결과가 나온다.        덧셈의 경우, 덧셈의 결과를 n으로 나눈 나머지가 결과값이 된다. (mod-7, 5 + 4 = 2)        곱셈의 경우, 곱셈의 결과를 n으로 나눈 나머지가 결과값이 된다. (mod-7, 3 * 5 = 1)  이진 연산에서는 수에 $2^k$를 곱하는 것은 k 만큼 왼쪽으로 쉬프트(shift)하는 것과 같다.즉 fig 6.6에서 만들었던 에러 감지를 위한 새로운 데이터 d+r은 기존 데이터 D에 대하여 $D\\cdot 2^r\\ XOR\\ R$와 결과값이 같다.d+r = R을 구하는 방법을 알아보자먼저 수신자가 뒤에 덧붙이는 데이터 R을 구하는 방법을 알아보려며,이전에 새로운 데이터가 G에 나누어 떨어져야 한다고 했으므로,$D \\cdot 2^r\\ XOR\\ R= nG$앞서 XOR의 결과물은 덧셈, 뺄샘과 같다고 했으므로 XOR 연산자를 양 식에 XOR 해주면, 위 식은 아래와 같이 다시 표현할 수 있다.$D \\cdot 2^r\\ XOR\\ R\\ XOR\\ R=D \\cdot 2^r+R - R = D \\cdot 2^r = nG\\ XOR\\ R$양 식에 G로 나누면 다음과 같이 된다.$R=remainder\\ \\frac{D\\cdot 2^r}{G}$아래 Figure 6.7은 D=101110, d= 6, G = 1001, r =3인 경우의 R의 계산법이다. 결과적으로 R은 011이 생성된다.$D \\cdot 2^r=101011\\cdot G\\ XOR\\ R$ (101011은 결국 n이 된다)이러한 R에 대해서 길이 값 r은 국제 표준으로 8, 12, 16, 32가 나와있으며, 각각 CRC-8, CRC-12, CRC-16, CRC-32라고 부른다.$G_{CRC-32}=100000100110000010001110110110111$위는 연결 계층 IEEE 프로토콜에서 사용하는 33자리 생성자이다.각 CRC의 r 값 이하의 길이를 가진 연속된 비트 에러를 100% 탐지할 수 있으며, 그 이상의 길이(&gt;=r+1)의 비트 에러를 탐지할 비율은 $1-0.5^r$이다.추가로 CRC는 모든 숫자의 홀수 갯수의 비트 에러를 감지할 수 있다.  즉, CRC-32의 경우 에러를 탐지하지 못할 비율은 $0.5^{32}=2.33\\times10^{-10}$이다.  에러를 탐지못할 확률 $0.5^r=\\frac{1}{2^r}$은 R이 나눠질 때 생길수 있는 나머지의 범위가 $0 \\sim 2^r-1$까지 총 $2^r$개이므로 무작위 수가 0(=나누어 떨어져서 에러로 탐지 안될 확률)이  $\\frac{1}{2^r}=0.5^r$이 된다.더욱 자세한 내용은 [Williams 1993, Schwartz 1980]을 참조6.3 다중 접근 링크와 프로토콜 (Multiple Access Links and Protocols)네트워크 링크에는 두가지가 있다.지점간 링크(point-to-point)링크 끝 단의 송수신자가 1:1로 연결하는 링크, PPP(point-to-point protocol, 지점간 프로토콜), HDLC(high-level data link control, 고레벨 데이터 링크 컨트롤) 같은 프로토콜이 존재.브로드캐스트 링크(boradcast link)송신자 여러명, 혹은 수신자 여러명이 하나의 브로드캐스트 채널(broadcast channel)을 공유하여 서로 연결되어 있는 링크,브로드캐스트(broadcast)란, 한 노드가 프레임을 보내면 채널 측에서 프레임을 여러개로 복사해서 여러 노드로 보내는 것이더넷(ethernet), 무선 LAN(wireless LAN)이 속한다.어떻게 다수의 송수신자가 공유된 하나의 브로드캐스트 채널을 공유할 수 있는가? - 다중 접근 문제(multiple access problem)에 대해서 알아보자.먼저 **지정학적으로 한 지역에 집중되어 다중 접근 채널을 활용하는 LAN 또한 **알아보자.흔히 브로드캐스트(broadcast, 방송, 송출) 채널은 학교의 한 반에서 선생님과 학생들이 동시에 이야기하는 것을 비유로 들 수 있다.당연히 모든 사람들이 동시에 이야기하면 시끄러워 말이 서로 전달이 안되니,“손 들고 질문할 것”, “다른 사람이 이야기 할 때 떠들지말 것”, “말하는 도중에 잠자지 말것”, “제한된 시간까지만 말할 것”등의 규칙을 정하여 이야기하면 되는데 이는, 다중 접근 프로토콜(multiple access protocols)에 해당한다.위의 figure 6.8 은 여러가지 네트워크 설정에 따른 다중 접근 프로토콜의 필요를 보이고 있다. 사실, 각 노드는 브로드캐스트 채널을 어답터를 이용해 접근하지만 일단 우리는 각 노드를 장치로 가정하고, 브로드캐스트 채널에 접근하는 것으로 하자.만약 여러 노드가 동시에 프레임을 보내게 된다면, 프레임들은 서로 간에 충돌된 채로 여러 노드에게 보내지고, 보내진 충돌난 프레임은 그대로 버려지게 된다.많은 충돌이 일어나 재전송이 일어나면, 더욱더 브로드캐스트 채널의 한정된 대역폭 내에서 많이 충돌이 일어나게 되고, 결국 대역폭은 낭비되고 만다.이러한 활성된 노드들의 전송들을 조정하는 것이 다중 접근 포로토콜의 일이며, 다음과 같이 크게 세가지로 분류된다.  채널 나눔 프로토콜(channel partitioning protocols)  무작위 접근 프로토콜(random access protocols)  순번 프로토콜(taking turns protocols)전송속도가 R bps인 브로드캐스트 채널에 대하여 다음과 같은 특성이 있어야 한다.  하나의 노드만 데이터를 보낼 때 처리율이 R bps 여야 한다.  M 개의 노드가 데이터를 보낼 때 처리율이 일정 시간동안 평균적으로 R/M bps여야 한다.  단일지점 장애를 예방하기 위해 하나의 마스터 노드가 관리하는 것이 아니라 프로토콜은 분산적이어야 한다.  프로토콜은 간단하고 비용이 적게 들어야 한다.6.3.1 채널 나눔 프로토콜 (Channel Patitioning Protocols)섹션 1.3에서 설명했던 시간 분배 멀티플렉싱(TDM, time-division multiplexing)과 주파수 분배 멀티플렉싱(FDM, frequency-division multiplexing)은 브로드캐스트 채널의 대역폭을 여러 노드에게 나눠줄 수 있는 방법이다.브로드캐스트 채널을 공유할 N개의 노드가 있다고 가정하면,TDM은 시간을 여러 개의 시간 프레임(time frames)로 나누어 추가로 각 시간 프레임을 N 개의 시간 슬롯(time slot)으로 나누어 각 슬롯에 N개의 노드들을 배정한 뒤, 라운드 로빈 방식으로 각 노드가 각 시간 프레임에 한번씩 시간 슬롯 동안 보내도록 하는 방식이며, 이때 슬롯 사이즈와 시간 프레임 사이즈는 보낼 단일 패킷의 크기에 따라 정해진다.비유하자면, 한 반에 학생들이 번호 순서대로 일정한 발언 시간을 가지는 것과 같으며, 패킷 간의 충돌을 막을 수 있다.아래 figure 6.9는 간단한 4개의 노드 TDM과 나중에 설명할 FDM의 예시이다.하지만 TDM은 2가지 큰 단점이 있는데, 바로 첫번째는 노드들 중 패킷을 보내고 싶은 노드가 하나여도 R/N bps로 대역폭이 제한된다는 점, 두번째는 마찬가지 상황에서 추가로 보내고 싶을때 바로가 아니라 언제나 자신의 순번을 기다리고 있어야한다는 점이다.비유로 치자면, 반에서 1명을 제외하곤 할말이 없는데 다들 똑같은 시간을 공유하고, 발언자는 원하는 시간에 말하지 못한다는 점이다.FDM은 이러한 TDM 대신 다른 주파수의 영역을 N개로 나누어 각각 R/N 만큼의 대역폭을 사용하는 방법으로 TDM과 같은 장점을 공유하지만, 마찬가지로 TDM의 첫번째 단점을 그대로 공유하고 있다.이외의 새로운 채널 나눔 프로토콜인 CDMA(코드 나눔 다중 접근, code division multiple access)은 노드 별로 다른 코드를 통해 데이터를 인코딩하여 보내며, 이미 해당 코드를 알고 있는 수신자측에서 디코딩하는 방식이며, 모든 노드가 동시에 전송할 수 있다.추가로 반 재밍(anti-jamming) 특성이 있어 군에서 사용하다가 민간 무선 통신에 사용되는 중이며, 자세한 설명은 7장에서 다룰 예정이다.6.3.2 무작위 접근 프로토콜 (Random Access Protocols)무작위 접근 프로토콜은 채널을 공유하는 각 노드가 전체 처리율인 R bps로 보내다가, 다른 노드의 전송에 의해 충돌이 감지되면, 무작위 지연 시간(random delay) 동안 전송을 멈췄다가, 재전송을 시작한다.이때 각 노드 별로 무작위로 지연 시간을 고르므로, 몇번 더 충돌이 일어나더라도, 결국에는 각기 다른 시간에 전체 처리율 R로 프레임을 보낼 수 있게 된다.우리는 무작위 접근 프로토콜인 ALOHA 프로토콜과 CSMA(반송파 감지 다중 접속 프로토콜,carrier sense multiple access)을 배울 것이며, CSMA 중 가장 유명한 이더넷에 대해 배울 것이다.일자형 ALOHA (Slotted ALOHA)간단한 일자형 ALOHA에 대해서 배우기 전에 가정을 하자면  모든 프레임의 크기는 정확히 L 비트이다.  시간은 L/R 초(=정확히 한 프레임을 보낼 수 있는 시간)의 슬롯으로 나눈다.  슬롯의 시작 지점에 바로 전송을 시작  각 노드들은 동기화되어 있어서 슬롯들의 시작과 끝의 타이밍을 잘 안다.  2개 이상의 프레임이 슬롯에서 충돌이 나면, 슬롯이 끝나기 전에 이미 충돌 이벤트를 노드들이 인지한다.각 노드에서의 일자형 ALOHA의 동작은 간단하다.  노드가 보낼 프레임이 있으면, 다음 슬롯에 시작할 때까지 기다렸다가 슬롯을 통해 전체 프레임을 보낸다.  만약 충돌이 없었다면, 새로 보낼 프레임을 준비하고 위의 행동을 반복한다.  만약 슬롯이 끝나기 전에 충돌이 있었다면, 충돌이 일어나지 않을때까지 노드는 연속된 슬롯에 p의 확률로 재전송한다.  만약 재전송하지 않는 확률(1-p)에 걸렸다면, 다음 슬롯까지 기다렸다가 다시 p의 확률로 재전송한다.일자형 ALOHA는 활성화된 노드의 경우 전체 처리율인 R로 전송이 가능하고, 충돌 감지와 재전송 확률 계산 등이 노드 별로 분산화되었으며, 간단한 프로토콜이다.비록, 노드들이 슬롯에 동기화되어야 한다는 점이 완전한 분산화의 발목을 잡지만, 다음에 배울 ALOHA 프로토콜과 다른 CSMA 프로토콜들은 이를 해결하였다.위 그림 figure 6.10을 보면, 일자형 ALOHA의 다른 문제점이 보인다.바로 슬롯이 두 가지 방법으로 낭비된다는 점인데 첫번째는 아무도 전송하지 않기로 결정한 시간대의 슬롯이 낭비되고, 두번째로 충돌이 일어난 슬롯도 결국 아무 패킷도 보내지 못하고 낭비되는 셈이다.전체 슬롯 중에 성공적으로 패킷을 보낸 슬롯의 비율을 효율성으로 보자면, 일자형 ALOHA의 최대 효율은 0.37 정도로 전체 R bps의 0.37정도로 효율이 낮다.아래는 ALOHA의 효율성을 구하는 공식이다.성공적인 슬롯은 노드 하나만 패킷을 보내고, 나머지는 패킷을 보내지 않을 확률이므로, $p(1-p)^{N-1}$이며, 이러한 노드가 N개 있으므로 N을 곱해 $Np(1-p)^{N-1}$이다.\\(Np^*(1-p^*)^{N-1}\\)N은 노드의 갯수, $p^*$는 가장 효율성이 최적의 재전송 확률 p이다.$p^* = $N의 값을 무한대로 발산시키면 $1/e$로 수렴하는데, 이 값이 0.37이다.ALOHA위에 설명했듯이 일자형 ALOHA 프로토콜은 각 슬롯의 시작지점에 동기화되야한다는 단점이 있다. 최초의 ALOHA 프로토콜은 슬롯을 사용하지 않는 분산화된 프로토콜이었다.순수 ALOHA에서는 보낼 프레임은 지체없이 바로 브로드캐스트 채널 보내었고, 만약 충돌을 겪었다면, 노드는 바로 p의 확률로 재전송 하거나, 1-p의 확률로 보내야 될  프레임에 걸리는 전송 시간 만큼 기다렸다가 다시 p의 확률을 계산했다.순수 ALOHA의 효율성은 $1/2e$=0.18 정도로 일자형 ALOHA의 정확히 절반이다.figure 6.11 을 살펴본면 순수 ALOHA는 단순히 특정 슬롯 시간($[t_0 \\sim t_0+1]$ 구간)에 둘 이상의 노드가 전송하면 안될 뿐만 아니라,(확률 :$(1-p)^{N-1}$)같은 단위시간 이전의 시간($[t_0-1 \\sim t_0]$ 구간)에도 노드가 전송될 경우 충돌이 일어나므로,(확률 :$(1-p)^{N-1}$), 충돌할 확률이 두배로 커지기 때문에(확률 :$(1-p)^{2(N-1)}$) 효율이 절반으로 줄어든다.완전히 분배된 프로토콜인 대신, 안그래도 비효율적이었던 ALOHA가 더욱 비효율적으로 변했다.반송파 감지 다중 접속 (Carrier Sense Multiple Access, CSMA)ALOHA 계열 프로토콜들은 전송을 시작할 때, 다른 노드의 전송상태를 살피지 않고, 전송 중일 때, 다른 노드의 전송 요청 여부도 확인하지 않는다.비유하자면, 반에서 누가 말하든, 누가 말하고 싶어하든 상관하지 않는 거만한 학생과도 같다.현실에서 이러한 행동을 막기위해 규칙이 예절이 있는 것처럼, 프로토콜에도 다음과 같은 규칙이 있다.      반송파 감지(carrier sensing), 말하기 전에 먼저 들어보기    전송하기 전에 채널의 상태를 감시하며, 아무도 패킷을 보내지 않게되면 전송한다.        충돌 감지(collision detection), 누군가와 동시에 이야기하기 시작하면, 발언을 멈추기    전송하고 있어도 채널의 상태를 계속 감지하다가 누군가 갑자기 전송을 시작하면, 무작위 시간만큼 기다렸다가 반송파 감지를 실시한다.  위 두 규칙은 carrier sense multiple access(CSMA, 반송파 감지 다중 접속)와 CSMA with collision detection(CSMA/CD, 충돌 감지 CSMA)의 기능이다.CSMA와 CSMA/CD의 기본적이고 중요한 질문 중 하나로, CSMA의 프로토콜의 특성 대로 모든 노드들이 다른 노드의 전송을 감지 중(반송파 감지)이라면, 충돌은 절대 일어나지 않을텐데, 어째서 우리는 충돌을 방지해야하는가? 이다.일단, 반송파 감지가 있어도 충돌은 일어난다.그 이유는, 패킷을 전파할 때 다른 노드에게 전송되는데 시간(end-to-end channel propagation delay, 지점간 채널 전파 지연)이 걸리기 때문에, 먼 거리의 노드는 반송파를 감지하기 전에 이미 비어있는 채널로 착각하고 전송하여 충돌이 일어난다.figure 6.12로 알아보자면, 아무도 채널에 전송하지 않는 것을 깨닫고, B는 A, C, D를 향해 시간 $t_0$에 전파를 시작했지만, $t_0$보다 조금 늦은 $t_1$ 시점에 D 입장에는 아직 B의 전송 데이터가 도착하지 않아, 아무도 채널에 전송하지 않는다고 착각하여 $t_1$에 전파를 시작하고, 충돌이 일어난다.이를 통해 CSMA 계열 프로토콜에게 지점간 채널 전파 지연이 성능에 중요함을 알 수 있다.충돌 감지 반송파 감지 다중 접근(CSMA/CD, Carrier Sense Multiple Access with Collision Detection)CSMA/CD는 CSMA에 추가로 충돌 방지를 추가한 것이다.아래의 figure 6.13은 충돌 방지가 추가된 그래프이다.B와 D가 서로의 전파를 처음 맞닥뜨리는 시간으로 부터 충돌 감지 및 중단 시간(Collision detect/abort time)의 지연이 발생한 뒤 전송이 중단된다.이를 통해 의미없이 충돌이 생긴 프레임이 전송되는 것을 최대한 빨리 막아 성능 향상을 이룰 수 있다.CSMA/CD 프로토콜의 활동을 브로드캐스트 채널을 이용하는 노드의 어답터 기준으로 알아보자면,      어답터가 네트워크 계층으로 부터 데이터그램을 가져오고 연결 계층 프레임으로 바꾼 뒤, 프레임 어답터 버퍼(frame adapter buffer)에 집어넣는다.        만약 어답터가 채널로부터 들어오는 신호 에너지가 없으면 채널이 사용가능한 상태라고 판단하고 전송을 시작한다.    만약 반대로 채널이 붐비다고 판단되면, 더이상 채널로부터 신호 에너지가 들어오지 않을 때까지 기다리다가 전송을 시작한다.        전송 중에 어답터는 다른 어답터가 보내는 신호 에너지의 존재를 모니터링한다.        만약 어답터가 아무런 다른 어답터의 신호 에너지를 감지 하지 못한 채 전체 프레임의 전송을 완료하면, 전송을 끝마친다.    만약, 도중에 다른 신호 에너지를 감지하면, 전송을 멈춰버린다.        전송이 멈춰졌다면, 무작위 시간 만큼 기다린 뒤, 다시 2번 과정부터 시작한다.          다들 동일한 일정 시간을 기다리게 하면, 모두 같은 시간만큼 기다렸다가 동시에 신호를 측정하므로 영원히 충돌을 반복하게 되어, 무작위 시간을 기다리게 한다.      무작위 시간의 정할 때의 시간 범위는 너무나 넓으면, 어답터들이 비효율적으로 오래 기다려야 하여 비효율적이고, 너무나 좁으면 충돌이 자주 일어나 비효율적이다.이상적인 무작위 시간 범위는 충돌 경험의 수가 적으면 좁게, 충돌 경험의 수가 많으면 채널이 상당히 붐빈다는 의미이므로 넓게 가지는 것이 좋을 것이다.이진 지수적 백오프(binary exponential backoff)는 이더넷과 DOCSIS 유선 네트워크 다중 접근 프로토콜에 사용되는 방법으로, 위의 이상적인 시간범위 설정을 해결해 준다.원리는 노드가 특정 프레임에 대하여 n번째 충돌을 겪는다면, 무작위 백오프 시간 K는 시간 범위는 ${0,1,2…,2^n-1 }$에서 고른다.즉 매 충돌마다 무작위 백오프 시간 범위는 2배씩 늘어나는 것이며, 이더넷의 경우, 무작위 백오프 시간 K * 512 bit times(해당 채널에서 512bit 데이터를 전송하는데 걸리는 시간) 만큼 기다리며 최대 충돌 제한은 10으로 놓는다.또한, 전송이 끝난 뒤, 다음 프레임을 준비할 때, CSMA/CD의 변수들은 초기화되고 다시 시작한다.예를 들어 저번에 6번의 충돌이 일어났고, K의 시간이라는 백오프를 기다렸었다면, 해당 프레임이 전송된 뒤로, 충돌 횟수는 1로, 백오프는 0으로 초기화한다.이를 통해 노드가 빠르게 전송 시간을 차지할 수 있다.CSMA/CD 효율 (CSMA/CD Efficiency)CSMA/CD의 효율성은 $d_{prop}$은 최대 크기의 프레임을 보내는데 걸리는 시간일 때 아래 처럼 유도한다.([LAM 1980, Bertsekas 1991])\\[Efficiency =\\frac{1}{1+5d_{prop}/d_{trans}}\\]$d_{prop}$이 0에 가까워지거나 반대로 무한대에 발산할수록, 효율은 1에 가까워진다.이는  전파 지연시간(propagation delay)가 0에 가까울 수록, 채널을 낭비하지않고 바로 전송을 그만둘 수 있기 때문에  프레임 전송하는 시간이 늘면서 단일 노드가 채널의 전송속도를 가지고 있는 시간이 늘어서 효율적으로 자원을 쓰기 때문에이다.6.3.3  순번 프로토콜 (Taking-Turns Protocols)다중 접근 프로토콜(multiple access protocol)의 필요한 특성으로 첫번째, 한 노드만 채널 사용시 R bps의 처리율, 두번째, 여러 노드가 채널 사용시 R/N bps를 기억하자면, ALOHA와 CSMA의 랜덤 접근 방법은 첫번째 특성은 만족하지만 두번째 특성은 만족하지 않는다는 것을 알 수 있다.두 가지 특성을 모두 성립하기 위해 연구된 것이 순번 프로토콜(Taking-Turns Protocols)이며 다음과 같은 종류가 있다.      폴링 프로토콜 (polling protocol)    하나의 마스터 노드가 각 노드에게 라운드 로빈 순으로 다른 노드들을 폴링(하나의 장치가 다른 장치들을 주기적으로 검사 후, 조건에 만족하면 송수신 등의 자료처리하는 것)하는 방식이다.    예를 들어, 마스터 노드가 노드 하나에게 메시지를 보내 제한된 프레임의 전송을 허락한 뒤, 채널의 신호가 사라짐을 감지하면, 다른 노드에게 메시지를 보내어 전송을 허락하는 방식의 반복이다.    블루투스(Bluetooth) 프로토콜이 해당한다.    충돌과 사용하지 않는 시간대 또는 슬롯을 없앨 수 있어서 높은 효율을 보이지만, 다음과 같은 단점이 있다.          폴링 지연(polling delay) : 마스터 노드가 노드에게 권한을 부여하기 위해 생기는 시간의 지연, 폴링 프로토콜에서는 한 노드만 전송할 시, 미묘하게 효율이  100%가 못되는데, 마스터 노드가 비활성 노드를 포함해서 채널의 노드들에게 폴링하는 데 시간이 걸리기 때문이다.      중앙집중식 프로토콜(centralized protocol) : 만약 마스터 노드에 장애가 발생시, 전체 채널이 가동할 수 없게 된다.            토큰 전달 프로토콜 (token-passing protocol)    마스터 노드가 존재하지 않으며, 대신 토큰(token)이라 불리우는 특수 목적 프레임이 노드들 사이에 일정한 순서로 건네지면서 채널 자원을 활용할 수 있다.    보통 제한된 프레임의 전송을 끝내면 각 노드가 라운드 로빈식으로 다음 노드에게 토큰을 넘겨주며, 보낼 프레임이 없으면 즉시 다음 노드로 넘겨주는 방식으로 진행된다.    높은 효율과 분산화된 알고리즘의 장점이 있지만, 한 노드가 먹통이되거나 악의적으로 토큰을 독점하면, 반환 절차 등을 시행하면서 오버헤드가 생긴다.    FDDI(fiber distributed data interface, 광섬유 분산 데이터 인터페이스) 프로토콜, IEEE 802.5(WiFi) token ring 프로토콜 등이 존재한다.  6.3.4 DOCSIS: 유선 인터넷 접근을 위한 연결 계층 프로토콜 (DOCSIS: The Link-Layer Protocol for Cable Internet Access)앞서 배웠던 채널 나눔 프로토콜, 무작위 접근 프로토콜, 순번 프로토콜을 유선 접근 네트워크를 이용해 살펴보자.이전 섹션 1.2.1에 배웠던 유선 접근 네트워크는 보통 수많은 주거지 유선 모뎀을 유선망 종점의 유선 모뎀 종결 체계(CMTS, cable modem termination system)와 연결하여 생성된다. DOCSIS(유선상 데이터 전달 서비스 인터페이스 설명서, Data-Over-Cable Service Interface Specificationis)는 유선 데이터 네트워크 구조와 프로토콜에 대해 설명한다.DOCSIS는 브로드캐스트 채널을 FDM을 통해 두 채널로 나누는데,각각 다운 스트림(CMTS-&gt;모뎀, 24MHz ~ 192MHz, 1.6 Gbps/채널)과 업 스트림(모뎀-&gt;CMTS, 6.4MHz ~ 96MHz, 1Gbps/채널)로 나눈다.다운 스트림 채널은 CTMS 단일 노드가 사용하므로 다중 접근 문제가 없지만, 업 스트림 채널은 수많은 모뎀이 사용하므로 그렇지 않다.figure 6.14를 보면, 업스트림 채널을 TDM 형식으로 여러 미니 슬롯으로 나눈 뒤, CMTS 측에서 컨트롤 메시지인 MAP 메시지를 다운스트림 채널을 통해 특정 유선 모뎀에게 특정 미니 슬롯을 부여하여 전송하는 방식을 사용하여, 충돌을 없앴다.이때, 미니슬롯을 배정받으려면 미니슬롯 요청 프레임(mini-slot-request frame)을 랜덤 접근 프로토콜 방식으로 보내어 CMTS에게 보낼 데이터가 있음을 어필해야 하며, 이때의 방식에는 충돌이 존재할 수 있다.모뎀 측에서는 반송파 감지나 충돌 감지가 불가능한 대신, 보낸 요청 프레임에 대한 응답으로 MAP 메시지가 돌아오지 않으면, 충돌로 생각하고, 앞서 배웠던 이진 지수적 백오프(binary exponential backoff)를 실시한다.만약 업스트림 채널의 트래픽이 여유있다면, 유선 모뎀은 명시적인 미니슬롯 할당을 받기 전까지 원하는 대로 데이터를 보내어 폴링 지연을 막는다.DOCSIS는 이런식으로 FDM, TDM, 무작위 접근, 중앙 할당이 모두 포함된 좋은 네트워크 예시이다6.4 스위치 근거리 네트워크 (Switched Local Area Networks)figure 6.15는 세 부서, 두 서버, 4개의 스위치와 한개의 라우터에 연결된 스위치드 LAN의 예시이다.스위치들은 연결 계층 장비이므로, 연결 계층 프레임을 주고 받으며, 네트워크 계층의 OSPF 등을 이용해 라우팅하지 않고, IP 주소를 통해 구분하지도 않으며, 대신 연결 계층 주소를 이용해 통신한다.우리는 연결 계층 주소와 이더넷 프로토콜, 연결 계층 스위치의 동작 등에 대해서 배울 것이다.6.4.1 연결 계층 주소와 ARP(Link-Layer Addressing and ARP)호스트와 라우터들은 연결 계층 주소와 네트워크 계층 주소를 둘다 가지고 있어야 하는 이유와 연결 계층 주소의 기능과 문법, IP 주소와 연결 계층 주소를 서로 번역해주는 ARP(주소 확인 프로토콜, Address Resolution Protocol)에 대해 알아볼 것이다.MAC 주소(MAC Addresses)연결 계층 주소는 주로 MAC 주소라고 불리우며 이외에 LAN 주소, 물리적 주소 등으로도 불린다.한 라우터와 호스트에 인터페이스별로 여러 IP 주소를 가질 수 있듯이, MAC 주소 또한, 장치들(정확히는 장치들의 연결 계층 어답터)이 여러 개의 연결 계층 주소를 가질 수 있다.다만 아래 figure 6.16 처럼 연결 계층 스위치의 경우 MAC 주소를 가지지 않는데, 이는 스위치에는 패킷이 도착하지 않고 경유만할 뿐이기 때문이다.MAC 주소는 아래처럼 6 바이트(=48 비트)의 길이로 주로 16진수(hexadecimal notation)로 표현되며, 과거와 달리 최근 어답터에서 소프트웨어로 MAC 주소가 정의되면서  바꿀 수 있게 되었지만, 보통 고정하여 바꾸지 않고 사용하며, IEEE에서 MAC 주소 공간을 관리하여 여러 장치의 MAC 주소는 장치마다 유일한 값이다.IEEE에서 $2^{24}$개의 주소 공간중에 하나를 구매하여 사용하고, 나머지 $2^{24}$개의 주소를 기기 별로 유일한 MAC 주소로 이용한다.MAC 주소는 평등 구조(flat structure)로, IP 주소의 계층 구조(hierarchical structure)와 대비되는데,이는 IP 주소의 경우, 호스트가 네트워크를 변경할 때마다 바뀌지만, 기기의 MAC 주소는 네트워크가 바뀌어도 동일하게 유지하기 때문이다.비유하자면, IP 주소는 이사갈때 마다 바뀌는 집 주소, MAC 주소는 언제나 일정한 주민등록번호로 생각할 수 있고 둘다 따로 있어야 편하다는 것을 알 수 있다.어뎁터가 프레임을 특정 어뎁터에 보내고 싶다면, 해당 어뎁터의 MAC 주소를 프레임에 넣은 뒤, LAN으로 보내면, 연결 계층 스위치가 해당 프레임을 LAN 내부의 모든 인터페이스에 브로드캐스트한다.LAN 내의 모든 어뎁터들은 받은 프레임의 주소 정보를 확인하여 자신에게 온 것이면 네트워크 계층으로 전달하고, 아니면 프레임을 버린다.만약, LAN 내의 모든 어뎁터들에게 전파되어야할 브로드캐스트 메시지라면, MAC 브로드캐스트 주소(FF-FF-FF-FF-FF-FF)로 목적지 주소를 설정하여 프레임을 만들고 스위치로 넘기면 모든 어뎁터들이 대상이 된다.IP 주소가 MAC 주소를, 또는 MAC 주소가 IP 주소를 대체하지 못하는 이유는      IP 프로토콜이 대다수의 인터넷 프로토콜을 차지하지만 유일한 네트워크 계층 프로토콜이 아니다.    즉, IP 주소가 존재하지 않는 프로토콜을 사용한다면? 그렇다면 또 다른 식별자와 네트워크 라우팅 방법이 있을 수 있겠지만, 연결 계층에서 모든 프로토콜에 대응해 새로운 ARP 프로토콜 역할을 하는 프로토콜을 짜는것 보다는 그냥 고유의 주소를 가지고 있는 편이 낫다.    즉, 연결 계층이 IP 프로토콜 이외의 프로토콜 아래에서 일할 수 있게 유연성을 제공해준다. 실제로 SPX/IPX 주소 체계의 경우 IP 프로토콜을 MAC이 대체해, 네트워크 주소와 MAC 주소를 이용해 라우팅하고, ARP 프로토콜이 없어 상대적으로 성능상 이득을 봄에도 불구하고, IP 프로토콜의 서브넷 개념이 주는 유연성에 밀려 많이 사용하지 않는다.        MAC 주소는 IP 주소와 달리 네트워크 내의 위치를 추정할 수 없다.    MAC 주소는 IP 주소와 달리 서브넷 마스크로 자신의 소속을 나타내지않으며, 그렇게 되게끔 MAC 주소를 고안했다고 해도, 고정된 값이기 때문에 서브넷을 바꾸면 엉망이 된다. 마치 집주소 대신 주민등록번호로 편지 배달을 하는 것과 비슷하다.        IP 주소는 MAC 주소와 달리 수시로 변한다.    MAC 주소는 단순히 라우팅 뿐만 아니라 일종의 기기의 식별자로도 사용되곤 한다. 만약 IP 주소를 식별자로 사용한다면, 내가 사용하던 IP 주소를 내 노트북이 네트워크에서 나간 뒤에 DHCP가 다른 기기에 부여하게 된다면, 그 기기는 내 노트북으로 구별될 것이다.  그러니 유연성을 위해 네트워크를 찾는데는 IP 주소, 불변성으로 네트워크 내에서 특정 기기를 찾아내는 데는 MAC 주소를 사용한다.주소 분석 프로토콜 (Address Resolution Protocol(ARP))주소 분석 프로토콜(Address Resolution Protocol(ARP))은 네트워크 계층 주소와 연결 계층 주소를 서로 번역하는 역할을 한다.figure 6.17은  네트워크 계층 주소와 연결 계층 주소가 하나씩 할당된 네트워크의 예시이다.만약 222.222.222.220이 222.222.222.222에게 패킷을 보내려면, 목적지의 IP 주소 뿐만 아니라 MAC 주소를 프레임의 목적지 MAC 주소 헤더에 적어 LAN으로 보내야한다.이때 송신자 측은 목적지의 MAC 주소를 얻기 위해 호스트에 포함된 ARP 모듈을 이용해 IP 주소(222.222.222.222)에서 MAC 주소(49-BD-D2-C7-56-2A)를 알아낸다.ARP의 IP 주소를 이용해 MAC 주소를 얻어낸 다는 점이 도메인을 이용해 IP주소를 얻어내는 DNS와 유사한데, 한가지 차이점은DNS는 전체 인터넷에서 IP 주소를 얻어내지만, ARP는 같은 서브넷에 한해서만 호스트와 라우터 인터페이스의 MAC 주소를 얻어낸다는 점이다.각각 호스트와 라우터는 figure 6.18과 같은 ARP 테이블을 메모리에 가지고 있으며, IP 주소와 MAC 주소의 맵 형식으로 되어 있다.TTL(Time-to-live)는 해당 항목이 유효한 시간이며, 해당 항목은 시간을 넘으면 지워지게 되며, 보통 생성부터 20분 뒤 이다.ARP 테이블은 서브넷 내부의 모든 IP 주소와 MAC 주소에 대한 정보를 가지고 있지 않고, 추가로 TTL에 의해 지워지므로, 존재하지 않은 항목은 ARP 프로토콜을 이용해서 생성해야한다.      먼저 ARP 쿼리 패킷(ARP query packet)이라는 ARP 형식을 가진 패킷을 생성한다.        해당 패킷의 목적지 IP 주소를 설정하고, 목적지 MAC 주소를 MAC 브로드캐스트 주소(FF-FF-FF-FF-FF-FF)로 설정한 뒤, 프레임으로 캡슐화해 서브넷에 보낸다.        스위치는 ARP 쿼리 프레임을 브로드캐스트하여 서브넷 내의 모든 호스트에게 보낸다.        각 호스트의 어답터는 ARP 패킷을 ARP 모듈로 보내고, 목적지 IP 주소를 자신의 IP 주소와 비교해본다.        다르다면 무시하고, 같다면 ARP 응답 패킷(ARP response packet)을 생성한다. ARP 응답 패킷은 ARP 쿼리 패킷과 같은 형식 구조를 가지고 있다.        ARP 응답 패킷은 ARP 쿼리 패킷을 참조해 목적지 주소를 설정하고, 출발지 주소는 자신의 IP 주소와 MAC 주소로 설정해서 스위치로 보낸다.        스위치에 의해 원래 브로드캐스트했던 호스트에게 돌아가고, 해당 호스트의 어답터의 ARP 모듈은 응답 패킷의 정보를 토대로 ARP 테이블을 갱신한다.              IP 주소      MAC 주소      TTL                  222.222.222.221      88-B2-2F-54-1A-0F      13:45:00              222.222.222.223      5C-66-AB-90-75-B1      13:52:00      [Figure 6.18 222.222.222.220에서 가능한 ARP 테이블 (A possible ARP table in 222.222.222.220)]ARP의 추가적인 특징은 다음과 같다.      ARP 쿼리 패킷은 브로드캐스트 프레임, ARP 응답 패킷은 기존 프레임에 담겨진다.        ARP는 plug and play로, 운영체제나 시스템 관리자와 관계 없이 ARP 테이블이 자동으로 생성되며, 스스로 운영된다.        만약 해당 호스트가 서브넷에서 지워지면 서브넷 내의 다른 호스트의 ARP 테이블에서 관련 항목이 즉시 지워진다.  ARP는 ARP 패킷이 프레임에 캡슐화된다는 점과 IP 주소를 다룬다는 점에서 네트워크 계층 프로토콜 특성이 있고, MAC 주소를 다룬다는 점에서 연결 계층 프로토콜 특성이 있으므로, 양 계층의 경계에 해당하는 프로토콜로 보면 된다.서브넷 바깥에 데이터그램 보내기 (Sending a Datagram off the Subnet)그렇다면 다른 서브넷에 속한 호스트와 통신하려면 어떻게 할까? figure 6.19는 두 서브넷으로 이루어진 라우터의 네트워크 그림이다.각 호스트, 라우터의 인터페이스에는 IP 주소와 MAC 주소, ARP 모듈과 어답터가 존재하며, 라우터의 경우는 인터페이스가 2개이므로 2개씩 존재하는 셈이다.위 네트워크에서 좌측 서브넷의 111.111.111.111이 우측 서브넷의 222.222.222.222에게 데이터그램을 보내는 과정을 알아보자.먼저 111.111.111.111 측에서 데이터그램을 만들때, 목적지 IP 주소는 알지만, 목적지 MAC 주소는 모르므로, 목적지 MAC 주소에 적을 정보를 알아내야 한다.물론, 무슨 이유로 222.222.222.222의 MAC 주소 49-BD-D2-C7-56-2A를 안다고 하여도 해당 MAC 주소를 집어 넣으면, 스위치 측에서 해당 MAC 주소가 좌측 서브넷 에 존재하지 않으므로, 데이터그램을 파기해버린다. MAC 주소는 같은 서브넷에서만 유효하다는 것을 상기하자.  먼저, 111.111.111.111은 데이터그램의 목적지 IP 주소가 원래 목적지인 222.222.222.222로 설정하고, 이를 프레임으로 캡슐화한 뒤, 프레임의 목적지 MAC 주소는 좌측 서브넷에 연결된 라우터의 인터페이스 MAC 주소인 E6-E9-00-17-BB-4B로 보낸다.          라우터의 인터페이스 MAC 주소는 ARP 프로토콜을 통해서 가져올 수 있다.        라우터 인터페이스 111.111.111.110은 해당 프레임을 받고, 네트워크 계층으로 옮기면서 디캡슐화하여 데이터그램으로 바꾼다.  네트워크 계층에서 데이터그램과 포워드 테이블을 대조하여 목적지 IP 주소가 우측 서브넷 소속임을 알고, 라우터 인터페이스 222.222.222.220로 포워딩한다.  라우터 인터페이스 222.222.222.220은 받은 데이터그램의 우측 서브넷 소속인 것을 알고, 222.222.222.222의 MAC 주소를 ARP 테이블에서 가져와 프레임으로 만들고 스위치에 보내 서브넷 내부에 전파한다.          이때 ARP 테이블에 없으면 역시나 ARP 프로토콜을 통해서 가져온다.      이더넷에서의 ARP의 행동은 RFC 826에 적혀있으며, ARP TCP/IP 튜토리얼은 RFC 1180에 존재한다.6.4.2 이더넷(Ethernet)유선 LAN 기술은 token ring, FDDI, ATM 등이 있었지만 단연 최고의 인기는 이더넷(Ethernet)이다.인기의 원인은 다음과 같다.  오래전에 소개된 고속 LAN, 선점 효과  토큰 링, FDDI, ATM은 상대적으로 더 복잡하고 비용이 높은 프로토콜  최신 LAN 기술을 따라잡을 수 있도록 업데이트 되어 왔음  이더넷이 인기가 많자, 이더넷 하드웨어들 또한 가격이 싸지고, 제품이 다양함.원본 이더넷 LAN은 동축 버스(coaxial bus)를 통해서 서로 연결하였고, 브로드캐스트 LAN이라서 전송하는 모든 프레임은 버스에 연결된 모든 어뎁터들에게 전송되었다. (섹션 6.3.2 그래프 참조)그후, 허브(hub)라고 불리우는 비트 정보를 증폭시켜주는 역할만 하는 물리 계층 장비를 이용한 허브 기반 LAN이 유행하였고, 이때도 여전히 브로드캐스트 LAN으로, 충돌이 발생하고, 이를 재전송해줘야 했다.2000년대 초에는 허브 중심을 링크 계층 장치인 스위치로 바꾼 스위치드 이더넷(switched ethernet)이 유행하게 되었고, 충돌이 존재하지 않으며, 패킷을 저장 및 포워드(store-and-forward)하였다.이더넷 프레임 구조 (Ethernet Frame Structure)이더넷 프레임 구조에 대해 알아보기 위해 네트워크를 가정해보자.두 호스트 A, B는 같은 이더넷 LAN(figure 6.17 같은)에 존재하며, 어뎁터 A의 MAC 주소는 AA-AA-AA-AA-AA-AA이고, 어뎁터 B의 MAC 주소는 BB-BB-BB-BB-BB-BB이며, 이더넷 프레임은 페이로드로 IP 데이터그램을 집어 넣어 통신한다. 어뎁터 A가 B에게로 통신하려 한다고 가정하자.이제 figure 6.20을 참조하며 이더넷 프레임 구조를 알아보자.      Data field (데이터 필드, 46 바이트 ~ 1500 바이트)    IP 데이터그램이 담기는 곳, 이더넷의 최대 전송 단위(maximum transmission unit, MTU)는 1500 바이트이며, 이 이상일 경우 호스트가 데이터그램을 여러개로 나누어야 하며, 최소 전송 크기는 46 바이트로, 이 이하일 경우 추가로 의미없는 데이터 집어넣어야 한다. IP 데이터그램의 length 필드를 통해 의미없는 데이터와 데이터 그램을 구분한다.        Destination address (목적지 주소, 6 바이트)    목적지 어뎁터의 MAC 주소가 담기는 곳, 시나리오 같은 경우 BB-BB-BB-BB-BB-BB이며, MAC 브로드캐스트 주소(FF-FF-FF-FF-FF-FF)일 경우 이더넷 LAN 내부의 모든 어뎁터에게 프레임이 복사되어 전달된다. 자신의 MAC 주소와 일치하지 않거나 MAC 브로드 캐스트 주소가 아니면 프레임을 버린다.        Source address (출발지 주소, 6 바이트)    전송을 시작한 출발지 어뎁터의 MAC 주소가 담기는 곳, 이번 경우에는 AA-AA-AA-AA-AA-AA        Type field (종류 필드, 2 바이트)    이더넷이 멀티플렉스할 네트워크 레이어 프로토콜의 고유 코드가 담기는 곳, 보통은 IP이지만 ARP(0806)나 AppleTalk 같은 다른 네트워크 계층 프로토콜이 들어올 수 있다. 이를 보고, 어뎁터는 어느 네트워크 계층 프로토콜로 데이터그램을 보낼지(디멀테플렉스) 결정한다.    이 부분은 네트워크 계층의 데이터그램이나 전달 계층의 포트번호 필드와 비슷한 역할을 한다; 상위 계층 어느 프로토콜로 데이터를 보내야하는가?        Cyclic redundancy check(CRC, 4 바이트)    이전에 배웠던 CRC 필드, 프레임의 비트에러를 탐지한다.        Preamble (서문, 8 바이트)    프레임의 비트들의 처음 시작으로 등장하는 필드로, 총 8 바이트중 7바이트는 10101010이고, 마지막 바이트는 10101011이다. 첫 7바이트는 수신측 어뎁터를 깨우는 역할을 하며, 내장 시계를 송신측 클럭과 동기화한다.    클럭의 동기화는 이더넷 LAN은 특정 수치(10 Mbps, 100 Mbps, 1 Gbps)의 처리율을 노리지만, 언제나 조금씩 처리율의 차이가 나므로, 송신자측에서 1과 0의 반복 신호의 도착 속도를 측정하여 비트 속도를 계산해 송신자 측에서 수신자 측의 클럭과 맞추는 것이다. 비트 동기화라고도 한다.    마지막 바이트는 SFD(프레임의 시작 구분자, Start of Frame delimiter)라고 따로 분류되어 불리우기도 하며, 마지막 두 비트(11)은 이제 Preamble이 끝나고 본문이 시작됨을 알린다. 이를 프레임 동기화라고 한다.  이더넷은 IP와 UDP처럼 비연결성 서비스이며, 데이터 비신뢰성(unreliable service) 서비스이며, 즉 아무런 예고없이 프레임이 도착하고, 해당 프레임이 CRC 에러 감지를 통과하든 못통과하든 ACK나 negative ACK 메시지를 돌려주지 않는다.덕분에 이더넷의 구현이 간단하고 저렴하지만 대신 데이터 흐름에 빈 공간이 생길 수 있다. 이러한 갭은 보통 전달 계층의 TCP가 재전송시켜 해결하거나 UDP처럼 그냥 내버려둔다.이더넷 기술(Ethernet Technologies)사실 이더넷은 IEEE 802.3 CSMA/CD (Ethernet) working group에서 지정한 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX,10GBASE-T, 40GBASE-T 등 많은 접두어(acronyms) 버전이 있으며,각각 앞 부분은 10, 100, 1000, 10G, 40G은 10 Mbit/s, 100 Mbit/s, 1 Gbit/s, 10 Gbit/s, 40 Gbit/s 의 이더넷을 의미하며, BASE는 물리적 매질이 오직 이더넷 트래픽만 사용하는 것을 의미하며, 802.3 기준의 대부분을 차지한다. 마지막 부분은 동축케이블, 광섬유, 구리선 등의 물리적 매질을 의미하며, T의 경우 구리 연선(twisted-pair copper wire)를 의미한다.역사적으로 이더넷은 동축 케이블의 한 부분으로 개발되었으며, 10BASE-2와 10BASE-5 기준은 10Mbps 이더넷에 2가지 종류의 500m 내의 동축 케이블로, 이뤄진다. 이보다 길면 리피터라는 물리 계층 장비를 이용해 입력 신호를 증폭하여 출력해 먼 곳까지 도달하게 했다.동축케이블은 브로드캐스트 매질로, 자원을 공유하며 여러 충돌이 일어났고, 이를 CDMA/CD 프로토콜로 해결하여 LAN을 만들 수 있게 되었다..지금의 이더넷은 과거의 bus기반과 달리 스위치를 이용해 노드들은 연결하며, 사용하며, 속도도 월등히 빨라졌다.  이때는 100m 내에서는 구리 연선, 수킬로 미터 단위는 광섬유를 사용해 연결한다.figure 6.21은 이러한 다른 기준들과 MAC 프로토콜, 프레임 포맷을 보여준다.최신의 기가비트 이더넷은 40000Mbps 급의 처리율을 보이며, 거대한 규모에도 높은 호환성을 보인다.다음은 IEEE 802.3z에서 설명한 기가비트 이더넷의 기준이다.      표준 이더넷 프레임 포맷(fig 6.20)을 사용하고, 10BASE-T, 100BASE-T와 호환이 되어야 한다. 이를 통해 기존의 이더넷 장치 기반에 기가비트 이더넷을 구현하기 쉬워진다.        스위치를 사용하는 지점간(point-to-point) 연결과 허브를 사용하는 공유 브로드캐스트 채널을 사용함. hub는 이더넷에서 버퍼드 분배자(buffered distributor)라고 부른다.        브로드캐스트 채널에는 CSMA/CD 프로토콜 사용하며, 성능 향상을 위해 노드 간의 최대 거리가 엄격히 지켜져야 한다.        지점간 통신에서 40Gbps의 완전 양방향 통신이 가능해야한다.  기가비트 이더넷과 광섬유로 인해 카테고리 5 UTP 케이블을 사용하여 이더넷을 구축할 수 있게 되었다.사실 store-and-forward 패킷 스위칭을 사용하는 최신 스위치 기반 성형 이더넷의 경우, 스위치가 자동으로 전송을 조정해주고, 완전 양방향 통신과, 포워딩 버퍼를 지원하는 등의 이유로, 더이상 MAC(매체 접근 제어, medium access control) 프로토콜을 이용해 충돌을 관리하지 않아도, 스위치가 해준다.하지만 이더넷 프레임 포맷은 전혀 변하지 않았으므로, 여전히 이더넷의 정의에 부합한다.6.4.3 연결 계층 스위치 (Link-Layer Switches)스위치는 연결 계층 프레임을 입력받아 나가는 링크로 포워딩하는 장치이며, 서브넷 내의 호스트나 라우터들에게 투명(transparent)하다.이 뜻은 호스트나 라우터가 프레임을 보낼 때, 스위치의 존재와 포워딩 기능 등을 전혀 신경쓰지 않는다는 의미이다.스위치 또한 갑작스런 통신 증가에 따른 부하를 방지하기 위해 버퍼(buffer)를 가지고 있다.포워딩과 필터링 (Forwarding and Filtering)필터링(filtering)은 스위치의 기능으로, 프레임을 포워딩 또는 버림을 결정하는 것이고, 포워딩(forwarding)은 또한 스위치의 기능으로 프레임이 어떤 인터페이스로 전달되어야 하는지 스위치 테이블을 통해서 결정한다.스위치 테이블은 항목들을 가지고 있지만 LAN 내부의 모든 필요한 호스트와 라우터 정보를 가지고 있지 않다.스위치 내부의 항목은 각각 (1) MAC 주소, (2)해당 MAC 주소가 포워딩 되야하는 스위치 인터페이스, (3) 항목 생성 시간아래 figure 6.22는 figure 6.15(책의 오류인듯, 해당하는 주소를 가진 figure가 이 장에 없음) 네트워크의 스위치 테이블 예시이다.            주소      인터페이스      시간                  62-FE-F7-11-89-A3      1      9:32              7C-BA-B2-B4-91-10      3      9:36              …      …      …      [Figure 6.22 상단 스위치 테이블 일부 (Portion of a switch table for the uppermost switch in Figure 6.15)]이는 네트워크 계층 라우터의 라우팅 테이블과 비슷하게 생겼는데, 실제 현재의 SDN 스위치들은 설정을 통해 네트워크 계층 데이터그램 스위치로 쓸 것인지, 연결 계층 프레임 스위치로 쓸지 설정할 수 있다. 하지만 우리는 여기서는 SDN을 사용하지 않는 전통적인 스위치 테이블을 이용할 것이다.먼저 스위치 인터페이스 x에 도착한 데이터그램의 목적지 MAC 주소가 DD-DD-DD-DD-DD-DD라고 가정하고 세가지 시나리오를 알아보자.  DD-DD-DD-DD-DD-DD 주소에 관한 항목이 x의 스위치 테이블에 없는 경우, 스위치는 프레임의 복사본을 인터페이스 x(왔던 길)을 제외한 모든 인터페이스에 보낸다.(=프레임을 브로드캐스트 한다.)  DD-DD-DD-DD-DD-DD 주소에 관한 항목이 x의 스위치 테이블에 있고, 해당 주소가 가야하는 인터페이스가 x 자신일 경우, 스위치가 필터링 기능을 수행하여 프레임을 버린다.  DD-DD-DD-DD-DD-DD 주소에 관한 항목이 x의 스위치 테이블에 있고, 해당 주소가 가야하는 인터페이스가 x 자신이 아닌 경우, 프레임이 목적지가 있는 LAN 부분의 인터페이스로 포워딩되고, 프레임은 해당 인터페이스의 출력 버퍼에 들어간다.스위치는 허브보다 위와 같은 기능을 추가로 수행한다.자가 학습(Self-Learning)스위치의 스위치 테이블을 처음 부터 가지고 있지않으며, 라우터와 같이 라우팅 프로토콜도 실행하지 않는다.대신 스위치 테이블은 자동, 동적, 자체적으로 테이블을 생성하며, 이를 자가 학습(Self-Learning)이라고 한다.  스위치 테이블은 시작시 비어있다.  인터페이스에 프레임이 도착할 때마다, (1) source address 필드의 MAC 주소와, (2) 프레임이 입력된 인터페이스, (3) 현재 시간을 이용해 테이블에 기록한다. 만약 LAN 내부의 모든 호스트가 프레임을 보냈다면, 모든 호스트에 대한 항목을 가진 테이블이 완성된다.  일정 시간마다(이를 에이징 시간(aging time)이라고 한다.), 특정 주소로 부터 프레임이 오지 않으면, 해당 주소가 출발지 주소인 항목을 지워버린다. 이를 통해 호스트가 추가, 제거, 변경되는 상황에 대비할 수 있다.            주소      인터페이스      시간                  01-12-23-34-45-56      2      9:39              62-FE-F7-11-89-A3      1      9:32              7C-BA-B2-B4-91-10      3      9:36              …      ….      …      [Figure 6.23 스위치가 01-12-23-34-45-56의 주소를 가진 어답터의 위치를 학습(Switch learns about the location of an adapter with address 01-12-23-34-45-56)]스위치는 plug-and-play 장치로, 네트워크 관리자나 사용자의 관심이 필요 없으며, LAN 세그먼트(랜의 연결선?)에 스위치를 아무 설정없이 설치하기만 하면 된다.스위치는 완전 양방향 통신(full-duplex)로, 스위치 인터페이스는 동시에 프레임을 보내거나 받을 수 있다.연결 계층의 스위칭 속성(Properties of Link-Layer Switching)버스나 허브 기반 성형 구조 같은 브로드캐스트 링크보다, 연결 계층 스위치를 사용하는 것의 장점은 다음과 같다.      충돌 없음(Elimination of collisions)    스위치 기반 LAN은 충돌에 의한 대역폭 손실이 없다.    스위치가 프레임을 버퍼에 집어넣어 순서대로 LAN 세그먼트에 보내주므로 효율적이다.        다차원적 링크(Heterogeneous links)    스위치로 인해 링크들이 분리되어 있으므로, 다른 매체나 속도로 링크를 고를 수 있다. 와이파이와 광섬유, 구리 연선이 섞여있는 스위치가 존재할 수 있다. 이를 통해 레거시 시스템으로 인한 다른 장치의 시스템에 새로운 장치를 추가할 수 있다.        관리(Management)    추가적인 보안성과 쉬운 네트워크 관리체계를 제공한다.    만약 어뎁터가 이상해서 끊임없이 프레임을 보낸다면(jabeering 어뎁터라고 함), 스위치가 그걸 감지하고 해당 어뎁터를 무시할 수 있다.    스위치가 트래픽, 충돌율, 링크 상태 등의 통계 데이터를 모아 관리에 도움을 줄 수 있다.  SNIFFING A SWITCHED LAN: SWITCH POISONING스위치는 프레임을 모든 호스트에게 브로드캐스트하지 않으므로 패킷 스니핑에 강하다.공격자가 목적지 주소가 브로드캐스트 주소(FF-FF-FF-FF-FF-FF)로 되어있고, 출발지 주소는 무작위 가짜 주소로 되어있는 프레임을 스위치에 마구 보내면, 스위치는 그 내용을 토대로 의미없는 항목을 스위치 테이블에 계속 넣게 되고, 이에 따라 결국 정상적인 호스트들의 주소는 추가되지 않게 되며, 이를 스위치 포이즈닝(switch poisoning)이라고 한다.또한, 이러한 프레임은 브로드캐스트 되기 때문에 공격자에게 알림이 간다.복잡한 공격에도 대비할 수 있는 스위치가 상대적으로 허브나 무선 랜보다 보안상 낫다.스위치 VS 라우터(Switches Versus Routers)스위치와 라우터는 일부 공통점과 차이점을 아래처럼 공유한다.                   스위치      라우터(비 SDN 계열)                  계층      연결 계층      네트워크 계층              대상      프레임      데이터그램              사용 주소      MAC 주소      IP 주소              하는 역할      패킷을 올바른 링크로 보냄      패킷을 올바른 링크로 보냄              스위칭 방식      버퍼를 이용한 store-and-forward      버퍼를 이용한 store-and-forward      하지만 오픈플로우를 사용하는 라우터는 IP 주소뿐만 아니라 프레임, 데이터그램, 세그먼트의 필드를 활용한다.그러므로, 네트워크를 만들 때 스위치와 라우터 중에 사용할 장치를 잘고려해서 설치해야 한다.장단점은 다음과 같다.                   스위치      라우터                  장점      plug-and-play 방식, 따로 설정해줄 필요 없음, 필터링과 포워딩 속도가 빠름 (네트워크 계층이 구현되지 않아 오버헤드가 적음)      IP 주소는 계층적 구조+ IP 데이터그램 TTL 헤더덕분에 설정을 잘못하지 않는 이상, 패킷의 무한 순환이나 중복 경로가 없음, 네트워크 구조가 스패닝 트리(spanning tree: 노드간에 군더더기없는 엣지 없이 최소한의 링크로 연결)로 제한되지 않고 다양한 구조가 가능(방화벽, bypass 링크)              단점      프레임 브로드캐스트 순환(cycling of broadcast frames: 프레임이 도착하지 못하고 끊임없이 네트워크 내부를 돌아 성능을 잡아먹음)을 막기 위해 스위치드 네트워크의 형상이 스패닝 트리로 제한되어있음, 네트워크 규모가 커지면 큰 ARP 표가 호스트와 라우터에 필요하고, ARP 트래픽과 프로세싱이 성능을 잡아먹음, 브로드캐스트 폭풍(broadcast strom: 호스트 하나가 오작동으로 끊임없이 브로드캐스트 프레임을 보내고 스위치가 이를 브로드캐스하여 네트워크가 마비되는 현상)에 취약      plug-and-play 방식이 아님, 설치시 설정 필요,  패킷 처리 시간이 비교적 오래 걸림, 발음을 라우터로 부를지 루터로 부를지 논란이 있음      주로 LAN 세그먼트(링크, 스위치 같은 장비를 의미하는 듯?)가 적은 소규모 네트워크의 경우, 스위치가 만들어내는 브로드캐스트 트래픽을 감당할만 하므로, IP 설정이 필요없고 성능이 더 빠른 스위치 덕분에 스위치를 도입하는게 더욱 좋다.하지만 규모가 커지면 라우터의 트래픽 고립 기능, 브로드캐스트 폭풍 방지, 좀 더 지능적인 라우팅을 위해 스위치에 추가적으로 라우터를 섞어 사용한다.                   허브      라우터      스위치                  트래픽 고립(Traffic isolation)      X      O      O              플러그 앤 플레이(Plug and play)      O      X      O              최적 라우팅(Optimal routing)      X      O      X      [Table 6.1 흔한 상호 연결 장치의 전형적인 특징 비교 (Comparison of the typical features of popular interconnection devices)]추가적인 장단점과 스위치드 LAN을 이용한 이더넷의 성능향상은 [Meyers 2004; Kim 2008]에서 볼 수 있다.6.4.4 가상 근거리 네트워크(VLANs) (Virtual Local Area Networks(VLANs))최신 기업 LAN들은 마치 figure 6.15 처럼 각 부서별로 계별 스위치드 LAN을 만든 뒤, 서로 계층적으로 연결 시켜 계층적 구조를 이루기도 한다.계층적 구조는 이상과 달리 세가지 단점이 있는데.      트래픽 고립의 부족(Lack of traffic isolation)    계층 구조가 그룹의 트래픽을 각자 스위치에 고립하더라도, ARP나 DHCP, 자가 학습되지 않은 프레임의 브로드캐스트 트래픽에 의해 LAN의 성능이 떨어질 수 있으므로, 세심한 트래픽 고립이 필요하다.    트래픽 고립은 추가로, 인사과의 메시지는 다른 부서에 절대로 가지못하게 하는 등의 보안이나 프라이버시 등에 사용될 수 있다.    이러한 트래픽 고립을 수행하려면 보통 라우터를 이용해야하지만, 나중에 스위치를 이용한 방법을 배워볼 것이다.        스위치들의 비효율적인 사용(Inefficient use of switches)    계층적 구조에서는 각 그룹마다 트래픽 고립을 위해 스위치가 하나씩 필요한데, 규모가 작은 그룹이 많으면, 할당된 스위치의 성능과 포트가 남아돌 수 있다.        사용자 관리(Managing users)    만약 사용자가 다른 그룹으로 이동한다면 물리적인 연결선을 바꿔줘야 하며, 두 그룹에 속해야하는 사용자는 더 큰 문제다.  이와 같은 문제들은 VLAN(가상 근거리 네트워크,virtual local area networks)을 지원하는 스위치를 통해 해결할 수 있다.VLAN은 물리적으로 하나에 속하는 LAN을 가상으로 여러개로 나누며, 호스트들은 같은 VLAN에 속해야만 서로 통신할 수 있다.아 figure 6.25같은 경우, 포트 기반 VLAN으로, 일정 포트 번호들을 네트워크 관리자가 VLAN으로 지정하며, 각 VLAN들은 브로드캐스트 도메인을 형성하여 같은 도메인끼리만 연결가능하다.예를 들어 아래에는 2~8 포트는 전기공학 부서, 9~15는 컴퓨터공학 부서에 할당하여 VLAN을 형성하였고, 할당되지 않은 1, 16번은 기본 VLAN 그룹이 된다.만약 8번 포트 전기공학 사용자가 컴퓨터 공학으로 옮기게 되면, 물리적으로 바꿔낄 필요없이 단순히 8번 포트의 소속을 컴퓨터 공학 VLAN으로 옮기면된다.이렇게 스위치 관리 소프트웨어로 port-to-VLAN 매핑이 된 표를 유지 관리하여, 스위치의 성능 및 포트 낭비 없이 여러 VLAN을 형성할 수 있다.만약 각기 다른 VLAN의 호스타가 통신할 수 있게 설정하고 싶다면,첫번째 방법은 설정하지 않은 1, 16번 같은 포트에 추가적인 라우터를 연결하고, 연결된 포트를 양쪽 VLAN에 모두 속하게 한 뒤, 라우터를 통해 통신하게 끔 하면 된다.두번째 방법은 스위치와 라우터 기능을 둘다할 수 있는 장치의 경우, 위와 같은 기능을 가상으로 할 수 있게 제공하는 경우가 많다.이번에는 물리적으로 다른 곳에 있는 호스트드과의 VLAN 설정에 대해 알아보자.figure 6.26은 두 부서에 속한 호스트들 중 일부는 포트 8개의 스위치에 속한 상황에서 VLAN 설정을 하는 그림이다.첫번째 방법은 figure 6.26(a)처럼 각 스위치에 부서마다 하나씩 대표 포트를 설정한 뒤, 이를 서로 연결하는 방법이다. 이 방법은 간단하지만 부서 N개에 총 N개의 포트가 필요하고, 스케일링이 힘들다는 단점이 있다.두번째 방법은 figure 6.26(b)처럼 VLAN 트렁킹(VLAN trunking)을 이용하는 것이다.각 스위치에 하나씩 모든 부서에 속하는 트렁크 포트를 설정하고, 이를 스위치 간에 연결하면, VLAN 내부의 브로드캐스트 되는 프레임은 트렁크 포트를 통해 다른 스위치로 전달된다.하지만 스위치는 트렁크를 통해서 들어온 프레임을 알맞은 부서에 브로드캐스트해줘야 하는데, 이를 위해 확장된 이더넷 프레임 형식인 802.1Q에는 4바이트의 VLAN tag 필드를 추가하여 VLAN을 구분하게 만들었다.figure 6.27에 보이는 것 처럼 프레임이 VLAN 트렁크를 통과하려 할때, 송신자 측 스위치에서 VLAN tag를 추가하고, 수신자 측 스위치에서 VLAN tag를 확인하고 없앤다.2바이트의 TPID(태그 프로토콜 식별자, Tag Protocol Identifier) 필드(보통, 16진수로 표현 xx-xx)와 2바이트의 Tag Control Information(태그 제어 정보)필드로 이루어져 있고, Tag Control Information은 12비트의 VLAN identifier 필드와 IP의 TOS 필드와 비슷한 3비트의 priority(우선순위) 필드로 이루어져 있다.  나머지 1비트는 CFI(canonical format identifier) 필드로, 이터넷의 경우 0, 토큰 링의 경우 1포트 기반 VLAN 이외의 방법은 다음이 있다.MAC 기반 VLAN은 특정 MAC 주소를 VLAN 그룹으로 묶는 방식이며, 포트에 장치가 연결될 때마다 해당 장치의 MAC 주소를 읽고 해당 포트를 VLAN 그룹에 추가하는 방식이다.네트워크 계층 주소 기반 VLAN과 IP 라우터를 이용한 LAN들을 묶은 VLAN이 존재한다.6.5 연결 가상화: 연결 계층으로서의 네트워크 (Link Virtualization: A Network as a Link Layer)지금까지 학습을 통해 링크를 단순한 물리적 도선에서 부터 복잡한 스위치 구조까지 인식을 올리게 되었다.하지만 호스트 입장에서 연결 계층의 일은 신경 쓰지 않아도 되므로, 어떠한 매질의 연결인지, 공유되는 브로드 캐스트 채널인지, LAN 세그먼트로 연결 되었느닞, 아니면 스위치드 LAN이나 VLAN인지 신경쓸 필요가 없다.MPLS (다중 프로토콜 레이블 스위칭 Multiprotocol Label Switching) 네트워크는 패킷 스위칭을 이용한 가상의 서킷 스위치 네트워크로 고유한 패킷 포맷과 포워딩 행동을 가지고 있다.MPLS는 IP 장비들을 연결해주는 연결 계층 기술처럼 볼 수 있으며, Frame-relay와 ATM이 비슷한 역할을 하지만 이번엔 다루지 않는다.6.5.1 다중 프로토콜 레이블 스위칭(MPLS) (Multiprotocol Label Switching (MPLS))다중 프로토콜 레이블 스위칭(MPLS)은 가상 서킷 네트워크의 키인 고정 길이 레이블(fixed-length-label)을 이용해 IP 라우터의 포워딩 속도를 높이기위해 등장하였다.목적은 IP의 목적지 기반 포워딩을 대체하는 것이 아니라, 라우터에게 포워딩의 새로운 선택권을 주도록 하는 것이었으며, IP 주소와 라우팅에 많은 영향을 받았고, 이러한 VC(가상 서킷, Virtual Circuit) 기술과 데이터그램 라우팅의 결실이 MPLS 프로토콜로 나타났다.위 figure 6.28은 MPLS 헤더의 위치와 구조에 대해 잘알 수 있다.MPLS 헤더는 MPLS 지원 라우터에 의해 연결 계층과 네트워크 계층 헤더 사이에 추가되며, 레이블 필드, 실험용으로 남긴 3 비트, 쌓여진 MPLS 헤더의 연속의 끝을 알리는 1 비트의 S 필드, TTL 필드로 이루어져 있다.MPLS 헤더는 MPLS 지원 라우터 간의 연결일때만, 라우터에 의해 추가된다.  즉, 송신 라우터와 수신 라우터가 둘다 MPLS 지원 라우터일 때만 추가되며, 지원하지 않는 라우터 입장에서는 IP 헤더가 있어야할 자리에 오류가 난 비트가 있는거롤 보일 것이다.MPLS 헤더는 IP 주소 대신 Label 필드를 보고 빠르게 라우팅하므로 레이블 스위치드 라우터(Label switched router)라고도 부른다.즉, MPLS 지원 라우터는 굳이 목적지 IP 주소를 보고 포워드 테이블을 보고 최장 접두어 매치를 할 필요가 없다.어떻게 Label 필드로 빠르게 라우팅하는 지, 상대가 MPLS 지원 라우터인지는 어떻게 아는 지 등을 알아보기 위해 figure 6.29 같은 시나리오를 알아보자.그림의 파란색 라우터(R1~R4)는 MPLS 지원 라우터이다.먼저 연결된 MPLS 지원 라우터들은 자신이 갈 수 있는 MPLS 라우터가 아닌 링크와 필요한 레이블에 대한 정보를 주변 이웃의 다른 MPLS 지원 라우터 한정으로 전파한다.예를 들어 R1은 레이블 6을 포함한 패킷은 A로 갈수 있게 할 것임을 R2와 R3에 알려주고, 이 정보를 받은 R2와 R3는 자신에게 각각 레이블 8번과 레이블 12번으로 온 패킷을 레이블 6으로 바꾸어 A에게 넘겨주는 형식으로 A로 갈 수 있음을 R4에게 알려준다.이제 R4는 A로 갈 수 있는 레이블 변경 경로를 2개나 알고 있게 된다.MPLS 구조는 ATM과 스위치드 LAN 처럼 패킷의 IP 헤더를 이용하지 않고, A, D 그리고 R5, R6 사이를 이어주고 있다.자신의 레이블 정보를 다른 라우터에게 보내는 RSVP-TE[RFC 3209] 프로토콜(시그널링(signaling)이라고도 함),MPLS 라우터들의 구조 안에서 최적의 경로를 계산하는 프로토콜,  실제로 공시적인 프로토콜은 없고, 네트워크 장치 회사마다 알아서 구현한다.경로 계산을 위해 연결 상태 정보를 수집하는 프로토콜  보통 기존의 OSPF 같은 연결 상태 라우팅 알고리즘이 MPLS 지원 라우터에게 정보를 넘겨 주도록(flooding: 자신에게 패킷을 준 경로 이외의 경로들에게 패킷을 복사해서 뿌리는 알고리즘) 확장되있다.등의 자세한 내용은 다루지 않겠다.MPLS의 포워딩 속도 상승 이외의 또 다른 강력한 점은 트래픽 관리 능력(traffic management capability)의 향상이다. IP 라우팅 프로토콜은 언제나 최소 비용의 경로 하나만을 선택하지만, MPLS는 여러 경로를 선정하고 패킷의 순서, 정책, 성능, 트래픽 상황 등에 따라 다른 경로 또한 가능하게 만들어져있다. 이를 MPLS를 이용한 트래픽 공학(traffic engineering)이라고 한다.[RFC 3346; RFC 3272; RFC 2702; Xiao 2000]MPLS를 이용해서 링크 연결이 실패할 시에 대비해 미리 계산한 예비 경로로 트래픽을 보낼 수도 있고, VPN(가상 사유 네트워크, virtual private network)를 구현하는데 사용할 수 있다.  VPN 서비스를 제공 ISP는  각 사용자들의 VPN을 MPLS 네트워크를 통해, 이를 통해 각 네트워크의 자원과 주소, 링크 상태를 격리시키면서 동시에 서로 연결시켜줄 수 있다.MPLS는 현재 나중에 개발된 SDN, 일반화된 포워딩 패러다임과 여러 기능이 겹치며 경쟁중이다.6.6 데이터 센터 네트워킹 (Data Center Networking)구글, 아마존, 같은 거대 기업들은 수 천의 호스트로 이루어진 데이터 센터들을 외부 뿐만 아니라 내부적으로 서로 연결하여 데이터 센터 네트워크(Data Center Networking)를 형성했다.클라우드 어플리케이션을 위한 데이터 센터 네트워킹에 대해서 배워보자.데이터 센터는 보통 세가지 이유로 만든다.  웹 페이지, 검색 결과, 이메일, 비디오 등의 컨텐츠를 유저들에게 보내기 위해  분산 검색 엔진 색인 계산 같은 빅데이터 처리를 위한 거대 병력 컴퓨팅 자원 확보  다른 회사에게 클라우드 컴퓨팅 서비스 제공6.6.1 데이터 센터 구조 (Data Center Architectures)데이터 센터 설계는 보통 회사의 이득을 위해 극비에 부쳐져있다. 10만 호스트 데이터 센터의 경우 월 12억원 정도의 비용이 들어가며,전체 비용의 45%는 3~4년 주기로 교체하는 호스트 서버 교체 비용, 25%는 변압기, UPS(무중단 전력 공급, uninterruptable power supplies) 시스템, 예비 발전기, 냉각 시스템 같은 기반 시설, 15%는 전기 사용을 위한 유틸리티, 15%는 스위치, 라우터, 부하 밸런서, 연결선, 트래픽 이동용 네트워크 장비 등에 쓰인다.네트워크 비용은 가장 큰 비용은 아니지만, 성능을 늘리면서 비용은 줄일 수 있다.호스트들은 일벌과 같은 존재로, 블레이드(blades)라고 불리우는 피자박스처럼 생긴 CPU, 메모리, 디스크 등을 포함하고 있는 상용 호스트를 이용한다.블레이드는 랙(rack)이라고 불리우는 곳에 20~40개가 적층되며, 랙의 꼭대기는 TOR(랙의 꼭대기, Top of Rack) 스위치라고 불리우는 스위치가 위치하여, 자신이 속한 랙을 다른 랙, 데이터센터 내의 다른 스위치들과 연결하는 역할을 한다.블레이드 호스트들은 자신의 렉에 위치한 TOR 스위치와 40 ~ 100 Gbps 수준의 이터넷 연결이 되고, 각자 데이터 센터 내의 유일한 IP 주소를 가지고 있다.데이터 센터 네트워크의 트래픽은 크게 호스트와 외부 클라이언트와의 트래픽, 내부 호스트 간의 트래픽으로 트래픽을 나누며,데이터 센터 네트워크는 한 개 이상의 경계 라우터(border router)를 이용해 외부 인터넷과 연결된다.이러한 데이터센터 네트워크와 프로토콜, 랙과 경계 라우터들의 연경 방법을 데이터 센터 네트워크 설계는 최근 아주 중요하게 연구되고 있다.부하 밸런싱 (Load Balancing)클라우드 데이터 센터는 이메일, 검색, 비디오 같은 여러 응용 프로그램 서비스를 동시에 제공하기 위해 존재하며, 공개 IP 주소를 통해 외부 클라이언트들로부터 요청을 받고, 이를 처리한다.외부 클라이언트의 요청들은 제일 먼저 부하 밸런서(Load Balancer)에게 보내지고 호스트들에게 호스트 부하를 고려하여 분산되어 전해진다.거대한 데이터 센터에는 여러개의 부하 밸런서가 특정 클라우드 서비스에 소속되어 존재하며, 부하 밸런서는 목적지 포트 번호와 IP 주소를 이용해 특정 호스트에게 포워딩하므로 “4 계층(전달 계층) 스위치(layer-4 switch)”라고도 불리운다.단순 성능 뿐만 아니라 NAT와 비슷하게 외부 IP 주소에 따라 특정 호스트로 연결해주거나 아니면 그 반대의 역할을 수행할 수 있으며, 이를 통해 높은 보안과 클라이언트가 직접 호스트와 연결되거나 내부 구조를 파악하는 행동 방지 등이 가능하다.계층적 구조 (Hierarchical Architecture)작은 규모의 데이터센터와 달리 큰 규모의 데이터 센터는 쉬운 확장과 효율적인 연결을 위해 계층적 구조(Hierarchical Architecture)로 만들 수 있다.경계 라우터(Border router)가 각각 접근 라우터(access rotuer)들과 연결되고, 각 접근 라우터들은 Top-tier 스위치와, Tier-1 스위치는 그아래 Tier 2 스위치와 연결되는 식으로 반복하여 계층 구조를 만든다.  이때 부하 밸런서(load balancer)는 TOR 스위치와 함께 Tier-2 스위치에 붙는다.모든 연결들은 보통 이터넷과 물리 계층 프로토콜로 만들어졌으며, 라우터에 원하는 규모의 특정 티어 스위치를 추가하는 것으로 쉽게 스케일링할 수 있다.가동율을 높이기 위해 계층적 구조 이외의 추가적인 연결을 다른 라우터와 만들기도 하고, ARP 브로드캐스트 트래픽을 고립시키기 위해 각 서브넷은 추가로 여러 수백 호스트 규모의 VLAN 서브넷으로 나뉜다.하지만 이런 구조는 하위의 호스트들이 동시에 통신을 시작하면 호스트간 전송 능력이 제한되는 문제가 발생하는데, 예를 들면 figure 6.30  Tier-2 스위치 A 하의 호스트들과 Tier-2 스위치 C 하의 호스트들이 동시에 수많은 패킷을 서로 교환하면 링크 AB와 링크 AC가 붐비게 된다.이러한 문제를 막기 위해 다음과 같은 방법이 있는데      고성능 스위치와 라우터, 링크로 교체하기 X    이는 비용이 비싸지고 관리가 어려우며, 교체에 힘이 든다는 단점이 있다.        호스트간 패킷 교환이 많은 관련 서비스별 호스트들이나 데이터별 호스트들을 지리상, 라우터 홉상 가까운 위치에 모아놓기 X    이는 데이터 센터 서비스의 유연성을 저해할 수 있다. 예를 들어 사용자들의 가상 머신 서비스를 돌리는 호스트들을 같은 서비스로 구분해 모아놓으면, 해당 데이터센터와 지정학적으로 먼 사용자들에게는 성능 저하가 일어날 것이다.        스위치 간의 연결성을 증가시킨다. O    figure 6.31처럼 자신의 바로 직속 스위치들과의 연결 뿐만 아니라 다른 스위치들과의 연결을 추가시켜 네트워크의 역량과 신뢰성을 모두 올릴 수 있다.    페이스북의 경우 각 TOR 스위치는 다른 4개의 tier-2 스위치와 연결하도록 되어있고, 각 tier-2 스위치는 다른 tier-1 스위치와 연결되도록 하고 있다.  스위치간 연결성을 증대시킬 시, 다중 경로 라우팅(multi-path routing)이 정말 중요한 프로토콜이 된다.보통 다중 경로 라우팅 프로콜들은 플로우(흐름, flow) 별로 여러 경로를 설정할 수 있게 하며,예시를 들자면ECMP(동비용 다중 경로, Equal Cost Multi Path) [RFC 2992]는 무작위로 다음 홉의 스위치를 골라서 다중 경로를 생성한다.세분화된 부하 밸런싱을 이용해 다중 경로 생성이 가능하다.[Alizadeh 2014; Noormohammadpour 2018]플로우 수준이 아니라 같은 플로우 내의 다른 패킷들을 각각 다른 경로로 보내는 방법도 연구 중이다.[He 2015; Raiciu 2010]6.6.2 데이터 센터 네트워크의 트렌드 (Trends in Data Center Networking)데이터 센터 네트워크는 비용 절감, 가상화, 물리적 제한, 모듈성, 커스터마이제이션을 위해 빠르게 진화하고 있다.비용 절감 (Cost Reductioin)비용과 지연 시간 절감과 처리율, 확장성, 배포의 향상을 위해 회사 개인, 또는 오픈소스로 많은 데이터 센터 네트워크 설계가 연구되고 있다.앞서 figure 6.31에서 배웠던 계층 구조내 스위치 간 고연결성을 이룩한 데이터 센터 네트워크는 크로스바, 또는 망형 구조의 라우터와 비슷한 원리로 구현되며, 다중 경로 라우팅으로 인한 성능과 신뢰성 증대가 장점이다.이러한 구조는 많은 량의 작은 스위치들의 상호연결로 이루어지며, 거대기업들이 자주 사용하며 구조 전체가 세트로 하나의 장치로써 팔리기도한다.다중 스위치 계층 상호연결 구조는 과거 전화 연결 스위칭을 연구하던 Charles Clos의 이름을 따 Clos 네트워크라고도 불리우며, 데이터 센터 네트워크와 다중프로세서 상호 연결 네트워크에서 계속 연구되고, 사용되고 있다.중앙화된 SDN 제어와 관리 (Centralized SDN Control and Management)데이터 센터 네트워크는 주로 거대 기업에서 중앙집중식으로 관리되기 때문에 SDN 형태의 논리적 중앙화된 제어가 연구되고 있다.앞서 우리가 SDN에서 배웠던 것처럼, 소프트웨어 기반 중앙 통제와 경로 계산을 하는 컨트롤 측면과, 개개의 상용 스위치의 라우팅을 의미하는 데이터 측면으로 나누는 구조를 가지고 있으며, 데이터 센터들의 거대한 스케일로 인해 자동화된 설정과 운영 상태 관리도 중요하다.가상화 (Virtualization)가상화는 가상 머신(VM, Virtual Machine)을 이용해 소프트웨어를 물리적인 한계에서 벗어나 좀더 유연하고 관리가 편한 네트워크를 생성할 수 있다.물리적으로 떨어져 위치한 서버 사이에 VM을 이동시키거나, 전체 네트워크를 하나의 레이어로 보는 등의 일이 가능하다.물리적 제한 (Physical Constraints)데이터 센터 네트워크는 인터넷과 달리 강력한 성능과 낮은 지연을 가지고 있으므로, 버퍼 크기 같은 기존의 하드웨어 설정과 TCP 혼잡 제어 같은 기존의 프로토콜이 잘 맞지 않을 수 있다.데이터 센터에서는 혼잡 제어 반응 속도가 빠르고, 낮은 손실하에 진행되어야 하며, 타임 아웃이나 fast-recovert 같은 상태는 비효율적일 이다.이를 위해 TCP 프로토콜에서 파생된 전용 프로토콜이나 RDMA(원격 직접 메모리 접근, Remote Direct Memory Access) 기술 등이 연구되고 있다.스케쥴링 또한 적용되어 흐름 스케듈링과 전송율 제어가 분리되어 간단하게 흐름 제어를 함과 동시에 높은 가동율을 달성할 수 있다.하드웨어 모듈성과 커스터마이제이션 (Hardware Modularity and Customization)운송용 컨테이너 기반 모듈형 데이터 센터(MDC, modular data center)는 12 미터 길이의 운송용 컨테이너에 십여개의 랙, 수천개의 호스트가 들어가 있는 공장 생산 소형 데이터센터이다.여러 MDC를 연결에 추가 및 제거하는 것으로 스케일링 할 수 있으며, MDC는 기기 이상으로 네트워크 성능이 저하됬을 때를 감지하여 연산량을 줄이거나, 제거되는 기능을 갖추었다.수많은 컨테이너를 중심 네트워크에 연결하는 것 이외에도 컨테이너 내의 수천 호스트를 서로 연결하는 것은 힘들일이고 많은 연구가 진행되고 있다.또한 각 데이터 센터마다, 스위치, 어뎁터, TOR, 프로토콜, 소프트웨어를 실정에 맞게 커스터마이징하는 것 또한 트랜드이다.예를 들어 아마존은 신뢰성을 늘리기 위해 사용 가능 지역이라는 수 키로 떨어져있는 빌딩에 데이터센터를 복제한 듯한 데이터센터를 이용해 시스템 장애와 성능 이슈에 대항하고 있다.6.7 회고 : 웹 페이지 요청의 하루(Retrospective: A Day in the Life of a Web Page Request)이제 이 책의 프로토콜 스택에 대한 이야기는 끝났으며, 전체적이고 집약된 프로토콜의 정리를 보기 위해 예시 시나리오를 만들어서 설명해보자.Figure 6.32는 학생이 학교의 이터넷에 노트북으로 접속한 뒤, 웹페이지를 요청하는 시나리오이다.6.7.1  시작하기 : DHCP, UDP, IP, 이터넷(Getting Started: DHCP, UDP, IP, and Ethernet)학생은 이터넷 케이블을 통해 학교의 이터넷 스위치에 접속하고, 이 스위치는 DHCP 서버를 겸하고 있는 학교 라우터를 거쳐 ISP comcast와 연결되어 있으며, comcast.net은 자신의 네트워크로 학교에 DNS 서버와 DNS 서비스를 제공한다고 가정하자.먼저 학생은 IP 주소를 얻기위해 DHCP 프로토콜을 돌리게 된다.      이터넷 케이블을 꽂은 뒤, 학생의 노트북의 운영체제가 DHCP 요청 메시지를 생성하고, UDP 세그먼트에 집어넣은 후, IP 데이터그램으로 캡슐화한다.    IP 데이터그램의 address 필드는 출발지의 경우 IP 주소 0.0.0.0, 포트 번호 68, 목적지의 경우  IP 주소 255.255.255.255,포트 번호 67 이다.        DHCP 요청 메시지가 담긴 IP 데이터그램이 이더넷 프레임에 캡슐화된 후, 목적지 MAC 주소를 FF:FF:FF:FF:FF:FF로 놓아 스위치에 연결된 기기들 모두에게 브로드캐스트되도록 한다. 출발지 주소는 학생의 노트북 MAC 주소이며, 여기서는 00:16:D3:23:68:8A로 가정한다.        브로드캐스트 된 이터넷 DHCP 요청 프레임이 스위치에 도착하면 스위치는 해당 프레임을 복사하여 학교 라우터가 연결된 포트를 포함해 모든 나가는 포트에 뿌린다.        라우터는 DHCP 요청 메시지가 들어있는 이더넷 프레임을 라우터 인터페이스(MAC 주소 00:22:6B:45:1F:1B)로 받고, IP 주소를 추출한다.    브로드캐스트 IP이므로 라우터는 해당 데이터그램을 디멀티플렉스하여 세그먼트로 만든 뒤,    UDP 프로토콜에 보내고, DHCP 서버는 세그먼트 내의 요청 메시지를 확인한다.        DHCP 서버가 Comcast ISP 측에게 할당 받은 IP 주소 범위의 CIDR 표현은 68.85.2.0/24이며, 68.85.2.101을 학생의 노트북에 할당하기로 했다.    DHCP 서버는 할당할 IP 주소(68.85.2.101), DNS 서버 주소(68.87.71.226), 기본 게이트웨이 라우터(68.85.2.1, 현재 DHCP 서버가 존재하는 라우터이기도 하다.), 서브넷 범위(68.85.2.0/24, 또는 네트워크 마스크) 정보들이 들어가 있는 DHCP ACK 메시지를 생성 뒤,    UDP 세그먼트와 IP 데이터그램, 이터넷 프레임으로 차례차례 캡슐화 하고, 출발지 MAC 주소는 라우터의 주소(00:22:6B:45:1F:1B), 목적지 MAC 주소는 학생의 노트북 주소 (00:16:D3:23:68:8A)로 지정한다.        DHCP ACK 메시지를 포함한 이터넷 프레임은 유니캐스트(unicast, 1:1 통신)를 통해 라우터에서 스위치로 전달되고,    스위치는 이전 학생이 보낸 DHCP 요청 메시지로 이미 노트북의 주소를 자가학습(self-learning)하였으므로, DHCP ACK 이터넷 프레임을 00:16:D3:23:68:8A로 향하는 나가는 포트로 보낸다.        학생은 DHCP ACK 이터넷 프레임을 받고 페이로드 추출을 반복하여 DHCP 메시지 내의 정보(할당 받은 IP 주소, DNS 서버 주소 등)을 기록하고, IP 포워드 테이블에 기본 게이트웨이 주소를 등록한다.    이제 노트북이 보낼 서브넷(68.85.2.0/24) 바깥이 목적지인 데이터그램은 모두 기본 게이트웨이 라우터로 향할 것이다.    (DHCP 4 단계 중 마지막 2단계는 생략됨.)  6.7.2 계속 시작하기 : DNS와 ARP (Still Getting Started: DNS and ARP)이제 학생이 www.google.com 을 웹 브라우저에 입력하면, 브라우저에 화면이 나타날 때까지 많은 일이 생기게 된다.먼저 브라우저는 TCP 소켓을 형성해서 HTTP 요청 메시지를 www.google.com으로 보내야 하며, TCP 소켓을 생성하기 위해서는 도메인명 www.google.com에 대응하는 IP 주소가 필요하며, 이는 DNS 프로토콜을 통해 변환 가능하다.      학생의 노트북의 운영체제가 DNS 쿼리 메시지를 생성하고, DNS 메시지의 question 부문에 www.google.com이 입력된다.    DNS 메시지는 이후 목적지 포트 번호를 53번(DNS 서버 포트번호)으로 설정한 UDP 세그먼트에 캡슐화되고    목적지 IP 주소를 68.87.71.226(DNS 서버 주소), 출발지 IP 주소를 68.85.2.101(할당 받은 IP 주소)로 설정한 IP 데이터그램에 캡슐화 된다.        DNS 쿼리 메시지가 담긴 데이터그램을 이터넷 프레임에 캡슐화하고, 서브넷의 게이트웨이 라우터로 보내야 하는데, 게이트웨이 라우터의 IP 주소(68.85.2.1)만 알고 MAC 주소는 모르므로, ARP 프로토콜을 통해 IP 주소로 MAC 주소를 가져와야 한다.        노트북의 어뎁터 내부의 ARP 모듈이 target IP 주소가 기본 게이트웨이 IP 주소(68.85.2.1)인 ARP 쿼리 메시지를 생성한다.    ARP 쿼리 메시지는 브로드 캐스트 주소(FF:FF:FF:FF:FF:FF)로 설정된 이터넷 프레임으로 캡슐화 된 뒤 스위치로 보내지면,    게이트웨이 라우터를 포함한 서브넷 내부의 모든 호스트에게 프레임이 전달된다.        게이트웨이 라우터가 ARP 쿼리 메시지가 담긴 프레임을 받고, ARP 메시지의 target IP 주소가 자신의 IP 주소(68.85.2.1)와 일치함을 확인한 게이트웨이 라우터의 ARP 모듈은 ARP reply 메시지를 준비한다.    ARP reply 메세지에는 target IP 주소(68.85.2.1)의 MAC 주소가 00:22:6B:45:1F:1B 임이 적혀있다.    ARP reply는 프레임에 캡슐화되서 노트북의 MAC 주소(00:16:D3:23:68:8A)로 보내진다.        노트북은 받은 프레임에서 ARP reply 메시지를 추출하고, 게이트웨이의 MAC 주소를 알아낸다.        이제 학생의 노트북은 DNS 쿼리 메시지가 담긴 데이터그램을 게이트웨이 MAC 주소가 담긴 프레임으로 캡슐화하고, 스위치를 지나 게이트웨이 라우터로 보낸다.    이때, 데이터그램의 IP 주소는 위 8번에 적었음을 상기하자.  6.7.3 계속 시작하기 : DNS 서버를 향한 도메인 내부 라우팅  (Still Getting Started: Intra-Domain Routing to the DNS Server)      게이트웨이 라우터는 DNS 쿼리 메시지가 담긴 IP 데이터그램을 받은 프레임으로 부터 추출한다.    추출한 IP 데이터그램의 목적지 주소(DNS 서버 주소)를 살펴보고 포워딩 테이블과 대조하여 Comcast 네트워크의 말단(leftmost)의 라우터에 보내기 위해 해당 라우터와 연결 경로에 연결되어 있는 나가는 링크에 라우팅 해준다.        Comcast 네트워크의 말단 라우터는 프레임을 받고 거기서 IP 데이터그램을 추출한 뒤, 목적지 주소인 DNS 서버 주소를 확인하고, 포워딩 테이블과 대조하여 Comcast 내부의 DNS 서버 쪽으로 라우팅 해준다.    이때, 포워딩 테이블은 Comcast의 도메인 내부(intra-domain)  프로토콜(IP, OSPF, ISIS 등)과 도메인간(inter-domain) 프로토콜(BGP)을 이용해 채워넣었다.        DNS 서버에 DNS query 메시지가 담긴 패킷이 도착하면, DNS 서버는 역시 패킷에서 DNS 쿼리 메시지를 꺼내 www.google.com을 DNS 데이터베이스에서 찾아보기 위해 DNS 자원 기록(DNS RR, DNS resource record)을 찾아본다.    이 정보는, 이미 Comcast의 DNS 서버에 캐싱되어 있었고, 캐싱 되지 않았으면 구글의 authoritative DNS 서버에서 RR을 얻어야 했다.    캐싱된 결과에서  www.google.com의 IP 주소(64.223.169.105)를 알아내고, DNS 서버는 DNS reply 메시지에 호스트명과 IP 주소 맵핑 정보를 넣고 UDP 세그먼트, IP 데이터그램에 캡슐화하고, 노트북의 주소를 목적지 주소로 설정하고 라우터를 통해 보낸다.        DNS 서버가 보낸 DNS reply 메시지가 담긴 패킷이 학생의 노트북에 도착하면, 노트북은 해당 DNS reply 메시지를 추출하고, reply 메시지에 적힌 정보로부터 www.google.com의 IP 주소를 얻어올 수 있다.  6.7.4 웹 클라이언트-서버 상호작용: TCP와 HTTP (Web Client-Server Interaction: TCP and HTTP)      학생의 노트북은 이제 구글 IP 주소를 통해 TCP 소켓을 형성할 수 있다. TCP 소켓을 생성하려 하면 노트북 내의 TCP 프로토콜이 three-way handshake를 www.google.com 측과 진행한다.    먼저 TCP SYN 세그먼트를 웹서버 포트인 80번 포트를 목적지 포트로 설정하고 생성한 뒤, 목적지 IP 주소가 64.233.169.105 (www.google.com)인 IP 데이터그램 내부에 캡슐화한다. 이후 목적지 MAC 주소가 00:22:6B:45:1F:1B (게이트웨이 라우터)인 프레임으로 캡슐화해서 스위치로 보낸다.        TCP SYN 데이터그램이 학교 네트워크, Comcast의 네트워크, 구글의 네트워크를 거쳐 www.google.com에 도착한다.    이때, 네트워크 간의 경로 정보를 담은 포워딩 테이블은 BGP 프로토콜에 의해 생성되었다.    NAT 부분은 생략되었으나, 정확히는 패킷의 출발지 주소가 학교의 라우터로 바뀌었고, 대신 포트번호를 따로 할당받았을 것이다        결국 TCP SYN이 담긴 데이터그램은 www.google.com에 도착한 후, TCP SYN 메시지가 패킷으로부터 추출된 후, 구글 서버의 80번 포트에 존재하는 환영 서버로 디멀티플렉스된다.    이후, 구글 HTTP 서버와 학생의 노트북 사이에 TCP 연결을 위한 연결 소켓이 생성된 후, 응답을 위한 TCP SYNACK 세그먼트가 생성되어 IP 데이터그램, 연결 계층 프레임(이때의 MAC 주소는 학생 노트북이 아니라 해당 네트워크의 말단 라우터의 MAC 주소이다.)으로 캡슐화 된다.        TCP SYNACK 세그먼트가 구글, Comcast, 학교 네트워크 들을 거쳐 학생 노트북의 이더넷 컨트롤러에 도착하면, 운영체제에 의해 디멀티플렉싱 되어 생성해놨던 TCP 소켓으로 들어가 연결 상태를 만든다.        이제 학생의 노트북의 브라우저는 패치(fetch)할 www.google.com에 대한 HTTP GET 요청 메시지를 만들어 소켓으로 보내지고, TCP 세그먼트로 캡슐화된다. 이후 18번의 데이터그램 캡슐화 부터 20번의 구글 웹 서버 도착까지의 과정을 답습한다.        구글의 HTTP 서버가 HTTP GET 메시지를 TCP 소켓으로 부터 읽고 난 뒤, HTTP 응답 메시지를 만들고 응답 메시지의 body에 요구한 웹 페이지의 컨텐츠를 넣어준 뒤 TCP 소켓의 연결을 통해 보낸다.        노트북의 TCP 소켓으로 다시 되돌아온 HTTP 응답 메시지 패킷에서 HTTP 응답 메시지를 추출한 뒤, html과 컨텐츠를 메시지 body로부터 가져와 브라우저가 화면상에 표시해준다.  학교 라우터의 NAT, 학교 네트워크로의 무선 접근, 보안 프로토콜, 네트워크 관리 프로토콜 등은 생략되었고, 웹 캐싱이나 DNS 계층 같은 미들박스들도 생략되었다."
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%207-%EB%AC%B4%EC%84%A0%EA%B3%BC%20%EB%AA%A8%EB%B0%94%EC%9D%BC%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC.html": {
    title: "네트워크 정리-Chap 7-무선과 모바일 네트워크",
    date: " Aug 22, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%207-%EB%AC%B4%EC%84%A0%EA%B3%BC%20%EB%AA%A8%EB%B0%94%EC%9D%BC%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 7. 무선과 모바일 네트워크(Wireless and Mobile Networks)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)  student resources : Companion Website, Computer Networking: a Top-Down Approach, 8/e무선 네트워크와 장치들은 점점 인기있어지고 있다.무선 네트워크의 통신법, 특히 네트워크 계층과 연결 계층이 기존의 유선망과 상당히 다른 경향을 보이므로 따로 챕터를 할애하게 되었다.모바일 사용자, 무선 연결, 네트워크, 그리고 이들과 연결되는 유성 네트워크에 대해서 설명하고, 무선 네트워크의 어려움과 이동성(mobility)의 방법을 알아보자.먼저 가장 처음에는 무선 접근 기반과 관련 용어에 대해 배우고 무선망의 특성에 대해 배우자.이외에도 CDMA(code division mutiple access), IEEE 802.11(WiFi), Bluetooth, 4G, 5G, 이동성 서비스(mobility service), 모바일 IP 기준, 전달 계층과 응용 계층에 준 영향 등에 대해 배울 것이다.7.1 소개(Introduction)figure 7.1은 무선 네트워크의 통신과 이동성에 대해 생각해볼 수 있게 만든 자료이다.우리는 이 그림에서 다음과 같은 요소들을 알 수 있다.      무선 호스트들(Wireless hosts)    유선 네트워크에서는 호스트는 응용프로그램을 돌리는 말단 시스템 장치(end-system devices)였지만, 무선 호스트는 스마트폰, 노트북, IOT (Internet of Thinigs) 장비, 센서, 등의 장비가 될 수 있으며, 이동성을 가지고 있을 수 있다.        무선 연결들(Wireless links)    무선 호스트는 기지국이나 다른 무선 호스트와 무선 통신 연결(wireless communication link)로 연결된다.    각기 다른 무선 연결 기술마다 전송율과 통신 거리가 다르며, figure 7.2에서 연결 전송율(link transmission reate)와 커버리지 영역(coverage range)이라는 무선 네트워크의 특별한 특성 및 기준에 대해 알 수 있다.    비트 에러 비율 및 원인 같은 기타 다른 특성은 7.2 절에 배운다.          물론 위와 같은 특성의 값은 사용자 수, 채널 상태, 거리 등에 의해 증가할 수도 있고 줄어 들 수 있으며, 절대적 기준이 아니다.        무선 연결이 언제나 네트워크의 말단에서만 일어나지만은 않으며, 라우터, 스위치 등을 연결하는데 사용되기도 하지만 여기서는 여러 문제와 어려움이 나타나고 있는 말단만 다룬다.        기지국(Base station)    기지국은 무선 네트워크의 핵심 기반시설이다. 유선망과 적절하게 들어맞는 예시가 없다.    기지국은 연관된 무선 호스트와 패킷 같은 데이터를 주고 받는 것과 전송들을 조정하는 역할을 맡는다.    휴대폰 네트워크의 셀 타워(cell tower)와 와이파이 LAN의 접근 지점(access point)이 기지국의 예시이다.    기지국은 연결 계층 릴레이 처럼 호스트와 인터넷, 사내망 처럼 더욱 커다란 네트워크를 연결해주는 역할을 한다.    이러한 기지국으로 운영되는 호스트들을 기반시설 모드(infrastructre mode)라고 하며, 라우팅, 주소 할당 같은 전통적인 네트워크 서비스는 연결된 기지국을 통해서 이루어지면,    기지국을 사용하지 않는 무선 호스트들의 네트워크를 애드 혹 네트워크(ad hoc network)라고 부르며, 각각 호스트들이 스스로 라우팅, 주소할당, DNS 등의 기능을 제공해야 한다.    만약 이동성이 있는 호스트가 기지국의 범위를 벗어나 다른 기지국의 범위 들어서면, 네트워크의 소속과 연결을 바꾸어야 하며, 이를 핸드 오프(handoff), 핸드 오버(handover)라고 부르며, 그러한 이동성 서비스는 어려운 문제이다.    예를 들어, 핸드오버되면 TCP 연결은 어떻게 되는가? 중간에 겹치는 범위에 있으면 어떻게 해야하는가? 끊이없이 바뀌는 네트워크 주소상 위치는 어떻게 추적하는가? 등의 문제가 있다.        네트워크 기반시설(Network infrastructure)    무선 호스트가 연결하고 싶은 인터넷 같은 더욱 규모가 큰 네트워크를 의미한다.  위와 같은 요소들은 무선 네트워크의 형태와 종류에 따라 여러 방식으로 조합될 수 있으며, 무선 네트워크는 크게 2가지 방법으로 구분될수 있다.      패킷이 한번의 무선 홉으로 호스트들에게 전달되는가? 아니면 여러번의 무선 홉으로 전달되는가?        기지국(base station)같은 기반 시설이 네트워크에 존재하는가?          단일 홉, 기반시설 기반 (single-hop, infrastructure-based)      호스트들이 단 한번의 무선 홉과 기지국의 연결을 이용해 인터넷 같은 더욱 규모가 큰 네트워크와 연결된다.대다수의 무선 연결을 차지하며 와이파이, 4G LTE 등이 예시이다.      단일 홉, 기반시설 비의존적 (single-hop, infrastructure-less)    기반시설이 존재하지 않으며, 무선 호스트들이 스스로 다른 노드들과 전송을 조율해야 한다.    블루투스(Bluetooth: 키보드, 스피커, 헤드셋 등의 근거리 소규모 통신에 사용)가 대표 예시이다.        다중 홉, 기반시설 기반 (multi-hop, infrastructure-based)    기반 시설이 존재하지만 릴레이나 다른 무선 노드들을 이용해 통신할 수 있다.    일부 무선 센서 네트워크나 무선 메시 네트워크(wireless mesh network)가 대표적인 예시이다.        다중 홉, 기반시설 비의존적 (multi-hop, infrastructure-less)    기반시설이 없으며, 목적 노드와 연결을 위해 여러 다른 노드들과 메시지를 릴레이한다.    다른 노드들 또한 이동성을 가지고 있어 연결성(connectivity)가 언제나 바뀔 수 있으며, 이러한 네트워크를 MANET(모바일 에드 혹 네트워크, mobile ad hoc network)이라고 부른다. 만약 모바일 노드가 탈 것이라면, VANET(탈것용 에드 혹 네트워크, vehicular ad hoc network)라고 부른다.    상당히 어렵고 여러곳에서 연구되는 분야이다.  이번 챕터에서는 대다수의 이야기가 단일 홉, 기반시설 기반 (single-hop, infrastructure-based)으로 이루어진다.7.2 무선 연결과 네트워크 특성 (Wireless Links and Network Characteristics)무선 연결은 다음과 같은 점이 유선망과 다른 특성이다.      신호 강도 약화(decreasing signal strength)    전자기적 복사는 매질을 통과할 수록 약해지며, 자유 공간 상에서도 송수신자 간의 거리가 멀어지면 점점 분산되어 신호 강도가 약해지진다.(경로 손실(path loss)라고도 부른다.)        다른 기반으로 부터의 방해(Interference from other sources)    동일한 주파수 대역을 가진 무선 기반들은 서로 간섭하여 방해 될 수 있다.    같은 대역을 사용하는 2.5 GHz 무선폰과 802.11b 와이파이 LAN, 모니터, 전자렌지 등에서 나오는 주파수 노이즈 등이 예시이다.    최근에는 이러한 방해를 막으려고 몇몇 802.11 기준이 5GHz 주바수 대역으로 바뀌었다.        다중 경로 전파(Mutipath propagation)    다중 경로 전파는 지면이나 몇몇 물체에 전자기파가 반사되어 송수신 거리가 다른 경로들이 있을 때 생겨난다. 이는 수신자가 받는 신호가 흐려지는 결과를 낳으며, 물체가 이동함에 따라 강도가 바뀌거나 생기거나 할 수 있다.  무선 채널의 추가적인 특성은 [Anderson 1995; Almers 2007]를 참고하자.위와 같은 특성 덕분에 비트 에러가 유선에 비해 자주 일어나며, 이를 막기위해 무선 프로토콜들은 재전송을 이용한 데이터 신뢰성 프로토콜이나 강력한 CRC 에러 감지 등을 이용한다.호스트들은 전자기적 신호를 받을 때, 주변 환경의 노이즈와 상기했던 이유들로 약해진 원본 전송 신호가 섞인 신호를 받게 된다. 이때 SNR(신호대 노이즈 비율, signal-to-noise ratio)은 받은 전송 신호대 노이즈의 상대적 비율로, 주로 데시벨(dB, decibel)이라는 전기 공학적 용어를 이용해 측정한다.dB로 측정된 SNR은 받은 신호의 진폭 대 노이즈의 진폭을 10이 밑인 로그함수로 표시한 비율의 20배로 측정되며, 쉽게 말해 SNR 값이 클수록 노이즈가 적고 많은 전송신호가 온 양질의 신호라고 생각하면 된다.figure 7.3은 각기 다른 전송을 위한 정보 인코딩 조정(modulation) 방법에 따른 기술에 따른 NSR, 비트 에러 비율(BER, bit error rate: 전송한 비트가 에러일 확률, 낮을 수록 좋다), 전송비율에 대한 그래프이다.이는 몇몇 상위 계층의 무선 통신 프로토콜을 이해하는데 중요한 물리 계층 특징을 보여준다.      조정(modulation) 방법이 같으면, SNR을 높이면 BER이 줄어든다.(For a given modulation scheme, the higher the SNR, the lower the BER.)    송신자가 SNR을 높이기 위해 전송 세기(transmission power)를 높이면, 비트 에러 비율, BER을 낮출 수 있다. 하지만 BER읠 $10^{-12}$에서 $10^{-13}$로 줄이는 정도의 일정 한계(threshold)를 넘기면 그 뒤 부터 전송 세기를 높여도 비효율적이며, 모바일 장치 같은 경우 배터리가 빨리 달게되며, figure 7.4(b)처럼 전송세기가 강할 수록 주변 기기의 신호와 서로 방해가 되게 된다.        SNR이 같으면, 높은 비트 전송율을 보이는 조정 방법이 높은 BER을 가진다.(For a given SNR, a modulation technique with a higher bit transmission rate (whether in error or not) will have a higher BER.)    figure 7.3을 보면 같은 SNR이여도 전송율(transmission rate)가 높을 수록 BER이 높은 경향이 있다. 이러한 전송 비율은 조정 방법에 따라 달라진다.    이러한 특성은 다음과 같은 특성을 야기한다.        동적인 물리 계층 조정 기술 선택을 이용한 채널 상태에 따른 조정 기술(Dynamic selection of the physical-layer modulation technique can be used to adapt the modulation technique to channel conditions)    SNR과 BER은 주변 환경과 이동성에 따라 바뀔 수 있다. 802.11 WiFi와 4G, 5G에 사용된 적응형 조정 및 인코딩 방법을 나중에 배울 것이다.    즉 현재의 SNR, BER 그리고 주변 환경과 조건에 따라 조정 방법을 바꾸는 방법이 가능하다.  높고 시간에 따라 변하는 BER이 유선과 무선의 유일한 차이가 아니며 Figure 7.4(a)의 경우 처럼 무선 통신 경로 상의 빌딩, 산 같은 물리적 장애물로 인해 생기는 숨겨진 터미널 문제(hidden termnial problem)로 인해 A와 C 간의 통신이 힘들 수 있다.두번째 시나리오는 무선 매체를 통해 전파되는 신호의 강도가 사라지는(fading) 현상에 의해 수신자 측에서 감지할 수 없는 충돌이 생기는 현상이다.Figure 7.4(b)는 A와 C가 서로를 인지할 수 있을 정도로 신호가 강하지 않지만, 수신자 측인 B 입장에서는 A와 C를 감지할 수 있을 정도로 신호가 강하여, A와 C의 신호가 서로 충돌하게 된다.만약 서로 감지할 수 있었다면, 신호에 대한 상호 조정이 가능하여 충돌을 방지할 수 있었을 것이다.위와 같은 두 문제는 유선망보다 무선망의 문제가 더욱 복잡하게 만드는 원인이다.7.2.1 CDMA앞선 챕터에서 공부했듯이 공유하고 있는 채널, 매질에서 다수의 노드가 서로의 신호를 방해하지 않기 위해 조정하는 프로토콜이 필요했으며, 3가지 분류가 있었다.채널 나눔(channel partitioning), 무작위 접근(random access), 순번(taking turns) 프로토콜 이며, CDMA(코드 나눔 다중 접근, code division multiple access)는 채널 나눔 프로토콜에 속했으며, 무선 네트워크에 널리 사용되는 방식이다.CDMA 프로토콜에서는 보내진 각 비트는 원본 데이터 비트 순서보다 훨씬 빠른 처리율(chipping rate 라고도 한다. 물리적인 DSSS(direct-sequence spread spectrum)의 파동?)로 처리하는 신호(또는 코드)로 인코딩된다.  아래 예시의 Data bits 아래의 Code 부분을 의미하며, 아마 하나의 비트를 더욱 잘게 쪼개 처리하는 듯 하다.Figure 7.5는 간단하고 이상적인 CDMA의 인코딩 디코딩 시나리오이다.각 비트 하나를 보내는데 걸리는 시간을 비트 슬롯 시간이라는 단위라고 가정하고, $d_i$는 i번째 비트 슬롯의 비트 값이다.여기서는 수학적인 편의를 위해 비트 0의 값을 -1로 가정하겠으며, 각 비트 슬롯은 M개의 미니슬롯으로 나뉘며 여기서 M은 8이다.각 미니슬롯은 CDMA code를 나누며, code의 i번째 값들은 $c_m$이라고 하며 1~M까지 있다고 하며 $c_m$은 -1 또는 1이다.  예를 들어 Figure 7.5의 송신자가 사용하는  M-비트 CDMA 코드는 (1,1,1,-1,1,-1,-1,-1)이다.CDMA 인코더의 출력 $Z_{i,m}$은 다음과 같이 구한다.$Z_{i,m}=d_i\\cdot c_m$송신 측으로 부터 간섭이 없었다면, 수신자 측은 수신받은 인코딩된 비트 $Z_{i,m}$를 이용하여 아래와 같은 식으로 원본 데이터 비트$d_i$를 구할 수 있으며 위의 Figure 7.5에 묘사되어 있다.$d_i=\\frac{1}{M}\\sum^M_{m=1}Z_{i,m}\\cdot c_m$하지만 실제 현실에서는 외부 환경에 의한 간섭으로 신호가 바뀌고, CDMA 코드 또한 제각기 송신자 마다 다르다.현실에서는 송신자 각자의 신호가 간섭하여 더해졌다고 가정하고 진행된다. 예를 들어 3명의 송신자가 1을 보낸 타이밍에 1명의 송신자가 -1을 보냈다면 도착한 값은 (1+1+1-1)=2 라는 식으로 신호를 바라본다.이 식을 이용해 수신자 S의 인코딩된 전송신호 $Z^S_{i,m}$을 구하는 것은 이전과 동일하고, N명의 송신자가 간섭하여 받은 전송신호 $Z_{i,m}^*$은 다음과 같이 구한다.$Z_{i,m}^*=\\sum^N_{s=1}Z^s_{i,m}$만약, 송신자가 CDMA 코드를 잘골라 수신자 측에 알려줬다면, 수신자 측에서 해당 코드를 이용해 아래와 같은 식으로 원본 데이터 비트를 구할 수 있다.$d_i=\\frac{1}{M}\\sum^M_{m=1}Z^*_{i,m}\\cdot c_m$Figure 7.6에서 보이듯이 두명의 송신자가 보내어 간섭 받은 인코딩 신호 $Z_{i,m}^*$에서 1번째의 송신자의 CDMA 코드를 알고 있는 수신자 1이 1번째 송신자의 원본 데이터 비트를 구할 수 있다. 만약 2번재 송신자의 CDMA 코드를 알고 있다면, 2번째 송신자의 원본 데이터도 알 수 있을 것이다.이는 마치 시끄러운 반 내부에서 목소리를 아는 두 친구가 서로의 대화를 특정 목소리(코드)를 잘 구별하여 대화하는 것과 같다.CDMA는 이렇게 코드(목소리)를 송신 노드 별로 나누어 동시에 송수신할 수 있으므로 채널 나눔 프로토콜에 속한다. (이전에 CDMA 배울 때 설명하지 못한 부분)실제로는 현실에서 이보다 더욱 어려운데, 첫번째로 CDMA 송신자가 CDMA 코드를 세심하게 골라야 한다는 점이고, 두번째는 현재 우리는 모든 송신자가 같은 강도로 신호를 보내 같은 비율로 신호가 더해져 간섭한다고 가정했지만 실제로는 송신자의 신호마다 강도가 다르므로 단순히 1+1+1-1 식으로 계산할 수 없다.자세한 CDMA의 문제 해결은 [Pickholtz 1982; Viterbi 1995]에서 볼 수 있다.7.3 WiFi: 802.11 무선 LANs (WiFi: 802.11 Wireless LANs)무선 LAN(wirelss LAN)은 가장 중요하고 인기있는 접근 네트워크중 하나이며, 그 중 으뜸은 단연 IEEE 802.11 wireless LAN, 또는 WiFi 이다.802.11 wireless LAN의 프레임 구조, 매체 접근 프로토콜, 이더넷 LAN 과의 상호작용에 대해서 알아보자.Table 7.1에 요약된 대로, 802.11 기준은 여럿이 있다. (802.11 생략) b, g, n, ac, ax는 802.11 기술의 WLAN(wireless local area network)의 산물이며, 70m 범위의 사무소, 가정 등에 사용되었다.n, ac, ax 기준은 WiFi 4, 5 ,6이라고 불리우며 4G와 5G 기술과 경쟁하였다.af, ah 기준은 IOT, 센서 네트워크 등의 좀 더 먼 거리의 통신을 위해 운용된다.각기 다른 b, g, n, ac ,ax 버전은 프레임 구조, 매체 접근 프로토콜(medium access protocol)인 CSMA/CA, 무선 장비들의 전송을 위한 기지국에 의한 중앙화 스케듈링, 과거 버전과의 호환 등을 공유한다.하지만 각 기준들은 Table 7.1에서 보듯이 물리 계층의 특색이 다른데,예를 들어 각기 다른 두 주파수 대역에서 운영된다      2.4 GHz 범위(2.4~2.485 GHz 범위) : 라이센스가 필요없는 주파수 범위로, 2.4GHz의 핸드폰, 전자레인지와 주파수 경쟁을 해야한다.        5 GHz 범위(5.1~5.8 GHz 범위) : 2.4 GHz 범위에 비해 같은 전송세기에서 비교적 짧은 전송거리와 많은 다중경로 전파 문제를 겪는다.  또한, n, ac, ax 기준에는 다른 MIMO(다중 입력, 다중 출력, multiple input multiple-output) 안테나를 이용하며, 이를 이용해 좀더 다양한 주파수 대역을 받아들인다.ac, ax는 여러 스테이션에게 동시에 전송할 수 있으며, 스마트 안테나를 이용해 적응적으로 수신자의 물리적 방향을 향해 전송하여 간섭을 줄이고 최대 전송 거리를 늘릴 수 있다.Table 7.1에서의 값들은 이상적인 상황에서 값들이며, 실제와는 다를 수 있다.7.3.1 802.11 무선 LAN 구조 (The 802.11 Wireless LAN Architecture)Figure 7.7은 802.11 Wireless LAN Architecture의 기본적인 요소들의 그림이다.가장 기본이 되는 블록인 BSS(기본 서비스 셋, basic service set)은 하나 이상의 무선 스테이션, 그리고 하나의 AP(접근 지점, access point)라고도 불리우는 중앙 기지국(central base station)로 이루어져 있다.Figure 7.7은 각 BSS에 속하는 AP와 상호작용하는 기기들과, AP와 연결되어 인터넷에 연결시켜주는 라우터를 볼 수 있다.이더넷에서 각 802.11 무선 스테이션에는 6바이트의 MAC 주소가 어뎁터에 펌웨어 안에 존재하며, 각 AP 또한 그러하다.AP가 배포 중인 무선 LA을 기반시설 무선 LAN(infrastructure wireless LANs)이라고 불리우는데 AP와 이더넷이 라우터와 AP를 상호 연결하기 때문이다.Figure 7.8의 경우 IEEE 802.11 스테이션들이 스스로 그룹 네트워크를 이루어 에드 혹 네트워크를 이룬 모습이며, 중앙 제어 네트워크나 외부 연결이 없다.서로 통신해야 하는 장치들 간에 모여 중앙 제어나 기반 네트워크, AP 없이 서로간에 통신할 수 있으며, 상당한 관심을 끌어모았지만, 우리는 이곳에서  기반시설 무선 LAN(infrastructure wireless LANs)에만 관심을 둘 것이다.채널과 연관 (Channels and Association)802.11에서는 각 무선 스테이션이 AP와 연결되어야 데이터를 주고 받을 수 있다. IEEE 802.11b, g, ,b, ac, ax에 대해서 알아보자.네트워크 관리자가 AP를 설치하면 1~2 단어의 SSID(서비스 셋 식별자, Service set identifier)를 AP에 설정해준다. (스마트폰 WiFi 설정에 보이는 리스트)그 뒤에 네트워크 관리자는 AP에게 채널 번호(channel number)를 할당해줘야 한다.채널 번호는 802.11에 할당된 2.4GHz ~ 2.4835GHz 사이의 85MHz 대역폭을 11개의 채널로 나누어 사용한다.이때 각 채널들은 주변 번호의 채널들과 조금씩 대역폭이 겹치는데, 대략 한번에 4개 채널 정도가 겹치므로, 서로 간섭하게 되며 만약 전혀 겹치지 않은 3개의 채널을 사용하고 싶다면 3개의 AP를 각각 1, 6, 11 채널로 설정해주면 최대 전송율을 가진 채널 3개를 같은 물리적 공간 안에 설정해줄 수 있다.여러 AP들이 WiFi를 설정하다 보면, 수 많은 AP들과 통신이 가능한 물리적 위치가 존재하는 데 이를 WiFi 정글(WiFi Jungle)이라고 한다.사용자는 그 중에서 수동적으로 선택하거나 장치의 알고리즘(어떤 AP를 고를까는 공식화되지 않았으며, 회사마다 다르지만 보통 신호 강도가 센 AP를 고른다.)에 따라 참여(associate: AP와 장치가 가상의 연결을 가지게 됨)할 AP를 고르게 된다.  사실 신호세기가 무조건 최고의 선택이 아닐 수도 있는데, 여러 AP가 같은 채널을 사용하고 있을 수도 있고, 너무 많은 장치가 연결되어 있을 수도 있다.  좋은 선택을 하는 방법은 [Vasudevan 2005; Nicholson 2006; Sundaresan 2006]에서 볼수 있으며, 신호 세기를 측정하는 방법은 [Bardwell 2004] 참조,이때, 장치가 AP의 존재를 알 수 있는 방법은 두가지가 있다.      수동 탐지(Passive scanning): Figure 7.9 (a)    802.11의 기준, AP가 주기적으로 AP의 SSID와 MAC 주소가 적힌 비컨 프레임(beacon frame)을 주변 장치들에게 보낸다.    이후 사용자가 참여하고 싶은 AP과 요청, 응답 프레임을 주고 받아서 참여한다.        **능동 탐지(Active scanning) **: Figure 7.9 (b)    무선 장치가 주변 도달가능 범위 내의 모든 AP에게 탐지 프레임(probe frame)을 보내면 AP가 그에 반응해 응답 프레임을 보내고, 장치가 AP 중 하나를 선택하여 참여하는 방식이다.  능동적 탐지에서 사용자의 장치가 참여할 AP를 골랐다면, 참여(association) 요청 프레임을 AP에게 보내게 되고 AP는 응답 메시지를 보내게 되는데, 이러한 요청/응답 handshake는 필수적이다.DHCP와 유사하게, AP 입장에서 사용자가 여러 제안 중 자신의 제안을 받아들일지 알 수 없기 때문에, 사용자가 명시적으로 제안을 받아 들이겠다고 요청해야하며, 요청이 받아들여지면 장치에게 IP를 주기 위해 DHCP 절차가 행해지고, 해당 AP의 서브넷에 참여하게 된다.AP에 참여하려면, 인증(authenticate)을 하게 만들 수 도 있다.802.11 무선 LAN의 인증과 접근 방법은 여럿이 존재하며,예시를 들자면 장치의 MAC 주소를 기반으로 하는 것과 패스워드 방식이 있다.두 방법 모두 AP가 인증 서버와 RADIUS[RFC 2865], DIAMETER[RFC 6733] 같은 보안 프로토콜을 이용해 통신하여 확인 받으며, 여러 AP를 운영하는 경우 하나의 인증 서버에 여러 AP와 통신하도록 하게하여, 보안성, 비용 효율성, 구조의 간단함 등을 높일 수 있다.보안의 자세한 부분은 8장에서 다루겠다.7.3.2 802.11 MAC 프로토콜 (The 802.11 MAC Protocol)AP의 서브넷에 참여했지만, AP에 참여한 다른 장치들이나 AP 측에서 보내는 전송에 의해 충돌이 일어날 수 있으므로, AP를 포함한 통신 장치들은 다중 접근 프로토콜(multiple access protocol)에 의해 제어되어야 한다.이터넷의 영향으로 802.11은 무작위 접근 프로토콜(random access protocol), 그중에서도 CSMA/CA(CSMA with collision avoidance)을 사용하며, CSMA의 반송파 감지(carrier sense)의 의미는 전송 하기전에 채널의 상태를 감지하고, 채널이 바쁘면 전송을 보류한다는 의미이다.CSMA/CA는 이더넷에서 사용하는 CSMA/CD와 다르다는 것을 유의하자,다만 이더넷의 MAC(다중 접근 제어, multil access control, MAC 주소의 Media acess control과 헷갈리지 않게 주의) 프로토콜과는 두가지가 다른데,첫번째는 충돌 감지 대신 802.11은 충돌 회피 기술을 사용한다는 점,두번째는 비교적 높은 비트 에러율 때문에, 802.11은 연결 계층 답신(acknowledgement)/재전송 (ARQ)를 사용한다는 점이다.앞서 이터넷(802.3)의 충돌 감지에서는 전송 중에 다른 전송이 감지되면 즉시 전송을 멈추고 무작위 시간동안 기다렸다가 재전송을 시도하는 방식이였다.하지만 802.11 MAC 프로토콜에서는 충돌 감지 기능이 존재하지 않는데 두가지 이유 때문이다.  충돌 감지를 위해서는 동시에 송신(적절한 순간에 나의 패킷 송신을 위해)과 수신(다른 노드의 전송을 감지하기 위해)하는 기능이 있어야한다. 하지만 무선 환경에서는 송신 신호에 비해 수신 신호는 상대 송신측이 멀리서 보내는 걸테니 비교적 약하게 측정되므로, 송신 신호를 보내면서 동시에 송신 신호에 간섭받고, 멀리서 오느라 미약해진 수신 신호를 감지하는 기능을 구현하는 하드웨어가 비싸진다.  만약, 그러한 하드웨어를 구현한다고 해도, 숨겨진 터미널 문제(hidden terminal problem)와 감쇠(fading)로 인해 모든 충돌을 감지할 수 없다. 예를 들어 A와 C가 동시에 B에 송신하고 있는 충돌 상태를 A와 C 사이가 너무 멀어 감쇠(fading)되어있거나 A와 C 사이에 장애물이 있어 서로를 감지못하면 충돌 또한 감지 못한다.충돌 방지 기능이 없으므로, 결국 802.11 무선 LAN에서는 전송을 한번 시작하면 전체 프레임이 도착할 때까지 멈추지 않는다.하지만 충돌이 일어난 상태로의 전송은 결국 성능 저하를 초래하므로 802.11은 대신 다양한 충돌 회피 기술을 사용한다.이전에 먼저 비트 에러와 패킷 손실 비율이 높은 무선 환경에서의 특징을 극복하기위한 802.11의 연결 계층 acknowledgment에 대해서 알아보자. (Figure 7.10)      목적지 스테이션에서 프레임을 받고 CRC 검사를 통과하면 SIFS(프레임 간 단시간,Short Inter-frame Spacing)라는 잠시 동안의 대기시간을 거친 뒤, ACK 프레임을 돌려준다.        만약 송신자 스테이션에서 ACK를 timeout 될 때까지 받지 못한다면, 에러로 판단하고 CSMA/CA 프로토콜 과정을 거친 뒤 프레임을 재전송한다.        만약 일정한 시도 횟수의 재전송에도 ACK를 받지 못한다면 전송 스테이션은 포기하고 해당 프레임을 버린다.  이제 Figure 7.10의 802.11의 CSMA/CA 프로토콜에 대해 알아볼 차례이다.Station(무선 장치 또는 AP를 포함한 전송 가능한 장치를 의미)이 보낼 프레임이 있다고 가정하자.      station이 채널의 대기상태를 감지하면 DIFS(분산 프레임 간 시간, Distributed Inter-frame Space)라는 짧은 대기시간을 가지고 전송한다.        만약 채널 상태 바쁘다면, 스테이션은 이진 지수적 무작위 백오프 시간(binary exponential random backoff)을 이용한 카운터를 설정하고, 대기상태가 감지되면 DIFS 시간 만큼 기다린 뒤 부터 카운트를 세기 시작한다. 여전히 채널 상태가 바쁘다면 카운터는 줄어들지 않는다.        채널이 대기상태여서 카운터가 0이 된다면, station은 전체 프레임을 전송하고 ACK 프레임을 기다린다.        만약 ACK를 받았다면, 정상 도착한 것으로 알게 되고, 다음 보낼 프레임을 준비한 뒤 위의 2번째부터 CSMA/CA 절차를 밟는다.(백오프 시간은 초기화 된다.)    만약 ACK를 받지 못했다면 위의 2번째 부터 CSMA/CA 절차를 밟되 백오프 시간이 초기화 되지 않고, 더욱 커진다.  이터넷의 CSMA/CD에서는 채널의 대기상태가 확인되면 즉시 전송했던 것과 달리 CSMA/CA는 DIFS + 백오프 카운터 시간을 기다린다.이 차이의 원인은 802.11은 충돌 감지를 하지 않는다는 점에 있다.예시로 A와 B 두 스테이션이 전송을 기다리고 있고 채널 상태는 누군가가 사용중이라고 하자.CSMA/CD에서는 채널이 비자마자 A와 B 둘다 전송을 시작할 것이고, 충돌이 일어날 것이다.이터넷의 경우 둘다 서로 충돌을 감지하고 곧바로 전송을 취소하고 무작위 백오프 시간을 가진 뒤 재전송할 것이므로 크게 문제가 안된다.하지만 802.11에는 충돌 감지가 없으므로 A와 B 둘다 프레임 하나가 전송이 끝날 때 까지 기나긴 충돌을 겪어야하며, 이는 크나큰 성능상의 낭비가 된다.그러므로, 802.11의 CSMA/CA는 곧 바로가 아닌 한번의 무작위 백오프 시간을 가진 뒤, 무작위로 먼저 전송을 시작한 쪽의 채널 전송을 다른 한쪽이 알게 하여 카운터를 멈추게 하고, 전송이 끝날때 까지 기다리게 한다. 덕분에 충돌을 최대한 방지할 수 있다.단, 두 가지 상황에서는 여전히 충돌이 발생할 수 있다.  두 전송자가 숨겨진 터미널 효과나 감쇠(fading)효과로 인해 전송되고 있는 상태를 감지하지 못할 때  우연히 두 전송자의 무작위 백오프 시간이 거의 유사하게 잡혀서, 전송을 거의 동시에 시작할 때          “일치”가 아니라 “동시에”인 이유는 미세한 차이라면, 상대방의 전송을 감지하기 전에 전송을 시작해버릴 수 있기 때문이다.      숨겨진 터미널 다루기 : RTS와 CTS(Dealing with Hidden Terminals: RTS and CTS)802.11의 MAC 프로토콜은 숨겨진 터미널 문제를 어느 정도 해소해주는 방법 또한 존재한다.Figure 7.11의 그림과 같은 상황이라면, H1과 H2는 서로의 커버리지에 존재하지 않아 존재를 모르는 상태에서 충돌이 일어나는 전송을 하겠지만, 이를 RTS(송신을 위한 요청, Reqeust to Send) 제어 프레임과 CTS(송신 가능, Clear to send) 제어 프레이미을 이용해 채널의 접근을 예약하여 피할 수 있다.먼저 보내고 싶은 데이터가 있는 스테이션에서 AP 측에 데이터를 보내기 위해 필요한 시간을 적어 놓은 RTS 프레임을 브로드캐스트한다.이후, AP에서 RTS 프레임을 받으면, CTS 프레임을 브로드캐스트로 송출한다.이를 통해 RTS 프레임을 보낸 스테이션에는 채널 사용 허락을, 보내지 않은 다른 스테이션에게는 일정 시간 동안 사용 금지를 알릴 수 있다.Figure 7.12는 이 과정을 나타낸 그림이다.RTS와 CTS 프레임을 이용하면 두가지 중요한 이유로 성능이 좋아진다.  예약된 시간에만 전송되므로 숨겨진 터미널 문제가 경감된다.  RTS와 CTS 용량이 적어 전송시간이 적으므로, 충돌이 나도 큰 충격이 없으며, RTS와 CTS가 도착한 뒤에는 데이터와 ACK로 답신해줘야 한다.단, RTS/CTS는 지연시간과 채널 자원의 소모가 있으므로, 데이터 크기 기준(threshold)이 정해져 있어 데이터 프레임이 기준이상으로 클때만 사용되며, 보통의 경우 프레임의 최대 크기 제한보다 기준이 커서 RTS/CTS가 사용되지 않는 경우도 많다.802.11를 사용한 지점간 연결(Using 802.11 as a Point-to-Point Link)각 노드가 지향성 안테나를 가지고 서로를 가리키면 802.11 프로토콜을 통해 지점간 연결이 가능하다.신호 세기에 따라 수십 키로미터의 통신이 가능하며, 여러 홉 무선 네트워크를 이용해 광범위한 지점간 연결 또한 가능하다.7.3.3 IEEE 802.11 프레임 (The IEEE 802.11 Frame)figure 7.13은 802.11 프레임의 예시이며, 이터넷과 유사한 점도 있지만, 특별한 점도 있다.위의 프레임의 구조의 경우 숫자의 단위가 바이트이며, 아래의 frame control 필드의 서브 필드의 숫자의 단위는 비트이다.페이로드와 CRC 필드(Payload and CRC Fields)페이로드는 주로 IP 데이터그램이나 ARP 패킷이 담기며, 2312 바이트까지 가능하지만 대체로 1500바이트 이하이다.CRC의 경우, 32비트 CRC(순환 중복 검사, cyclic redundancy check)를 이용하며, 에러가 잦은 무선 환경에서는 더욱 중요하다.Address 필드(Address Fields)802.11 프레임에서는 주소 필드가 4개나 존재하며 각각 6바이트의 MAC 주소가 들어가야 한다.  Address 1: 전송받는 무선 장치의 MAC 주소, 호스트 장치가 전송 시에는 도착할 AP AMC 주소, AP가 전송시에는 도착할 호스트 MAC 주소  Address 2: 전송하는 무선 장치의 MAC 주소, AP가 전송할 시 AP MAC 주소, 호스트 장치가 전송 시 호스트 장치 MAC 주소, 일종의 출발지 MAC 주소  Address 3: AP와 무선 장치가 소속된 서브넷의 게이트웨이 라우터 인터페이스의 MAC 주소, 아래에 보충 설명이 있다.  Address 4 : 에드 혹 네트워크 모드에서 AP 역할을 하는 장치가 다른 장치들에게 포워드하기 위한 MAC 주소주소 3번의 필요성을 알기 위해 figure 7.14의 시나리오를 생각해보자.먼저 AP는 연결 계층 장비이므로, IP와 통신하거나 IP 주소를 이해하지 못한다.또한 AP는 라우터 입장에서 보이지 않고, H1과 라우터는 직접 연결되있는 것 처럼 보인다.      전송받은 데이터그램에서 H1의 주소를 읽은 라우터는 ARP를 통해 H1의 MAC 주소를 얻는다. 이후 R1은 데이터그램을 프레임으로 캡슐화하고, 출발지 MAC 주소로 라우터의 R1 인터페이스 MAC 주소를 적어서 H1의 MAC 주소로 브로드캐스트한다.        AP에 프레임이 도착하면 AP는 802.3(이터넷) 프레임을 802.11 프레임으로 바꾸고,    address 1을 H1의 MAC 주소, address 2 필드를 자신, AP의 MAC 주소를 넣고 address 3 필드에는 R1의 MAC 주소를 넣는다.    H1은 이제 address 3 필드를 통해 서브넷의 라우터 인터페이스 MAC 주소를 알 수 있게 된다.  이제, H1이 R1으로 응답하려고 하는 경우  H1이 802.11 프레임을 생성하고 address 1을 AP의 MAC 주소, address 2를 H1의 MAC 주소, address 3을 R1의 MAC 주소를 넣는다.  AP가 H1으로 부터 802.11 프레임을 받으면, 이를 이터넷 프레임으로 바꾼다. 출발지 주소는 H1의 MAC 주소, 도착지 주소는 R1의 MAC 주소이며, 이때 address 3에서 R1의 MAC 주소를 가져온다.이렇게  address 3은 라우터 인터페이스의 MAC 주소를 알려주는 역할을 한다.Sequence Number, Duration, 그리고 Frame Control 필드(field)Sequence Number 필드는 전달 계층의 그것과 같으며, 스테이션이 진행한 acknowledgment에 사용한다. 새로 보낸 프레임과 이전에 보냈으나 재전송한 프레임을 구별하는 역할을 해준다.Duration 필드는 앞서 설명했던 RTS와 CTS 프레임이 데이터나 ack 프레임을 보내기 위해 채널 자원을 예약할 때 사용한다.Frame Control 필드는 여러가지 비트 단위의 서브 필드로 이루어져 있는데, 자세한 내용 들은 [Held 2001; Crow 1997; IEEE 802.11 1999]에서 확인 가능하다.  type, subtype 필드는 연관(association, 어느 AP의 서브넷에 참여 중인가?), RTS, CTS, ACK와 데이터 등의 프레임 종류를 구별하는데 사용한다.  to, from은 address 필드들을 정의하는데 사용하는데, 에드 혹 네트워크와 기반시설 네트워크 모드일 때 마다 의미가 다르다. 기반 시설 모드일 경우, 해당 프레임을 보낸(from), 받는(to) 스테이션이 AP냐, 무선 장치이냐를 결정한다.  WEP 필드는 암호화(encrpytion)이 사용중이냐 아니냐를 표시한다. 8장 참조7.3.4 같은 IP 서브넷에서의 이동성(Mobility in the Same IP Subnet)무선 LAN의 물리적 범위를 증가시키기 위해 같은 IP 서브넷에 같은 SSID를 가진 BSS를 여럿 두기도 한다.  SSID가 같으면 자동으로 다시 연결해주는 듯?이는 사용자가 이동하면서 소속 BSS가 변함을 의미하며, TCP 연결 유지 등의 해결 방안을 생각해봐야 한다.그래도 BSS가 같은 서브넷 소속이면 쉽지만, 다른 서브넷 소속이면 복잡한 이동성 관리 프로토콜(mobility management protocol)이 필요하다.Figure 7.15 처럼 같은 서브넷 BSS 간의 이동을 알아보자면,스위치로 연결된 두 BSS는 같은 IP 서브넷을 공유하므로, IP가 유지된 채로 소속이 바뀌면서 이로 인해 TCP 연결 등이 종료되지 않는다.이때, AP2는 새로 들어온 호스트 H1의 MAC 주소를 발신지 주소로 한 브로드캐스트 프레임을 뿌려 스위치의 자가학습(self-learning) 기능으로 포워딩 테이블을 갱신하게 한다.LAN subnet 뿐만 아니라 VLAN이 다른 경우에도 위와 같은 방법으로 해결할 수 있다.하지만 만약 두 BSS가 라우터로 연결되어 다른 IP 서브넷을 가지게 되면 호스트 H1은 이동 후에 새로운 IP를 할당받아야 하고, TCP 연결을 종료해야 하며, 세상 입장에서는 갑자기 해당 주소의 호스트가 사라지는 것 처럼 보일 것이다.이를 해결하기 위해 네트워크 계층 이동성 프로토콜인 모바일 IP를 이용할 수 있으며, 자세한 내용은 7.6절에서 설명할 것이다.GPS와 WIFI 위치 지정 (LOCATION DISCOVERY: GPS AND WIFI POSITIONING)스마트폰을 이용해 위치를 측정하는 방법으로 GPS(Global positioning system)와 WiFi Positioning System(WPS)가 있다.GPS는 30개 이상의 위성들이 자신들의 위치와 시간 정보를 라디오 신호로 전달하며, 이 신호들 중 각기 다른 인공위성 4개 이상의 정보를 GPS 장치가 확보하면 삼각 측량(triangulation equations)을 통해 자신의 위치를 알 수 있는 원리이다.미국에서 개발, 누구나 사용할 수 있게 관리하고 있으며, 언제나 지상의 정말힌 시계와 끊임없이 동기화하고 있다.하지만 GPS 신호가 거대한 건물에 막히거나 강력한 신호로 교란되면 사용하기 힘들다.이떄 WPS를 사용하면 되는데, 구글, 애플, 마이크로 소프트 같은 거대 기업이 수 많은 WiFi AP의 위치와 SSID를 데이터베이스에 저장하여, 사용자가 자신의 주변 WiFi들의 SSID와 신호 강도, 가능하다면 GPS 정보를 보내 클라우드 상의 데이터베이스에서 가져온 주변 WiFi 위치를 통해 삼각 측량(triangulation equations)으로 자신의 위치를 알 수 있다.그렇다면 처음에 WiFi 위치 정보는 어떻게 알고 있었을까? 바로 사용자들이 보낸 정보를 토대로, 이미 알고 있는 WiFi 위치 정보와 사용자가 보낸 GPS 정보를 이용해 사용자가 보내온 새로 생긴 WiFi의 위치를 삼각 측량(triangulation equations)으로 예측하며, 이 예측을 수천명의 정보로 확실하게 만들어 추가한다.7.3.5 802.11의 진보된 특성 (Advanced Features in 802.11)802.11의 공식적인 기능은 아니지만 802.11 네트워크를 이용해 할 수 있는 뛰어난 기능에 대해 설명한다.주로 장치 판매 회사들이 경쟁성을 위해 구현한다.802.11 전송율 적응 (802.11 Rate Adaptation)현재나 지난 채널의 성향에 따라 적응적으로 물리 계층 조정 방법을 바꾸는 처리율 적응 능력을 구현할 수 있다.전송율이 높은 조정 방법일 수록 비트 에러율이 높고 반대로 전송율이 낮을 수록 비트 에러율이 낮아지는 특성을 이용해 비트 에러율이 높은 시기에는 전송율이 낮은 조정방법을, 반대로 비트 에러율이 낮다면 전송율을 높여줄 수 있다.이때 에러율을 탐지하는 방법으로 ACK 메시지의 응답 성공율을 이용하는 경우가 많으며, 연속으로 ACK가 성공했을 때는 전송율을 높이다가, ACK가 실패하는 순간 전송율을 떨어뜨리는 방식을 이용할 수 있으며, 이는 마치 TCP 혼잡 제어 때와 비슷하다.전력 관리 (Power Management)전력은 모바일 장치에서 귀중한 자원이므로 802.11 기준에서 전력 관리 역량 기능으로 802.11 노드에게 전송, 수신, 감지, 등을 담당하는 회로망을 전원이 켜져 있는 시간을 최대한 줄일 수 있게 한다.노드에는 수면 (sleep) 상태와 각성(wake) 상태로 나누고, 노드는 AP(Access Point)에게 장치가 곧 수면 모드로 돌입할 것이라는 사실은 802.11 프레임의 power-management 비트를 1로 설정해 알려 줄 수 있다.노드의 타이머는 AP가 비콘 프레임을 보내기 바로 전(보통 100msec 주기로 보낸다.)에 노드가 깨어날 수 있게 설정되고, AP는 해당 노드가 자고 있다는 것을 알고 있으므로, 해당 노드가 자고 있는 중에 해당 노드로 가야 되는 프레임들은 AP에 버퍼되고, 해당 노드가 깨어 났을 때 보내준다.노드는 AP가 비컨 프레임을 보내기 바로 전에 일어나 활성 상태가 되며, 비컨 프레임에는 프레임이 버퍼된 노드들의 리스트가 들어있어서, 해당 노드에 자신이 존재하면 해당 프레임을 받아서 처리하고, 그렇지 않으면 다시 수면상태로 돌아가고 다음 비컴 프레임을 기다린다.비콘 프레임의 주기는 100msec, 노드가 각성 상태가 된 후, 비컨 프레임을 확인하는 시간은 250 마이크로초 내외이므로 99% 시간 동안은 노드가 자고 있을 수 있으므로, 전력 소모를 크게 줄일 수 있다.7.3.6 개인 지역 네트워크: 블루투스 (Personal Area Networks: Bluetooth)블루투스는 유선 대체 기술로 점점 중요해지고 있으며, 특유의 좁은 범위와 낮은 전력 소모, 비용으로 WPAN(무선 개인 지역 네트워크, wireless personal area network) 또는 피코넷(piconet)이라고 불리운다.블루투스 네트워크는 작고 간단하도록 설계되었으며, TDM, FDM, 무작위 백오프, 폴링, 에러 감지 및 수정, ACKs와 NAKs 같은 수많은 연결 계층 네트워크 기술이 많이 사용된다.블루투스는 비허가 사용이 가능하며 ISM(산업적, 과학적, 의학적 라디오 대역, Industrial, Scientific and Medical)이라고 불리우는 2.4 GHz를 사용하므로 높은 노이즈와 방해를 염두에 두고 디자인 됬다.무선 채널을 TDM으로 나누며, 각 시간 슬롯은 625 마이크로초이며, 매 슬롯 마다 79 개의 채널 중 무작위를 모방한 방법으로(pseudo-random) 끊임 없이 채널을 바꾸면서 전송하게 된다.  이 방법을 FHSS(주파수 도약 확산 스펙트럼, frequency-hopping spread spectrum) 이라고 하며, 이를 통해 ISM 대역 내 다른 기기의 신호 간섭이 일부 슬롯에만 일어나게끔 할 수 있으며, 블루투스 경우 최대 3 MBps까지 가능하다.블루투스 네트워크는 에드 혹 네트워크 이므로 AP 같은 기반 시설이 없고, 대신 figure 7.16처럼 각 장치가 최대 8개의 장치가 스스로 네트워크를 이루며, 장치 중 하나가 마스터 노드(master node)역할을 하고 나머지는 클라이언트 노드 역할을 한다.마스터 노드의 시계를 기준으로 피코넷의 TDM 슬롯 경계를 나누며, FHSS 절차, 피코넷의 장치들의 항목, 피코넷의 전송 강도(100mW, 2.5mW, 1mW)를 결정, 관리 하며, 마지막으로 폴링을 이용해 클라이언트에게 전송 권한을 부여한다.8개의 활성 노드 이외에도 추가로 255개의 보류된(parked) 장치가 존재할 수 있으며, 장치들은 수면 상태에서 에너지를 아끼다가 마스터 노드의 비컨 메시지 계획에 따라 주기적으로 일어나서 통신할 수 있다.블루투스는 에드 혹 네트워크 이므로 스스로 조직되어야 하며(self-organizing), 다음과 같은 방법으로 네트워크 구조가 생성 된다.마스터 노드가 블루투스 네트워크를 생성하고 싶을 때, 먼저 범위 내의 다른 블루투스 장비를 탐색한다. 이를 이웃 탐사(neighbor discovery) 문제라고 한다.마스터 노드는 조사(inquiry) 메시지 별로 다른 주파수 대역을 할당한 총 32 개의 조사 메시지를 128회 반복 브로드캐스트하여 이웃을 탐색하며,이때, 클라이언트 장치는 선택한 주파수 대역에 조사메시지를 듣게 되면, 충돌을 막기위해 0~0.3초 정도의 무작위 백오프 시간을 가진 뒤, 장치 ID가 포함된 메시지를 마스터 노드에게 보낸다.마스터 노드가 모든 클라이언트를 찾으면, 피코넷에 클라이언트들을 초대한 뒤, 블루투스 페이징(bluetooth paging)이라고 부르는 802.11에서 기기들을 기반 스테이이션에 연관(associatie)시키는 것과 비슷한 과정을 거쳐야한다.블루 투스 페이징 시, 마스터는 클라이언트들에게 주파수 도약 패턴과 마스터의 시계 정보를 알려주기 위해 위의 조사메시지 처럼 32개의 구분되는 페이징 초대 메시지를 총 32개의 대역폭에 보낸다.  클라이언트들과 직접 통신하지 않는 이유는 아직 클라이언트들이 주파수 도약 패턴을 모르기 때문각 클라이언트들이 ACK 메시지로 페이징 초대 메시지에 답신하면, 마스터노드는 이제 주파수 도약 정보와 시계 동기화 정보, 활성화된 멤버들의 주소를 클라이언트에게 다시 보내고, 클라이언트를 주파수 도약 패턴에 따라 폴링(polling)하여 클라이언트가 네트워크에 참여했는지 확인한다.블루투스 무선 네트워크는 이외에도 데이터 신뢰성 전송, 서킷 스위칭 비슷한 비디오, 오디오 스트리밍, 전송 강도 세기 조절, 활성화/대기 상태 노드 전환, 저전력 모드 및 보안 등의 중요한 기능등이 있으며 이는  [Bisdikian 2001, Colbach 2017, and Bluetooth 2020]에서 확인하자.7.4 무선 네트워크 : 4G와 5G (Cellular Networks: 4G and 5G)WiFi는 아쉽게도 짧은 커버리지 범위를 가지고 있으며, 지나치는 모든 AP에 참여할 수 있지도 않으므로, WiFi로는 사용자의 이동에 따라 완전한 서비스를 제공하기 힘들다.반대로 4G 무선전화(cellular) 네트워크는 빠르게 성장하고 있고, 20Mbps 수준의 빠른 속도로 90%가 넘는 4G 신호 탐색율을 자랑하여, 비디오 스트리밍과 회의 등을 자동차, 버스 같은 곳에서 사용할 수 있게 되었다. 최근에는 IOT 장비나 모바일 결제, 인터넷 베이스 메시지 등에도 사용된다.무선전화(cellular) 네트워크의 cellular(무선전화의, 또는 직물의)의 의미는 각 지역이 셀(cell)이라고 불리우는 여러 개로 서로 다른 지정학적 커버리지 지역으로 나뉘어 무선전화(cellular) 네트워크에 참여하기 때문에 붙여진 이름이다.각 셀에는 데이터를 송수신하는 기지국(base station)과 기지국을 이용해 셀을 이동하면서 송수신하는 모바일 장비(mobile device)로 구성된다.셀의 크기는 기지국의 전송 강도, 모바일 장비의 전송 강도, 빌딩, 산 등의 장애물, 기지국 안테나의 높이와 종류에 따라 달라진다.우리는 이곳에서 모바일 장치와 기지국의 무선 첫 홉 연결과 무선전화 제공자 네트워크와 연결된 all-IP 중심 네트워크을 통한 인터넷 연결에 대해서 배울 것이며, 전화 연결선에서 기원했음에도 놀랍게도 인터넷과 유사하면서, 앞서 배웠었던 프로토콜 계층, 엣지/중심 구분, 컨트롤 측면과 데이터 측면 나눔과 같은 개념을 사용하는 4G 네트워크의 구조적 원칙에 대해서 배워볼 것이다. 그 이후 4G 이동성 관리나 4G 네트워크에서의 인터넷 프로토콜(IP)에 대해 배울 것이다.간단하게 설명할 것이며 더욱 알고 싶다면 [Goodman 1997; Kaaranen 2001; Lin 2001; Korhonen 2003;Schiller 2003; Palat 2009; Scourias 2012; Turner 2012; Akyildiz 2010, Mouly 1992; Sauter 2014 ]를 참조.RFC처럼 4G와 5G 네트워크는 [3GPP 2020]에 기술적 상세 정의가 되어있다.7.4.1 4G LTE 무선 네트워크: 구조와 요소들 (4G LTE Cellular Networks: Architecture and Elements)4G Long-Term Evolution standard(4G 장기 진화 표준), 줄여서 4G LTE 네트워크는 크게 무선전화 네트워크 가장자리의 라디오 접근 네트워크(radio access network)와 코어 네트워크(또는 EPC(all-IP Enhanced Packet Core))로 나뉘며, 네트워크의 각 요소들은 IP를 통해 통신한다.먼저 4G LTE의 모호한 용어와 약어에 의한 혼란을 막기 위해 각 요소들의 기능과 데이터 측면, 컨트롤 측면의 상호작용을 알아보자.      모바일 장치(mobile device)    무선장치 네트워크에 연결된 응용 프로그램(웹 브라우저, 전자결제 앱, 메신저 등)이 돌아가는 장치(스마트폰, 태블릿, 노트북, IOT 장치 등)를 의미하며, 4G LTE에서는 UE(유저 장비, User Equiment)라고 한다.    모바일 장치는 보통 5개의 TCP/IP 5계층을 전부 구현하며, 네트워크의 말단에 위치하며, NAT가 제공한 IP 주소로 이야기한다.    모바일 장치는 SIM(Subscriber Identity Module) 카드에 64비트 식별자인 IMSI(국제 모바일 구독자 식별,International Mobile Subscriber Identity)를 가지고 있어, 전세계 무선전화 네트워크 상에 유일하게 식별 될 수 있다.    SIM 카드에는 이외에도 서비스에 대한 정보와 접근 및 암호화 키 정보를 포함하고 있다.        기지국(Base station)    기지국은 네트워크 말단에 위치하여 범위 내(figure 7.17의 육각형 모양)의 무선 라디오 자원과 모바일 장치를 관리한다.    모바일 장치는 기지국과 통신을 통해 네트워크에 연결될 수 있는데, 기지국은 라디오 접근 네트워크에서 장치들의 인증과 채널 접근 같은 네트워크 자원 할당을 맡는다.    AP와 비슷하지만, 몇몇 추가적인 역할을 맡는데, 게이트웨이와 모바일 장치를 잇는 장치별 IP 터널을 만든다.    추가로 다른 기지국과 상호작용하여 셀 내부에서 장치의 이동과 셀 간 라디오 주파수의 간섭을 최소화하기 위해 관리한다.    4G LTE에서는 eNode-B라고 부른다.        HSS(홈 구독자 서버, Home Subscriber Server)    컨트롤 측면의 요소로, HSS가 소속하는 네트워크에 소속된 모바일 장치에 대한 정보를 담고 있는 데이터베이스이다. MME와 함께 장치의 인증에 사용된다.        S-GW(서빙 게이트웨이, serving gateway), P-GW(패킷 데이터 네트워크 게이트웨이,Packet Data Network Gateway, PDN gateway), 다른 네트워크 라우터들        Figure 7.18에서 보듯이, S-GW와 P-GW는 인터넷과 모바일 장치 경로상에 위치한 라우터로, P-GW는 외부 인터넷을 위해 모바일 장치의 이동성을 관리한다.    PDN 게이트웨이(P-GW)는 NAT IP 주소를 모바일에게 부여하며 NAT 기능도 수행하며, 데이터그램이 인터넷에 들어가기 전 마지막 LTE요소이자 관문이다.    추가적으로 통신사의 all-IP 중심 네트워크에는 평범한 라우터 역할을 하는 라우터들도 많다.        MME(이동성 관리 개체,Mobility Management Entity)    Figure 7.18에서 MME 또한 컨트롤 측면 요소로, HSS와 함께 연결을 원하는 장치의 인증하고 장치나 PDN 인터넷 게이트웨이와의 데이터 터널을 생성하며, 활성화된 모바딜 장치의 셀 위치를 유지한다. 하지만 figure 7.18의 사용자 데이터 측면에서 경로상에 위치하진 않는다.                  인증(Authentication)        모바일 장치 측에서는 적접한 네트워크인지, 네트워크 측에서는 등록된 IMSI를 가진 장치가 맡는지 확인하는 것이 중요하며, MME는 장치와 HSS 사이의 중간자 역할을 하며, 정확히는 모바일 장치의 요청을 받아, HSS와 접촉해 암호화된 정보를 받는다.        이 암호화된 정보는 모바일 장치 측에서는 적접한 네트워크인지, 네트워크 측에서는 등록된 IMSI를 가진 장치가 맡는지 증명하기 위한 정보이다.        위의 경우 MME와 HSS, 모바일 장치는 같은 네트워크에 속해 있을 때며, 만약 모바일 장치가 다른 회사 소속의 네트워크에 로밍 중이면, 모바일 장치의 원래 홈 네트워크의 HSS와 로밍 중인 MME가 통신하며 인증한다.                    경로 설정(Path setup)        통신하는 데이터들은 모바일 장치와 기지국 간에 한 홉 연결 이후, S-GW와 P-GW로 이루어진 IP 터널을 지나게 된다.        이 터널은 MME가 제어하며, 라우터간의 패킷 라우팅을 대체하여 포워딩하여 장치의 이동에 대응할 수 있게 해준다.                  장치의 이동이 감지되면 터널의 끝단만 바뀌며, 그 이외의 터널 지점이나 QOS(Quality of Service) 등의 기타 정보는 바뀌지 않게한다.                            셀 위치 추적(Cell location tracking)        장치가 셀 간 이동을 할때, 기지국은 장치의 새로운 위치를 MME에 업데이트하지만, 만약 장치가 수면 모드인 상태에서 셀간 이동을 한다면, 기지국이 더이상 추적을 하지 못하므로, MME가 페이징(Paging)이라는 절차를 통해 장치를 깨우게 된다.            아래 Table 7.2는 LTE 구조 요소들에 대한 간략한 설명과 WLAN 요소와의 비교이다.            LTE 요소      설명      유사한 WLAN 기능(s)                  모바일 장치(mobile device) (UE: User equipment, 유저 장비)      사용자의 IP 지원 무선/ 모바일 장치(스마트폰, 온도계, 노트북)      호스트, 말단 시스템              기지국(Base station)(eNode-B)      LTE 네트워크 측의 무선 접근 링크      Access point(AP)와 비슷,  기지국이 몇몇 기능을 더욱 제공함              MME(이동성 관리 개체,Mobility Management Entity)      모바일 장치들의 인증, 이동성 관리      Access point(AP)와 비슷,  MME가 몇몇 기능을 더욱 제공함              HSS(홈 구독자 서버, Home Subscriber Server)      모바일 장치의 홈 네트워크에 소속되어, 해당 네트워크에서의 장치의 인증, 접근 우선순위 정보 등을 제공      없음              S-GW(서빙 게이트웨이, serving gateway), P-GW(패킷 데이터 네트워크 게이트웨이,Packet Data Network Gateway)      외부 네트워크와의 패킷 포워딩을 담당하는 통신사의 네트워크의 라우터      ISP 접근 네트워크에서의 iBGP와 eBGP 라우터              라디오 접근 네트워크(Radio Access Network)      모바일 장치와 기지국 간의 무선 연결      모바일 장치와 AP 간의 802.11 무선 링크      [Table 7.2 LTE 요소들과 유사한 WLAN(WiFi) 기능들(LTE Elements, and similar WLAN (WiFi) functions)]2G, 3G, 4G의 구조적 혁명(THE ARCHITECTURAL EVOLUTION FROM 2G TO 3G TO 4G)2G의 듀얼 서킷 기반에서 4G의 패킷 기반까지 어떻게 구조를 변화시켰는지 알아보자.figure 7.19의 2G 네트워크는 서킷 스위치 기반 모바일 전화 네트워크였지만, 데이터 대신 목소리를 옮기는 방식이였고, 구조 자체는 크게 다르지 않았다.이후 Figure 7.20의 3G 무선전화 구조로 바뀌었을 땐 2G의 음성 데이터와 패킷 데이터를 둘다 활용하는 구조가 되었다.중간 RNC에서 두 갈래로 갈라져 음성 처리와 패킷 처리로 나뉜다.이렇게 기존의 시설을 활용하면서, 새로운 구조를 적용하였다.7.4.2 LTE 프로토콜 스택(LTE Protocols Stacks)4G LTE 구조가 all-IP 구조가 되면서, 새로운 LTE 프로토콜 스택은 이전에 배웠던 TCP, UDP, IP 프로토콜 등 익숙한 개념을 많이 찾아 볼 수 있다.Figure 7.21은 LTE 모바일 노드의 유저 측면 프로토콜 스택과 서빙 게이트웨이, 기지국 등을 보여준다.LTE는 모바일 장치 연결 계층을 3개의 부계층으로 나누는데,      패킷 데이터 전환(Packet Data Convergence)    연결 계층 내에서 가장 상위 부계층으로 IP 바로 아래에 존재하며, PDCP(패킷 데이터 전환 프로토콜, Packet Data Convergence Protocol)은 무선 통신에 보낼 비트양을 줄이기 위해 IP 헤더 압축 실시하며,  MME와 장치간의 첫 메시지 교환때 확보한 키를 이용한 IP 데이터그램의 암호화/복호화 또한 이뤄진다.        라디오 링크 컨트롤(RLC)(Radio Link Control)    RLC 프로토콜은 크게 두가지 중요한 역할을 수행한다.          연결계층 프레임에 들어가기 너무 큰 IP 데이터그램의 분화와 조립      ACK/NAK 기반 ARQ 프로토콜(섹션 3.4.1)을 이용한 연결 계층 데이터 신뢰성 전송            매체 접근 제어(MAC)(Medium Access Control)    MAC 계층은 전송 스케듈링, 즉 라디오 전송 슬롯의 요청과 사용 허가를 관리하며, 중복 비트 전송을 이용한 추가적인 에러 감지 및 수정 또한 이루어진다.  Figure 7.21을 보면 MME가 관리하는 터널을 이용해 데이터를 통신하는 것을 알 수 있는데, 모바일 장치가 네트워크에 연결되면, 두 끝점 사이의 터널들은 각각 유일한 터널 끝점 식별자(TEID, tunnel endpoint identifer)를 가진다.모바일 장치가 기지국에 데이터그램을 보내면, 데이터그램은 GPRS Tunneling Protocol에 따라 TEID를 포함해 캡슐화되며, UDP 세그먼트에 추가로 캡슐화되어 서빙 게이트(Serving Gateway, S-GW)로 보내진다.반대로 모바일 장치가 패키을 받을 때는 기지국에서 UDP 세그먼트에서 IP 데이터그램을 추출해 보내준다.7.4.3 LTE 라디오 접근 네트워크 (LTE Radio Access Network)LTE는 다운스트림 채널에 대해 주파수 나눔과 시간 나눔을 둘다 이용하는 OFDM(직교 주파수 나눔 멀티플렉싱, orthogonal frequency division multiplexing)을 이용한다. 신호를 서로 다른 주파수 대역에 아주 촘촘하게 배치하여 서로 간선을 최소화하여 붙여진 이름이다.LTE에서는 각 활성된 모바일 장치에게 1개 이상의 채널 주파수 내의 1개 이상의 0.5 ms 시간 슬롯들을 제공한다. 예를들어 Figure 7.22에는 8개의 시간 슬롯을 4개의 주파수에 나누어 할당한 모습을 볼 수 있다.많이 할당 받을 수록 더 빠른 전송 시간을 자랑하며, 슬롯의 할당은 매 ms마다 이루어진다.또한, 조정(modulation) 방법이 바뀜에 따라 전송율이 달라질 수도 있다.모바일 장치들에게 타임 슬롯을 할당하는 방법은 공식화되지 않았고, 네트워크 관리자나 장비 판매사의 알고리즘에 의해 결정된다.송수신자 사이의 채널 상태에 맞게 물리 계층 프로토콜을 고르고, 수신자에게 채널 상태에 따라 보낼 패킷을 고르게 하면, 기지국이 최대한 무선 매체를 이용할 수 있다.추가로 사용자 우선순위나 서비스 등급에 따라 다운스트림 전송 스케듈에 차등을 줄 수 있다.LTE-Advanced 에서는 채널을 모바일 장치에 할당하여 수백 Mbps의 다운스트림 대역폭을 할당할 수 있다.7.4.4 추가적인 LTE 기능들 : 네트워크 결합과 전력 관리(Additional LTE Functions: Network Attachment and Power Management)네트워크 결합 기능은 해당 모바일 장치가 처음으로 네트워크에 참여할 때의 절차이고,전력 관리 기능은 모바일 장치가 중심 네트워크 요소와 함께 전력 사용을 관리하는 것이다.네트워크 결합 (Network Attachment)모바일 장치가 통신사의 네트워크에 참여할 때의 절차는 3 단계로 나뉜다.      기지국에 결합(Attachment to a Base Station)    LTE에서의 네트워크 참여 첫 단계는 섹션 7.3.1에서 배웠던 무선 LAN에서의 방법과 목적은 비슷하지만 방법은 다르다.    첫번째는 모바일 장치가 주변의 기지국을 알아보고, 참여하는 과정이다.                  모바일 장치가 기지국이 발하는 주기적인(5ms) 최초 신호를 잡기 위해 모든 채널의 모든 대역폭을 찾아본다.                    최초 신호가 잡히면 이 주파수를 유지하면서 두번째 동기화 신호를 찾는다.                    2번째 신호가 잡히면 장치는 채널 대역폭, 설정, 기지국의 통신사 정보 등의 정보를 얻는게 가능하다.                    모바일 장치는 잡힌 여러 기지국 신호 중 하나를 골라(주로 홈 네트워크) 무선 홉을 이용해 컨트롤 측면 신호 연결을 수립한다.                    장치와 기지국간의 채널은 나머지 절차 동안 유지된다.                  상호 인증 (Mutual Authentication)    기지국이 MME(Mobility Management Entity)와 연락해 상호 인증을 실시한다.    MME를 매개로 장치는 해당 네트워크가 적법한지, 네트워크는 해당 장치의 IMSI가 일치하고, 믿음직한지 확인하게 된다.    상호 인증이 끝나면, MME와 모바일 장치는 상호 인증되었고, MME 또한 모바일 장치와 연결된 기지국의 정보를 알고 있게 된다.    MME는 이 정보들을 가지고 장치-PDN 게이트웨이 데이터 경로를 설정한다.        모바일 장치-PDN 게이트웨이 간 데이터 경로 설정 (Mobile-device-to-PDN-gateway Data Path Configuration)    MME는 PDN 게이트웨이(NAT 서비스도 제공함), 서빙 게이트웨이, 기지국과 만나 figure 7.21과 같은 터널 두개를 만든다.    이제 모바일 장치는 IP 데이터그램을 터널과 기지국을 통해 인터넷과 주고받을 수 있다.  전력 관리: 수면 모드(Power Management: Sleep Modes)4G LTE에는 블루투스와 802.11과 다르게 두 가지의 전력관리 모드가 존재한다.첫번째 비연속적 수신 상태는 활동을 하지 않은 순간부터 수백 밀리초 이후에 진입하며, 일종의 가벼운 수면 상태이다.이때, 모바일 장치와 기지국은 주기적으로 모바일 장치가 깨어나 다운스트림 채널을 모니터링할 시간을 정하고, 모바일 장치의 라디오 송수신 장치는 비활성화 된다.두번째 대기 상태는 5~10초 정도 활동을 하지 않은 순간에 진입하며, 일종의 깊은 수면 상태이다.이때, 모바일 장치는 비연속적 수신 상태때보다 더 드물게 일어나고 채널을 모니터링하며, 심지어 셀(cell) 간 이동이 일어나도 모바일 장치는 셀 이동을 알리지 않는다.그러므로 드물게 깨어났을 때, 새로운 셀에 도착해 있다면, 기지국과 새로운 연결을 수립하고, MME가 이전 속했던 기지국에서 가져와 브로드캐스트한 페이징 메시지를 체크한다.해당 페이징 메시지에는 셀에 소속하는 모든 장치에게 이전 소속 기지국이 브로드캐스트했던 메시지로, 해당 모바일 장치를 대상으로 도착한 패킷이 있으니, 활성 상태로 돌아와 기지국과 연결을 재수립해야하는 모바일 장치들에 대한 정보가 있다.모바일 장치에 자신이 있다면, 수면 상태 동안 들어온 패킷이 있다는 의미이니 이전 기지국과 연결을 재수립하고 패킷을 가져온다.7.4.5 범지구적 무선전화 네트워크: 네트워크들의 네트워크 (The Global Cellular Network: A Network of Networks)Figure 7.23은 사용자의 스마트폰이 4G 기지국을 통해 홈 네트워크(home network)에 연결되어 있는 구조이다.사용자의 홈 네트워크는 통신사측에서 지원하며, 다른 통신사나 인터넷에 하나 이상의 게이트웨이 라우터를 통해 연결되어 있다.이런식으로 모바일 네트워크들은 인터넷이나 IPX(Internet Protocl Pacekt eXcahnge) 네트워크를 통해 서로 연결되어 있으며,특히, IPX는 IXP(Internet eXchange Point)와 비슷하게 각기 다른 통신사, ISP의 모바일 네트워크 간의 연결을 위해  관리되고 있는 네트워크이다.범지구적 무선전화 네트워크는 인터넷처럼 이러한 네트워크들의 네트워크이며, 4G와 3G, 그 이전의 전화 네트워크와도 통신할 수 있다.7.4.6 5G 휴대전화 네트워크(5G Cellular Networks)5G, 정확히는 3GPP에서 표준으로 택한 5G NR(New Radio)는 4G의 개선된 버전으로, 최대 비트 전송율 10배, 지연 시간 1/10배, 트래픽 허용량은 100배 정도로 추정되며, 더욱 광범위한 커버리지를 자랑한다.5G가 상용되면, VR, AR, 무인 자동차 제어 등의 강력한 무선 전송이 필요한 기술의 발전과, 유선망으로 이루어졌던 주거지 접근 네트워크를 고정 무선 인터넷 서비스(fixed wireless internet service)로 바꿀 수 있을 것이다.5G 표준은 주파수를 FR1(450 MHz-6GHz)와 FR2(24 GHz-52 GHz) 둘로 나누었다.FR2가 버라이즌의 5G TF 라는 다른 표준으로 스마트폰용이 아닌 고정 무선 인터넷 서비스 용으로 배포된 바가 있지만, 초기에는 FR1이 널리 배포될 것이다.또한, 5G는 이전의 4G 모바일 의사소통 체계와 호환되지 않으므로, 5G를 배포하려는 통신사는 막대한 시설 투자와 스프트웨어 업그레이드가 필요할 것이다.FR2 주파수는 밀리미터 파동 주파수(millimeter wave frequencies)로 알려져 있으며, 더욱 빠른 데이터 속도를 자랑하지만 두가지 단점이 있다.      더욱 좁은 범위를 가지고 있어 지정학적으로 광범위, 많은 양의 기지국 수립이 필요하다.        대기 간섭에 취약함, 나뭇잎이나 비에 의해서도 문제가 생길 수 있음  5G에는 추가로 3개의 표준이 존재한다.      eMBB (Enhanced Mobile Broadband, 향상된 모바일 광대역)    5G NR 배포의 초기 배포의 주역,    4G LTE 기준으로 향상된 대역폭에 의한 데이터 송수신 속도 증가 및 지연속도 감소 등이 있었으며,    이로인해 360도 비디오 스트리밍이나 AR, VR 등의 거대 미디어 응용 프로그램이 가능해 졌다.        URLLC (Ultra Reliable Low-Latency Communications, 초 신뢰성 저지연 의사소통)    URLLC은 낮은 지연속도가 아주 중요한 곳에 쓰인다. 주로 공장 자동화와 자율 주행 등에 사용되면 1msec 이하의 지연시간이 목표이다.        mMTC (Massive Machine Type Communications, 거대 머신 타입 의사소통)    감지, 측정, 모니터링을 위한 협대역(좁은 대역폭) 접근 유형의 표준으로, 낮은 지연시간과 저전력 소모, 낮은 연결 장벽이 목표로 주로 IOT 장비등에 사용된다.  5G와 밀리미터 파동 주파수 (5G and Millimeter Wave Frequencies)많은 5G 혁신들은 밀리미터 파동 주파수에 의한 4G LTE 대비 100배의 용량 상승으로 인해 이루어질 것이다.용량은 다음과 같이 계산한다.용량(bps/$km^2$) = 셀 밀도(cells/$km^2$) X 사용가능 스펙트럼(대역폭)(Hz)X 스펙트럼 효율(bps/Hz/Cell)셀 밀도는 일정 면적 당 셀의 갯수, 스펙트럼 효율(spectral efficiency)은 각 기지국이 얼마나 사용자와 효율적으로 통신하는 지이다.용량이 크게 늘어나는 이유는 다음과 같다.  밀리미터 파동 주파수는 범위가 짧으므로 셀을 여러개 둘 수 밖 없으므로 셀 밀도가 늘어난다.  5G FR2의 대역폭은 52-24=28GHz 이므로, 4G LTE의 2GHz에 비해 14배 넓다.  스펙트럼 효율을 2배 올리려면 17배의 전력 증가가 필요하지만, 5G 기지국은 여러개의 MIMO 안테나를 이용해 사용자 방향에 빔 형식으로 주사(beam forming)하므로 같은 시간, 같은 대역폭에서 동시에 10~20명의 사용자에게 전송 가능하다.용량 구하는 식의 3개 요소가 전부 증가하므로 100배의 용량 증가를 이루어 낼 수 있다. 뿐만 아니라 대역폭이 더 넓어 최대 다운로드 속도가 1Gbps 이상이다.하지만 밀리미터 파동 주파수는 빌딩이나 나무 등에 쉽게 막히므로 작은 크기의 셀을 여러개 두는 식(도심지 기준 10~100m 사이)으로 해결해야 한다.5G 중심 네트워크(5G Core Network)5G 중심 네트워크(5G Core Network)는 모든 5G 모바일 음성, 데이터, 인터넷 연결을 관리하는 데이터 네트워크이다.더욱더 나은 인터넷, 클라우드, 분산 서버, 캐쉬와의 호환성과 지연시간 감소를 위해 설계되었으며, 네트워크 기능 가상화(NFV, SDN 부분 참조)와 각기 다른 서비스와 응용 프로그램을 위한 네트워크 나누기 등이 관리되고 있다.4G의 요소들(MME, HSS, 셀, PDN 게이트웨이 등)을 많이 공유하고 있지만, 5G 중심 네트워크는 완전한 컨트롤 측면과 유저 측면의 분리가 이루어졌고, 순수하게 가상화 소프트웨어(virtualized software-based) 기반 네트워크 기능을 가지고 있어, 다양한 응용 프로그램의 요구에 유연성있게 대응할 수 있다.      유저 측면 기능(User-Plane Function) (UPF)    컨트롤 측면과 유저 측면의 분리는 패킷 처리를 분산적으로, 네트워크 말단(edge)에서 처리할수 있게 해준다.        접근 및 이동성 관리 기능(Access and Mobility Management Function) (AMF)    5G 중심은 MME를 두가지 요소, AMF와 SMF로 나누었다.    AMF는 모든 연결과 세션 정보를 사용자 장비로 부터 받아 오직 연결과 이동성 관리 업무를 처리한다.    SMF는 아래 참조        세션 관리 기능 (Session Management Function) (SMF)    SMF는 분리된 데이터 측면과 상호작용 하는 것과 세션을 관리한다. 또한 추가로 IP 주소 관리와 DHCP의 역할도 겸한다.  5G 표준은 여전히 개발 중이고, 앞으로 미래에 궁극의 광역 무선 서비스가 될 수 있을 것이다.7.5 이동성 관리: 원칙(Mobility Management: Principles)모바일 장치은 넓은 의미로, 시간에 따라 자신이 속한 네트워크를 끊임없이 바꾸는 장치이다.이동성은 분야마다 여러 의미로 쓰인다.7.5.1 장치 이동성: 네트워크 계층 관점 (Device Mobility: a Network-layer Perspective)장치 이동성의 도전 정도는 장치가 얼마나 능동적으로 네트워크를 이동했느냐이다.시나리오에 따라 이동성을 Figure 7.24 처럼 정리할 수 있다.            a      b      c      d                  장치가 접근 네트워크 사이를 전원이 내려간 채로 이동      장치가 같은 통신사의 같은 무선 접근 네트워크 사이를 이동      장치가 같은 통신사의 같은 무선 접근 네트워크 사이를 연결을 유지한 채로 이동      장치가 여러 다른 통신사의 네트워크를 연결을 유지한 채로 이동      예를 들어 (a) 시나리오는 전원이 켜져있는 동안은 네트워크에 존재하며, 스스로 연결을 종료하고 새로운 연결을 시도하는데, 이는 기존의 네트워크 기능으로도 충분히 가능하므로 네트워크 계층 관점에서 모바일 장치가 아니라고도 볼 수 있다.시나리오 (b)의 경우 장치가 물리적으로 이동하지만, 소속 네트워크가 바뀌거나 나간적 없으므로, 이 또한 네트워크 계층 관점에서 모바일 장치가 아니다.만약 AP나 LTE 기지국 무선 범위 내에 나간적도 없다면, 연결 계층 관점으로 봐도 모바일 장치가 아니다.시나리오 (c)의 경우, 소속 네트워크가 바뀌며, 심지어 TCP 연결 등을 유지하고 있으므로 이 시점부터 네트워크 이동 시 핸드오버(handover) 과정이 필요하다.핸드오버는 특정 AP나 기지국에서 다른 AP나 기지국으로 데이터그램의 송수신 책임을 전달하는 것이다.이 핸드오버 과정은 같은 통신사 네트워크 하에서는 쉽지만, 시나리오 (d)처럼 다른 통신사 네트워크라면 절차가 복잡해진다.7.5.2 홈 네트워크와 방문한 네트워크에서의 로밍(Home Networks and Roaming on Visited Networks)각 구독자들은 홈에 해당하는 네트워크를 가지고 있고, 해당 홈 네트워크의 HSS에는 해당 구독자의 유일하게 구분되는 장비 ID와 구독자가 접근하는 서비스, 암호화 키, 서비스 구매 기록 등이 있다.이러한 홈 네트워크가 아닌 다른 네트워크에 연결되는 것을 로밍(roaming)이라고 하며, 이때 다른 네트워크는 방문 네트워크(visited network)라고 한다.인터넷 프로토콜 측에서는 이러한 홈 네트워크, 방문 네트워크 같은 개념을 찾아보기 힘들고, 이를 위해 모바일 IP 프로토콜이 생겨나, 해당 하는 부분을 대체하려 하지만, 일부 한계를 보이자, 방문한 네트워크 전반에 인증 네트워크 접근을 제공하기 위해 기존의 IP 프로토콜 위에서 작동하는 Eduroam 같은 프로토콜도 있다.홈 네트워크의 개념이 주는 이점은 두가지가 있는데, 하나는 해당 장치에 대한 정보를 찾을 수 있는 장소가 확정되어있으며, 두번째는 로밍 중인 모바일 장치와 의사소통하는 조정 지점으로 활용 가능하다.홈네트워크를 비유하자면 마치 계속 이사다니는 사람(모바일 장치)의 부모님댁(홈 네트워크)과 같다고 생각하면 된다, 만약 스마트폰이 없다고 가정하고, 이사다니는 사람과 편지을 주고 받으려면, 계속 바뀌는 주소를 얻기위해 부모님댁에 물어보아 정보를 얻어낼 수 있고, 여기서 얻은 새로운 주소 정보를 물어봐서 직접 편지를 보내는 직접적인 방법과, 부모님댁에 맡겨두어 나중에 주는 간접적인 방법이 있을 수 있다.7.5.3 모바일 장치에 대한 직간접적 라우팅(Direct and Indirect Routing to/from a Mobile Device)figure 7.25는 홈 네트워크에 속해있거나 로밍중인 호스트와 통신에 대한 그림이다.모바일 장치는 4G LTE로 치면, IMSI 같은 세계적으로 유일한 식별자와 전화번호 등의 정보가 SIM 카드에 들어있으며, 모바일 인터넷로 치면, 홈 네트워크 IP 법위내의 귀속된 IP 주소를 가지고 있다.모바일 네트워크 구조에서 어떻게 해당 모바일 장치와 통신할 지는 3가지 방법이 있고, 마지막 2개는 실무에서도 사용되고 있다.기존 IP 주소 기반 활용하기 (Leveraging the Existing IP Address Infrastructure)방문 네트워크의 호스트와 통신하는 가장 간단한 방법은 기존 IP 주소 구조를 사용하는 것이다.ISP들이 BGP 알고리즘을 이용해 보유하고 있는 CIDR화 IP 주소 범위를 전파하고, 도달가능하게 만들었기 때문에, 모바일 장치의 IP 정보 역시 주변 네트워크에 전파해 놓는다면, 주변 네트워크들 또한 포워딩 테이블과 라우팅 정보를 갱신하여 패킷이 방문 네트워크와 모바일 장치 주소로 포워딩해올 수 있을 것이다.네트워크를 떠나는 장치나 새로 등록한 장비 또한, 새로운 라우팅 정보를 삭제하거나 추가하는 방식으로 전파하면 될 것이다. 즉, 기존의 IP 네트워크 구조에서 바꾸지 않아도 통신 문제와 이동 문제가 둘다 해결된다.하지만 치명적인 단점이 있는데, 바로 확장성이 떨어진다는 점으로, 수많은 장치가 떠나고 들어올 때 마다, 수많은 라우터들이 수많은 포워드 테이블 항목을 바꾸고, 갱신해줘야 한다. 이는 현실적으로 불가능하다. 추가적인 단점은 챕터 끝자락에 나온다.현실적인 다른 방법으로는 홈 네트워크 구조를 이용하는 것이다. 홈 네트워크의 MME가 방문 네트워크 내의 홈 네트워크 소속 장치를 추적하면서 HSS 데이터베이스의 정보를 갱신하고, 필요할 때는 방문 네트워크에 제공한다. 마치 이전에 비유했던 부모님댁과 비슷하다.기존의 MME와 HSS는 4G LTE의 비유지만 우리가 익숙하므로 IP 구조인 여기에서도 사용하겠다.방문 네트워크에서 장치는 방문 네트워크 내의 IP 주소가 필요하며 그 방법은 3가지가 있다.홈 네트워크와 관련된 영구 주소(홈 네트워크 내에 속한 영구 IP 주소) 사용, 방문 네트워크에게 새로운 IP 주소 받기, 방문 네트워크의 NAT 서비스 이용하기후자 2개의 방법은 사용자가 계속 바뀌는 IP 주소에 직접 패킷을 보내는 방법이다.추가로 NAT 방식이면, 방문 네트워크의 NAT 게이트웨이 라우터에 도착할 것이고, 패킷의 주소는 NAT 주소 번역에 의해 모바일 장치로 이동할 것이다.이제 모바일 장치가 이동 뒤, IP 주소를 얻는 방법을 알았다. 하지만 어떻게 모바일 장치로 패킷을 보낼까? HSS만 모바일 장치의 현재 위치를 알고 있으므로, 모바일 장치의 영구 주소에 보낼 수 없다. 그에 대한 해답은 아래에 배울 두 방법이 있다.모바일 장치를 향한 간접 라우팅 (Indirect Routing to a Moblie Device)간접 라우팅 방식(Indirect Routing)에는 송신자가 모바일 장치의 홈 네트워크 영구 주소에 패킷을 보낸다. 이 방법은 송신자가 현재 장치의 네트워크 위치가 어디있는지 신경쓰지 않아도 된다. 마치 영원히 홈 네트워크에 속해있는 것 처럼 보일 것이다. 이 방법은 Figure 7.26에 묘사되어 있다.HSS는 방문 네트워크와 상호작용해 모바일 장치의 위치를 추적하고, 홈 네트워크의 게이트웨이 라우터와 상호작용한다.게티으웨이 라우터는 도착한 데이터그램의 주소가 홈 네트워크 소속이지만 다른 방문 네트워크에 위치한 모바일 장치의 것이라면, HSS와 상호작용하여 방문 네트워크 게이트웨이 라우터 주소를 알아내고, 해당 방문 네트워크 게이트웨이로 포워딩 한다.(Figure 7.26의 2번 과정)그렇게 도착한 게이트웨이 라우터에서 데이터그램은 모바일 장치에게 라우팅되거나 NAT 네트워크라면 NAT 번역으로 패킷을 받는다 (Figure 7.26 3번 과정)2번 과정을 좀더 자세하게 설명하자면, 홈 네트워크 게이트웨이 라우터는 데이터그램의 패킷을 방문 네트워크 게이트웨이 라우터로 향하게 해야하는데, 이때 목적지 주소를 HSS에게 받은 방문 네트워크에 존재하는 모바일 장치의 임시 IP 주소로 바꿔줘야 한다.하지만, 원본 IP 데이터그램의 값을 변경하면, 네트워크 철학에도 어긋나고, 응용 프로그램들이 데이터그램이 변조된 것으로 인식할 수 있으므로, 원본 데이터그램은 그대로 두고, 새로운 더욱 큰 데이터그램으로 캡슐화한 뒤, 그곳에 목적지 주소를 모바일 장치 주소로 바꾼 뒤, 방문 네트워크 측으로 보내면, 방문 네트워크 측의 게이트웨이 라우터가 디캡슐화하여 원본 데이터그램으로 바꾼 뒤, 손님으로 있는 모바일 장치에게 넘겨 준다.이 과정은 마치 IPv6와 IPv4의 상호 호환을 위한 과정과 비슷하고, 4G LTE에도 존재하는 터널링(tunnerling)의 개념이다그렇다면 반대로, 방문 네트워크의 모바일 장치가 원래 송신자였던 호스트에게 답신하려면 어떻게 해야할까? 두가지 방법이 있는데,  Figure 7.26(a)의 방법, 데이터그램을 다시 홈 네트워크로 보낸 뒤, 홈 네트워크 측에서 되돌려주는 방법  Figure 7.26(b)의 방법, 데이터그램을 방문 네트워크에서 바로 보내는 방법, 통칭 LTE에서의 지역 발생(local breakout) 방법이동성을 위한 간접 라우팅 방법을 이용하기 위한 네트워크 계층의 필요 기능을 정리해보자.      모바일장치-방문 네트워크 간 참여 프로토콜(A mobile-device–to–visited-network association protocol)    모바일 장치가 방문 네트워크에 참여하고, 떠나는 프로토콜이 필요하다.        방문 네트워크-홈네트워크 HSS 간 등록 프로토콜(A visited-network–to–home-network-HSS registration protocol)    방문 네트워크 측은 모바일 장치의 홈 네트워크 내부의 HSS와 의사소통해, 현재 모바일 장치의 위치를 알려주고, HSS에게서 모바일 장치의 정보를 가져와 인증을 실시한다.        홈 네트워크와 방문 네트워크, 각각 게이트웨 라우터들 간의 데이터그램 터널링 프로토콜(A datagram tunneling protocol between in the home network gateway and the visited network gateway router)    터널링에서 송신자측은 데이터그램을 캡슐화한 뒤, 터널 반대편 수신자 측으로 포워딩하고, 터널링 수신자 측은 디캡슐화를 진행한 뒤, 원본 데이터 그램을 방문 중인 모바일 장치에게 포워딩해준다.  어떻게 장치가 네트워크 사이를 이동하는가, 그리고 어떻게 방문 네트워크에 있는 장치와 통신하는가를 알 수 있었다. 여기서 의문점이 하나 생긴다.만약, 모바일 장치가 한 네트워크에서 다른 네트워크로 이동하는 사이에 터널을 통과하고 있던(figure 7.26의 2번), 또는 TCP 연결 등지로 전송하려던(figure 7.26 1번) 패킷은 어떻게 되는가?네트워크 이송 사이에 전송된 패킷은 보통은 손실 처리되며, 만약, 데이터 신뢰성 전송이 필요하면, 송수신자의 호스트 간의 상위 계층의 데이터 신뢰성 전송이 재전송 처리한다.간접 라우팅은 모바일 IP 표준과 4G LTE 네트워크에서 사용하는 방법이며 자세한 사항은 [RFC 5944], [Sauter 2014] 참고 바람.하지만 이러한 간접 라우팅은 단점이 있는데, 다음의 직접 라우팅에서 설명하겠다.모바일 장치를 향한 직접 라우팅(Direct Routing to a Mobile Device)앞서 설명한 간접 라우팅은 삼각 라우팅 문제(triangle routing problem)이라는 문제가 존재하는데 홈 네트워크를 거쳐서 방문 네트워크로 도착해야하는 데에서 생기는 비효율성에 대한 문제이다.예를 들어, A라는 네트워크가 홈 네트워크인 기기와 방문 네트워크인 기기 둘이 통신하려면, 거리상으로는 라우터를 거쳐 두 홉이면 충분하지만, 간접 라우팅의 경우 굳이 홈 네트워크 까지 거쳐야 가능하다.직접 라우팅 (Direct routing)은 삼각 라우팅 문제에서 자유롭지만 추가적인 복잡함이 있다.figure 7.27에 묘사 되어있는 방법으로, 먼저 통신하려는 기기의 홈 네트워크의 HSS와 통신해 현재 기기가 위치한 방문 네트워크를 알아낸다.(1번, 2번 과정)그 뒤부터 방문 네트워크 측의 게이트웨이 라우터와 직접 터널을 만들어 통신하면 된다.(3번, 4번 과정)직접 라우팅 (Direct routing)은 삼각 라우팅 문제에서 자유롭지만 두가지 중요한 문제가 있다.  모바일 사용자 위치 프로토콜이 필요하다. 간접 라우팅 프로토콜에서는 HSS가 모바일 기기의 위치를 보고받고, 홈 네트워크 게이트웨이 라우터에게만 제공했지만, 이제는 외부의 사용자에게도 알려줘야 한다.  모바일 기기가 다른 네트워크로 이동하게 되면, 이 이동한 새로운 위치를 HSS 뿐만 아니라 송신자에게도 갱신해주는 프로토콜이 필요하다. 간접 라우팅에서는 모바일 기기가 알아서 HSS에 보고 하였고, 홈 네트워크 측은 새로운 네트워크와 터널링 하면 됬지만, 직접 라우팅에서 송신자는 처음 한번만 HSS에게사 수신받을 모바일 기기의 네트워크 위치를 가져가므로, 이를 갱신할 필요가 있다.이 두가지 문제는 챕터 끝단에서 해결법을 알려준다.7.6 실무에서의 이동성 관리 (Mobility Management in Pratice)홈, 방문 네트워크의 개념, 홈 네트워크의 역할과 가지고 있어야할 모바일 기기 정보, 홈 네트워크의 MME의 컨트롤 측면 기능, 직간접 라우팅을 통한 통신에 대한 데이터 측면 접근 등을 배웠다.4G/5G에서의 이동성 관리와 모바일 IP 에 대해서 배워보자.7.6.1 4G/5G 네트워크에서의 이동성 관리(Mobility Management in 4G/5G Networks)앞서서 4G/5G의 중요한 요소와 역할에 대해서 배웟었다. 이번에는 이러한 요소들이 모바일 이동성을 관리하기 위한 2G와 3G가 기반인 상호작용 구조를 배워보자.먼저 Figure 7.28에서 기기 사용자가 4G/5G 네트워크에서 자동차로 이동하면서 동영상 스트리밍을 받는다는 시나리오에서의 4가지 단계에 대해서 배워보자.      모바일 기기의 기지국 참여 (Mobile device and base station association)    모바일 기기가 방문 네트워크의 기지국에 참여한다.        모바일 장치의 네트워크 요소에 대한 컨트롤 측면 설정 (Control-plane configuration of network elements for the mobile device)    방문 네트워크와 홈 네트워크가 방문 네트워크에 모바일 기기가 있다는 것을 가르키는 컨트롤 측면 상태를 수립한다.        모바일 기기의 터널 포워딜을 위한 데이터 측면 설정 (Data-plane configuration of forwarding tunnels for the mobile device)    모바일 장치가 영상 스트리밍 서버로 부터 IP 데이터그램을 간접적 라우팅을 통해 주고받기 위해 방문 네트워크 게이트웨이 라우터와 홈 네트워크의 PDN 게이트웨이 라우터 사이의 터널을 수립한다.        다른 기지국으로의 모바일 장치 핸드오버 (Mobile device handover from one base station to another)    모바일 기기가 방문 네트워크 소속을 핸드오버를 통해 바꾼다.  위 네 가지를 좀더 자세히 설명해 보면      기지국 참여(Base station association)    7.4.3에서 배웠던 것처럼, 장치가 모든 주파수에서 기지국이 발하는 최초 신호를 획득 후, 두번째 신호에서 기지국의 정보를 받아 네트워크에 참여하고, 모바일 기기와 기지국 간의 컨트롤 (시그널링) 채널을 시작한다. 모바일 기기는 이때, 기지국으로 IMSI를 보내어 모바일 기기를 인증하고, 홈네트워크와의 상호작용을 할 수 있게 한다.        모바일 기기 LTE 네트워크 요소의 컨트롤 측면 설정(Control-plane configuration of LTE network elements for the mobile device)    모바일 기기-기지국 시그널링 채널이 만들어지면, 기지국은 방문 네트워크의 MME에게 하여금 홈 네트워크와 방문 네트워크의 4G/5G 요소들을 설정해 모바일 노드의 행동에 따른 상태를 수립한다.          MME는 IMSI와 다른 정보들을 통해 모바일 장치에게 인증, 암호화, 사용가능한 서비스 정보 등을 제공한다. 이러한 정보들은 해당 방문 네트워크, 이전 방문 네트워크, 기기의 홈 네트워크 등에서 가져올 수 있으며, 이때의 상호 인증 과정으로 모바일 장치는 방문 네트워크를, 방문 네트워크는 모바일 장치의 정체를 인증하게 된다.      MME는 모바일 기기의 홈 네트워크 측의 HSS에게 현재 자신의 방문 네트워크에 해당 기기가 존재하고 있다고 알려주고, HSS는 이 사실을 데이터베이스에 갱신한다.      기지국과 모바일 장치는 기지국과 모바일 기기간의 데이터 측면 채널을 수립하기 위해 매개변수를 고른다.(컨트롤 측면 채널은 이미 세웠음)            모바일 기기의 터널 포워딜을 위한 데이터 측면 설정 (Data-plane configuration of forwarding tunnels for the mobile device)    MME는 데이터 측면 채널을 Figure 7.29처럼 수립한다. 이때 두개의 터널이 수립되는데,    하나는 기지국과 방문 네트워크 서빙 게이트웨이 라우터 사이,    나머지 하나는 방문 네트워크 서빙 게이트웨이 라우터와 모바일 기기의 홈 네트워크의 PDN 게이트웨이 라우터 사이에 존재한다.    4G LTE는 이러한 대칭적 간접 라우팅(symmetric indirect routing)을 수립하는데, 5G와 마찬가지로 GTP(GPRS Tunneling Protocol)[3GPP GTPv1-U 2019]를 이용한다.    TEID (터널 끝점 ID, Tunnel Endpoint ID)를 GTP 패킷의 헤더에 사용해 데이터그램의 터널 소속을 알려주어 여러 흐름이 GTP에 의해 멀티플렉스, 디멀티플렉스되게 해준다.    figure 7.29와 figure 7.18(홈 네트워크에서만 이동하는 모바일 이동성 처리)을 비교하면, 서빙 게이트웨이는 모바일 기기와 같은 네트워크에 존재하지만, PDN 게이트웨이는 모바일 기기와 다른 곳에 있으며(figure 7.18에는 같은 곳에 있다.), 이것은 간접 라우팅의 특성이다.    간접 라우팅과 다르게 서빙 게이트웨이가 같은 방문 네트워크 내에 존재하는 PDN 게이트웨이와 터널을 만드는 지역 발생(local breakout)이라는 방법도 있지만 많이 사용되진 않는다.    이제 장치는 외부 인터넷과 통신할 수 있다.        핸드오버 관리(Handover management)    핸드오버는 모바일 장치가 다른 네트워크의 기지국에 참여하려 할때 발생된다.    figure 7.30처럼 같은 네트워크 내의 다른 기지국으로 바꿀 때는 서빙게이트-기지국 간의 터널만 바뀌지만, 만약 다른 네트워크 소속의 기지국으로 바꾼다면 좀더 상황이 복잡해진다[Sauter 2014; GSMA 2019a].    핸드오버가 일어나는 이유는 여러가지가 있다. 예를 들어, 기지국과 기기 사이의 신호가 심각하게 약해지거나, 한 기지국이 너무나 많은 트래픽을 처리해야해서, 주변의 다른 기지국에 넘겨주는 등이다.    모바일 장치는 주변 기지국의 신호를 주기적으로 측정해 서비스의 상태를 예측할 수 있으며, 이러한 정보는 기지국에도 주기적으로 보고된다.    기지국은 이 정보를 토대로 자신의 혼잡상황을 알 수 있으며, 이때 다른 기지국에 핸드오버 해주는 신호 기준이나 기지국 선택 방법 등은 표준화되지 않았으므로, 통신사 등에서 알아서 구현한다.      ​        figure 7.30은 원(source) 기지국에서 타(target) 기지국으로 핸드오버할 때의 단계를 나타낸 그림이다.    1. 원 기지국에서 핸드오버할 타 기지국을 고른다. 이후 핸드오버 요청 메시지를 타 기지국에 보낸다.    1. 타 기지국은 모바일 기기를 도울 자원이 되는지, QOS(quaility of service) 등을 확인하고, 가능하다면, 타임 슬롯 같은 자원들을 미리 선점한다. 이러한 선점 행위 덕분에 기지국 참여 프로토콜을 최대한 생략하고 빠르게 참여할 수 있다. 타 기지국은 원 기지국에 모바일 기기가 네트워크 참여시 필요로 할 기지국 정보를 포함해서 핸드오버 요청 Acknowledge 메시지를 보낸다.    1. 원 기지국은 ACK 메시지를 받고 모방리 기기에게 타 기지국의 정보와 채널 접근 정보를 알려준다. 이때부터 모바일 기기는 타 기지국에서 데이터를 주고받을 수 있고, 모바일 기기 입장에서는 핸드오버가 종료된다.    1. 원기지국은 나간 모바일 기기로 향하던 데이터그램 터널을 종료하지 않고 유지하되, 모바일 기기에게 포워딩하지 않고 데이터그램들을 타 기지국으로 넘긴다.    1. 타 기지국은 MME에게 자신이 새로운 모바일 기기의 방문 기지국이라고 알리고, MME는 서빙게이트웨이 라우터와 타 기지국 사이의 터널을 설정해준다.    1. 타 기지국은 원 기지국에 데이터 터널이 만들어졌음을 알리고, 원 기지국에게 터널을 종료하고, 모바일 기기를 위해 할당한 자원을 풀게 함.    1. 타 기지국은 이제 터널을 통해 핸드오버 기간동안 도착했던 데이터그램을 포함한 데이터그램들을  모바일 기기와 주고 받을 수 있다.이러한 로밍 설정 방법은 5G에도 사용될 것이며, 5G는 기지국이 좀더 밀집되게 설정해야 하므로, 핸드오버가 자주 일어날 것이다. 또한, 컨트롤 측면을 SDN 프레임워크으로 바꾸게 된다면 더욱 큰 용량에 저 지연의 5G 무선전화 네트워크 컨트롤 측면이 될 것이며 활발히 연구 중이다.7.6.2 모바일 IP (Mobile IP)모바일 IP 구조와 프로토콜들을 이용한다면 4G/5G가 아닌 인터넷을 이용해서도 응용 프로그램에서 이동형 서비스를 제공할 수 있다.하지만 사람들의 사용례와 적절한 시기 놓침, 4G/5G 같은 강력한 라이벌으로 인해 모바일 IP의 발달이 느리다.아마 고정된 서비스 제공을 위한 WiFi와 이동성 서비스 제공을 위한 4G/5G 양강 체제는 수 십년간 지속될 것이다.하지만 모바일 IP 기술 또한 무선 전화 네트워크와 기본적인 이동성 원칙을 구현했으므로 살펴보도록 할 것 이다.모바일 IP와 무선전화 네트워크의 구조는 놀랍도록 유사하다. 모바일 기기는 영구 귀속 IP 주소를 가지고 있고, 외부(foreign = 4G에서의 방문(visited)) 네트워크에서는 의탁 주소를 받게 된다.모바일 IP에서 홈 에이전트는 모바일 기기의 위치를 추적하고, 외부 네트워크 소속 외부 에이전트에게 정보를 갱신받는다. 이는 마치 HSS와 비슷하다.4G/5G와 모바일 IP 둘다 터널을 이용한 간접 라우팅을 사용한다. 이러한 유사점은 아래 Table 7.3에 요약해놓았다.            4G/5G 요소      모바일 IP 요소      비고                  홈 네트워크(Home network)      홈 네트워크(Home network)                     방문 네트워크(Visited network)      외부 네트워크(Foreign network)                     IMSI 식별자(IMSI identifier)      영구 IP 주소 (Permanent IP address)      전세계적 유일한 라우팅 가능한 주소 정보              홈 구독자 서비스(HSS)(Home Subscriber Service)      홈 에이전트 (Home agent)                     이동성 관리 개체 (MME)(Mobility Management Entity)      외부 에이전트 (Foreign agent)                     데이터 측면: 홈 네트워크와 방문 네트워크 사이의 터널과 모바일 장치가 존재하는 터널을 통한 홈 네트워크 간접 포워딩      데이터 측면: 홈 네트워크와 방문 네트워크 사이의 터널을 통한 홈 네트워크 간접 포워딩                     기지국(eNode-B)(Base Station)      접근 지점 (AP)(Acess Point)      AP와 정확히 들어맞는 요소 모바일 IP에 없음              라디오 접근 네트워크(Radio Access Network)      WLAN      WLAN과 정확히 들어맞는 요소 모바일 IP에 없음      [Table 7.3 4G/5G와 모바일 IP 구조의 유사성 (Commonalities between 4G/5G and Mobile IP architectures)]모바일 IP는 3개의 주 부위로 이루어져 있다.      에이전트 탐색(Agent discovery)    모바일 IP에서는 외부 에이전트가 이동성 서비스를 모바일 기기에 알려 네트워크에 참여하게 하는 프로토콜을 사용한다.    외부 네트워크에서 기기가 사용할 의탁 주소를 할당하고, 모바일 기기의 참여를 기기의 홈 네트워크의 홈 에이전트 측에 등록하고, 외부 네트워크에서 모바일 기기가 데이트그램을 주고 받을 수 있게 하는 등의 서비스를 제공한다.        홈 에이전트를 등록(Registration with the home agent)    모바일 IP는 모바일 기기나 외부 에이전트가 의탁 주소를 모바일 기기의 홈 에이전트에게 등록하거나 삭제하는 프로토콜을 사용한다.        데이터그램의 간접 라우팅(Indirect routing of datagrams)    모바일 IP는 데이터그램을 홈 에이전트를 통해 모바일 기기로 포워딩하는 방식, 에러 조정, 터널링 방식 등을 정의한다.  모바일 IP는 아주 간략하게 설명하였으며, 더욱 자세히 알고 싶으면 7번째 개정판이나 다른 모바일 IP 관련 서적 참조7.7 무선과 이동성: 상위 계층 포로토콜에의 영향  (Wireless and Mobility: Impact on Higher-Layer Protocols)무선 네트워크는 유선 네트워크에 비해 연결 계층(페이딩, 다중경로, 숨겨진 터미널 등 물리적 장애 때문에)과 네트워크 계층(모바일 기기가 계속 네트워크를 바꾸는 특성 때문에)이 상당히 다르다는 것을 알 수 있었다.반대로 응용 계층과 전달 계층은 이러한 유무선의 영향을 적게 받아 거의 변화가 없어야하지만, TCP에서는 유무선의 차이가 크다.TCP의 재전송과 혼잡 제어는 패킷이 혼잡한 상황에 의해 손실되거나, 감쇠, 간섭 등의 원인에 의해 손실, 변형 됬음으로 예측하고 진행되는 활동이지만, 무선 네트워크에서는 단순히 전파 중 오류가 많이 일어날 뿐만 아니라 네트워크 상황이 혼잡하지 않아도, 핸드오버, 비트 에러 감지 등에 의해 손실처리되는 경우가 많으므로, 이러한 상황에서는 실제로 혼잡해서 벌어진 손실이 아니므로 혼잡 제어가 일어나지 않아야 되지만 TCP는 혼잡 윈도우를 줄여서 전송율을 낮추어버리기 때문에 성능이 크게 떨어진다.이 문제를 해결하기 위해 크게 3가지 방법을 사용하는데,      국지 회복(Local recovery)    국지 회복 프로토콜은 무선 연결에서 에러가 발생시 회복한다. 예를 들어 802.11 ARQ(Automatic Repeat reQuest) 프로토콜이나 ARQ와 FEC(Forward Error Correction) 동시 사용 등이다.        TCP 송신자의 무선 연결 인지(TCP sender awareness of wireless links)    TCP가 패킷의 손실을 감지할 때, 유선 링크에서 손실되었는지, 무선 링크에서 손실되었는지 구분하게 만드는 방법이다. 이를 통해 혼잡 제어가 올바른 상황에서만 일어나게 해준다.    [Liu 2003]에서는 구별하는 방법을, [Huang 2013]에서는 LTE에 좀더 맞는 응용 프로그램과 전달 계층 메커니즘 개발 방법에 대해서 이야기 한다.        연결 분배 방법 (Split-connection approaches)    모바일 사용자와 다른 지점 간의 지점간 연결을 모바일 사용자와 무선 접근 지점 까지, 그리고 무선 접근 지점부터 유선 통신 지점까지 총 2개의 전달계층 연결로 바꾼다.    이때 무선 연결 지점은 TCP 연결을 그대로 쓰거나, 전용으로 설계된 UDP 기반 에러 회복 프로토콜을 사용할 수도 있다.  전달 계층 이외에도 응용 계층 프로토콜들 또한 제한된 무선 연결 속도와 대역폭 때문에 유선과 달라져야하는 부분이 있다.예를 들어 웹 브라우저는 유선 통신일때 보다, 무선 통신일 경우 이미지의 화질이나 갯수 등에서 더욱 낮은 전송속도를 요구하게 줄이던지, 천천히 전송되도 되게 하던지 해야한다."
  }
  , 
  
  "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%208-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EB%B3%B4%EC%95%88.html": {
    title: "네트워크 정리-Chap 8-컴퓨터 네트워크 보안",
    date: " Aug 22, 2022 ",
    url: "/articles/computer_science/network/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EC%A0%95%EB%A6%AC-Chap%208-%EC%BB%B4%ED%93%A8%ED%84%B0%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EB%B3%B4%EC%95%88.html",
    tags: ["CS","NETWORK","요약"],
    content: "Chapter 8. 컴퓨터 네트워크에서의 보안(Security in Computer Networks)style: numbermin_depth: 2max_depth: 3varied_style: truetitle: 출처  Computer Networking: A Top-Down Approach(Jim Kurose, Keith Ross)의 강의를 정리한 내용입니다.(Jim Kurose Homepage)                    student resources : [Companion Website        Computer Networking: a Top-Down Approach, 8/e](https://media.pearsoncmg.com/ph/esm/ecs_kurose_compnetwork_8/cw/)            1.6절에서 간단하게 인터넷 공격에 대해서 배웠지만, 대처법들에 대해선 전혀 배우지 않았으며, 이번 장에서는 인터넷 보안에 대해 자세히 배울 것이다.앞으로 계속 사용될 앨리스와 밥 예시는 보안에서 자주 사용되는 두 사람으로 A, B를 재미있게 표현한 것이다.초장에서는 메시지 무결성과 인증, 통신 암호와에 사용될 기본적인 암호화 기술에 대해서 배우겠다.두번째 부분부터 그러한 기본적인 암호화 기술로 어떻게 상위 4개 계층의 네트워크 보안 프로토콜을 만드는지 알아보겠다.세번째 부분부터 방벽, IDS 같은 운영 보안에 대해 배우겠다.8.1 네트워크 보안이란? (What Is Network Security?)보안있는 통신이 하고 싶다는 것은, 통신하는 와중에, 의도했던 송수신자간의 통신 내용을 어떠한 매체에 관계없이 감청당하지 않고, 변경당하지 않고, 방해받지 않도록 하지 않는 것이다.이에 따른 보안 통신의 속성은 다음과 같다.      기밀성 (Confidentiality)    오직 송신자가 의도한 수신자만 전송된 내용을 이해할 수 있는 것, 메시지의 탈취자가 이해하지 못하게 암호화될 필요가 있다.    흔히 이해되는 보안 통신의 의미이기도 하다. 암호화, 복호화 기술은 8.2절부터 배운다.        메시지 무결성 (Message integrity)    송수신자가 주고받은 메시지가 악의적으로든, 사고로든 변경되지 않아야 한다.    이전에 배웠던 데이터 신뢰성 전송을 위한 체크섬 기술이 사용될 수 있다. 8.3절부터 배운다        엔드포인트 인증(End-point authentication)    송수신자는 자신의 상대방의 실제 신분을 확신할 수 있어야 한다. 눈으로 확인하면 되는 현실과 달리 인터넷 패킷 통신에서는 쉬운 일이 아니다.    엔드포인트 인증은 8.4절에 배운다.        운영 보안 (Operational security)    거의 모든 조직에서 인터넷과 연결된 조직망을 운영하는데, 이는 쉽게 표적이 될 수 있다.    조직망 내의 중요한 기업 비밀, 내부 인터넷 설정, DoS 공격용 좀비 피시 설정, 호스트에 웜 설치 등의 보안적 위험이 있다.    8.9절에서는 이를 막기위한 방화벽이나 침입 감지 체계에 대해서 배운다.    방화벽은 조직망과 인터넷 사이에 자리잡아 패킷의 접근과 유출을 제어하고, 침입 감지 체계는 심층 패킷 조사를 통해 의심스러운 활동이나 위험을 관리자에게 알리거나 조치를 취할 수 있다.  figure 8.1은 공격자가 취할 수 있는 행동에 대한 시나리오이다.앨리는 밥에게 메시지를 위에서 설명한 보안 통신의 속성대로 안전하게 보내고 싶으므로,메시지는 암호화되었고, 공격자는 다음과 같은 행동을 취할 수 있다.  도청, 채널의 제어와 데이터 메시지를 엿듣고, 기록한다.  메시지나 메시지 내용의 변경, 삽입, 삭제몇 가지 보안 대책을 세우지 않으면, 도청을 통한 데이터(암호, 기업기밀 등) 탈취, 다른 활동의 모방, 세션 탈취, 시스템 자원 과부화를 통한 서비스 무력화 등 다양한 보안 공격이 행해질 수 있다. 이러한 보안 공격의 종류는 CERT 조정 센터(CERT Coordination Center [CERT 2020])에서 알 수 있다.송수신자의 안전한 통신은 단순 메시지 뿐만 아니라 네트워크 기반 기능들에도 필요하다. 예를 들어 라우팅 정보를 포함하고 있는 DNS나 라우팅 데몬들은 공격자가 DNS 룩업 표, 라우팅 테이블, 네트워크 관리 기능들을 수정, 조작하여 인터넷에 크나큰 타격을 줄 수 있다.네트워크 보안의 중요성과 의미에 대해 알았으니, 암호화에 대해 알아보자.암호화는 단순 기밀성 유지 뿐만 아니라 메시지의 무결성과 종단간 인증에도 사용된다는 것을 배울 수 있다.8.2 암호학의 원리 (Principles of Cryptography)암호의 역사는 방대하고 오래됬으므로 여기서는 일부만 다룬다.암호는 네트워크 보안에서 기밀성 유지, 인증, 메시지 무결성, 부인 방지등에도 사용된다.암호화된 메시지는 침입자가 탈취한 메시지로부터 아무 정보도 얻을 수 없게 변형되어있어야 하며, 송수신자는 변형된 메시지로 부터 원본 데이터를 얻을 수 있어야 한다. figure 8.2는 암호의 중요한 요소에 대해 묘사되어 있다.앨리스가 밥에게 보내고 싶은 원본 데이터 메시지(“I love you Bob”)은 plaintext(원문) 또는 cleartext라고 부르며, 이는 암호화 알고리즘(encryption algorithm)에 의해 암호화된 메시지, ciphertext(암호문)가 되며, 이는 침입자에겐 의미없는 문자의 나열처럼 보인다. 이러한 암호화 방법은 오픈소스로 원리까지 완벽하게 알려져있는 경우가 많으며,[RFC 1321; RFC 3447; RFC 2420; NIST 2001], 이때, 침입자가 복호화 할 수 없게, 암호화하는 데 필요한 비밀 정보를 key(키)라고 한다.figure 8.2에서 앨리스가 숫자와 문자로 이루어진 키 $K_A$와 원문(plaintext) m를 암호화 알고리즘을 통해 암호문(ciphertext) $K_A(m)$를 내보내게 된다.밥 또한 키 $K_B$와 암호문 $K_A(m)$을 이용해 원문 $K_B(K_A(m))=m$을 얻게 된다.이때, $K_A$와 $K_B$에 대해서 대칭 키 체계(symmetric key systems)에서는 두 키는 동일하며 공개되지 않고, 공개 키 체계(public key systems)에서는 두 키는 서로 다르고, 키 하나는 세상에 널리 공개되어 있고, 하나는 앨리스와 밥만 알고 있다.이 두 키에 대해 자세히 알아 보자.8.2.1 대칭키 암호화 (Symmetric Key Cryptography)모든 암호화 알고리즘은 무언가를 다른 것으로 대체하는 과정이 들어가 있다. 예를 들어 원문을 암호화를 통해 암호문으로 바꾸는 과정이다.현대 암호화에 들어가기 앞서 과거의 암호화 방법인 카이사르 암호(cipher, 암호와의 한 방법)에 대해 알아보자.알파벳의 카이사르 암호는 알파벳 순번을 k번 만큼 밀어내기하는 방법이다.. 예를 들어 k = 3이면, a 는 d, b는 e가 되는 형식이다. plaintext로 “bob, i love you”가 들어가면 암호문으로 “ere, l oryh brx”가 나오게 된다. 이는 쉽고 간편하지만 26가지의 변형만 존재하고, 누구나 금방 해독이 가능하다.카이사르 암호에 대한 개선으로 단일 알파벳 암호(monoalphabetic cipher)가 가능하다. 각 알파벳 별로 대체 알파벳을 정한 뒤,(정확히는 각각 유일한 알파뱃으로 변환되도록 k 값을 다르게 두어) 암호화하는 방식으로, 도합 $10^{26}$가지의 암호가 가능하다. figure 8.3의 예시를 들자면, 원문 “bob, i love you”가 암호문 “nkn, s gktc wky”가 될 것이다.브루트포스로 유추하는게 불가능할 정도로 힘들긴 하지만,  통계학적인 방법으로 쉽게 유추할 수 있는데 예를 들어, 영어 문장 중 e와 t는 가장 많이 나오는 문자이므로, 암호문 문자 중 가장 많이 나온 문자를 e와 t로 설정한 뒤, “in, ion, it, the, ing” 같은 자주 나오는 단어들을 이용해 다른 문자들 또한 쉽게 유추가능하다. 추가로 침입자가 메시지의 내용을 예상하고 있다면, 더욱 쉬워지는데, 예를 들어, 위의 문장의 “bob, i love you”에서 침입자가 해당 메시지가 밥과 앨리스의 통신이라고 알게되면, bob이나 alice라는 문자가 하나 이상 나올 것이라 예상하게 되고, 이를 이용해 nkn이 bob을 의미할 수 있다고 예측할 수 있다.침입자가 얼마나 쉽게 암호를 해독하는 정도는 침입자가 얼마나 미리 정보를 가지고 있느냐에 따라 정해진다.      암호문 전용 공격 (Ciphertext-only attack)    침입자가 가로챈 암호문 이외의 정보를 가지고 있지 않은 상태이지만 통계학적 방법 등으로 암호를 해독할 수 있다.        알려진 원문 공격 (Known-plaintext attack)    위의 bob과 alice의 대화임을 아는 침입자처럼 만약 침입자가 일부의 (원문, 암호문) 쌍 예시를 아는 경우에는 알려진 원문 공격이라고 한다.        선택된 원문 공격 (Chosen-plaintext attack)    침입자가 원하는 원문 메시지에 대한 암호문 형태를 알 수 있는 경우를 의미한다. 예를 들어 침입자가 송신자에게 “abcdefghijklmnopqrstuvwxyz”를 보내게하여 이를 암호화한 문자 “mncvcxzasdfghjklpoiuytrewq”를 얻을 수 있게 한다면, 암호는 완전히 의미없게 된다. 하지만 이러한 치명적인 공격이 뚫리지 않게하는 방법이 복잡한 암호화 방법으로 존재한다.  단일알파뱃 암호화의 개선 버전으로 다중 알파벳 암호화(polyalphabetic encryption) 또한 존재한다. 기본적인 아이디어는 문자의 위치별로 다른 단일 문자 암호화를 적용하는 방식이다. 즉, 같은 문자라도 본문 내 위치에 따라 다른 문자를 가질 수 있다.figure 8.4은 다중 알파벳 암호화의 예시로 k=5, k=19인 2개의 카이사르 암호화 $C_1, C_2$를 선정하고, 각 두 개의 암호화의 순번을 결정한다. 예를 들어 $C_1,C_2,C_2,C_1,C_2$ 로 결정한다면, 원문의 첫번째와 네번째는 $C_1$을, 나머지는 $C_2$로 암호화될 것이고, 그 이상부터는 반복될 것이다. 원문 “bob, i love you”는 암호문 “ghu, n etox dhz”로 암호화 된다.블록 암호 (Block Ciphers)블록 암호는 PGP(보안 이메일), TLS(보안 TCP), IPsec(보안 네트워크 계층) 같은 여러 인터넷 프로토콜이 활용하는 암호화로, 메시지는 k 비트의 블록으로 나뉘어 처리된다.예를 들어 k = 3인 경우, 각 3비트의 원문은 3비트의 암호문으로 바꾼다는 의미이며, 이때의 암호화는 Table 8.1과 같이 입력값과 출력값의 1:1 매핑으로 되어있다.원문 010 110 001 111 의 경우 암호문 101 000 111 001로 변한다.또한 Table 8.1 같은 표는 도합 8! = 40320의 경우의 수를 가질 수 있으며, 이러한 매핑 표 하나를 일종의 키로 보고 송수신자가 암호화와 복호화 하는데 사용할 수 있다.물론 위의 예시 3 비트의 경우 브루트포스로 쉽게 무력화되므로 더욱 기다란 64 비트 정도의 블록을 이용해 매핑한다. 이 경우 $2^{64}!$개의 키가 나오므로 브루트포스로 유추는 불가능하며, 아주 강력한 암호화를 제공한다.하지만 위와 같이 전체 매핑 표를 이용한 암호화는 키를 보관하는데 필요한 공간만 $2^{64}$가 필요하므로, 이를 보관하기도, 이용해 매핑하기도, 새로 바꾸어 키를 재발급받는데도 불가능에 가까우므로, 실무에서 사용이 불가능하다.대신, 블록 암호화에서는 무작위 생성된 표를 모사(simulated)해서 사용한다.[Kaufman 2002]  예를 들어 figure 8.5에서는 64 비트를 8비트 블록 8개로 나누고, 각 블록을 8 비트 짜리 맵핑 표로 암호화한 뒤, 다시 합쳐서 64 비트 문자열로 바꾼다. 이후 공개적으로 알려진 스크램블 함수를 이용해 변형된 64 비트 결과를 만들고, 이 결과물을 다시 64비트 입력물로 되돌려 앞선 과정을 총 n번 반복한다.이를 통해서 비교적 가벼운 8비트 맵핑 표로도 복잡한 암호화를 만들 수 있다.이외에도 인기있는 블록 암호화로 DES(Data Encryption Standard), 3DES, AES(Advanced Encryption Standard)가 있으며, 이들은 미리 계산된 표 대신 figure 8.5 같지만 더욱 복잡한 함수를 사용하며 키로 비트들의 문자열을 이용한다.예를 들어 DES는 64 비트 블록 암호화로 56 비트 키를 사용하며, AES는 128 비트 블록에 설정에 따라 128, 192, 256 비트 길이의 키를 사용한다. 이러한 키들은 알고림즘을 통해 각각 특정한 미니 표 맵핑과 순열을 생성할 수 있다.이러한 키에 대한 브루트포스 공격은 사실상 불가능할 정도로 오래 걸리므로 안전하다.암호 블록 체이닝 (Cipher-Block Chaning)위와 같이 블록 암호화하는 경우를 ECB(electronic codebook, 전자 코드북) 방법이라고 부르는데, 이는 치명적인 단점이 있다.바로 같은 입력의 비트 값을 암호화하면 같은 출력의 비트값이 나온다는 점이다. 이 방법이 치명적인 이유는 우리가 단일 알파벳 암호화 당시, 통계학적 방법으로 해독해낸 방법을 기억해내면 된다. 공격자는 동일하게 반복되는 비트를 통해 원본 비트를 유추해낼 수 있으며, 더 나아가 전체 블록을 해독할 수 도 있다.이를 막기위해 각 블록마다 무작위 블록을 $\\otimes$(XOR)하여 같은 값이더라도 다른 값이 나오도록 조치할 수 있다. 예를 들어 10101010 $\\otimes$ 11110000 = 01011010이다.예를 들어 $c(i)$를 i번째 암호화 블록, $m(i)$를 i번째 원본 블록, $r(i)$를 i번째 무작위 비트블록, $K_s$를 송수신자가 공유한 블록 암호화 대칭 키라고 가정하면,송신자는 $c(i)=K_s(m(i)\\otimes r(i))$로 암호화 블록을 계산한 뒤, $c(i),r(i)$를 같이 함께 수신자 측에 보낸다.이에 수신자는 $m(i)=K_s(c(i))\\otimes r(i)$를 통해 복호화할 수 있고, 탈취자는 대칭키 $K_s$가 존재하지 않으므로, 중간에 데이터를 가로채더라도 복호화하지 못한다.또한 $m(i)=m(j)$이라고 하더라도 $r(i)!=r(j)$이므로 결과값인 $c(i)!=c(j)$가 되므로 상기한 ECB 블록 암호화의 단점은 없어진다.하지만 이때 문제가 하나 있는데, 각기 다른 무작위 블록 $r(i)$를 매 블록마다 보내면 전송해야하는 전체 데이터양이 2배로 많아진다.이를 해결하기 위한 방법이 암호 블록 체이닝 (Cipher-Block Chaning)이며 CBC라고 부른다.CBC 방법에서는 데이터의 맨 처음 비트 블록 $m(1)$에 $\\otimes$(XOR)할 $c(0)$ 딱 하나만 보내고, 나머지 블록들($c(n)$)은 자신의 이전 블록에서 나온 무작위 블록($c(n-1)$)을 이용한다.자세한 방법은 다음과 같다.  송신자가 데이터 암호화 시작시, 무작위한 k 비트 문자열을 만들어낸다. 이를 IV(초기치 벡터, Initialization Vector)라고 하며 $c(0)$라고 표현하겠다. 이 값은 수신자에게 원문으로 보내진다.  이후 첫번째 블록 $m(1)$에서 $c(1)=K_s(m(1)\\otimes c(0))$을 통해 암호화된 블록을 얻어낸다. 이를 수신자에게 보낸다.  이후 부터 i 번째 블록의 암호화 블록 $c(i)$는 $c(i)=K_s(m(i)\\otimes c(i-1))$로 생성해 보낸다. 즉, 여기서 $r(i)=c(i-1)$이다.CBC로 인한 효과로  수신자가 대칭키를 이용해 복호화 하는 방법은 $m(i)=K_s(c(i))\\otimes c(i-1)$이며 대칭키 $K_s$, 현재 암호화 블록 $c(i)$, 이전 암호화 블록 $c(i-1)$을 전부 알고 있으므로 원본 메시지를 복호화할 수 있다.  두 원문 블록이 같은 값이라고 하더라도 결과는 언제나 다르게 나오게 된다.  침입자는 IV와 암호화된 블록들을 모두 감청해도, 대칭키 $K_s$를 모르므로 원문을 얻어낼 수 없다.  송신자는 암호화를 위해 IV 단 한 블록만 보내면 되므로, 오버헤드나 대역폭 추가 사용은 무시할만 하다.CBC는 안전한 네트워크 프로토콜을 설계하는데 아주 중요하며, 이를 이용하는 프로토콜은 나중에 배우겠다.8.2.2 공개 키 암호화 (Public Key Encryption)공개 키 방법의 맹점은, 서로 암호화와 복호화가 가능한 공개 키를 안전한 통신을 통해 건네줘야 한다는 점이다. 보안 통신을 위해서 공개 키가 필요한데, 공개키를 주고 받으려면 중간 탈취를 막기위해 보안 통신이 필요하므로, 서로 모순적인 상황이 되버린다.유일한 방법은 통신을 이용하지 않고, 두 송수신자가 직접 만나 공개 키를 합의하는 방법 뿐이 없을까?바로 공개 키 암호화(public key cryptography)로 가능하다.[Diffie 1976], [RSA 1978], [Ellis 1987] 공개키 암호는 이외에도 인증과 디지털 서명등에도 쓰인다.공개키 암호화의 알고리즘은 아래에 배울 RSA가 대표적이며 Diffie-hellman 키 교환 방식, DSA 등이 존재한다.Figure 8.6은 공개키 암호화의 예시이다.밥과 앨리스는 대칭키 대신 공개키 암호화를 사용하고 있다.공개 키는 두개의 키로 이루어져 있는데, 침입자를 포함해 모든 대중이 알고 있는 공개 키 $K_B^+$와 밥 만이 알고 있는 비공개 키 $K_B^-$가 있으며,놀랍게도 이 둘은 서로의 암호를 상대로만 복호화 할 수 있다 즉, 원문 메시지 $m$에 대하여 $m=K_B^+(K_B^-(m))=K_B^-(K_B^+(m))$이다,만약 엘리스가 밥에게 메시지를 보내고 싶다면, 공개 암호화 키 대중에 널린 $K_B^+$를 이용해 원문 메시지 m을 암호화하고, $K^+_B(m)$ 밥에게 보내면,밥은 자신의 비공개 복호화키 $K_B^-$를 이용해 원문 메시지 m로 복호화한다. $m=K_B^-(K_B^+(m))$중간의 탈취자는 공개된 키 $K_B^+$만 가지고 있으며 이것으로는  $K_B^+$로 암호화된 메시지를 복호화할 수 없다.하지만 이때, 하나의 맹점이 있다. 만약 다른 누군가가 공개된 공개 키 $K_B^+$를 이용해 엘리스인척하고 메시지를 주고 받을 수 있다. 어떻게 엘리스가 보낸 메시지라고 확인할 수 있을까?이것을 가능하게 하는 방법이 바로 디지털 서명(digital signature)이며, 8.3절에 배운다.RSA알고리즘을 고안한 Ron Rivest, Adi Shamir, Leonard Adleman 세사람의 이름의 글자를 따 명명한 RSA 알고리즘은 위의 문제를 해결한 방법 중 하나로, 현재에는 공개키 알고리즘과 거의 동시에 같이 쓰이는 중요한 알고리즘이다.RSA는 모듈로-n 산술 연산을 이용한 방식으로, 모듈로 연산은 CRC 오류 검사 때 배웠던 것 처럼,  x mod n은 x를 n으로 나눈 뒤의 나머지란 의미이다.또한, 사칙연산과 지수연산도 가능한데 이 결과와 성질을 간단히 나타내자면 다음과 같다.\\([(a \\mod n) + (b \\mod n)] \\mod n = (a + b) \\mod n \\\\[(a \\mod n) - (b \\mod n)] \\mod n = (a - b) \\mod n\\\\[(a \\mod n) \\cdot (b \\mod n)] \\mod n = (a \\cdot b) \\mod n\\\\(a \\mod n)^d \\mod n = a^d \\mod n\\)figure 8.6 때와 같은 상황에서 RSA 암호화를 이용해 통신한다고 가정하고, 모든 비트의 나열은 숫자로 표현될 수 있음을 명심하자. 예를 들어 101은 5, 1001은 9이다. 즉, RSA 알고리즘으로 비트로 이루어진 메시지를 암호화한다는 것은 특정 숫자를 암호화한다는 것과 같다.또한, 아래 둘은 RSA에서 서로 연관된 요소들이다.  공개키와 비공개키의 선택  암호화와 복호화 알고리즘다음은 공개 RSA 키와 비공개 RSA 키를 생성하기 위한 밥의 행동이다.  아주 커다란 서로 다른 소수 p와 q를 고른다. 이때 p와 q의 값은 클 수록 보안에 좋으면 1024 비트가 권장 사항이다. 커다란 소수값을 구하는 방법은 [Caldwell 2020] 참조  p와 q의 곱 n과 $z=(p-1)(q-1)$을 준비한다.  암호화에 사용될 n보다 작고 z와 1을 제외한 공약수가 없는(상대적 소수) 숫자 e를 고른다.  복호화에 사용될 $ed-1$이 z에 나누어떨어지는 수, d를 고른다. 이제 다음 식이 성립할 것이다. $ed \\mod z = 1$  이제 공개키 $K_B^+$은 (n, e)이며, 비공개키 $K_B^-$은 (n,d)이다.이제 암호화와 복호화 과정을 살펴보자면,만약 비트 값의 정수표현이 m (m&lt;n)인 메시지를 암호화하고 싶다면, 공개키 $K_B^+$인 (n,e)를 이용해서 $c=m^e \\mod n$값을 구한다. 이 값 c는 메시지 m의 암호문이다.수신자는 c를 받고 이를 복호화 하고 싶으면, 비공개키 $K^-_B$인 (n,d)를 이용해 $m=c^d \\mod n$을 통해서 원문 메시지를 구한다.만약 p = 5, q= 7, e =5, n = 35, d = 29로 가정하고, love라는 메시지가 각 알파벳 순번으로(8비트 ASCII 표현이 좀더 현실적이겠지만) 표현할 수 있다고 가정하면 Table 8.2와 Table 8.3과 같은 암호화와 복호화를 실행할 수 있다.커다란 소수 구하는 법, e와 d를 도출하는 법, 어마어마하게 커다란 값들을 이용해 지수연산을 하는 하는 법 등은 [Kaufman 2002]에서 알아보자.세션 키들 (Session Keys)상기한 거대한 값들의 지수연산이 소요되는 RSA 계산은 자원이 많이 드는 방법이므로, 실무에서는 DES, AES 같은 대칭 키 암호화와 함께 사용한다.      송신자가 대량의 암호화된 데이터를 보내고 싶을때, 데이터를 암호할 대칭 키 암호화에 사용할 대칭 키이자, 세션 키(session key) $K_S$를 고른다.        수신자의 공개 RSA 키를 이용해 세션키 $K_S$를 RSA 암호화한다. $c=(K_S)^e \\mod n$        수신자가 송신자로 부터 RSA 암호화된 세션키 c를 받으면, 이를 자신이 가지고 있는 비공개 RSA 키로 복호화하여 세션키 $K_S$를 얻는다.        이때부터 송수신자간의 통신 암호화는 좀더 자원이 적게드는 세션키 $K_S$를 이용한 대칭키 알고리즘으로 진행된다  RSA이 가능한 이유는? (Why Does RSA Work?)RSA 암호화에서 메시지를 정수 m으로 표현하고, 이를 모듈로-n 연산으로 e로 지수연산하였다.\\[c=m^e\\mod n\\]복호화는 이 값을 모듈로-n 연산으로 d로 지수연산을 하였으며, 해당 값은 모듈러 연산의 성질이던 $(a \\mod n)^d \\mod n = a^d \\mod n$에 의하여, 다음과 같이 된다.\\[c^d\\mod n=(m^e \\mod n)^d \\mod n=m^{ed}\\mod n\\]이번에는 소수 p와 q에 대하여 , $n=pq$이고, $z=(p-1)(q-1)$일 때, 정수론적 증명에 의해 $x^y \\mod n = x(y \\mod z) \\mod n$이라고 한다.\\(m^{ed} \\mod n = m^{ed\\mod z} \\mod n\\)하지만 알다시피 우리는 $ed \\mod z = 1$이 나오도록(정확히는 $ed-1$이 z에 나누어 떨어지게) e와 d값을 설정했기 때문에 다음과 같이 된다.\\(m^{ed} \\mod n = m^1 \\mod n = m\\)이렇게 원문 메시지 값이었던 m으로 다시 되돌렸다.우리는 e 모듈로-n 지수연산을 먼저하고 d 모듈로-n 지수연산을 실시했는데, 이를 반대 순서로, 즉 복호화를 먼저하고, 암호화를 진행해도 결과는 똑같게 나온다.\\((m^d\\mod n)^e \\mod n = m^{de} \\mod n = m^{ed} \\mod n = (m^e \\mod n)^d \\mod n\\)RSA 암호화 알고리즘은 두 소수의 곱으로 만든 값 n을 소인수 분해하여 두 소수 p와 q를 알아내는 방법이 없다는 가정하에 진행된다.만약 소수 p와 q를 손쉽게 알아낼 수 있는 알고리즘이 있다면, p와 q, 그리고 공개키의 e를 이용해 비밀 키 d를 쉽게 알아낼 수 있으므로, RSA 알고리즘은 엄밀히 말해서 완벽히 보장된 보안은 아니다.예를 들어, 양자 컴퓨터가 등장하여 빠르게 소인수분해가 가능하다면, RSA 보안은 무용지물이 될 것이다.RSA 이외에도 Diffie-Hellman 알고리즘이라는 공개키 암호화 알고리즘이 있으며, 임의의 원하는 메시지의 길이를 정할 수 없지만, 대칭 세션 키를 수립할 때, 메시지를 암호화 하는데 사용한다.8.3 메시지 무결성과 디지털 서명(Message Integrity and Digital Signatures)이번 장에는 메시지 무결성(message integrity) 또는 메시지 인증(message authentication)에 대해서 알아보자수신자가 송신자에게서 메시지를 받았을 때, 이게 보안적으로 괜찮은 메시지인지 알려면 두 가지를 증명해야 한다.  내가 특정한 송신자에게서 부터 온 메시지가 맞는가?  통신 중간에 메시지가 다른 침입자에 의해 수정되거나 바꿔쳐지지 않았나?이는 개인간 뿐만 아니라 네트워크 전역에서 중요하게 생각하는 일이다.예를 포워드 테이블을 생성하기 위한 라우터 간의 메시지 전파에서 가짜 라우터 메시지를 만들어, 네트워크에 혼선을 주거나 자신에게 패킷이 향하게 만들 수 있다.그러므로, 정확히 메시지를 누가 보냈는지 구별하는 일은 정말 중요하며, 이를 알아보기 위해 가장 먼저 암호화 해쉬 함수에 대해 배워보자8.3.1 암호화 해쉬 함수 (Cryptographic Hash Functions)figure 8.7은 입력값 m을 받아 고정된 길이의 문자열 H(m)을 내놓은 해쉬 함수에 대한 묘사이다.인터넷 체크섬과 CRC가 그 예시이며, 암호화 해쉬 함수는 다른 해쉬함수와 달리 다음과 같은 추가적인 특성을 충족해야한다.  다른 입력값 x와 y에 대하여 해쉬 결과물 H(x)와 H(y)는 언제나 달라야 한다.이는 침입자가 메시지를 다른 메시지로 바꾸는 게 불가능하게 만든다. 만약, 결과물 H(x)가, x로도, y로도 만들 수 있다면, 칩입자는 원문 메시지를 y로 만들 수 있다.figure 8.8은 위의 특성을 지키지않는 예시로, 1의 보수를 취하지 않는 4비트 인터넷 체크섬의 예시이다.메시지의 내용은 “IOU100.99BOB”이며, 아스키 표현으로 49,4F,55,31,30,30,2E,39,39,42,4F,42이 된다.이를 체크섬으로 해쉬 결과물을 내놓으면 B2, C1, D2, AC가 된다.(figure 8.8 위)또 한, “IOU900.19BOB”라는 다른 결과물을 이용해도  B2, C1, D2, AC가 되므로, 다른 내용의 메시지지만 같은 결과물이 나오게 된다.이를 수신자가 착각하면 9배나 많은 빚을 지게 된걸로 이해할 것이며, 이러한 일을 막기 위해 강력하면서도 중복 결과값이 나오지 않는 해쉬함수가 필요하다.MD5 해쉬 알고리즘은 4개의 단계를 통해 128 비트 해쉬를 생성하는 인기있는 해쉬 알고리즘이다.  패딩 단계 : 맨 앞에 1, 이후 충분한 갯수의 0이 붙어있는 패딩을 메시지 길이 조건에 맞게 붙여줌  삽입(append) 단계: 패딩 이전 위치에 메시지 길이의 64비트 표현을 집어넣음  축적 시작(initialization of an accumulator) 단계 : 16 단어 블록으로 처리  반복(looping) 단계 : 3번을 4회 반복자세한 사항은 [RFC 1321]에서 참고바람.SHA-1(안전한 해쉬 알고리즘 Secure Hash Algorithm) 또한 MD4(MD5 이전 버전)와 비슷하면서 인기가 많은 160 비트 길이 미 연방 표준 알고리즘이다.8.3.2 메시지 인증 코드 (Message Authentication Code)먼저, 흠이 있는 메시지 무결성을 보장하는 단계에 대해 설명해보자면,  송신자가 메시지 m을 만든 뒤 해쉬 결과값 H(m)을 가져온다.(ex. SHA-1)  H(m)을 메시지 m에 붙여 더욱 기다란 메시지 (m, H(m))을 만든 뒤, 수신자에게 보낸다.  수신자는 받은 메시지 (m, H(m))을 통해 H(m) = h임을 확인해보고, 맞다면 확인 처리 한다.이 방법은 3번에서 칩입자가 m 대신 악의적인 메시지 m`을 이용해서 (m`,H(m`))으로 바꿔 보내도 수신자는 눈치 채지 못하므로 안전하지 않다.이를 막기위해 송수신자간에 공유하는 새로운 비밀 키 s가 필요하며 이를 인증키(authentication key)라고 부른다. 이를 이용한 새로운 방법은 아래와 같으며, 묘사는 figure 8.9에 나와있다.  송신자는 메시지 m과 인증키 s를 합쳐 m+s를 만들고, 이를 해쉬 함수를 통해 H(m+s)로 만든다. 이때 H(m+s)를 MAC(message authentication code, 메시지 인증 코드)라고 한다.          연결 계층의 MAC(media access cotnrol) 주소나, 공유 채널 접근 프로토콜인 MAC(medium access control)과 헷갈리지 말자.        MAC와 m을 합쳐 늘어난 메시지 (m, H(m+s))를 만들어서 수신자에게 보낸다.  수신자는 (m, H(m+s)) 메시지를 통해 m에 자신이 가지고 있는 인증키 s를 합친 뒤, H(m+s)를 계산해 보아 일치하면 확인 처리 한다.MAC 방법의 장점은 MAC을 위한 별도의 암호화가 필요 없어서 복잡하거나 자원이 많이드는 처리가 필요없다는 점이다.가장 인기있는 MAC은 HMAC이며, MD5, SHA-1같은 해쉬 알고리즘과 함께 사용할 수 있으며, 심지어 해쉬 과정을 두번 거치게 할 수 있다.상기했던 라우터 간의 네트워크 정보 교환에도 라우터 간에 같은 인증키를 공유해서 MAC 절차를 이용해서 통신한다.이때, 라우터를 설치하는 네트워크 관리자가 일일이 인증키를 설정해주거나, 아니면 앞서 배웠던 공개키 암호화 알고리즘을 이용해서 인증키를 주고 받는다.8.3.3 디지털 서명 (Digital Signatures)디지털 서명(digital signature)는 수기 사인처럼 검증 가능하고 위조 불가능한 디지털 암호적인 방법으로 자기 자신을 증명하는 방법이다.디지털 서명은 개인에게 유일하여야 하니, 두 사람이 같은 인증키를 공유해야하는 인증키 방식은 옳지 않다. 공개키 방식에서는 공개되어있는 공개키와 개인만 알고 있는 개인키로 나눠져 있고, 이 비공개키는 디지털 서명의 좋은 후보가 될 수 있을 것 같다.아래 figure 8.10에서 공개키 방식의 개인키를 이용한 디지털서명의 예시이다.      이때 송신자는 전송 전에 개인키를 이용해 원문 m을 암호화 하여 사인된 메시지 $K_B^-(m)$을 원문 메시지와 짝인 (m, $K_B^-(m)$)로 만들어 전송한다.          비공개키로 암호화는 것이 미심쩍다면, 공개키 방식에서 공개키와 비공개키는 서로 암호화하고 복호화할 수 있는 관계였음을 떠올리자.            수신자는 $K_B^-(m)$를 송신자의 공개키$K_B^+$로 이용해 복호화해보고, 원문 메시지 m으로 복호화된다면 송신자가 보낸 것이라는 증명이 된 것이다.  추가로, 침입자가 탈취하여 m 대신 m`라는 메시지로 바꾸어 보내려고 해도, 개인키 $K_B^-$가 없다면 $K_B^-({m}’)$를 만들 수 없고, 메시지 무결성 또한 증명된다.하지만 이 방법의 단점은 암호화와 복호화 과정은 자원이 너무 많이 드는 방법이라는 점이다. 이를 위해 위에서 사용했던 해쉬 함수 기능을 이용할 수 있다.메시지 전체(m)을 암호화($K^-_B(m)$)하는 대신 생성된 해쉬값($H(m)$)을 암호화($K_B^-(H(m))$)하면, 원문에 비해 상대적으로 길이가 짧으므로 드는 비용이 크게 줄어든다.figure 8.11은 디지털 서명 생성의 절차를 묘사한 것이다.밥은 원문 메시지를 해쉬함수로 처리한 결과물에 자신의 개인키를 이용해 디지털 사인을 한다.디지털 사인을 한 결과물과 원문 메시지를 함께 붙여 앨리스에게 보낸다.figure 8.12는 디지털 서명의 증명 절차를 묘사한 것이다.앨리스는 받은 메시지의 원문 m 부분에 해쉬함수를 적용해 $H(m)$을 얻어낸 뒤, 밥이 보낸 메시지의 암호화된 부분을 밥의 공개키로 복호화 해본다,이렇게 얻어낸 결과과 $H(m)$과 같다면 메시지 무결성이 확인된다.넘어가기 전에, MAC 방법과 디지털 서명법을 비교해보자면,MAC을 메시지로 부터 만들어내기 위해, 인증키를 메시지에 붙이고, 해쉬 결과 값을 취한다. 이때 대칭키도, 공개키도 사용하지 않는다.디지털 서명에서는 메시지의 해쉬값을 개인키로 대칭키 암호화를 적용한다. 디지털 서명법은 좀더 오버헤드가 큰 작업으로, 추가적인 인증된 기관의 PKI(Public Key Infrastructure, 공개 키 기반)이 필요하다.나중에 알아볼 PGP(보안 이메일)은 디지털 서명을 사용하고, OSPF(라우팅 알고리즘)는 MAC을 이용한다.이외에도 MAC은 여러 인기있는 프로토콜에 사용된다.(8.5, 8.6절 참조)공개 키 증명 (Public Key Certification)디지털 서명의 중요한 응용 프로그램으로 특정 개체의 공개키를 보증해주는 공개 키 증명 (Public Key Certification)이 있으며, TLS나 IPsec 같은 여러 프로토콜이 활용한다.공개키 증명이 필요한 이유는 아래 figure 8.13을 통해서 증명한다.인터넷 피자 가게를 운영하는 앨리스에게 트루디가 장난 전화를 거는 상황이다.트루디가 앨리스에게 자신이 밥이라는 사람이고, 피자를 주문하고 싶다고 메시지를 디지털 서명을 이용해 보낸다.그리고 이를 증명하기 위해 트루디는 자신의 개인키와 쌍을 이루는 공개키를 밥의 공개키라고 속이고 첨부한다.앨리스는 해당 디지털 서명이 첨부된 공개키를 이용해 풀리니 밥이라고 확인하고 밥에게 피자를 배달했지만, 밥은 모르는 일이다.위를 통해, 해당 공개키가 실제로 송신자의 공개키인지 증명할 필요가 있다는 것을 깨달았을 것이다.이렇게, 특정 개체의 공개키를 증명해주는 기관이 CA(Certification Authority, 인증기관)이며, 공개키의 유효성을 검사하고 인증서를 발급해주며, 정확한 역할을 다음과 같다.      CA는 각 객체(사람, 라우터, 기관 등)가 자신이 주장하는 그 객체가 맞는지 증명한다. 이러한 증명 방법에는 의무적인 절차가 없으므로, 만약 어떤 CA가 단순히, 그 객체가 “나는 OO이야, 니가 증명해줘”라고 주장했다는 이유로 적절한 절차없이 증명서를 발급해준다면, 그러한 CA는 믿으면 안된다. 즉 CA의 인증서를 믿는 다는 것은 CA, 더나아가 CA의 증명 절차를 믿는다는 의미이다. 보통 CA는 엄격한 보안 기준과 일정 이상의 규모를 인증해야 CA로 인정된다.        CA가 객체의 정체성을 증명했다면, CA는 해당 객체와 공개키를 짝지어주는 인증서(certificate)를 만든다. 인증서에는 공개키와 객체에 대한 전세계적으로 유일한 정보(이름, IP 주소 등), 인증 기관명 등이 적혀있다. 이러한 인증서는 CA가 디지털적으로 사인하였다.      이제 CA에 인증된 밥이 피자를 배달하는 과정을 알아보자.밥이 디지털 서명으로 암호화된 메시지를 보낼때, CA 인증서를 첨부해서 보낸다.앨리스는 해당 CA 인증서에 첨부된 증명서를 보고 CA 내부의 공개키로 디지털 서명을 해독해보고, 원문 메시지와 같음을 보고 밥임을 확신한다.만약, 트루디가 역시 장난을 치려하고 있고      트루디가 밥이라고 주장하며 아무런 인증서 없이 자신의 공개키를 보냈다면, 앨리스는 인증서가 없으니 믿지 않을 것이고,        밥의 CA 인증서를 가져와 메시지를 보낸다면, 앨리스가 CA 인증서 공개키를 사용해서는 트루디의 개인키로 디지털 서명된 암호화를 해독할 수 없으니 역시 믿지못한다.        밥의 개인키를 해킹으로 탈취해서 디지털 서명한 뒤, 밥의 CA 인증서를 보냈다면, 그것은 개인키 관리를 못한 밥의 잘못이다.        트루디가 CA의 데이터베이스를 해킹해서 밥의 인증서에 자신의 공개키를 넣도록 하였더라면, 트루디는 디지털 포렌식을 통해 추적되어 미 연방 1급 교도소에 갈 것이다.  ITU(International Telecommunication Union)와 IETF는 CA를 위한 엄격한 기준을 가지고 있으며, 키 관리 구조의 절차 및 의례에 대해 적인 ITU X.509 [ITU 2005a]은 인증을 위한 서비스와 정확한 구문을 정의하며, [RFC 1422]에는 보안 이메일을 이용한 CA 기반 키 관리에 대해 적혀있다.Table 8.4는 인증서의 필드에 대해 설명한다.8.4 엔드포인트 인증(End-Point Authentication)엔드포인트 인증(End-Point Authentication)은 한 객체가 다른 객체에게 자신의 정체를 증명하는 절차이다.인간은 서로를 목소리, 얼굴, 증명사진 등을 통해 증명한다.우리는 여기서, 이러한 네트워크 상에서의 실제로 통신하는 객체가 서로를 인증하는 법에 대해서 살펴본다.예를 들어 이메일 서버에서 사용자의 인증이다.이러한 인증은 라우터 간이나 클라이언트-서버 간에서도 필요하며, 인증(authentication) 프로토콜의 절차 내의 메시지와 데이터의 교환을 통해 이루어진다.보통 이런 인증 프로토콜은 다른 프로토콜들(이메일 프로토콜, 데이터 신뢰성 전송 프로토콜)을 이용하기 전에 먼저 서로의 정체를 수립하기 위해 이루어진다,전에 RDT 프로토콜에 배웠던 것 처럼 절차적으로 진화하는 프로토콜을 설명하는 방식으로 설명하겠다.figure 8.15 묘사에서 엘리스는 밥에게 자신을 증명해야하며, 가장 간단한 방법은 자신이 앨리스임을 메시지로 보내는 것이다.이 방법의 단점은 트루디(침입자)가 자신이 앨리스라고 주장할 수도 있다는 점이다.분명, 다른 방법이 필요하다.인증 프로토콜 ap2.0 (Authentication Protocol ap 2.0)만약 figure 8.16처럼 앨리스가 잘 알려진 자신만의 IP 주소를 발신지 IP 주소로 첨부해서 보낸다면 어떨까?아쉽게도, IP 데이터그램을 생성하고 조작하는 일은 그렇게 어려운 일이 아니다. 그저 오픈소스 리눅스 운영체제의 코드를 고치고, 자신만의 운영체제 커널을 만든 뒤 원하는 헤더 필드를 가진 데이터그램을 만들어 보내면 된다.이렇게 IP 데이터그램을 조작해 보내는 공격을 IP 스푸핑(IP Spoofing)이라고 한다.트루디는 자신의 IP 데이터그램에 잘알려진 앨리스의 IP 주소로 발신지 IP 주소를 수정해서 보내면 된다.몇 몇 최신 라우터는 이러한 IP 스푸핑 조작을 막게 하기 위해, 보내온 IP 데이터그램 발신지 IP 주소가 실제와 다르면 막게 하는 경우도 있지만, 모든 라우터가 최신 버전이 아닐 수도 있으며, 공격자 본인이 라우터의 설정을 건들 수 있다면 무력화할 수 있다.분명, 다른 방법이 필요하다.인증 프로토콜 ap3.0 (Authentication Protocol ap 3.0)비밀 번호를 활용하는 방법이 존재한다. 비밀번호는 객체와 인증 객체 사이만 공유하는 인증을 위한 번호이며, Gmail, Facebook, telnet, FTP 등 여러 서비스에서 이용 중이다.우리가 만든 인증 프로토콜 3.0 버전은 앨리스는 밥과 자신만이 공유하는 비밀번호를 메시지에 첨부해서 보내고, 밥은 이 비밀번호를 공유 받은 앨리스의 비밀번호와 비교해 앨리스임을 확신한다.하지만 이번엔 트루디가 중간에 메시지를 도청해서 가로채어 비밀번호를 알아버렸다. 이제 트루디는 해당 비밀번호를 첨부해 밥에게 메시지를 보내면, 앨리스로 변장할 수 있다.실제로 운영체제 원격 제어를 위한 Telnet 프로토콜은 이런식으로 비밀번호를 암호화하지 않고 보내기 때문에 서버나 클라이언트 LAN에 연결된 다른 침입자가 패킷을 sniff하고 변장하여 로그인할 수 있다. 이로 인해 오래되고 위험하고 구식이었던 Telnet은 더이상 사용하지 않고 SSH 프로토콜로 대체되었다.분명, 다른 방법이 필요하다.인증 프로토콜 ap3.1 (Authentication Protocol ap 3.1)이번에는 비밀번호를 대칭키로 암호화하여 전달하면 트루디가 메시지를 도청해도 이해할 수 없을 것이다.앨리스와 밥이 대칭키$K_{A-B}$를 공유한다고 가정하고, 비밀번호를 암호화하고 보내면 된다. 이를 인증 프로토콜 3.1이라고 하자!이제 트루디가 도청했지만, 대칭키를 가지고 있지 않은 트루디는 비밀번호 부분을 해독할 수 없고, 이해할 수 없다.하지만 상관없다. 트루디는 원하는 메시지를 적고 암호화된 비밀번호를 그대로 첨부해서 보내면 된다.밥은 암호화된 비밀번호를 해독하고 비교해보고, 같다는 것을 깨닫고 앨리스로 착각한다.이런 식으로 암호화된 정보를 해독하지 않고 재이용하는 방식을 플레이백(playback) 공격이라고 한다.우리의 ap 3.1 프로토콜은 공격자가 암호화된 정보를 해독하지 않고 재이용하는 플레이백 공격에 의해 무력화되었다.분명, 다른 방법이 필요하다.인증 프로토콜 ap 4.0 (Authentication Protocol ap 4.0)상기한 실패들은 잘 생각해보면 송신자가 현 시점에서 서로 인증한 뒤 실시간으로 통신을 진행 중인지, 아니면 이전에 진행했던 인증을 (공격자가) 재사용한 메시지인지, 알 수 있으면, 해결할 수 있다.TCP 연결을 예시로 들면, TCP 핸드셰이크 당시, 이전 TCP 연결이 사용하던 오래된 SYN 값은 더이상 사용하지 않으며, 만약 예전 SYN 값을 가진 세그먼트가 온다면 무시하게 된다. TCP 서버는 연결 시, SYN 값을 이전 연결에서 한번도 사용한적 없거나, 아주 오래전에 사용했던 SYN 값을 지정해 클라이언트에 보내고, 클라이언트는 해당 SYN 값을 ACK 세그먼트에 넣어 보낸다.이를 우리의 인증 프로토콜에 사용하기 위해 nonce(넌스)를 사용해보자,넌스(nonce)는 프로토콜에서 재활용이 불가능한 일생동안 단 한번만 사용가능한 값으로, 매 통신 시작시마다 주어진다.figure 8.18의 과정을 설명해 보자면,  앨리스는 자신이 앨리스라고 밥에게 알린다.  밥은 넌스 값 R을 생성하고 암호화하지 않고 앨리스에게 보내준다.          암호화하지 않는 이유는 암호화할 필요 없기 때문이다.        앨리스는 넌스 값 R을 미리 고유했던 대칭키 $K_{A-B}$로 암호화하고, 다시 밥에게 보낸다.  밥은 이를 복호화하고, 넌스 값 R과 일치하면, 현재 엘리스가 실시간으로 통신 중이라고 알 게 된다.          이 암호화 복호화 과정을 통해 2번에서 보냈던 넌스 값 R을 탈취해서 보낸 공격자를 구분할 수 있다.      넌스 값이 방금 보낸 것이라는 것을 통해, 현재 실시간 통신 중이라고 생각할 수 있다.      통신이 종료된 후, 밥이 복호화시 과거에 사용했던 R 값이면 이전 암호화된 R값을 탈취해서 모방하고 있는 공격자이다.대칭키 대신 공개키 암호화를 사용하면 어떨까? 그 해답은 이번 장의 끝부분에 나온다.8.5 이메일 보안 (Securing E-Mail)앞으로 PGP, TLS, IPsec이 속하는 응용 계층, 전달 계층, 네트워크 계층 순으로 보안 프로토콜에 대해 알아볼 것이다.기본적으로, 아래 계층이 제공한 기능은 위 계층들이 모두 누리게 된다. 예를 들어 응용 계층의 보안 프로토콜은 해당 프로토콜을 적용한 응용 프로그램만 보안을 누리지만, 연결 계층에서 적용된 보안 프로토콜은 해당 프로토콜을 기반으로 하는 그 위의 네트워크 계층, 전달 계층, 응용 계층의 모든 프로토콜과 응용 프로그램이 보안을 누리게 된다.하지만 그럼에도 불구하고 응용 계층, 전달 계층, 네트워크 계층 같은 상위 계층에 따로 보안 프로토콜이 필요한 이유는첫번째, 좀더 세세한 사용자 수준의 보안을 제공하기 위해 상위 계층 프로토콜이 필요하다.예를 들어 특정 IP가 보낸 것을 보장 하는 네트워크 계층 프로토콜이 있다고 해도, 해당 호스트를 이용한 다른 사람이 똑같이 쇼핑몰 계정을 공유하면 큰일 날 것이다. 쇼핑몰 계정을 나누기 위한 응용 계층 보안 프로토콜이 필요하다.둘째, 낮은 계층의 보안 프로토콜의 개발과 기능 갱신을 기다리는 것보다 자신의 응용 프로그램에 맞고, 빠르게 개발되는 상위 계층의 프로토콜이 필요할 수 있다.PGP (Pretty Good Privacy, 꽤 좋은 프라이버시)는 처음으로 널리 퍼진 보안 이메일 기능을 제공하는 보안 프로토콜이며, 이번 절에서 배워볼 것이다.8.5.1 안전한 이메일 (Secure E-Mail)안전한 이메일을 설계하기 위해 지금까지 배워왔던 보안의 기초와 밥과 앨리스, 트루디를 사용하자.우리는 기밀성(아무도 내용을 알수 없게), 송수신자 인증(내가 대화하는 사람이 그 사람이 맞는가), 메시지 무결성(메시지 내용을 누군가가 바꾸지 않았는가)을 가지고 있는 안전한 이메일을 만들 것이다.먼저 기밀성을 위해 DES와 AES 같은 대칭키 암호화 방식을 채택할 수 있겠지만, 대칭키를 공격자를 피해 보내줄 방법이 어려우므로, RSA 같은 공개키 방식을 이용할 수 있다. 하지만 공개키 방식은 메시지가 길어질 수록 비효율적이다.비효율성을 극복하기 위해, figure 8.19에 묘사된 세션키(8.2.2절) 방식을 이용할 수 있다.  Figure 8.19의 +는 메시지의 합체, -는 분리를 의미한다.          엘리스는 무작위 대칭 세션키 $K_S$를 생성한다.      그녀의 메시지 m을 세션키로 암호화한다. $K_S(m)$      세션키 $K_S$를 밥의 공개키 $K_B^+$로 암호화 한다.      암호화된 메시지와 세션키를 묶어 새로운 메시지(($K_S(m)$,$K_B^+(K_S)$))를 만든다.      이 메시지를 밥의 이메일 주소로 보내고 밥은 이 메시지를 둘로 분리한다. ($K_S(m)$ 따로 $K_B^+(K_S)$ 따로)      밥의 개인키 $K_B^-$를 이용해 대칭 세션키 $K_S$를 얻어낸다.($K_B^-(K_B^+(K_S))$)      대칭 세션키 $K_S$를 이용해 원문 메시지 m을 얻어낸다.      이제 기밀성과 비효율성을 모두 해결한 이메일을 만들었다. 이제 송신자 인증과 메시지 무결성을 제공해 보자.복잡함을 제거하기 위해, 위의 기밀성 부분은 생략하겠다. 나중에 이를 합칠 것이다.이번 일을 이뤄내기 위해, 디지털 서명(digital signatures)과 메시지 요약(message digests)을 활용하자, 과정은 Figure 8.20에 묘사되어 있다.  앨리스는 원문 메시지 m에 MD5 같은 해쉬 함수를 적용해 메시지 요약($H(m)$)을 얻어낸다.  앨리스의 개인키 $K_A^-$를 이용해 디지털 서명 $K^-_A(H(m))$을 생성한다.  원문 메시지 m과 디지털 서명을 합쳐 새로운 메시지를 만든다. $(m, K^-_A(H(m))$  밥의 이메일 주소에 넣어 보내면, 밥이 메시지를 원문과 디지털 서명으로 나눈다.  밥이 앨리스의 공개 키 $K_A^+$를 이용해 디지털 서명을 해독한다. $K^+_A(K^-_A(H(m)))=H(m)$  원문 메시지 m에 해쉬 함수를 비교해 5번의 결과물과 비교한다.만약 5번 결과물과 m의 해쉬 함수 결과물이 같다면, 이는 앨리스가 보낸 것이고, 메시지 또한 변경이 없었다는 것을 증명할 수 있다.이제 우리는 송신자 증명, 데이터 무결성, 기밀성이 확보된 이메일 서버를 만들어 볼것이다.앞서 배웠던 Figure 8.19와 Figure 8.20을 결합해보면,  앨리스는 먼저 디지털 서명과 원문 메시지가 합쳐진 메시지를 먼저 만든다. 이를 선두 메시지라 하겠다.  figure 8.19의 4번 과정처럼 선두 메시지를 세션키로 암호화하고, 이 세션키를 밥의 공개키로 암호화한 뒤, 이 둘을 이용해 새로운 메시지를 만든다.  이 메시지를 밥이 받으면 Figure 8.19와 Figure 8.20에서 했던 과정을 거친다.이를 통해 모두 합쳐진 이메일 서버를 만들 수 있었다.보다시피, 앨리스의 개인키(디지털 서명 생성시), 밥의 개인키(대칭 세션키 얻어낼 때) 공개키 방식을 두번 사용하였다.그리고 추가로, 서로의 공개키를 사용할 때, 별언급 없었지만 실제로는 CA의 인증서를 가져오는 과정이 있어야 한다.8.5.2  PGPPretty Good Privacy (PGP)는 위에서 우리가 만든 안전한 이메일 서버와 비슷하며, 버전에 따라 MD5나 SHA를 해쉬함수로, CAST, triple-DES, IDEA를 대칭키 암호화 알고리즘으로, RSA를 공개키 암호화로 사용하는 것이 다르다.PGP는 설치후, 공개키 암호화의 공개키와 개인키를 생성해주며, 공개키는 알아서 CA나 개인 홈페이지를 통해서 공개해야하고, 개인키는 비밀번호로 보호되어 접근할 때마다 비밀번호 입력을 필요로 한다.PGP는 설정에 따라 전체 메시지를 암호화하거나 디지털 서명화하거나 양쪽 다 선택할 수 있다.Figure 8.22는 디지털 서명이 설정된 PGP 메시지 예시이다. 이러한 메시지는 MIME 헤더 이후 나타난다.무작위 문자열로 보이는 인코딩된 데이터는 $K^-_A(H(m))$이며, 메시지 요약(digest)을 의미한다.메시지 요약은 송신자의 공개키를 이용해 복호화하여 메시지 무결성과 송신자 인증을 위해 사용한다.figure 8.23은 비밀 PGP 메시지로, 이 또한 MIME 헤더 앞에 나타난다. 기밀성있게 PGP를 설정하면 figure 8.23 처럼 생성된다.또한 PGP는 공개키 인증을 제공하며, 기존의 CA 방법과 조금 다른데, 신뢰의 웹(web of trust)라는 특별한 방법을 사용한다.한 사용자는 다른 사용자의 공개키/유저명 짝을 신뢰한다고 선언하면, 자신의 공개키/유저명 짝을 신뢰받을 수 있다. 만약 더 많은 인증서가 필요하다면, 더 많은 다른 사람의 인증서를 신뢰하면 된다. 이러한 키를 신뢰한다고 선언(=인증서를 생성하는) 하는 방법은, 개인키를 이용해 디지털 사인을 만들어 주는 것이다.몇몇 PGP 사용자는 키 인증 파티를 열어 많은 사람이 서로 신뢰성을 주고 받곤 한다.8.6 TCP 연결 보안화 : TLS (Securing TCP Connections: TLS)이번에는 어떻게 암호화가 TCP의 보안성을 증대키시는지 알아보자.TLS(Transport Layer Security, 전달 계층 보안)은 IETF에서 표준화한 TCP의 향상 버전으로 비슷한 프로토콜로 SSL 버전 3가 있다.SSL 프로토콜은 넷스케이프에서 고안했지만 비슷한 아이디어는 이전에도 있었고, SSL과 TLS는 모든 웹 브라우저와 웹 서버가 지원하고 여러 기업에서 전자 상거래 등에서도 활용하는 인기있는 프로토콜이 되었다.만약 http가 아닌 https를 주소창에 한번이라도 본적 있다면 당신은 TLS를 경험한 것이다.TLS가 없이 전자 상거래를 한다면 다음과 같은 문제가 생길 수 있다.  암호화 같은 기밀성이 없으므로, 침입자는 주문자의 주문 정보와 신용 카드 정보 등을 탈취하여 금전적인 피해를 입힐 수 있다.  데이터 무결성이 없다면, 침입자가 주문자의 주문 정보를 바꾸어 100배 더 많은 주문을 시도할 수 있다.  서버 인증이 없다면, 침입지가 가짜 전자 상거래 페이지를 띄워 고객의 정보를 가져가고, 돈을 갈취할 수 있다.TLS는 이와 같은 피해를 막을 수 있게 해준다.TLS는 주로 HTTP를 이용하는 전자상거래에 보안을 위해 사용되지만, TCP를 이용하는 서비스라면 어디든 사용 할 수 있으며, TCP와 유사한 소켓을 이용한 API를 제공한다.TLS은 SSL의 연장선으로 시작했기 때문에 TLS를 사용하는 응용 프로그램은 SSL 클래스와 라이브러리가 포함된다.Figure 8.24에서 보다시피 TLS는 엄밀히 말해서 응용 계층이지만, 개발자 입장에서는 TCP의 연장선이라는 느낌이라 전달 계층으로 분류된다.8.6.1 큰 그림 (The Big Picture)처음에는 쉬운 설명을 위해 간소화된 TLS로 시작하겠다.TLS에는 세가지 단계가 있다. 핸드셰이크(Handshake), 키 도출(key derivation), 데이터 전송(data transfer)예제는 클라이언트인 밥과 공개키 암호화와 인증서를 사용하는 엘리스로 보이겠다.핸드셰이크(Handshake)핸드셰이크에는 “(a) TCP 연결 수립”, “(b) 서버 인증”, “(c) TLS 세션간에 필요한 대칭키들을 생성하는데 사용할 마스터 비밀 키를 서버로 전송” 등의 활동을 할것이다.이 세가지 단계는 Figure 8.25에 묘사되어 있다.(a) TCP 연결이 수립된 이후 밥은 TSL Hello 메시지를 앨리스에게 보내면 공개키가 포함된 CA인증서를 응답 메시지로 받는다.(b) 이후 밥은 이번 세션에만 사용할 MS(Master Secret, 마스터 비밀)를 생성하고, 엘리스의 공개키로 암호화한다.이를 EMS(Encrypted Master Secret, EMS)라고 한다.(c) 이 EMS를 서버인 앨리스 측에 보내면, EMS를 앨리스의 개인키로 복호화해 MS를 확보한다.키 도출 (Key Derivation)이제 클라이언트와 서버 양측이 가지고 있는 MS를 이용해 모든 암호화와 데이터 무결성 체크에 사용할 대칭 세션 키들을 만드는데 사용한다.이를 이용해 4개의 키를 생성한다. 1개의 키로 모두 할 수 있음에도 4개의 키로 나누는 이유는 혹시 유출됬을 때의 피해를 최소화하기 위해서다.  $E_B$ = 클라이언트가 서버로 보낼 때의 데이터를 암호화하는 세션 암호화 키  $M_B$ = 클라이언트가 서버로 보낼 때의 데이터를 위한 세션 HMAC 키, HMAC(standardized hashed mesage authentication code)은 8.3.2에서 설명한 MAC의 일종  $E_A$ = 서버에서 클라이언트로 보낼 때의 데이터를 암호화하는 세션 암호화 키  $M_A$ = 서버에서 클라이언트로 보낼 때의 데이터를 위한 세션 HMAC 키4개의 다른 키를 생성하기 위해서 MS를 4개의 키로 쪼갠다.(정확히는 좀더 복잡한 방법이다.)키 생성이 끝나면 대칭키 4개가 생기며, 둘은 데이터를 암호화 하는데, 두 HMAC 키는 데이터 무결성을 확인하는데 사용한다.데이터 전달 (Data Transfer)이제 앨리스와 밥은 같은 4개의 세션 키 ($E_B$, $M_B$, $E_A$, $M_A$)를 가지게 되고 이제 데이터를 전송할 수 있다.하지만 TCP는 응용 계층과 바이트 흐름(byte-stream)으로 데이터를 전송하므로, 전송계층과 응용계층 사이에 있는 TLS의 데이터 또한 바이트 흐름 형태이다.(figure 8.24)  바이트 흐름(byte-stream)이란, 패킷 처럼 데이터들이 일정 형태로 나누어 지는 것이 아니라, 거대한 하나의 데이터 흐름으로 전달되는 것을 의미한다. 마치 드럼통 단위로 이동하는 석유가 패킷이라면, 송유관을 타고 흐르는 석유는 바이트흐름과 같다.  TCP는 IP 계층과 통신할 때는 세그먼트라는 패킷으로 보내지만, 응용 계층과는 사이에 버퍼를 두고 바이트 흐름으로 보낸다.  따라서, TCP와 응용 계층 사이에 존재하는 TLS는 바이트 흐름으로 이야기해야한다.하지만 바이트흐름 형태의 데이터는 일정한 단위가 없으므로, HMAC을 집어 넣어 무결성 검사를 할 위치가 애매하다.따라서 TLS에서는 바이트흐름 데이터를 레코드(rocord)로 나누어 레코드 별로 HMAC을 이어 붙여 암호화 한다.이 HMAC은 레코드 데이터 + $M_B$ 키를 이어 붙인 뒤, 해쉬 함수를 이용해 만들며 (8.3절 메시지 인증 코드 참조)이렇게 이어붙인 레코드 데이터 + HMAC를 $E_B$ 세션 암호 키를 이용해 암호화되고 TCP로 전해져 상대방에게 전달된다.하지만 여전히 칩입자가 TCP 헤더의 내용을 바꾸거나 세그먼트의 순서를 바꾸기, 삭제, 반복 등의 공격을 할 수 있으며, 이를 중간자(man-in-the-middle) 공격이라고 한다.이를 막기 위해  시퀀스 번호(sequence number)를 다음과 같이 활용한다.밥은 시퀀스 번호 카운터를 0부터 시작해 TLS 레코드를 보낼때 마다 증가시킨다.이때, 시퀀스 번호는 레코드에  포함시키지 않고 HMAC을 만들때 사용하는데,(데이터 m + HMAC 키 $M_B$ + 현재 시퀀스 번호)를 해쉬 함수를 이용해 HMAC을 만든다.이후 앨리스는 시퀀스 번호를 기록하면서, 데이터 무결성을 위해 HMAC을 비교할때, (데이터 m + HMAC 키 $M_B$ + 현재 도착했어야할 올바른 시퀀스 번호)를 이용해 HMAC을 만들고, 이 HMAC이 도착한 메시지의 HMAC과 같으면 해당 레코드를 취한다.이 방법을 이용해 중복되거나, 중간 지점의 여성(woman-in-the-middle) 공격을 방지할 수 있는데, 잘못된 세그먼트들은 그냥 버려지고, 오지 않은 시퀀스 번호는 재전송 요청을 하기 때문이다.TLS 레코드 (TLS Record)TLS 레코드의 구조는 figure 8.26에 묘사되어 있다.레코드는 Type 필드, Version 필드, Length 필드, Data 필드, HMAC 필드로 이루어져 있으며, 첫 세 필드는 암호화 되지 않는다Type 필드는 핸드셰이크 메시지, 데이터 메시지, TLS 연결 종료에 사용하는 메시지 등을 구분하는데 사용되며,Version 필드는 TLS의 버전을 의미한다.Length 필드는 수신자가 TCP 바이트 흐름에서 TLS 레코드를 추출하는데 사용한다.8.6.2 더욱 완벽한 그림 (A More Complete Picture)이때까지의 간단한 TLS가 아닌 실제의 좀더 복잡한 TLS 프로토콜에 대해 알아보자.TLS 핸드셰이크 (TLS Handshake)SSL에서는 사용자들이 사용할 대칭키나 공개키 알고리즘을 설정하지 않는다. 대신 TLS에서는 양측이 TLS 세션을 시작할 때 핸드셰이크를 통해 합의하도록 되어있다.추가로 핸드셰이크 중에는 양 측은 넌스(nonces)를 서로에게 보내 세션키($E_B$, $M_B$, $E_A$, $M_A$)를 만드는데 사용된다. 실제 TLS 핸드셰이크 과정은 다음과 같다.  클라이언트는 호환가능한 암호화 알고리즘들의 리스트와 클라이언트 넌스를 서버측에 보낸다. (클라이언트 Hello)  서버측은 클라이언트 측이 보내준 알고리즘 리스트 중에서 사용할 대칭키 알고리즘(AES 등)과 공개키 알고리즘(RSA 등), HMAC 알고리즘(MD5, SHA-1 등)를 고른 뒤, 이들의 리스트와 CA 인증서(서버 인증(Server Certificate)), 서버측 넌스를 함께 클라이언트측으로 보낸다. (서버 Hello, 서버 Hello 끝(Server Hello Done))          이때 클라이언트 측의 인증서를 요구하는 인증 요구 (Certificate Request)도 가능하다.      만약, CA 인증서를 이용해 공개키를 주는 방식이 아니라면 Diffie-Hellman 알고리즘을 통한 서버 키 교환(server key exchange) 이 필요하다.        클라이언트 측은 인증서를 확인한 뒤, 서버의 공개키를 추출하고 PMS(Pre-Master Secret, 선마스터 시크릿)을 생성한 뒤, 서버의 공개키로 암호화한 뒤 서버에게 보낸다. (클라이언트 키 교환(client key exchange))  TLS 표준에 명시되어 있는 같은 키 도출 함수로 클라이언트와 서버가 각각 PMS와 넌스를 활용해 MS(Master Secret, 마스터 시크릿)을 생성한다. MS를 나누어 두개의 암호화 키와 두개의 HMAC 키를 생성한다. 추가로 만약 대칭키 암호화 알고리즘(3DES, AES)이 CBC(Cipher block chaining, 암호화 블록 연쇄)를 이용한다면 MS를 이용해 서버와 클라이언트가 각각 한개, 총 두개의 IV(Initialization Vectors, 초기 벡터)를 생성한다. 이후로 모든 통신의 메시지는 이들로 암화회되고, HMAC를 이용해 인증된다.  클라이언트가 서버로 모든 핸드셰이크 메시지의 HMAC를 보낸다. 서버측은 자신이 클라이언트로부터 받은 HMAC와 자신이 가지고 있는 핸드셰이크의 HMAC가 전부 같은지 비교한다. 암호화 사양 변경 프로토콜-1(Change Cipher Spec Protocol-1)  서버가 클라이언트로 모든 핸드셰이크 메시지의 HMAC를 보낸다 클라이언트 측은 자신이 서버로부터 받은 HMAC와 자신이 가지고 있는 핸드셰이크의 HMAC가 전부 같은지 비교한다. 암호화 사양 변경 프로토콜-2(Change Cipher Spec Protocol-2)  이후 모든 과정이 끝났다면 서버측에서 Finished 메시지를 보내 TLS 핸드셰이크를 종료한다.종료 (Finished)암호화 사양 변경 프로토콜 (Change Cipher Spec Protocol)마지막 5번, 6번 두 과정은 암호화 사양 변경 프로토콜(Change Cipher Spec Protocol)라고 하며 핸드셰이크 과정 중 중간자의 템퍼링 공격(tempering, 위변조 공격)을 막기 위함이다. 이를 자세히 살펴보자1번 과정, 클라이언트 hello에서 클라이언트가 보낸 알고리즘의 리스트 정보는 아직 어떠한 알고리즘 선택이나 키 생성이 되지 않은 상태이기 때문에 아무런 보안 처리가 되지않은 원문 메시지로 보내게 되고, 이로 인해 공격자에게 노출되어 있다.중간자는 해당 메시지를 위변조하여 리스트에서 정상적이고 강력한 알고리즘들은 지워버리고 약하고 공격가능한 알고리즘만 남겨서 서버에 다시 보낸다.이러한 공격을 막기 위해 5번에서 클라이언트는 모든 핸드셰이크 과정 중 메시지의 HMAC들의 합체를 보낸다. 서버측은 자신의 HMAC들과 받은 HMAC들을 비교하여, 일치하지 않는게 있다면 연결을 종료해버린다. 이후 반대쪽의 클라이언트도 서버가 보내온 HMAC를 비교하여 같은 행동을 한다.즉, 1번 과정, 클라이언트 hello를 포함해서, 어떠한 핸드셰이크 메시지라도 자기가 보낸 메시지와 다르다면, 연결이 될 수 없다.예를 들어, 중간자가 리스트에서 정상적이고 강력한 알고리즘들은 지워버리고 약하고 공격가능한 알고리즘만 남겨서 서버에 다시 보냈다면, 이는 클라이언트 측에서는 자신이 보낸 알고리즘의 리스트와 다르게 되므로 서버측의 HMAC가 다르게되고 곧바로 종료시켜 버린다.유일한 방법은, 1번 과정의 알고리즘 리스트를 전부 가장 약한 알고리즘으로 만든 뒤, 마지막 5번 6번(Change Cipher Spec Protocol) 과정에 개입하여, 암호화가 걸려있는 메시지들을 복호화해 위변조하는 방법 밖에 없지만, SSL 측에서 (TLS와 library를 공유함) 사용가능한 알고리즘 리스트를 가장 약한 알고리즘이어도, 모든 복호화가 즉시 가능하지 못하도록 보안 수준을 유지하게 라이브러리를 유지하므로 사실상 불가능하다.서버와 클라이언트의 Hello 과정서버와 클라이언트 측의 Hello (1, 2번) 과정 중에 시퀀스 번호가 있음에도 넌스를 사용하는 이유는 뭘까?넌스는 연결 리플레이 공격을 방지하기 위해, 시퀀스 번호는 메시지 리플레이 공격을 방지하기 위해 존재한다.리플레이 공격이란, 중간자가 메시지를 도청한 뒤, 같은 메시지를 또 보내게 하여 보안으로 부터 회피하면서, 같은 명령을 다시 수행하게 만든 것이다.예를 들어, 이커머스 이용자의 구매 메시지를 도청한 뒤, 이를 다시 한번 보내는 것으로 이용자는 두 번의 구매를 한것으로 나타날 것이다.TLS 연결 중, 시퀀스 번호가 이미 사용했던 메시지가 다시 나타났다면, 상대 측에서 재전송했거나, 중간자가 리플레이 공격을 시도한 것이므로 무시하면 된다.넌스는 알다시피, 매 연결마다 설정되는 번호이므로, 같은 주체들의 연결이라고 해도, 넌스는 다르게 생성된다.즉, 연결 리플레이 공격을 시도하면, 이전에 사용했던 넌스를 재사용하게 되며, 또한 이 넌스는 상기 과정에서 PMS를 MS로 만들 때 사용되므로, 암호화 알고리즘에 사용할 키들 또한 이전에 사용했던 키를 가지게 된다.서로 다른 키에 의해 메시지들은 무결성 체크에 실패 하게 되고, 공격자의 리플레이 공격 메시지들은 무시되게 된다.연결 종료 (Connection Closure)만약 어느 순간에 더이상 주고 받을 통신이 없고 TLS 세션을 종료하고 싶을 때는 TCP FIN 세그먼트를 이용해 TCP 연결을 종료하는 방법이 있다.하지만 이 방법은 침입자가 진행중인 연결에 연결 종료 의사를 담은 메시지를 보내는 단절 공격(truncation attack)에 취약하다.트루디가 앨리스에게 TCP FIN을 보내면, 밥은 원하지 않았어도 앨리스가 연결이 끝난 줄 알고 종료를 할 수 있다.이를 막기 위해, TCP FIN 세그먼트를 보내기전에 TLS의 type 필드가 종료 레코드(closure TLS record)로 되어있는 레코드를 보내야 종료가 되게 만들면 된다.TLS 레코드는 이미 HMAC으로 송신자가 인증 되기 때문에 침입자가 단절 공격을 위한 위조 종료 레코드는 인증되지 못해 무시된다.우리의 TLS에 대한 공부는 여기서 끝났고, 더욱 자세한 내용은 [Rescorla 2001]에서 참고바란다.8.7 네트워크 계층 보안 : IPsec과 가상 사설망 (Network-Layer Security: IPsec and Virtual Private Networks)IP 보안 프로토콜(IP security Protocol, IPsec)은 네트워크 계층의 보안 프로토콜로, VPN(virtual private networks, 가상 사설망)을 만드는데 사용된다.네트워크 계층 보안은 기밀성 이외에도 인증, 변조 방지, 리플레이 공격 방지, 데이터 무결성 보장 등이 가능하며, 위가 배울 IPsec 또한 그러하다.8.7.1 IPsec과 가상 사설망 (VPNs) (IPsec and Virtual Private Networks (VPNs))일부 규모가 큰 조직들은 보안과 기밀을 이유로 외부에 연결되지 않은 네트워크를 가지고 싶어하고 이를 사설망(private network)라고 한다. 실제로 직접 라우터, 스위치 같은 물리적인 네트워크 장비를 구매해 설치하는 경우도 있지만, 이럴 경우 구매비용, 운영비용, 관리 비용이 엄청나진다.물리적인 사설망 대신, 많은 조직들은 공개 인터넷 내부에 VPN으로 사설망을 형성하다. VPN 간의 트래픽은 기밀을 위해 암호화된 후 물리적인 네트워크 대신 인터넷을 통해 전해진다.figure 8.27은 간단한 VPN의 예시이다. 본부(headquarter), 지부(branch office), 전근중인 영업사원(traveling Salesperson)으로 이루어져있고 영업사원은 호텔의 인터넷을 통해 연결되어 있다.VPN에서는 본부의 두 호스트 간이나 지부의 두 호스트 간, 즉 같은 장소에 있는 두 호스트는 서로와 통신할 때는 IPsec을 사용하지 않은 순수 IPv4를 사용하며, 만약 조직의 두 호스트의 통신 경로가 공용 인터넷을 지나간다면, 트래픽은 인터넷에 들어가기전에 IPsec을 통해 암호화되어야 한다.  단, 호스트가 같은 VPN 내의 호스트가 아닌 외부 인터넷의 호스트와 통신할 때는 IPsec이 아닌 순수 IPv4로 통신한다.  예를 들어 본부의 호스트가 구글의 메일서버를 이용할 때는 IPsec을 이용하지 않는다.예를 들어 본부의 호스트가 IP 데이터그램을 호텔의 영업사원에게 보내게 되었다면, 본부의 게이트웨이 라우터는 순수 IPv4 데이터그램을 IPsec 데이터그램으로 바꾼뒤 인터넷으로 포워딩 한다. IPsec 데이터그램은 IPv4의 헤더 또한 포함하므로, 평범한 IPv4 데이터그램처럼 경로를 찾아 간다.하지만 IPsec 데이터그램의 페이로드에는 IPsec 처리에 사용될 IPsec 헤더를 포함하고 있으며, 암호화되어있다.IPsec 데이터그램이 영업사원의 노트북에 도착하면, 노트북의 운영체제가 페이로드를 복호화하고, 데이터 무결성 등 다른 보안 사항을 체크한 뒤, 데이터를 UDP, TCP 같은 상위 계층에 보내준다.대략적으로 알아보았으니 좀더 자세하게 알아보자.8.7.2 AH와 ESP 프로토콜 (The AH and ESP Protocols)IPsec은 아주 복잡한 개념이며, 여러가지 RFC로 정의되어 있다. 대표적으로 RFC 4301(전체적인 IP 보안 구조), RFC 6071(IPsec 프로토콜들의 개요). 우리는 이를 간단하지만 실전적인 방법으로 알아보자.IPsec 프로토콜 집단에는 두가지 원리 프로토콜이 존재한다. AH(Authentication Header, 인증 헤더) 프로토콜, ESP(Encapsulation Security Payload, 캡슐화 보안 페이로드) 프로토콜, 둘다 IPsec 객체(호스트나 라우터)가 서로 보안 데이터그램을 주고 받을 때 사용하는 기본 프로토콜로,AH 프로토콜은 인증과 데이터 무결성을 보장하지만, 기밀성은 보장하지 않고, ESP 프로토콜은 인증, 데이터 무결성, 기밀성을 전부 보장한다.때문에 현실에서는 ESP 프로토콜이 더욱 널리 사용되고 있으며, 이에 대해 중점적으로 알아볼 것이다.8.7.3 보안 연관 (Security Associations)IPsec 개체 둘이 통신하기 전에 네트워크 계층의 단방향성 논리적 연결을 두개(오고 가고) 생성하는데 이것이 바로 SA(Security Association, 보안연관)이다.figure 8.28은 본부에서 지부 방향의 통신을 연결하는 SA를 가지고 있는 게이트웨이 라우터 R1과 게이트웨이 라우터 R2의 예시이다.라우터 R1과 라우터 R2는 SA의 상태 정보를 공유하고 있는데, 예를 들자면,  SA의 32비트 식별자, SPI(Security Parameter Index, 보안 매개변수 색인)  SA의 출발지 인터페이스(묘사상 200.168.1.100), SA의 도착지 인터페이스(묘사상 193.68.2.23)  암호화 알고리즘 종류 (예를 들어 CBC를 이용한 3DES)  암호화 키(The encryption key)  무결성 체크 알고리즘 종류 (예를 들어 MD5 HMAC)  인증 키(The authentication key)R1은 IPsec 데이터그램을 생성하고 암호화, 인증한 뒤, 포워딩 하기 위해 위의 상태 정보를 이용하고, R2는 수신받은 IPsec 데이터그램을 복호화하고, 인증하는데 위의 상태 정보를 이용한다.이러한 SA 정보는 VPN 내에서 연결이 많아질수록 통신 객체에 저장해야 할 정보가 많아지며, 보통 객체 운영체제 커널에 존재하는 SAD(Security Association Database, 보안 연관 데이터베이스)에 저장한다.8.7.4 IPsec 데이터그램 (The IPsec Datagram)IPsec 데이터그램은 터널 모드(tunnel mode)와 전달 모드(transport mode) 두가지가 존재하며, 터널모드가 좀더 VPN과 어울려 널리 사용되고 있다.우리는 여기서 터널 모드 IPsec 데이터그램만 미리 알아보겠다.Figure 8.29는 IPsec 데이터그램의 구조이다.Figure 8.28의 시나리오를 이용해서 순수 IPv4 데이터그램을 IPsec 데이터그램으로 바꾸는 과정을 알아보겠다.      IPv4 데이터그램의 뒷편에 ESP trailer(ESP 예고) 필드를 더한다.          ESP trailer 필드는 padding, pad length, next header 필드로 구성되어있으며, 원본 데이터그램과 함께 암호화 대상이다.                  padding 필드는 블록 암호화를 사용하기 위해 메시지를 정수배로 만드는데 사용 (예를 들어 8 비트 블록 암호화면 메시지는 8의 배수 길이여야 함.)          pad length 필드는 위의 padding 필드가 얼마나 포함됬는가?(나중에 제거하기 위해서)          next header 필드는 원본 IP 데이터그램 페이로드의 type(UDP, TCP 등) 정보                          SA 상태정보를 참고해 암호화 키를 이용해 암호화한다.        **암호화된 데이터그램의 앞에 ESP header(ESP 헤더) 필드를 더한다. **                  여기까지 패킷을 Enchilada(엔칠라다, 멕시코 음식) 라고 부르자.(실제로는 그렇게 부르지 않고, 이 책에서만의 비유인듯 하다.)                    ESP header 필드는 SPI 필드와 sequence number 필드로 이루어져 있다.                              SPI 필드는 소속 SA의 SPI(Security Parameter Index)값, 통신 객체는 이를 통해 SAD에서 SPI를 색인하여 적절한 알고리즘과 키를 가져옴                                sequence number 필드를 통해 리플레이 공격을 방지한다.                                    SA 상태정보를 참고해 엔칠라다에 인증 키를 더한 뒤 고정길이 해쉬 함수를 적용하여 엔칠라다 전체에 대한 인증 MAC를 생성한다.        생성된 MAC을 엔칠라다 뒤편에 더하여 여태까지의 결과물을 페이로드로 만든다.        마지막으로 이전 원본 20 바이트 길이 IPv4 헤더를 참조해 새로운 IP 헤더를 만든 뒤, 맨 앞에 더한다.                  이때의 새로운 IP 헤더의 출발지와 목적지 주소는 각각 SA의 출발지 인터페이스 주소와 도착지 인터페이스 주소이다.                    또한 protocol number 필드는 기존의 TCP, UDP 등이 아니라 50(ESP 프로토콜)이다.            이렇게 생성된 IPsec 데이터그램은 마치 페이로드로 ESP header, 암호화된 원본 IPv4 데이터그램, 암호화된 ESP trailer, ESP MAC을 가진 평범한 IPv4 데이터그램처럼 보이며, 공용 인터넷에서 평범한 IPv4 처럼 라우팅된다.도착지 인터페이스에 도착하면 도착지 통신 객체(보통 라우터)가 확인하고 ESP 처리 절차를 밟는다.  엔칠라다에서 SPI를 통해 SA 소속을 확인하고 사용해야할 알고리즘과 키를 가져온다.  ESP MAC값을 통해 데이터 무결성 확인  sequence number 필드를 확인하여 리플레이 공격 방지  원본 데이터그램과 ESP trailer 부분을 복호화  패딩을 제거하고 원본 IP 데이터그램을 가져온다.  원본 IP 데이터그램을 목적지 호스트에게 전달한다.통신 객체에는 SAD 이외에 추가로 SPD(Security Policy Database, 보안 정책 데이터베이스)라는 자료 구조를 가지고 있으며, SPD는 들어온 데이트그램의 프로토콜 종류, IP 주소를 이용해 IPsec으로 처리되어야 할 데이터그램과 순수 IPv4 데이터그램으로 보내져야할 데이터그램을 분류하고, 목적지로 가기 위한 올바른 SA를 배정해준다.즉, SPD는 데이터그램에 무엇을 해야하는가, SAD는 데이터그램을 어떻게 해야하는 가를 담는다.IPsec 서비스 요약 (Summary of IPsec Services)IPsec이 제공하는 서비스는 다음과 같다.      중간자(man-in-the-middle)이 데이터그램의 protocol number, IP 관련 주소를 알 수 없다.          오직 알 수 있는 것은 데이터그램의 출발지 라우터 인터페이스와 도착지 라우터 인터페이스 뿐이다.      이는 SSL/TLS에 비해 더욱 기밀성이 강화된 것이다.            중간자가 데이터그램을 변조하려고 시도하면, 도착 인터페이스에서 무결성 체크에 실패하게 된다.        만약 중간자가 라우터 인터페이스로 변장하여, 거짓 데이터그램을 보낼 경우, 동일한 인증키를 확보하지 못했으므로 무결성 체크에 실패하게 된다.        sequence number가 다르기 때문에 리플레이 공격도 불가능하다.  이러한 방법으로 IPsec은 두 통신 객체 간의 기밀성, 발신지 인증, 데이터 무결성, 리플레이 공격을 방지할 수 있다.8.7.5 IKE: IPsec에서의 키 관리(IKE: Key Management in IPsec)IKE(Internet Key Exchange, 인터넷 키 교환) 프로토콜은 거대해진 VPN에서 네트워크 관리자가 일일이 SA 정보와 키를 입력해주지 못하므로 이를 자동화하는 프로토콜이다.[RFC 5996]IKE는 SSL과 유사한 핸드셰이크를 가지고 있으며, 각 IPsec 객체는 인증서와 공개키를 가지고, SSL을 통해 인증서를 공유하고, 보안 알고리즘을 상의한 뒤, 안전하게 IPsec SA 세션 키를 생성하기 위한 정보를 공유한다.SSL과 달리 IKE는 두 단계의 핸드셰이크로 되어있으며, 이를 figure 8.28의 시나리오로 알아보자.먼저 첫번째 단계는 두 라우터 인터페이스 간에 두 메시지 쌍을 교환하는 것으로 시작한다.      첫번째 메시지쌍 교환 때는 Diffie-Hellman 알고리즘으로 양방향의 암호화되고 인증된 IKE SA를 생성한다.(이는 IPsec SA와 다르다. 주의!)    이때, IKE SA의 암호화와 인증을 위한 키와 두번째 단계 때 IPSec SA키를 생성하기 위한 MS(master secret)가 수립된다.        두번째 메시지 쌍 교환 때는 양쪽에서 자신의 메시지를 디지털 사인하여 정체를 밝힌다. (공개키 암호화 처음 사용) 이때 패킷 스니핑으로 보안 IKE SA 채널의 통신을 알지 못하며, 이때 IPsec SA에 사용할 암호화, 인증 알고리즘을 정한다.  두번째 단계는 양측에서 각각 서로의 방향으로 향하는 SA를 총 2개 만든다. 끝날때 쯤에는 양측에 두개의 SA를 위한 암호화, 인증 키들이 생성되고, SA를 이용해 데이터그램을 주고 받을 수 있게 된다.두 번째 과정중에는 공개키 암호화가 사용되지 않으므로, IKE는 성능상 괜찮고, 많은 양의 SA를 적은 비용으로 생성할 수 있다.8.8 무선 랜과 4G/5G 무선 망에서의 보안(Securing Wireless LANs and 4G/5G Cellular Networks)무선 통신에서는 공격자가 전송 범위에 있는 것만으로도 패킷을 받아볼 수 있기 때문에 무선 랜과 4G/5G를 막론하고 중요하다.우리는 무선망에서의 보안이 우리가 지금까지 배워온 인증을 위한 넌스의 사용, 메시지 무결성을 위한 해싱, 데이터 암호화를 위한 대칭키 도출, AES 암호화 표준 등 의 개념을 연장해서 사용한다는 것을 배울 것이다.더욱 자세한 내용은 802.11 보안 책 [Edney 2003; Wright 2015], 3G/4G/5G 보안 [Sauter 2014]과 최신 연구 [Zou 2016; Kohlios 2018]를 참고 바란다.8.8.1  802.11 무선 LAN에서의 인증과 키 합의(Authentication and Key Agreement in 802.11 Wireless LANs)802.11에서의 중요한 보안 사항에 대해 알아보자.      상호 인증(Mutual authentication) 네트워크는 무선 기기를, 무선 기기는 네트워크를 신뢰할 수 있는지 인증하는 것을 상호인증이라고 한다.        암호화(Encrpytion) 802.11 프레임이 도청되고 조작당할 가능성이 있으므로, AP와 장치간의 암호화가 중요하다. 이때, 암호화와 복호화의 속도가 빨라야 하므로, 대칭키 암호화방식이 사용된다.  figure 8.30 은 모바일 장치가 AP(Access Point)를 통해 802.11 네트워크에 참여하려하고 있다.AS(Authentication server, 인증 서버)는 모바일 장치의 인증을 책임지는 서버로, AP 내부에 존재하는 경우도 있지만 네트워크마다 하나의 서버가 같은 네트워크의 여러 AP의 인증 요청을 처리하기 위해 따로 존재한다.AP 서버는 AS와 모바일 기기 사이의 인증 절차 과정 중의 인증 및 키 도출 메시지를 실어나른다.위와 같은 시나리오에서 상호인증과 암호화 키 도출 과정은 크게 4 단계로 나눌 수 있다.      발견(Discovery)    이 단계에서는 AP가 자신의 존재와 제공할 수 있는 인증, 암호화 형식을 모바일 기기에게 전파한다. 모바일 기기는 이중에 원하는 보안 형태를 요청하고, 다음 절차를 시작한다.        상호 인증과 공유된 대칭 키 도출(Mutual authentication and shared symmetric key derivation)    802.11 채널 보안에서 가장 중요한 부분으로, 모바일 기기와 인증 서버(AS)가 이미 공유된 공통 비밀(shared common secret)을 가지고 있다고 가정하고 시작한다. (실무에서 비밀번호 등을 입력하므로 실제로 그러하다). 장치와 AS는 이 공통 비밀과 리플레이 공격을 막을 넌스, 무결성을 위한 암호화 해싱을 이용해 인증한다. 또한 추가로 모바일 장치와 AP가 암호화에 사용할 공유된 세션 키도 도출한다.        공유된 대칭 세션 키 분배(Shared symmetric session key distribution)    대칭 암호화 키가 도출된 후, 인증 서버가 AP들에게 대칭 세션 키를 전해줄 프로토콜을 사용한다.        모바일 장치와 원격 호스트간의 AP를 통한 암호화된 통신(Encrypted communication between mobile device and a remote host via the AP)    대칭키를 공유받은 AP와 이미 가지고 있던 모바일 장치간의 암호화된 통신이 가능하다. 암호화에는 실무에서 AES 대칭키 암호화가 사용된다.  상호 인증 및 공유된 대칭 세션 키 도출 (Mutual Authentication and Shared Symmetric Session Key Derivation)WEP(Wired Equivalent Privacy)라고 불리우는 802.11의 보안 사양에 몇 개의 보안 결점이 있었고, WLAN은 공격에 취약하게 되었다.따라서 WiFi Alliance(와이파이 연합)측에서 무결성 체크, 암호화된 메시지의 흐름을 관찰해 암호화 키를 예측하는 공격을 해결한 WPA1(WiFi Protected Access)과 AES 대칭 키 암호화를 강요한 WPA2를 내놓아 해결하려 했다.WPA는 4단계의 핸드셰이크 프로토콜을 통해 상호 인증과 공유 대칭키 도출을 실시하며, figure 8.31은 핸드셰이크의 간소화된 버전이다.처음에 비밀 키 $K_{AS-M}$(패스워드 등)를 모바일 장치(M)과 인증서버(AS)가 공유한 뒤, M과 AP 사이에서 프레임을 암호화/복호화할 공유된 대칭 세션키 $K_{M-AP}$를 도출해야한다.상호 인증과 공유된 대칭 세션 키 도출은 첫 두 단계 a와 b로 이루어지며, 단계 c와 d는 단체 통신을 위한 두번째 키 도출을 하는 단계이다.a. 첫 시작시, 인증서버(AS)는 넌스 $Nounce_{AS}$를 만들어 모바일 기기에게 보낸다. 이를 통해 리플레이 공격을 방지하고, 실시간 통신임을 증명한다.b. 모바일 장치 M은 $Nounce_{AS}$를 받은 후 새로운 넌스 $Nounce_M$을 만든다. 이후 시작시 공유되는 비밀키 $K_{AS-M}$, 장치 M의 MAC 주소, AS의 MAC 주소,$Nounce_{AS}$와 $Nounce_M$를 이용해 공유 대칭 세션 키 $K_{M-AP}$를 생성하고, $Nounce_M$와 $Nounce_{AS}$ 와 $K_{AS-M}$을 이용한 HMAC값을 보낸다.AS는 이 값을 받고 HMAC에서 $Nounce_{AS}$와 첫 시작시 공유한 $K_{AS-M}$를 알아보고 모바일 장치는 인증된다.이에 AS 또한 b의 모바일 장치 M과 똑같은 과정과 베이스로 같은 값을 가지는 $K_{M-AP}$를 생성하고, 이를 AP들에게 전달해 통신을 암호화할 수 있게 된다.WPA3는 WPA2의 갱신 버전으로, 핸드셰이크에서 같은 넌스를 이용하는 방법을 방지한다.802.11 보안 메시지 프로토콜 (802.11 Security Messaging Protocols)figure 8.32는 802.11 보안 프로토콜을 구현하는데 사용된 프로토콜의 스택이다.EAP(Extensible Authentication Protocol, 확장가능 인증 프로토콜)은 AS와 모바일 기기간의 간단한 요청/요구 메시지의 엔드투엔드 구조를 정의하며, WPA2로 인증된다.이러한 EAP 메시지는 무선 연결을 타고 EAPoL(EAP over LAN, LAN 상의 EAP)로 캡슐화되고, AP에서 디캡슐화 된 뒤, EAP 메시지를 UDP/IP로 AS측에 보내기  RADIUS 프로토콜로 재캡슐화한다.RADIUS 프로토콜과 서버는 필수가 아니며, 가까운 날 DIAMETER 프로토콜로 대체될 것으로 보인다.8.8.2 4G/5G 무선망에서의 인증 및 키 합의 (Authentication and Key Agreement in 4G/5G Cellular Networks)4G/5G 에서의 보안은 802.11에서와 달리 홈 네트워크와 방문 네트워크, 이동성의 개념이 존재한다는 점이다. 방문 네트워크와 홈 네트워크는 모바일 기기의 인증과 암호화 키를 생성하기 위해 서로 통신해야만 한다.4G/5G에서도 AP와 마찬가지로 기지국 또한 모바일 기기와 서로를 인증하고 공통의 대칭키를 생성해야 한다.Figure 8.33은 모바일 장치가 4G 무선망에 접속하려는 시나리오이다.익숙한 모바일 기기(M), 방문 네트워크의 기지국(BS), 이동성 관리 객체(MME), 홈 네트워크의 홈 구독자 서비스(HSS) 등이 보이며, 이를 figure 8.30과 비교하여 802.11과 4G 보안의 비슷한 점과 다른 점을 볼 수 있다.여기서는 4G MME와 HSS가 인증서버(AS)와 비슷한 역할을 하며, 모바일 기기가 생성된 $K_{BS-M}$이 프레임을 암호화/복호화 하는데 사용된다.또한 802.11 때와 마찬가지로 공유된 공통 비밀(Shared Common Secret) $K_{HSS-M}$을 가지고 서로의 인증을 시작하는데, 이는 모바일 기기의 SIM 카드와 HSS의 데이터베이스에 적혀있다.AKA(4G Authentication and Key Agreement) 프로토콜은 다음과 같은 과정을 거친다.a. HSS로 인증 요청 (Authentication request to HSS)모바일 기기가 기지국에 최초 네트워크 연결 요청 메시지를 IMSI(international mobile subscriber identity, 국제 모바일 구독자 식별)를 담아서 보내고, 기지국은 연결 메시지를 중계 목적으로 방문 네트워크의 MME(Mobility Management Entity, 이동성 관리 객체)에 보낸다. MME는 IMSI와 VN 정보를 홈 네트워크의 HSS에게 보낸다. MME와 HSS의 통신은 7.4절 참조b. HSS의 인증 응답 (Authentication response from HSS)HSS는 미리 정의되고 모바일 기기와 공유하고 있던 비밀키 $K_{HSS-M}$를 이용해 IMSI 등을 암호화해 인증 토큰 $auth_token$, 예상 인증 응답 토큰 $xres_{HSS}$ 토큰을 생성하고 이 둘과 나중에 기지국에 나눠줄 키를 MME로 보낸다.$auth_token$은 HSS가 $K_{HSS-M}$으로 IMSI를 암호화해서 생성한 암호화된 정보인데,아직 인증받지 않은 HSS가 $auth_token$를 보냈고, 이 $auth_token$을 모바일기기가 $K_{HSS-M}$을 이용해 풀었을 때 원본 IMSI로 해독이 된다면,모바일 기기와 HSS만 알고있는 비밀키 $K_{HSS-M}$을 이용했다는 의미이므로, 모바일 입장에서 HSS는 안전하다고 인증된다.반대로 MME에게(더나아가 HSS 입장에서도) 모바일 장치를 인증하는 방법으로, $xres_{HSS}$를 이용하는데,모바일 장치는 $K_{HSS-M}$을 이용해 암호화해 $res_M$을 만들고 MME에게 보내줘야하고,MME는 이 값을 $xres_{HSS}$과 비교해 맞다면 모바일 장치가 $K_{HSS-M}$을 가지고 있다는 의미이므로, 모바일 장치도 인증이 되는 구조이다.MME는 오직 경유지 역할을 하며, 인증 응답 메시지를 받고 $xres_{HSS}$는 가지고 있다 나중에 모바일 장치가 보낸 것과 비교하고,auth_token을 모바일 장치에게 주어 HSS를 믿게 하는 등의 역할을 하며, $K_{HSS-M}$은 받지도, 보내지도, 존재를 알지도 않는다.c. 모바일 기기의 인증 응답 (Authentication response from mobile device)모바일 기기는 $auth_token$을 건네받고, 이를 가지고 있던 $K_{HSS-M}$로 해독해보아 자신의 IMSI가 나온다면, HSS가 같은 $K_{HSS-M}$를 가지고 있다는 의미이므로 HSS를 믿을 수 있게 된다.그리고 모바일 기기는 $K_{HSS-M}$를 이용해 HSS와 같은 과정을 거쳐 $res_M$을 만들어 MME에게 보낸다.d. 모바일 기기 인증 (Mobile device authentication)MME는 $res_M$과 보내지 않고 간직하고 있던 $xres_{HSS}$를 비교하여 같다면,모바일 장치가 $K_{HSS-M}$를 보유하고 있다는 의미이므로 모바일 기기인증이 되며,이 사실을 기지국과 장치에게 알려 상호인증이 끝났음을 알리고, 다음 단계에 기지국이 사용할 키를 보내준다.e. 데이터 측면과 제어 측면 키 도출 (Data plane and control plane key derivation)모바일 장치와 기지국은 데이터 측면과 제어 측면 무선 전송 프레임의 암호화/복호화에 사용할 키를 도출한다. 이때 AES 암호화 알고리즘이 사용된다.이상은 4G에서의 인증과 키 생성 과정이었고, 5G에서는 조금 다르다.  먼저 4G에서는 방문 네트워크의 MME가 인증의 결과를 결정했지만, 5G에서는 홈 네트워크 측에서 결정하되, 방문 네트워크 측에서 거절할 수 있다.  5G 네트워크는 위와 같은 AKA 프로토콜을 포함해, 추가로 두개의 인증과 키 합의 프로토콜을 지원한다.          하나는 $AKA’$라고 불리우며, AKA와 비슷하지만, EAP 프로토콜을 이용하며 메시지가 조금 다르다.      나머지 하나는 IOT 환경에서 사용되며 미리 공유된 비밀키 $K_{HSS-M}$가 사용되지 않는다.        5G에서는 공용 키로 IMSI를 암호화해서 보내므로 전송간에 IMSI의 원문이 탈취될 수 없다.더욱 자세한 4G/5G 보안은 [3GPP SAE 2019; Cable Labs 2019; Cichonski 2017]에서 확인바란다.AES(Advanced Encryption Standard) 대칭키 암호화DES를 대체하는 대칭키 알고리즘 정확히는 Rijindael 알고리즘 중 블록 크기가 128 비트인 알고리즘.DES에 비해 가변 길이의 블록과 가변 길이의 키 사용이 가능하며, 속도와 코드 효율성이 뛰어남,페이스텔 구조가 아닌 SPN(Substitution - Permutation Network) 구조를 이용대칭키 암호화에서 가장 널리 쓰이고 있음8.9 운영 보안: 방화벽과 침입 감지 체계(Operational Security: Firewalls and Intrusion Detection Systems)현대에서는 네트워크 관리자가 네트워크의 패킷을 감시, 허용, 차단하고, 보안을 체크하고, 포워드하기 위해 방화벽(firewalls), IDS(intrusion detection systems), IPS(intrusion prevention systems)를 사용한다.8.9.1 방화벽 (Firewalls)방화벽(Firewalls)은 조직 내부망을 인터넷으로 부터 격리하고 일부 패킷의 출입을 차단하는 하드웨어와 소프트웨어의 조합이다.방화벽은 네트워크 관리자로 하여금 트래픽 조절을 통해 외부 세계와 자원의 상호 접근을 조정하게 해준다. 다음은 방화벽의 목적이다.      모든 트래픽은 방화벽을 통해서만 출입(All traffic from outside to inside, and vice versa, passes through the firewall)    figure 8.34는 관리 네트워크와 인터넷 사이에 위치한 방화벽에 대한 묘사이다. 거대 조직은 묘사와 달리 다단계 방화벽이나 분산 방화벽을 이용하며, 이럴 경우 묘사보다 더욱 관리가 힘들고, 보안 접근 정책 준수가 힘들어진다.        로컬 보안 정책에 따라 허용된 트래픽만 출입(Only authorized traffic, as defined by the local security policy, will be allowed to pass)    모든 트래픽 출입은 방화벽이 허용한 경우에만 가능하다.        방화벽은 침입에 면역이어야 함(The firewall itself is immune to penetration)    만약 방화벽이 공격 당해 공격자의 입맛대로 트래픽이 조절된다면, 방화벽은 없으니만 못하게 되므로, 보안에 각별한 주의가 필요하다.  Cisco, Check Point 같은 상용 방화벽도 있지만, 오픈소스 Linux 계열 소프트웨어 Linux box의 iptable을 손보는 것도 가능하고, 또는 최신 라우터들에 포함되어있는 기능이나 SDN을 통해 원격으로 트래픽을 조종하는 것도 가능하다.방화벽은 크게 전통 패킷 필터(Traditional Packet Filters), 상태 분석형 필터(Stateful Packet Filters), 응용 계층 게이트웨이(Application Gateway)로 나뉜다.전통적인 패킷 필터 (Traditional Packet Filters)라우터로 ISP, 더나아가 인터넷과 조직 내부망이 연결되어 있는 형태라면, 해당 라우터에 패킷 필터링을 이용해 방화벽을 만들 수 있다.패킷 필터는 각 데이터그램을 조사하여 관리자가 정한 규칙에 따라 통과와 차단을 결정한다.보통 필터링의 결정 요소로는 다음이 존재한다.  출발지, 도착지 IP 주소  데이터그램의 Protocol type 필드(TCP, UDP, ICMP, OSPF 등)  TCP, UDP 출발지, 도착지 포트번호  TCP flag bit 필드: SYN, ACK 등  ICMP 메시지 type  출입 라우터 인터페이스, 출입 네트워크            정책      방화벽 설정                  외부 웹 접근 금지      포트번호 80번인 외부로 나가는 패킷 막기              조직의 웹서버를 제외한 TCP 연결 금지      특정 IP 주소, 포트 80번인 IP를 제외하고 모든 TCP SYN 패킷 막기              대역폭을 잡아먹는 웹 라디오 금지      DNS를 제외한 모든 UDP 패킷 막기              DoS 공격 방지      브로드캐스트 주소의 ICMP 핑 패킷 막기              traceroute의 핑 금지      나가는 ICMP TTL expired 패킷 막기      Table 8.5는 조직내 정책과 그에 따른 방화벽 설정의 예이다.예를 들어 외부 인원이 우리 측에 TCP 연결을 걸어오는 것은 막고, 우리가 외부에 TCP 연결을 거는 것은 허용하고 싶다면, TCP의 첫 연결 요청의 경우는 TCP ㅁ차 bit가 0이고, 이후로의 모든 TCP 세그먼트는 TCP ACK가 1이므로, TCP ACK bit가 0인 세그먼트가 들어오는 것을 막으면 된다.Table 8.6은 조직을 위한 라우터의 접근 조정 리스트 예시이다.기본적으로 맨 아래에 deny 항목에 all이 적혀있으므로, 일반적인 상황에서는 접근을 막고, 예외 사항을 두어 푸는 방식으로 되어있다.위의 두 규칙을 통해 외부에서 들어오는 TCP 연결은 막돼, 우리가 외부로 보내는 TCP 연결은 가능해지며, 3번째 4번째 규칙을 통해 DNS 패킷은 통과할 수 있게 되었다.위와 같은 방호벽 설정 규칙은 라우터의 접근 조정 리스트로 구현되며, 각 라우터 인터페이스는 각기 다른 리스트를 가지고 있다.이러한 접근 제어 리스트는 외부 ISP와 연결되어있는 라우터 인터페이스를 위한 것이며, 지나가는 모든 데이터그램 마다 적용된다.[CERT Filtering 2012]에서는 알려진 보안 취약점을 대비하는 추천하는 포트, 프로토콜 패킷 필터링 규칙을 알려주고 있다.이러한 접근 조정 리스트는 일반화된 포워딩(generalized forwarding) 규칙과 비슷하다.상태 분석형 패킷 필터(Stateful Packet Filters)상태 분석형 패킷 필터는 패킷 별로 규칙을 적용하는 것이 아니라, TCP 연결을 추적하여 필터링한다.예를 들어 ACK 비트가 1인 TCP 패킷을 허용하게 된다면, Dos 공격, 내부 네트워크 맵핑 시도, 기형 패킷으로 인한 내부 시스템 공격 등을 받을 수 있으며, 그렇다고해서 이를 막아버리면, 내부 사용자가 외부에 인터넷 웹 서핑하는 것을 막지못한다.방화벽은 TCP의 three-way 핸드셰이크(SYN, SYNACK, ACK)를 감지하여 연결의 시작에 추적을 시작하고, FIN 패킷이나 오랜 시간 동안 패킷의 주고받음이 없으면 연결 종료로 감지하여 추적을 멈출 수 있다.Table 8.7은 방화벽의 연결 테이블의 예시이다. 세션 테이블(session table)이라고도 불리운다.연결 테이블은 내부에서 외부로 TCP 연결을 진행하기 위해 SYN 세그먼트를 보내면 감지되어 추가된다.위의 연결 테이블을 보아, 현재 3개의 TCP 연결이 진행중이며, 내부에서 외부로 연결된 TCP 연결임을 알 수 있다.Table 8.8은 상태 분석형 패킷 필터의 접근 조정 리스트로, “check connection” 필드가 추가되어 이를 통해 해당 규칙을 적용하기 전에 먼저 연결 테이블을 확인해야 하는지 표시할 수 있다.만약 표시가 되어있다면, 연결 테이블을 확인해 연결이 존재한다면 해당 규칙을 적용해준다.예를 들어 Table 8.8의 경우 두번째 규칙이 표시되어 있기 때문에, 외부에서 뜬금없이 보내오는(보통 악의를 담은) ACK 세그먼트는 allow에 포함되더라도 연결이 없으므로 막히며,  내부 인원이 외부 인터넷으로 웹서핑하다가 답신으로 오는 ACK 세그먼트는 연결 테이블에 추가되어있을 것이므로 허용될 것이다.응용 계층 게이트웨이 (Application Gateway)만약 IP 주소나 포트 번호, 기타 패킷의 정보가 아닌, 보낸 인력의 직책, 권한, 또는 비밀번호 등으로 트래픽을 제한하고 싶다면 어떻게 할까?이럴때는 방화벽이 패킷 필터와 응용 프로그램 게이트웨이 기능을 둘다 제공되는 응용 계층 게이트웨이를 사용해야 한다.응용 계층 게이트웨이는 응용 프로그램 사양 서버로, 모든 출입 응용 계층 데이터가 출입한다.예를 들어, 내부에서 일부 인원은 외부로 텔넷 요청을 보낼 수 있고, 반대로 외부의 모든 인원들은 내부에 텔넷 요청을 보낼수 없게 하고 싶다면 figure 8.35처럼 라우터의 패킷 필터와 텔넷 응용 계층 게이트웨이를 함께 사용하여 구현할 수 있다.응용 계층 게이트웨이를 통하지 않는 모든 텔넷 연결은 라우터 필터 측에서 막도록 설정하면, 모든 외부로 향하는 텔넷 연결은 응용 계층 게이트웨이를 지나가야만 한다.이때 응용 계층 게이트웨이에서는 그렇게 들어오는 텔넷 연결들에게 유저 ID와 비밀번호를 요구할 수 있고, 이에 통과하는 경우에 연결할 호스트명을 받아 대신 telnet 연결을 생성해 입력값과 응답 값을 중계할 수 있다. 이때 라우터는 연결에 성공한 텔넷 연결에 대해 추가적인 필터링을 한다.사내망은 또한, HTTP, FTP, email, Web cache 등 여러 응용 계층 게이트웨이를 두는 것이 일반적이며, 각기 다른 응용 프로그램마다 게이트웨이를 둬야 된다는 점과 응용 프로그램 기능들이 게이트웨이를 중계하여 지나가야하기 때문에 성능상의 이슈가 있을 수 있으며, 사내망 외부와 내부 사용자들은 응용 프로그램 사용을 위해 응용 계층 게이트웨이와 통신하는 방법을 숙지해야 한다는 단점들이 존재한다.익명성과 프라이버시만약 사용자가 내가 방문하는 웹사이트에게 나의 IP를 알리고 싶지 않고, 나의 ISP에게 내가 방문한 웹사이트와 데이터 교환 내용을 알리고 싶지않으면, 즉 익명성과 프라이버시를 지키고 싶으면 어떻게 해야할까? SSL 같은 강력한 기밀성을 제공하는 프로토콜을 쓴다고 해도, 출발지 IP 주소와 도착지 IP 주소는 남게 되어 ISP 측에서 확인할 수 있다.익명성과 프라이버시를 해결하기 위해, 믿음직한 프록시 서버 서비스와 SSL을 이용하면 된다.figure 8.36은 프록시 서버의 사용 예시이다.먼저 프록시 서버와 SSL 연결을 맺고, 통신하고 싶은 URL의 HTTP 요청 메시지를 암호화해 프록시 서버로 보내면, 프록시 서버는 SSL 패킷에서 HTTP 요청 메시지를 복호화해 추출하고, 원문의 HTTP 요청을 웹사이트로 보내준다.웹사이트의 응답은 프록시 서버로 가게되고, 이후 프록시 서버는 해당 메시지를 SSL을 통해 다시 나에게 되돌려 준다.ISP나 웹사이트 입장에서는 웹사이트는 프록시 서버와 통신하였고, 당신은 프록시 서버와 통신한 것이므로, 프록시를 이용한 사람이 혼자가 아닌 이상, 누가 어느 웹사이트에 들려 어떤 메시지를 주고 받았는지 알 수 없다. 또한, 프록시와 웹사이트가 SSL 같은 보안 통신을 지원한다면, 대화내용 조차 아무도 알 수 없다. 이렇나 프록시 서버 서비스는 상용으로도 많이 나와있다.단, 내가 고른 프록시 서버 측에서는 출발지와 도착지 IP 주소, 주고 받은 패킷이 모두 알 수 있으므로, 만약 믿음직하지 못한 프록시 서버라면 정반대의 결과가 나올 수 있다.이를 해결하기 위해서는 TOR 서비스를 이용할 수 있다.TOR는 비공모식 프록시 서버를 이용해 트래픽을 숨기는데, 자세한 방법은 다음과 같다.TOR는 각각 독립적인 개인이 프록시 서버를 프록시 풀에 제공할 수 있게 되어있는데, 유저가 TOR을 사용하면, TOR은 프록시 풀에서 무작위의 3개 서버를 가져와    연쇄적으로 3개의 서버를 경유해서 다른 서버에 접근하게 된다.이로 인해, 각각의 무작위로 골라진 프록시 서버들은 앞, 뒤, 또는 양쪽에 다른 프록시 서버가 위치하므로 패킷을 주고받은 사용자, 패킷을 주고받은 서버, 이 둘 중에 최소 한가지는 모르게 된다.무작위로 골라졌으므로, 특정 사용자를 노려 데이터를 도청하는 것은 불가능하고, 도청한 데이터는 누가 보냈는지, 또는 누구에게 보냈는지 모르는 불완전한 데이터가 된다.8.9.2 침입자 감지 체계(Intrusion Detection Systems)방화벽만으로는 공격을 완벽히 막기 힘들고, 추가적으로 헤더 필드와 패킷의 페이로드까지 검사하는 심층 패킷 분석(deep packet inspection)이 필요하다.일부 응용 계층 게이트웨이가 심층 패킷 분석을 실시하나, 일부 응용 프로그램에 한해서이며, 패킷의 헤더, 심층 패킷 분석을 하고, 이상 징후가 발견되면 패킷들의 사설망 진입을 차단하거나 네트워크 관리자에게 경고를 줄 수 있는 장치인 IDS(intrusion detection system, 칩입 감지 체계)를 사용하면 좋다.만약 아예 의심스러운 패킷을 필터링해버리면 IPS(intrusion prevention system, 침입 방지 체계)라고 부르기도 한다.IDS는 네트워크 매핑, 포트 탐지, TCP 스택 탐지, DoS 대역폭 범람 공격, 웜과 바이러스, 운영체제 취약점 공격, 응용 프로그램 취약점 공격 등 수많은 범위의 공격을 막을 수 있으며, 오늘날에는 수많은 조직들이 IDS 체계를 갖추고 있다.몇몇은 Cisco, Check Point 같은 상용 IDS를, 몇몇은 Snort 같은 오픈소스 IDS를 이용하기도 한다.조직은 Figure 8.37처럼 조직망에 하나 이상의 IDS 센서를 놓기도 한다. 다수의 IDS 센서들은 의심스러운 트래픽 활동에 대한 정보를 모아 중앙 IDS 처리장치에 보내고, 중앙 IDS 처리장치는 이들을 취합하여 네트워크 관리자에게 경고하고 조치를 취하게 만든다.IDS를 방화벽 뒤나, 방화벽의 기능으로 놓고 하나만 사용하지 않는 이유는, IDS가 심층 패킷 분석 이외에도 패킷의 시그너처를 비교하는데, 이 과정이 아주 오래 걸리는 과정이므로, 각 IDS는 지나가는 패킷의 일부분만 검색하고 다른 부분은 다른 IDS가 처리한다. 최근의 고성능 IDS들은 입구에 하나만 놓기도 한다.Figure 8.37에서 조직은 네트워크 내부를 패킷 필터 라우터, 응용 계층 게이트웨이, IDS 센서 등으로 무장한 높은 보안 지역과 외부 인터넷과의 통신이 잦은 DNS, 웹서버 등이 존재하는 낮은 보안 지역인 DMZ(demilitarized zone)으로 나누곤 한다. DMZ의 보안은 보통 패킷 필터(방화벽)과 IDS 센서의 감시 정도이다.IDS는 크게 시그너쳐 기반 체계(signature-based systems)와 이상현상 기반 체계(anomaly based systems)로 나뉜다.시그너처 기반 IDS는 공격 패킷들의 시그너쳐를 데이터베이스에 저장해 놓고, 이를 지나가는 패킷들과 비교해 관리자 측에 경고하거나 기록한다. 각 시그너처는 일종의 공격 활동과 관련된 패킷의 패턴들이며, 이는 한 패킷의 특성(헤더 필드 값, 페이로드의 패턴 등)이거나 여러 패킷들의 연속일 수도 있다. 이러한 시그너처는 관리자에 의해 수정되거나 추가될 수도 있다.다만, 시그너처 기반 IDS에게 단점들이 있는데, 과거에 경험했던 공격만 막을 수 있고, 새로 생긴 공격이나 처음 보는 공격은 막지 못한다는 점, 시그너쳐가 일치하여도 공격과 관련이 없는 정상적인 패킷은 경우에는 거짓 알람이 된다는 점, 시그너처를 비교하는 과정이 너무 느리기 때문에 가끔 IDS가 일을 전부 처리하지 못하고 공격 패킷을 허용하거나 먹통이 될수 있다는 점이다.이상현상 기반 IDS는 평상시에는 트래픽에 대한 프로필을 작성하다가 비정상적인 패킷 흐름 통계를 감지하는 방식이다. 예를 들어 비정상적으로 ICMP 패킷이 많아지거나 포트 스캔, 핑 메시지가 많이 들어오게 된다면, 이상 현상 기반 IDS가 이를 경고하게 된다. 이러한 방식이므로, 처음보는 공격, 새로운 공격에도 대처할 수 있다는 장점이 있다. 단점으로는 이러한 이상 현상과 정상 상태를 구분하기 힘들다는 점이다.최근의 대부분의 IDS는 시그너처 기반이며, 일부는 이상 현상 탐지 기능도 포함하는 경우도 있다.스노트 (Snort)스노트는 널리 사용되고 있는 Linux, UNIX, Window에서 사용가능한 오픈소스 IDS로, 패킷 스니핑 인터페이스인 libpcap을 이용해 100Mbps 정도의 패킷을 탐지하며, 이 이상은 여러 스노트 센서를 사용해서 늘려야 한다.다음은 스노트 시그너처의 예시이다.alert icmp $EXTERNAL_NET any -&amp;#38;#62; $HOME_NET any(msg:&amp;#34;ICMP PING NMAP&amp;#34;; dsize: 0; itype: 8;)이는 외부($EXTERNAL_NET)에서 조직망($HOME_NET)에 들어오는 type(itype) 필드가 8(ICMP), 페이로드 크기(dsize)가 0인 모든 ICMP가 감지되면 “ICMP PING NMAP”이라는 메시지로 경고하게끔 설정한 구문이다. 이를 통해 nmap이 보내는 nmap 핑 스윕 공격에 대해 경고를 받을 수 있을 것이다.스노트의 장점은 거대한 커뮤니티로 수많은 보안 전문가들이 시그너처 데이터베이스를 유지, 갱신하고 있다. 또한 추가로 스노트 시그너처 구문을 이용해 네트워크 관리자가 조직의 시그너처 데이터베이스에 커스텀 시그너처를 추가하거나 기존의 것을 수정하여 알맞게 활용할 수 있다."
  }
  , 
  
  "/articles/computer_science/algorithm/Python%20for%20Algorithms-libraries.html": {
    title: "Python for algorithms-Libraries",
    date: " Sep 19, 2022 ",
    url: "/articles/computer_science/algorithm/Python%20for%20Algorithms-libraries.html",
    tags: ["알고리즘","PYTHON"],
    content: "Python for Algorithms-Librariesmin_depth: 2max_depth: 3varied_style: true간편하게 사용할 수 있는 파이썬 코드들Template.py  시간 비교, 테스트 케이스 입력과 출력, 정답 비교 등을 한 파일에 담은 template.title: Template.py#!/usr/bin/env python3# -*- coding: utf-8 -*-{: #coding-utf-8}# ----------------------------------------------------------------------------{: #}# Created By  : Junseok Yun(markkorea@naver.com){: #created-by- -junseok-yun-markkorea-naver-com}# Created Date: 2022-08-10{: #created-date-2022-08-10}# ---------------------------------------------------------------------------{: #}\"\"\" Python template file for algorithm competition. \"\"\"# ---------------------------------------------------------------------------{: #}import sysimport time# input = sys.stdin.readline {: #input-sys-stdin-readline}# Change `input` func to `readline` func.# print(sys.getrecursionlimit()) {: #print-sys-getrecursionlimit}# Print maximum recursion limit.sys.setrecursionlimit(987654321)   # Change maximum recursion limit.arrayOfInput = [    # 입력값 리스트]arrayOfAnswer = [    # 정답 리스트]if len(arrayOfInput) != len(arrayOfAnswer):    raise Exception(        f\"Error: (len(arrayOfInput)={len(arrayOfInput)}) != (len(arrayOfAnswer)={len(arrayOfAnswer)})\")  def solution(    # your arguments):    result = []    return result  print(\"---------------------------------------------------------------------------------------------------------------------------------------\")print(\"|case|               Input               |                correct               |              submitted              | result|time(ms)|\")print(\"=======================================================================================================================================\")  for case, (example_input, correct_answer) in enumerate(zip(arrayOfInput, arrayOfAnswer)):    startTime = time.time() * 1000    answer = solution(*example_input)    responseTime = time.time() * 1000 - startTime    print(f\"|{case:^4}|{str(example_input):^35.35}|{str(correct_answer):^37.37}|{str(answer):^37.37}|{'OOO' if answer == correct_answer else 'XXX':*^7}|{responseTime:^8.3f}|\")    print(\"---------------------------------------------------------------------------------------------------------------------------------------\")functools고차 함수와 콜러블 객체에 대한 연산, 즉 함수를 입력 값 혹은 출력 값으로 주로 다루는 함수 들이다.어렵게 구현해야 할 코드들이 쉽게 쓸 수 있게 되어 있다.만약 표준 라이브러리를 사용할 수 없는 시험에 대비하여  해당 구현 코드들을 올려놓았다.cmp_to_key()sorted, sort 함수를 이용해 정렬 시 비교 기준을 줄 수 있음title:cmp_to_key()의 활용 예시collapse: truefrom functools import cmp_to_keypoints = [{'x': 1, 'y': 2}, {'x': 2, 'y': 2},          {'x': 2, 'y': 1}, {'x': 1, 'y': 1}]          # 단순 비교 설정은 lambda를 이용해 가능{: #단순-비교-설정은-lambda를-이용해-가능}points.sort(key=lambda x: x['x']) def cmp(a, b):\t# 필요 리턴 값\t# 1 : 좌측이 더 큼\t# 0 : 두 값이 같음\t# -1 : 우측이 더 큼\tif a['y'] == b['y']:        if a['x'] &gt; b['x']:\t\t\treturn 1\t\telif a['x'] == b['x']:\t\t\treturn 0\t\telif a['x'] &lt; b['x']:\t\t\treturn -1\treturn 1 if a['y'] &gt; b['y'] else -1points.sort(key=cmp_to_key(cmp))sorted(points, key=cmp_to_key(cmp), reverse=True)print(points)# [{'x': 1, 'y': 1}, {'x': 2, 'y': 1}, {'x': 1, 'y': 2}, {'x': 2, 'y': 2}] 'y': 2}]{: #x-1-y-1-x-2-y-1-x-1-y-2-x-2-y-2-y-2}title: cmp_to_key()의 구현def cmp_to_key(mycmp):    'Convert a cmp= function into a key= function'    class K(object):        def __init__(self, obj, *args):            self.obj = obj        def __lt__(self, other):            return mycmp(self.obj, other.obj) &lt; 0        def __gt__(self, other):            return mycmp(self.obj, other.obj) &gt; 0        def __eq__(self, other):            return mycmp(self.obj, other.obj) == 0        def __le__(self, other):            return mycmp(self.obj, other.obj) &lt;= 0        def __ge__(self, other):            return mycmp(self.obj, other.obj) &gt;= 0        def __ne__(self, other):            return mycmp(self.obj, other.obj) != 0    return K  points = [{'x': 1, 'y': 2}, {'x': 2, 'y': 2},          {'x': 2, 'y': 1}, {'x': 1, 'y': 1}]def cmp(a, b):    if a['y'] == b['y']:        if a['x'] &gt; b['x']:            return 1        elif a['x'] == b['x']:            return 0        elif a['x'] &lt; b['x']:            return -1    return 1 if a['y'] &gt; b['y'] else -1  points.sort(key=cmp_to_key(cmp))print(points)title: 사용해본 알고리즘 문제백준 11650번 문제  다른 사람은 x와 y값의 합을 통해 정렬하되, y에 가중치를 적게 주어 (x의 최대값으로 나눈 수준의 가중치) 구별하는 방법으로 빠르게 정렬했다.reduce()자료 구조 내의 각 요소들을 순서대로 함수를 적용해 누적된 값을 보여준다.  백준 문제 1010번title: accumulate 함수와 달리 결과값이 iterator가 아닌 값이다.from itertools import accumulatefrom operator import addprint(list(accumulate([1, 2, 3, 4, 5], add))) # [1, 3, 6, 10, 15]# iterator로 나오므로 list 함수를 통해 list로 바꿈{: #iterator로-나오므로-list-함수를-통해-list로-바꿈}# add 함수를 만드는 대신 operator로 이용 가능{: #add-함수를-만드는-대신-operator로-이용-가능}title: reduce()의 활용 예시from functools import reduceprint(reduce(lambda accumulated, element: accumulated + element, [1, 2, 3, 4, 5])) #15 title: reduce()와 accumulate()의 구현def reduce(function, iterable, initializer=None):    it = iter(iterable)    if initializer is None:        value = next(it)    else:        value = initializer    for element in it:        value = function(value, element)    return valuedef accumulate(iterable, func=operator.add, *, initial=None):    it = iter(iterable)    total = initial    if initial is None:        try:            total = next(it)        except StopIteration:            return    yield total    for element in it:        total = func(total, element)        yield total@cache함수 위에 데코레이터로 이용하여 손쉽게 Memoization을 구현할 수 있다.title: 최신 버전(3.9++) 파이썬의 함수이므로 지원하지 않는 알고리즘 시험도 많다.title: @cache 데코레이터의 활용 예시from functools import cacherecursiveCall = 0@cachedef factorial(n):    global recursiveCall    recursiveCall = recursiveCall + 1  # 재귀함수가 실행될 때마다 증가    return n * factorial(n-1) if n else 1print(\"value: \", factorial(10))  # 3628800print(\"recursive called : \", recursiveCall)  # 11recursiveCall = 0print(\"value: \", factorial(5))  # 120print(\"recursive called : \", recursiveCall)  # 0# 앞서 실행한 결과가 저장되어 바로 값을 가져옴  {: #앞서-실행한-결과가-저장되어-바로-값을-가져옴}recursiveCall = 0print(\"value: \", factorial(12))  # 479001600print(\"recursive called : \", recursiveCall)  # 2# 앞서 실행한 결과에 추가로 두번만 더 실행해 값을 가져옴{: #앞서-실행한-결과에-추가로-두번만-더-실행해-값을-가져옴}itertools효율적인 루핑을 위한 이터레이터를 만드는 함수cycles()같은 값을 무한히 반복하여 돌려주는 iterator를 생성title: cycles()의 활용 예시from itertools import cycle      for i in cycle(\"ABCD\"):        print(i) # ABCD 무한 반복      dispenser = cycle(\"ABCD\")   peoples = 10   for i in range(peoples):       print(dispenser.__next__())      # A B C D A B C D A B 출력title: cycles()의 구현def cycle(iterable):    saved = []    for element in iterable:        yield element        saved.append(element)    while saved:        for element in saved:            yield elementfor i in cycle(\"ABCD\"):    print(i)  # ABCD 무한 반복compress()True로 판명되는 인덱스의 원소만 추출하여 이터레이터로 생성.data 리스트나 selector 리스트 둘 중에 하나만 소진되어도 중단됨title: compress()의 활용 예시from itertools import compress  a = [1, 2, 3, 4, 5]b = [True, False, False, True, True]# filter 함수 등을 이용해 생성한 selector 리스트{: #filter-함수-등을-이용해-생성한-selector-리스트}print(list(compress(a, b)))  # [1, 4, 5]# selector 리스트에서 true 였던 값들만 출력됨{: #selector-리스트에서-true-였던-값들만-출력됨}title: compress()의 구현def compress(data, selectors):    return (d for d, s in zip(data, selectors) if s)  a = [1, 2, 3, 4, 5]b = [True, False, False, True, True]  print(list(compress(a, b))); # [1, 4, 5]조합과 순열을 위한 함수들product, permutations, combinations, combinations_wtih_replacement() 함수들은 알고리즘 문제로 자주 출제되는 조합과 순열을 생성할 수 있는 함수들이다.title: 조합과 순열 함수들의 활용 예시from itertools import product, permutations, combinations, combinations_with_replacementprint(list(product('ABCD', repeat=2))) # 중첩된 for 루프, 모든 조합과 순열을 생성하게 됨.# [('A', 'A'), ('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('B', 'D'), ('C', 'A'), ('C', 'B'), ('C', 'C'), ('C', 'D'), ('D', 'A'), ('D', 'B'), ('D', 'C'), ('D', 'D')]{: #a-a-a-b-a-c-a-d-b-a-b-b-b-c-b-d-c-a-c-b-c-c-c-d-d-a-d-b-d-c-d-d}print(list(permutations('ABCD', 2))) # 순열 생성, # [('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'A'), ('B', 'C'), ('B', 'D'), ('C', 'A'), ('C', 'B'), ('C', 'D'), ('D', 'A'), ('D', 'B'), ('D', 'C')]{: #a-b-a-c-a-d-b-a-b-c-b-d-c-a-c-b-c-d-d-a-d-b-d-c}print(list(combinations('ABCD', 2))) # 조합 생성# [('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D'), ('C', 'D')]{: #a-b-a-c-a-d-b-c-b-d-c-d}print(list(combinations_with_replacement('ABCD', 2))) # n번 중복 가능한 조합 생성# [('A', 'A'), ('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'B'), ('B', 'C'), ('B', 'D'), ('C', 'C'), ('C', 'D'), ('D', 'D')]{: #a-a-a-b-a-c-a-d-b-b-b-c-b-d-c-c-c-d-d-d}title: 조합, 순열 함수들의 구현def product(*args, repeat=1):    pools = [tuple(pool) for pool in args] * repeat    result = [](){: .wikilink}{:target=\\\"_blank\\\"}    for pool in pools:        result = [x+[y] for x in result for y in pool]    for prod in result:        yield tuple(prod)  def permutations(iterable, r=None):    pool = tuple(iterable)    n = len(pool)    r = n if r is None else r    for indices in product(range(n), repeat=r):        if len(set(indices)) == r:            yield tuple(pool[i] for i in indices)  def permutations_standalone(iterable, r=None):    pool = tuple(iterable)    n = len(pool)    r = n if r is None else r    if r &gt; n:        return    indices = list(range(n))    cycles = list(range(n, n-r, -1))    yield tuple(pool[i] for i in indices[:r])    while n:        for i in reversed(range(r)):            cycles[i] -= 1            if cycles[i] == 0:                indices[i:] = indices[i+1:] + indices[i:i+1]                cycles[i] = n - i            else:                j = cycles[i]                indices[i], indices[-j] = indices[-j], indices[i]                yield tuple(pool[i] for i in indices[:r])                break        else:            return  def combinations(iterable, r):    pool = tuple(iterable)    n = len(pool)    for indices in permutations(range(n), r):        if sorted(indices) == list(indices):            yield tuple(pool[i] for i in indices)  def combinations_standalone(iterable, r):    pool = tuple(iterable)    n = len(pool)    if r &gt; n:        return    indices = list(range(r))    yield tuple(pool[i] for i in indices)    while True:        for i in reversed(range(r)):            if indices[i] != i + n - r:                break        else:            return        indices[i] += 1        for j in range(i+1, r):            indices[j] = indices[j-1] + 1  def combinations_with_replacement(iterable, r):    pool = tuple(iterable)    n = len(pool)    for indices in product(range(n), repeat=r):        if sorted(indices) == list(indices):            yield tuple(pool[i] for i in indices)def combinations_with_replacement_standalone(iterable, r):    pool = tuple(iterable)    n = len(pool)    if not n and r:        return    indices = [0] * r    yield tuple(pool[i] for i in indices)    while True:        for i in reversed(range(r)):            if indices[i] != n - 1:                break        else:            return        indices[i:] = [indices[i] + 1] * (r - i)        yield tuple(pool[i] for i in indices)        print(list(product('ABCD', repeat=2)))print(list(permutations('ABCD', 2))) print(list(combinations('ABCD', 2))) print(list(combinations_with_replacement('ABCD', 2)))bisect이진 탐색을 이용하여 리스트 내의 원소의 위치를 탐색하거나 삽입하는 함수를 가지고 있다.이진 탐색을 직접 구현하지 않고 높은 성능을 낼 수 있다.title: bisect의 활용 예시from bisect import bisect_left, bisect_right, insort_left, insort_right  a = [2, 1, 4, 3]a.sort()  # 배열은 정렬되있어야 한다.  print(bisect_right(a, 3))  # 3 [1, 2, 3, #3번 인덱스 자리#, 4]print(bisect_left(a, 3))  # 2 [1, 2, #2번 인덱스 자리#, 3, 4]a = [1, 2, 3, 4]print(insort_left(a, 3))  # 함수 자체는 None을 리턴 # Noneprint(a)  # 대신 배열에 추가되어 있음 # [1,2,3,3,4]a = [1, 2, 3, 4]print(insort_right(a, 3))  # Noneprint(a)  # [1,2,3,3,4]title: bisect를 이용한 탐색 함수 구현def index(a, x):    'Locate the leftmost value exactly equal to x'    i = bisect_left(a, x)    if i != len(a) and a[i] == x:        return i    raise ValueErrordef find_lt(a, x):    'Find rightmost value less than x'    i = bisect_left(a, x)    if i:        return a[i-1]    raise ValueErrordef find_le(a, x):    'Find rightmost value less than or equal to x'    i = bisect_right(a, x)    if i:        return a[i-1]    raise ValueErrordef find_gt(a, x):    'Find leftmost value greater than x'    i = bisect_right(a, x)    if i != len(a):        return a[i]    raise ValueErrordef find_ge(a, x):    'Find leftmost item greater than or equal to x'    i = bisect_left(a, x)    if i != len(a):        return a[i]    raise ValueErrortitle: bisect 함수들의 구현def insort_right(a, x, lo=0, hi=None, *, key=None):    if key is None:        lo = bisect_right(a, x, lo, hi)    else:        lo = bisect_right(a, key(x), lo, hi, key=key)    a.insert(lo, x)  def bisect_right(a, x, lo=0, hi=None, *, key=None):    if lo &lt; 0:        raise ValueError('lo must be non-negative')    if hi is None:        hi = len(a)    if key is None:        while lo &lt; hi:            mid = (lo + hi) // 2            if x &lt; a[mid]:                hi = mid            else:                lo = mid + 1    else:        while lo &lt; hi:            mid = (lo + hi) // 2            if x &lt; key(a[mid]):                hi = mid            else:                lo = mid + 1    return lo  def insort_left(a, x, lo=0, hi=None, *, key=None):    if key is None:        lo = bisect_left(a, x, lo, hi)    else:        lo = bisect_left(a, key(x), lo, hi, key=key)    a.insert(lo, x) def bisect_left(a, x, lo=0, hi=None, *, key=None):    if lo &lt; 0:        raise ValueError('lo must be non-negative')    if hi is None:        hi = len(a)    if key is None:        while lo &lt; hi:            mid = (lo + hi) // 2            if a[mid] &lt; x:                lo = mid + 1            else:                hi = mid    else:        while lo &lt; hi:            mid = (lo + hi) // 2            if key(a[mid]) &lt; x:                lo = mid + 1            else:                hi = mid    return loCollections파이썬의 자료 구조의 특수한 형태의 대안들defaultdict초기 값을 설정할 수 있는 dictionarytitle: defaultdict의 활용 예시from collections import defaultdict  int_dict = defaultdict(int)print(int_dict['no_value'])  # 0int_dict = defaultdict(lambda: 1)  # 기본값 지정print(int_dict['no_value2'])  # 1print(int_dict['str_value'])int_dict['str_value'] = \"str\"print(int_dict) # 물론 다른 자료형으로 지정 또한 가능OrderedDictdictionary의 키,값 페어에 추가로 순서를 적용한 자료구조title: OrderedDict의 활용 예시from collections import OrderedDictordered_dict = OrderedDict()ordered_dict[\"1\"] = \"a\"ordered_dict[2] = 2ordered_dict[\"three\"] = \"3\"print(ordered_dict) # OrderedDict([('1', 'a'), (2, 2), ('three', '3')]print(ordered_dict.popitem()) # 맨 뒤의 키 값을 pop # ('three', '3')print(ordered_dict) # OrderedDict([('1', 'a'), (2, 2)])ordered_dict.move_to_end('1') # 키 1의 값을 맨 뒤로print(ordered_dict) # OrderedDict([(2, 2), ('1', 'a')])print(list(ordered_dict.items())[0]) # 첫번째 값 가져오기 # (2, 2)namedtuple튜플 내부에 내부에 키 값의 서브 클래스를 생성하여 접근 가능title: namedtuple의 활용 예시from collections import namedtuple  Point = namedtuple('Point', ['x', 'y'])p = Point(11, y=22)  # instantiate with positional or keyword argumentsprint(p[0] + p[1])  # indexing like plain tuple (11, 22)# 33{: #33}x, y = p  # unpack like a regular tupleprint(x, y)# (11, 22){: #11-22}print(p.x + p.y)  # fields also accessible by name# 33{: #33}print(p)ChainMap두 dictionary를 빠르게 합치는데 사용title: ChainMap의 활용 예시from collections import ChainMap, OrderedDictdict1 = {\"x\": 1, \"2\": \"a\"}dict2 = {\"y\": 3, \"three\": \"b\"}dict3 = OrderedDict({\"z\": 4, \"1\": \"one\"})  chained = ChainMap(dict1, dict2, dict3)  print(chained) # ChainMap({'x': 1, '2': 'a'}, {'y': 3, 'three': 'b'}, OrderedDict([('z', 4), ('1', 'one')]))print(chained['2']) # aprint(chained['y']) # 3print(chained['1'])# one # 각기 다른 키들에 한꺼번에 접근 가능중요하고 자주 사용되는 다른 자료 구조들단순히 편리한 수준이 아닌 핵심적으로 사용되는 자료 구조들deque스택 + 큐 기능이 합쳐진 리스트기본 리스트와 달리 pop()과 insert() 연산을 상수 시간 만에 끝낼 수 있다.title: deque의 활용 예시from collections import dequed = deque('12345')print(d)  # deque(['1', '2', '3', '4', '5'])  d.append('6')d.appendleft('0')print(d)  # deque(['0', '1', '2', '3', '4', '5', '6'])  print(d.pop())  # 6print(d)  # deque(['0', '1', '2', '3', '4', '5'])print(d.popleft())  # 0print(d)  # deque(['1', '2', '3', '4', '5']) d.extend('abc')print(d)  # deque(['1', '2', '3', '4', '5', 'a', 'b', 'c'])d.extendleft('efg')print(d)  # deque(['g', 'f', 'e', '1', '2', '3', '4', '5', 'a', 'b', 'c'])d.rotate(1)print(d)  # deque(['c', 'g', 'f', 'e', '1', '2', '3', '4', '5', 'a', 'b'])d.rotate(-1)print(d)  # deque(['g', 'f', 'e', '1', '2', '3', '4', '5', 'a', 'b', 'c'])  # d[0] : 최좌측 peek{: #d-0-최좌측-peek}# d[-1] : 최우측 peek{: #d-1-최우측-peek}heapq이진 트리를 이용해  최소 힙을 구현, 최대 힙을 구현 하기 위해서는 값에 -을 붙여 넣어줘야 한다.title: heapq의 활용 예시import heapqa = [2, 4, 6]# heap에 삽입{: #heap에-삽입}heapq.heappush(a, 3)print(a)  # [2, 3, 6, 4]  # 최소값 pop{: #최소값-pop}print(heapq.heappop(a))  # 2print(a)  # [3, 4, 6]  b = [3, 1, 6]heapq.heapify(b) # b를 힙을 이용해 정렬print(b)  # [1, 3, 6]title: heapq 키값 비교 방법 바꾸기  출처 hereheapq를 이용하여 우선순위 큐 등을 만들 시 아래와 같이 __lt__ 메소드를 오버라이딩하여 키값의 비교 방법을 정해줄 수 있다.import heapq  class Node(object):    def __init__(self, val: int):        self.val = val     def __repr__(self):        return f'Node value: {self.val}'      def __lt__(self, other):        return self.val &lt; other.val  heap = [Node(2), Node(0), Node(1), Node(4), Node(2)]heapq.heapify(heap)# output: [Node value: 0, Node value: 2, Node value: 1, Node value: 4, Node value: 2]{: #output-node-value-0-node-value-2-node-value-1-node-value-4-node-value-2}print(heap) heapq.heappop(heap)# output: [Node value: 1, Node value: 2, Node value: 2, Node value: 4]{: #output-node-value-1-node-value-2-node-value-2-node-value-4}print(heap)재귀함수재귀함수 문제"
  }
  , 
  
  "/articles/computer_science/algorithm/Python%20for%20Algorithms-Graph.html": {
    title: "Python for Algorithms-Graph",
    date: " Sep 22, 2022 ",
    url: "/articles/computer_science/algorithm/Python%20for%20Algorithms-Graph.html",
    tags: ["알고리즘","PYTHON"],
    content: "min_depth: 2max_depth: 3varied_style: truePython for Algorithms-Graph노드와 그래프, 간선에 대한 탐색과 자료구조그래프 순회BFS와 DFS 둘 다 $O(n^2)$의 시간 복잡도를 가진다.BFS(너비 우선 탐색)title: BFS 파이썬 코드 예시def BFS(node_num, start, graph):     queue = deque([start]) # 시작 점을 큐에 삽입    visited = [False for node in range(node_num)]     while queue: # 큐가 빌때 까지        node = queue.popleft() # 큐의 첫 번째 원소 반환        if not visited[node]: #             visited[node] = True # 방문 표시            do_something(node) # 원하는 연산 수행(탐색, 변경 등)        for i, accessible in enumerate(graph[node]): # 모든 노드 중            if accessible and not visited[i]: # 연결됬지만 아직 방문하지 않은 노드에 대하여                                visited[i] = True # 방문 표시                queue.append(i) # 큐에 넣기DFS(깊이 우선 탐색)title: DFS 파이썬 코드 예시def DFS(node_num, start, graph):    stack = deque([start])    visited = [False for node in range(node_num)]    while stack:        node = stack.pop()        if not visited[node]:            visited[node] = True            do_something(node)        for i, accessible in enumerate(graph[node]):            if accessible and not visited[i]:                stack.append(i)그래프 최단 경로다익스트라 알고리즘한 정점에서 다른 모든 정점까지 비용을 측정하는 알고리즘title: 조금 변형하여 최단 경로 또한 알 수 있다.(아래 코드 참조)title: 다익스트라 알고리즘 예시 $O(NlogM + M)$import heapqINF = 987654321class Edge(object):    def __init__(self, index, weight) -&gt; None:        self.index = index        self.weight = weight      def __repr__(self) -&gt; str:        return f\"Node:{self.index} Weight:{self.weight}\"      def __lt__(self, outer):        return self.weight &lt; outer.weight  # D: 출발점에서 각 정점까지의 최단 경로 가중치 합을 저장{: #d-출발점에서-각-정점까지의-최단-경로-가중치-합을-저장}# P: 최단 경로 트리 저장{: #p-최단-경로-트리-저장}def Dijkstra(G, r):    # G: 그래프, r: 시작 정점, N: 노드의 수    N = len(G)    D = [INF]*N        # 1 시작점부터 각 노드로 가능 최단 거리를 무한대로 설정    P = [None]*N    # 2 시작지점 부터 해당지점까지의 최단 거리를 구할 수 있게 끔 도와주는 배열    visited = [False] * N    # 3 방문 여부를 모두 False로 설정    D[r] = 0        # 4 시작 지점부터 시작 지점까지의 거리 0으로 설정    min_heap = [Edge(r, 0)]  # 5 시작 지점 설정    while min_heap:  # 6 더이상 갱신할 노드가 없을때 까지 실행        # 최소 거리 노드로 설정        min_node = heapq.heappop(min_heap)  # 7 힙을 통해 새로 갱신된 최소 거리 노드 값 구함        visited[min_node.index] = True  # 8 이제 최소거리를 구할 것이므로 visited 설정        for node in G[min_node.index]:  # 9 해당 최소 거리 노드와 연결된 노드들에 대해            # 10 해당 노드의 기존의 최소 거리보다 더 낮은 비용으로 도달할 수 있다면            if not visited[node.index] and D[min_node.index] + node.weight &lt; D[node.index]:                D[node.index] = D[min_node.index] + node.weight  # 11 새로운 최소 거리 갱신                heapq.heappush(min_heap, node) # 12 갱신된 정보를 통해 다른 노드를 갱신하기 위해 힙에 저장                # 13 node의 부모 노드(한번에 한해 어디 노드로부터 이 노드로 오는것이 최단경로인가?) 저장                P[node.index] = min_node.index                  # P 집합을 역순은 시작정점까지 가는 경로가 시작 노드 -&gt; 해당 노드로 가능 최단 거리    return D, P  # 14 더이상 갱신할 것이 없는 경우 구한 최소 거리와 최소 거리 경로를 도출title: 코드는 더럽지만 미묘하게 더 성능이 좋은 코드import sys, heapqINF = 9999999nodeNum, edgeNum = map(int, input().split())startNode = int(input())edgeList = [[] for i in range(nodeNum+1)]for i in range(edgeNum):    fr, to, wt = map(int, input().split())    edgeList[fr].append((wt, to))    distance = [INF for i in range(nodeNum+1)]visited = [False for i in range(nodeNum+1)]distance[startNode] = 0hq = [(0,startNode)]while(hq):    minweight, minIndex = heapq.heappop(hq)    visited[minIndex] = True    for weight, node in edgeList[minIndex]:        if not visited[node] and (distance[node] &gt; (distance[minIndex] + weight)):            distance[node] = distance[minIndex] + weight            heapq.heappush(hq,(distance[node],node))for i in range(1, len(distance)):    print(distance[i] if distance[i]!=INF else \"INF\")벨만 포드 알고리즘음의 가중치가 존재하는 경우에도 사용할 수 있는 최단 경로 알고리즘title: 벨만 포드 알고리즘 $O(n^2)$ 예시# 정점 갯수, 시작 정점, 간선 정보{: #정점-갯수-시작-정점-간선-정보}def bellman_ford(node_num, start_node, edge_list):    INF = 99999999999    distance = [INF for _ in range(node_num)]    distance[start_node] = 0  # 1. 시작 정점 비용 초기화    for _ in range(node_num):  # 2. 각 정점 수 만큼 비용을 업데이트        for edge in edge_list:            from_node, to_node, weight = edge            # 3. 만약 시작 정점에서 해당 간선의 시작지점으로 못가면 보류            if (distance[from_node] == INF):                continue            # 4. 만약 더 적은 비용의 경로가 존재하면 비용 업데이트            if (distance[to_node] &gt; distance[from_node] + weight):                distance[to_node] = distance[from_node] + weight    # 5. 한번 더 비용 업데이트를 시도.    for edge in edge_list:        from_node, to_node, weight = edge        if (distance[from_node] == INF):            continue        # 6. 각 정점의 수만큼 업데이트 해도 새로 비용이 줄어든다면 음의 경로가 존재한다는 뜻        if (distance[to_node] &gt; distance[from_node] + weight):            return \"Infinity loop caused by - weight.\"    # 7. 새로 비용이 줄지 않는다면, 음의 경로가 존재 하지 않는다.    return distance플로이드 워셜 알고리즘음의 가중치가 존재하는 경우에 모든 정점 간의 최단 경로 비용을 측정하는 알고리즘title: 플로이드 워셜 알고리즘 $O(n^3)$ 예시# graph: 간선 정보, 간선이 존재하지 않는 노드 간은 무한대로 초기화{: #graph-간선-정보-간선이-존재하지-않는-노드-간은-무한대로-초기화}def floyd_warshall(graph):    # 모든 정점 간의 경로를 구하므로 보통 정사 배열 형태의 간선 정보를 사용    node_num = len(graph)    # 모든 정점의 시작, 경유, 도착 순서쌍에 대해 비교    for mid in range(node_num):  # 반드시 중간 경유 정점부터 루프를 돌려야 한다.        for start in range(node_num):            for end in range(node_num):                if (graph[start][end] &gt; graph[start][mid] + graph[mid][end]):                    graph[start][end] = graph[start][mid] + graph[mid][end]    return graph # 모든 정점 간의 최소 경로 비용 최소 신장 트리(Minimum Spanning Tree)가중치 그래프에서 신장 트리를 구성하는 간선들의 가중치의 합이 최소인 신장 트리프림 알고리즘한 정점에서 연결된 간선들 중 하나씩 선택하면서 최소 신장 트리를 만들어 가는 방식title: 프림 알고리즘 $O(n\\log n)$ 예시import heapq# G: 그래프, 인덱스에서 갈 수 있는 정점 v에 대한 가중치 val들이 존재, s: 시작 정점{: #g-그래프-인덱스에서-갈-수-있는-정점-v에-대한-가중치-val들이-존재-s-시작-정점}def MST_PRIM(G, s):    INF = 99999999    N = len(G)    key = [INF]*N    # 1. 가중치를 무한대로 초기화    pi = [None]*N    # 2. 트리에서 연결될 부모 정점 초기화    visited = [False]*N    # 3. 방문 여부 초기화    key[s] = 0        # 4. 시작 정점의 가중치를 0으로 설정    minHeap = [(key[s], s)]    while minHeap:  # 5. 힙이 빌때까지(=더 이상 갱신할 정점이 없을때 까지) 실행        _minWeight, minIndex = heapq.heappop(minHeap)  # 6. 갱신할 최소 가중치 정점 찾기        if not visited[minIndex]:            visited[minIndex] = True  # 7. 최소 가중치 정점 방문처리            for v, val in G[minIndex]:  # 8. 선택 정점의 인접한 정점들에 대해                if not visited[v] and val &lt; key[v]:  # 9 방문하지 않았고 갱신될 수 있는 정점이면,                    key[v] = val    # 10. 가중치 갱신                    pi[v] = minIndex  # 11. 트리에서 연결될 부모 정점                    # 12 갱신된 정보를 처리하기 위해 힙에 입력                    heapq.heappush(minHeap, (key[v], v))    return key, pi  # 갱신된 가중치와 연결 상태 출력크루스컬 알고리즘사이클이 생기지 않도록 최소 가중치 간선을 하나씩 선택해서 최소 신장 트리를 찾는 알고리즘사이클은 상호 배타 집합을 이용해 감지한다.title: 크루스컬 알고리즘 예시 def MST_KRUSKA(G):    mst = []  # 1. 최소 거리 간선 집합으로 공집합 생성       for i in range(N):  # 2. 각 노드를 집합으로 만들기        Make_Set(i)    G.sort(key = lambda t:t[2]) # 3. 가중치 기준으로 정렬       mst_cost = 0  # 4. MST 가중치       while len(mst) &lt; N-1:  # 5. 필요한 간선의 갯수(노드 갯수 - 1)만큼        u, v, val = G.pop(0)  # 6. 최소 가중치 간선 가져오기        if Find_Set(u) != Find_Set(v): # 7 만약 to node와 from node가 같은 집합에 있지 않다면            Union(u, v)  # 8 두 노드를 같은 집합으로 합치기            mst.append((u,v)) # 9 최소 거리 간선 집합에 (u, v) 추가            mst_cost += val 위상 정렬(topological sort)유향 비사이클 그래프에서 사용 가능한 차수(degree)에 따라 정렬하는 알고리즘title: 차수(degree)란, 해당 정점에 연결된 간선의 수를 의미하며, 들어오는 Indegree와 나가는 Outdegree로 나뉜다.만약, 알고리즘 종료된 뒤, 정렬에 포함되지 않은 정점이 있다면, 사이클이 존재한다는 의미이다.title: 위상 정렬 $O(N)$ 예시# 들어오는 차수와 나가는 차수에 대한 정보가 필요{: #들어오는-차수와-나가는-차수에-대한-정보가-필요}def topol_sort(indegree, outdegree):    node_num = len(indegree)    stack = []    sorted_node = []    for i in range(node_num):        if len(indegree[i]) == 0:  # 1. 들어오는 간선이 없는 정점부터 처리            stack.append(i)    while stack:  # 2. 더이상 처리할 정점이 없을 때 까지        min_node = stack.pop()  # 3. 차수가 0인 정점 하나 꺼내기        sorted_node.append(min_node)  # 4. 정렬 배열에 입력하고 정점을 제거        for to_node in outdegree[min_node]:  # 5. 정점에서 나가는 방향의 정점들의            indegree[to_node].remove(min_node)  # 6. 간선 삭제            if len(indegree[to_node]) == 0:  # 7. 만약 간선 삭제로 인해 차수가 0이 되면                stack.append(stack, to_node)  # 8. 다음 처리될 정점으로 스택에 입력    return sorted_node  # 9. 차수 별로 정렬된 배열 도출스택 대신 큐를 사용할 수 있으며, 이 경우 정답인 다른 결과가 나온다."
  }
  , 
  
  "/articles/computer_science/algorithm/Python%20for%20Algorithms-Tree.html": {
    title: "Python for Algorithms-Tree",
    date: " Sep 23, 2022 ",
    url: "/articles/computer_science/algorithm/Python%20for%20Algorithms-Tree.html",
    tags: ["알고리즘","PYTHON"],
    content: "min_depth: 2max_depth: 3varied_style: truePython for Algorithms-Tree비선형 구조, 원소들 간에 1:n, 계층형 관계를 가진다.상호 배타 집합서로소 집합(disjoint set) 혹은 상호 배타 집합은 집합 간에 중복되는(교집합) 원소가 없는 집합들이다.문제를 풀 때 서로 교집합이 없는 집합들의 수 등이 주제라면 생각해볼 만 하다.집합의 대표자(루트)로 각 집합을 구분한다.title: 상호 배타 집합 (배열 구현, $O(N)$) 함수p = [i for i in range(node_num)]  # p[x]: 노드 x의 부모 저장rank = [i for i in range(node_num)]  # rank[x]: 루트 노드가 x인 트리의 랭크 값 저장def make_set(x):    \"\"\"유일한 멤버 X를 포함하는 새로운 집합을 생성하는 연산\"\"\"    p[x] = x  # 자기자신이 루트    rank[x] = 0  def find_set(x):    \"\"\" x를 포함하는 집합을 찾는 오퍼레이션 \"\"\"    if x != p[x]:    # x가 루트가 아닌 경우    # Path Compression: 특정 노드에서 루트까지의 경로에 존재하는 노드가 루트를 부모로 가리키도록 갱신, 단방향 그래프에서만 가능함        p[x] = find_set(p[x])  # 양방향일 시 다음으로 대체 : return find_set(x)    return p[x]  def union(x, y):    \"\"\" x와 y를 포함하는 두 집합을 통합하는 오퍼레이션 \"\"\"    link(find_set(x), find_set(y))  def link(x, y):    \"\"\" 두 루트를 통합하는 오퍼레이션 \"\"\"    if rank[x] &gt; rank[y]:        p[y] = x    else:        p[x] = y    if rank[x] == rank[y]:        rank[y] += 1이진 탐색 트리이진 트리는 모든 노드들이 2개의 서브트리를 갖는 특별한 형태레벨이 $i$라면 노드의 최대 갯수는 $2^i$개이진 탐색 트리는 탐색 작업을 효율적으로 하기 위한 자료 구조, 다음과 같은 특성을 가짐  각 노드는 서로 다른 유일한 키(데이터)를 가진다.  $왼쪽\\ 자식\\ 값 &lt;= 자신의\\ 값 &lt;= 오른쪽\\ 자식\\ 값$  트리를 중위 순회 시 오름차 순 정렬을 얻을 수 있다.title: 이진 탐색 트리 예시AVL 트리title: 이진 탐색 트리 예시세그먼트 트리 (segment tree)수열의 총 합, 총 곱, 최댓값, 최솟값 등을 $O(\\log N)$의 시간에 해결할 수 있는 자료구조title: 세그먼트 트리 $O(\\log n)$ 예시 - 재귀 버전import mathdef create_seg_tree(start, end, index=1):    '''    재귀를 통한 세그먼트 트리 생성    index: 처리 중인 노드, index *2, index * 2 + 1 : 노드의 자식 노드들      - 세그먼트 트리의 최고 부모인 1부터 시작, 자식을 가르키는 방향으로 진행함    start - mid :왼쪽 자식이 저장하는 값, mid+1 - end : 오른쪽 자식이 저장하는 값    '''    global num_list    global tree    # tree의 leap에 도달했으면 num_list 값 그대로 삽입    if start == end:        tree[index] = num_list[start]        return tree[index]    mid = (start + end) // 2        # 좌측 노드와 우측 노드 값을 구해 부모 노드의 값을 합쳐 만든다. (이때 합이 아니라 구간곱이면 바꿔주자)    tree[index] = create_seg_tree(        start, mid, index * 2) + create_seg_tree(mid + 1, end, index * 2 + 1)    return tree[index]  def interval_sum(start, end, left, right, index=1):    '''    재귀를 통한 구간합 계산 함수    index: 현재 처리 중인 인덱스    start - end : 현재 index 노드가 값을 저장한 구간    left, right : 구간 합을 구하고자 하는 범위    '''    # 원하는 구간이 tree 밖인 경우    if left &gt; end or right &lt; start:        return 0  # 곱일 경우 1!    # 현재 인덱스가 포함하는 구간이 전부 내가 원하는 구간 안에 있는 경우 더한다.    if left &lt;= start and right &gt;= end:        return tree[index]    # 일부만 겹친 경우는 두 자식으로 나누어 구간내에 있는 자식만 더함    mid = (start + end) // 2    return interval_sum(start, mid, left, right, index * 2) + interval_sum(mid + 1, end, left, right, index * 2 + 1)def update(start, end, where, diff, index=1):    '''    특정 원소의 값을 수정하는 함수    index : 현재 수정 필요 여부를 판단하고 있는 함수    start - end: index 노드가 포함하고 있는 범위    where : 구간 합을 수정하고자 하는 노드    value : 수정할 값    '''    # 범위 밖에 있는 경우는 아무것도 안하고 재귀 중단    if where &lt; start or where &gt; end:        return      # 범위 안에 있으면 내려가면서 다른 원소도 갱신    tree[index] += diff    if start != end:        mid = (start + end) // 2        update(start, mid, where, diff, index*2)        update(mid + 1, end, where, diff, index*2+1)# segment tree{: #segment-tree}numCount, changeCount, caseCount = map(int, input().split())num_list = [0 for _ in range(numCount)]for i in range(numCount):    num_list[i] = int(input())tree = [0] * (1 &lt;&lt; (math.ceil(math.log2(numCount))+1))# 필요한 노드 수 == 잎새 노드(N) + 부모 노드(N-1) == 2N - 1{: #필요한-노드-수-잎새-노드-n-부모-노드-n-1-2n-1}# 따라서 높이를 H라고 놓을 때 ∑^{H}_{n=0}2^n &gt;= 2N -1을 만족해야 한다.{: #따라서-높이를-h라고-놓을-때-∑-h-n-0-2-n-2n-1을-만족해야-한다}# tree의 크기는 2의 (예상 트리의 높이 + 1)승이면 절대 부족하지 않다.{: #tree의-크기는-2의-예상-트리의-높이-1-승이면-절대-부족하지-않다}seg = create_seg_tree(num_list)for j in range(changeCount + caseCount):    cmd, par1, par2 = map(int, input().split())    if cmd == 1:        loc, val = par1-1, par2        update(seg, loc, val, numCount)    elif cmd == 2:        fr, to = par1-1, par2-1        print(interval_sum(numCount, seg, fr, to))아래는 다른 버전의 세그먼트 트리이다.+ PROS    - 반복문과 비트 연산을 활용해 좀 더 빠르다.    - 자료형의 크기가 입력 값의 2배로, 비교적 효율적이다.    - 자료형에 배치된 원소들이 위치적으로 직관적임.        - 위치 n의 $n\\times 2,\\ n\\times 2 + 1$이 자식이며, 반대로 $n//2$는 부모        - 원소들이 층 별로 일렬로 배치됨.- CONS    - 코드를 이해하기 힘들다. 특히, left 인덱스와 right 인덱스의 활용 부분    - 자료형에 배치된 원소들이 실제 원리의 노드와 조금 다르다.        - 예를 들어, 원소의 갯수가 홀수 일 경우, 서로 다른 층 값의 합을 가지고 있는 부모 노드가 생김.        - 따라서 lazy propagation 같은 추가적인 알고리즘 적용 힘듬.잠깐, 이거 혹시 BIT 트리인가? 나중에 확인 바람title: 세그먼트 트리 $O(\\log n)$ 예시 - 반복 버전# 반복문을 이용한 세그먼트 트리 생성{: #반복문을-이용한-세그먼트-트리-생성}def create_tree(num_list):    # 부모 노드들이 들어갈 공간 + 원본 값 공간    tree = [0 for _ in range(len(num_list))] + num_list    # 부모 노드 생성     for i in range(len(num_list) - 1, 0, -1): # 거꾸로 시작하여 말단 노드에 가까운 부모부터 생성        tree[i] = tree[i &lt;&lt; 1]+tree[i &lt;&lt; 1 | 1]    return tree  # 반복문을 이용한 구간합 구하기{: #반복문을-이용한-구간합-구하기}def interval_total(num_count, tree, left, right):    # 실제 범위는 부모 공간에 의해 밀려났으므로 그만큼 더해줌    left += num_count    right += num_count    result = 0    while (left &lt;= right):        # 인덱스의 홀수 짝수 여부에 따라 두가지로 나뉜다.        # 1. 이 노드의 값을 포함한 부모 노드가 있음(=더하지 않음)        # 2. 이 노드의 값을 포함한 노드가 없음 (=더해줘야 함)        if (left &amp; 1):  # 왼쪽 인덱스가 홀수이면            result += tree[left]  # 해당 값을 더해 주고            left += 1  # 인덱스 증가        if not (right &amp; 1):  # 오른쪽 인덱스가 짝수이면            result += tree[right]  # 해당 값을 더해주고            right -= 1  # 인덱스 감        # 인덱스를 절반으로 줄임 == 해당 값의 부모 노드로        left &gt;&gt;= 1        right &gt;&gt;= 1    return result# 반복문을 이용한 트리 갱신{: #반복문을-이용한-트리-갱신}def update(tree, index, val, num_count):    index += num_count    tree[index] = val    # 부모 노드 업데이트    while index &gt; 1:        tree[index &gt;&gt; 1] = tree[index] + tree[index ^ 1]          index &gt;&gt;= 1    return tree    # segment tree{: #segment-tree}numCount, changeCount, caseCount = map(int, input().split())  arr = [0 for _ in range(numCount)]for i in range(numCount):    arr[i] = int(input())  seg = create_tree(arr) for j in range(changeCount + caseCount):    cmd, par1, par2 = map(int, input().split())    if cmd == 1:        loc, val = par1-1, par2        update(seg, loc, val, numCount)    elif cmd == 2:        fr, to = par1-1, par2-1        print(interval_total(numCount, seg, fr, to))동적 세그먼트 트리(Dynamic segment tree)title: 동적 세그먼트 트리 예시지속 세그먼트 트리(Persistent segment tree)title: 지속 세그먼트 트리 예시다차원 세그먼트 트리 (multi dimensional segment tree)title: 다차원 세그먼트 트리 예시게으른 전파 세그먼트 트리 (segment tree with lazy propagation)  하단 코드는 이해를 돕기 위해 dictionary를 활용했으며, 실제로 아래 코드는 필요한 메모리가 어마어마하므로, 다차원 배열이나 lazy와 value를 다른 배열로 나누자.title: 게으른 전파 세그먼트 트리 예시import sysimport mathinput = sys.stdin.readline  def create_seg_tree_lazy(start, end, index=1):    '''    기존 방식과 동일한 방법으로 트리 생성    '''    if start == end:        tree[index][\"value\"] = number_list[start]    else:        mid = (start + end) // 2        create_seg_tree_lazy(start, mid, index * 2)        create_seg_tree_lazy(mid + 1, end, index * 2 + 1)        tree[index][\"value\"] = tree[index*2][\"value\"] + tree[index*2 + 1][\"value\"]def propagation(where, start, end):    '''    lazy값이 존재하는 경우 값을 추가하고 자식 노드로 lazy 값을 전파하는 함수    '''    if start != end:        tree[where * 2][\"lazy\"] += tree[where][\"lazy\"]        tree[where * 2 + 1][\"lazy\"] += tree[where][\"lazy\"]    tree[where][\"value\"] += tree[where][\"lazy\"] * (end - start + 1)     tree[where][\"lazy\"] = 0 def interval_update_lazy(left, right, start, end, diff, index=1):    '''    lazy 구간 업데이트 구하기    left, right : 수정하고자 하는 범위    start, end : 현재 계산 중인 범위(현재 index번의 노드가 커버하는 공간)    index : 현재 update하는 노드    diff : 변화되는 값    '''    if tree[index][\"lazy\"]: # 이전에 쌓은 lazy값이 존재하는 경우 수정 뒤 lazy 전파        propagation(index, start, end)    if end &lt; left or right &lt; start: # 구하는 구간이 관계 없는 경우        return    if left &lt;= start and end &lt;= right: # 해당 노드가 커버하는 구간이 완전히 포함될 경우        tree[index][\"value\"] += (end - start + 1) * diff        if start != end: # 말단 노드가 아닐 경우,            tree[index * 2][\"lazy\"] += diff            tree[index * 2 + 1][\"lazy\"] += diff        return    else:        mid = (start + end) // 2        interval_update_lazy(left, right, start, mid, diff, index*2)        interval_update_lazy(left, right, mid + 1, end, diff, index*2 + 1)        tree[index][\"value\"] = tree[index*2][\"value\"] + tree[index*2+1][\"value\"]def interval_sum_lazy(left, right, start, end, index=1):    '''    lazy 구간 합 구하기    index : 현재 처리 중인 노드의 인덱스    start, end : 현재 처리 중인 인덱스 노드가 값을 포함하고 있는 범위    left, right : 값을 구할 범위    '''    if tree[index][\"lazy\"]:        propagation(index, start, end)      if end &lt; left or right &lt; start:        return 0    if left &lt;= start and end &lt;= right:        return tree[index][\"value\"]    else:        mid = (start+end)//2        return interval_sum_lazy(left, right, start, mid, index*2) + interval_sum_lazy(left, right, mid + 1, end, index * 2 + 1)number_count, change_count, sum_count = map(int, input().split())number_list = [int(input()) for _ in range(number_count)]tree = [{\"value\": 0, \"lazy\": 0} for _ in range(1 &lt;&lt; (math.ceil(math.log2(number_count))+1))]# 필요한 노드 수 == 잎새 노드(N) + 부모 노드(N-1) == 2N - 1{: #필요한-노드-수-잎새-노드-n-부모-노드-n-1-2n-1}# 따라서 높이를 H라고 놓을 때 ∑^{H}_{n=0}2^n &gt;= 2N -1을 만족해야 한다.{: #따라서-높이를-h라고-놓을-때-∑-h-n-0-2-n-2n-1을-만족해야-한다}# tree의 크기는 2의 (예상 트리의 높이 + 1)승이면 절대 부족하지 않다.  {: #tree의-크기는-2의-예상-트리의-높이-1-승이면-절대-부족하지-않다}create_seg_tree_lazy(0, number_count-1) for i in range(change_count + sum_count):    cmds = list(map(int, input().split()))    if cmds[0] == 1:        _, fr, to, chng = cmds        interval_update_lazy( fr-1, to-1, 0, number_count-1, chng)    else:        _, fr, to = cmds        print(interval_sum_lazy( fr-1, to-1, 0, number_count-1))BIT (Binary Indexed Tree, 펜윅 트리, fenwick tree)title: BIT 예시트라이(Trie)주로 문자열과 관련되어 사용되나, 트리를 이용하기 때문에 이곳에 편입n진 트리 구조를 이용해 찾고자 하는 검색할 문자열의 길이를 $M$이라 할 때, $O(M)$이 된다.title: Trie 예시class Trie:    def __init__(self, value):        self.value = value        self.childs = {}     def insert(self, word):        currNode = self        for letter in word:            if currNode.childs.get(letter):                currNode.childs[letter].value += 1            else:                currNode.childs[letter] = Trie(1)            currNode = currNode.childs[letter]    def search(self, query):        currNode = self        for i, letter in enumerate(query):            if currNode.childs.get(letter):                currNode = currNode.childs[letter]            else:                return 0        if i == len(query) - 1:            return currNode.value        def solution(    words, queries):    trie = Trie(0)    for word in words:        trie.insert(word)    result = [trie.search(query) for query in queries]    return result"
  }
  , 
  
  "/articles/computer_science/algorithm/Python%20for%20Algorithms-etc.html": {
    title: "Python for Algorithms-etc",
    date: " Sep 23, 2022 ",
    url: "/articles/computer_science/algorithm/Python%20for%20Algorithms-etc.html",
    tags: ["알고리즘","PYTHON"],
    content: "min_depth: 2max_depth: 3varied_style: truePython for Algorithms-etc스위핑title: 스위핑 예시컨벡스 헐CCW(Counter Clock Wise)벡터의 외적(Cross Product)를 이용해 점의 쌍을 기준으로 다른 점의 상대적 위치를 알아냄title: CCW 예시def ccw(fr_point, to_point, target_point):    fr_x, fr_y = fr_point    to_x, to_y = to_point    tg_x, tg_y = target_point        result = (to_x - fr_x)*(tg_y - fr_y) - (tg_x - fr_x)*(to_y - fr_y)    if result &gt; 0:        return 1 # 반시계 방향    elif result &lt; 0:         return -1 # 시계 방향    else:        return 0 # 평행그라함 스캔title: 그라함 스캔 $O(n\\log n)$ 예시def ccw(a, b, c):    t = (b['x']-a['x'])*(c['y']-a['y']) - (c['x'] - a['x'])*(b['y'] - a['y'])    if t &gt; 0:        return 1    elif t &lt; 0:        return -1    else :        return 0def comp_by_ccw(a, b):    global firstPoint    return -ccw(firstPoint,a,b)def comp_by_pos(a, b):    if a['y'] == b['y']:        return 1 if a['x'] &gt; b['x'] else -1     else:        return 1 if a['y'] &gt; b['y'] else -1firstPoint = {'x':0, 'y':9999099} # 정렬을 위한 기준points.sort(key=functools.cmp_to_key(comp_by_pos)) # 맨 왼쪽 위 지점부터 작 위한 정렬points.sort(key=functools.cmp_to_key(comp_by_ccw)) # 기준에서 외부 껍질 우선으로 정렬 st = [points[0], points[1]] # 첫 두 시작 지점 입력for point in points[2:]: # 모든 지점에서    while(len(st) &gt;=2):         if (ccw(st[-2], st[-1], point) &gt; 0): # 다음 점이 더 외부 껍질 부분에 있는지 확인            break # while 문이므로 스택 내의 모든 점에 대해서 검사함         st.pop() # 만약 더 외부 껍질이면 2번째 기준점을 내보냄    st.append(point) # 새로운 기준점 집어 넣음print(len(st))누적합배열의 크기를 $N$, 구간 합의 갯수를 $M$이라고 할때, 기존의 시간 복잡도 $O(M\\times N)$에서 $O(M+N)$으로 바꿀 수 있다.일반 누적합title: 누적합 예시def solution(board, sums):\t# 누적합은 마지막 인덱스 + 1에 변화값의 항등원을 더하기 때문에\t# 기존 배열보다 크기가 1커야 한다.    new_board = board + [0]    for props in sums:        # x1 ~ x2까지 degree 값만큼 변화시킨다.        x1, x2, degree = props\t\t# 누적합은 첫 인데스와 마지막 인덱스 + 1 부분을 각각 변화되야할 값과 그 값의 항등원을 더해야 한다. \t\t# 예를 들어 인덱스 0 ~ 2까지 값을 X 만큼 변화시킨다면 다음과 같이 설정한다.\t\t#     0  1  2  3 \t\t#   [+X, 0, 0, -X]\t\tnew_board[x1] += degree\t\tnew_board[x2] -= degree            \t# 이후 각 가로 줄 먼저 가로 방향 누적합을 구한다. 인덱스 0를 제외하고 이전 인덱스 값을 순차적으로 더해주면 된다.    for i in range(len(new_board)):        new_board[i] += new_board[i-1]        \t# 해당 누적합을 원래 2차원 배열에 적용시키기    for i in range(len(board)):        board[i] += new_board[i]                return board 2차원 누적합title: 2차원 누적합 예시def solution(board, sums):\t# 누적합은 마지막 인덱스 + 1에 변화값의 항등원을 더하기 때문에\t# 기존 배열보다 가로 세로 크기가 1커야 한다.    new_board = [[0]*(len(board[0])+1) for _ in board] + [[0]*(len(board[0])+1)]    for props in sums:        # (x1, y1) ~ (x2, y2)까지 degree 값만큼 변화시킨다.        x1, y1, x2, y2, degree = props\t\t# 2차원 누적합은 1차원과 달리 두 방향으로 인덱스를 누적합을 설정해야 한다.\t\t# 예를 들어 (0,0) ~ (2,2)까지 값을 X 만큼 변화시킨다면 다음과 같이 설정한다.\t\t# /   0  1  2  3 \t\t# 0 (+X, 0, 0, -X)\t\t# 1 ( 0, 0, 0,  0)\t\t# 2 ( 0, 0, 0,  0)\t\t# 3 (-X, 0, 0, +X)\t\t# 참고로, (0,0) ~ (0, 2)처럼 한줄의 누적합 또한 \t\t# 예외 없이 다음과 같이 다음 줄에 인덱스를 설정해야 한다.\t\t# /   0  1  2  3 \t\t# 0 (+X, 0, 0, -X)\t\t# 1 (-X, 0, 0, +X)\t\t# 2 ( 0, 0, 0,  0)\t\t# 3 ( 0, 0, 0,  0)        for i, j in [[x1, y1], [x2+1, y2+1]]:             new_board[i][j] += degree                for i, j in [[x2+1, y1], [x1, y2+1]]:            new_board[i][j] -= degree            \t# 이후 각 가로 줄 먼저 가로 방향 누적합을 구하고    for i in range(len(new_board)):        for j in range(1,len(new_board[0])): # 이때, 가로줄은 인덱스가 1부터 시작해야한다.            new_board[i][j] += new_board[i][j-1]        \t# 이후 각 세로 줄의 세로 방향 누적합을 더하면 된다.    for i in range(1,len(new_board)):        for j in range(len(new_board[0])): # 이때, 세로줄은 인덱스가 1부터 시작해야한다.            new_board[i][j] += new_board[i-1][j]\t# 해당 누적합을 원래 2차원 배열에 적용시키기    for i in range(len(board)):        for j in range(len(board[0])):            board[i][j] += new_board[i][j]                return board "
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EB%AC%B8%EC%A0%9C%20%ED%95%B4%EA%B2%B0%20%EA%B3%BC%EC%A0%95.html": {
    title: "알고리즘 문제 해결 과정",
    date: " Nov 7, 2022 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EB%AC%B8%EC%A0%9C%20%ED%95%B4%EA%B2%B0%20%EA%B3%BC%EC%A0%95.html",
    tags: ["알고리즘","CRUDE"],
    content: "알고리즘 문제 해결 과정 alphastyle: bulletmin_depth: 2max_depth: 2varied_style: falsetitle: 출처_프로그래밍에서 배우는 알고리즘 문제 해결 전략(구종만 저)_를 참고한 후 내용을 추가한 포스트입니다.1. 문제를 읽고 이해한 뒤, 재정의, 추상화 하기문제의 조건과 지문을 읽는다, 이 때 편견이나 성급함으로 잘못 유추하게 되면 치명적이므로 꼼꼼히 읽어야 한다. 의외로 자주 일어나는 실수이다.이후 문제를 추상화 해야한다.  추상화 : 현실 세계 개념을 수학적, 전산학적 개념으로 표현이를 통해 간단하고 친숙하게 만들어 적용할 도구를 쉽게 선택할 수 있게 한다.  문제에 적힌 명칭들을 간략화된 이름으로 바꿔보자관광지 수, 여행 비용 =&gt; X 너무 현실 세계 개념a, b =&gt; X 너무 아무 의미 없음NodeCount, weight =&gt; O 간결하고 알고리즘에 맞게 추상화된 변수명  간략화된 그림이나 수식 등으로 표현해보자2. 문제 해결 계획 세우기문제 해결 방식, 즉, 사용할 알고리즘과 자료구조를 선택가장 좋은 경우는 여러 경험이 쌓인 후 나타나는 직관이다. 하지만 직관이 떠오르지 않거나 직관으로 풀었을 때 틀린 경우 다음과 같은 체계적인 접근을 시도할 수 있다.문제 접근 방법비슷한 문제 찾기다음의 두 경우, 비슷한 문제의 접근법을 적용가능title: 알고리즘의 원리를 완전히 이해하고 있어 응용하는 경우  두 도시를 잇는 가장 짧은 경로를 찾는 문제를 푼 경험이 있음  위 문제를 풀기 위한 최단 경로 알고리즘을 완전히 이해하고 있음  알고리즘을 응용하면 가장 긴 구간과 가장 짧은 구간의 길이 차이가 적은 경로를 찾는 문제를 풀 수 있음title: 해당 카테고리의 문제를 푼 경험이 많음발생 확률이나 경우의 수 문제의 경우, 십중 팔구는 동적계획법으로 해결 가능하다는 경험을 토대로 접근 가능따라서 문제가 어떤 카테고리(최적화, 경우의 수, 검색, 등)에 속했는지 찾아내는 능력이 필요하다.단순한 해결 방법으로 시작하기 : 브루트 포스만약, 갈피를 잡지 못하겠다면, 비효율적이여도 목표를 이룰 수 있는 단순한 알고리즘을 만들어 보자. 예를 들어 사탕을 모두 나누어 주는 모든 경우의 수를 따지게 만든다던가…그 이유는  종종 충분한 자원과 컴퓨터의 효율로 인해 해결되버리는 경우 있음  모든 알고리즘은 단순한 알고리즘을 기반으로 구성된 경우가 많음          가지치기, 효율적인 자료구조로 변경 등        앞으로 구현 시도해볼 알고리즘의 성능 기준선이 될 수 있음          어느 정도 더 빠른 알고리즘을 구현해야 하는가?      문제 푸는 과정 수식화 : 급할수록 돌아가기단순한 해결 방법으로 시작하기 : 브루트 포스는 가끔 오히려 완전한 발상의 전환이 필요할 때는 역효과를 일으키기도 한다.이때는 간단한 입력과 경우의 문제를 직접 손으로 풀어보자.(직접 구현 보다 왠만하면 빠르다.)그리고 해당 방법을 공식화한 알고리즘을 구상하거나 고려해야할 점을 얻어낼 수 있다.문제 단순화 : 문제를 좀더 쉽게 변형문제를 좀더 쉬운 변형판을 먼저 풀어보는 방법이 있다.앞선 문제 푸는 과정 수식화 : 급할수록 돌아가기와의 차이점은, 쉬운 입력을 이용하는 것이 아니라 문제 자체의 조건과 요구를 변경하는 것이다.  변수 수 줄이기, 차원 수 줄이기 등이를 통해 새로운 직관을 얻거나 간단히 문제를 풀어낼 수 도 있다.예를 들어 2차원의 문제를 1차원으로 줄이고 푼 뒤, 사실은 1차원 문제 값 2개를 풀어 합하면 답이 나오는 경우가 있다.그림이나 수식으로 표현해 보기수학적 능력이 있다면 수식으로 표현하여 알고리즘 전개나 축약 등에 도움이 될 수 있다.  동적 계획법의 점화식 등이 대표적수식으로도 안된다면 인간이 직관적으로 받아 들일 수 있는 기하학적 도형을 이용해볼 수 있다.  정수쌍의 경우 좌표 평면에 그려보자.문제를 더욱 작게 쪼게기한개의 복잡한 조건을 여러개의 단순한 조건으로 나눌 수 있는 경우가 많다.단순히 문제 두개가 이어진 형태부터, 특정 알고리즘의 값을 비교하기 위해 다른 알고리즘이 필요한 경우도 존재한다.순서를 뒤집기주어진 원소에서 목적을 찾는 것이 아니라, 목적지부터 반대로 향하는 방법 또한 존재한다.예를 들어  어느 입구에 들어가야 출구로 나갈 수 있는가?에 대한 문제는 모든 입구를 확인하지 않고 반대로 출구로 들어가서 나오는 입구를 확인하면 된다.순서를 강제하기주로 순서에 관계없는 수(경우의 수, 필요한 횟수)를 요구하는 문제에 사용되며, 일부분 순서대로 조금씩 수를 구한다는 점에서 스위핑이나 DP와 비슷하다.순서에 관계없을 경우 수많은 경우의 수가 나오는 경우 순서를 강제하여 경우의 수를 줄일 수 있다.예를 들어 5X5 게임판에서 각 격자를 누르면 격자의 상하좌우 격자 불이 토글되는 문제에서 모든 격자를 끄는데 필요한 최소 클릭 수를 구하는 문제 등이 있다.엄밀히 말하면 문제 해결 계획 보다는 알고리즘에 가까운것 같다?정규화(Canonicalization) 하기위의 순서를 강제하기의 연장으로, 우리가 고려해야 할 답들 중 형태가 다르지만 결과적으로는 똑같은 것들을 그룹으로 묶은 뒤, 각 그룹의 대표들만을 고려하는 방법이다.예를 들어 상단에 격자 토글 문제에서 순서가 다르지만 클릭 하는 부분이 같은 경우의 수는 순서와 관계없이 결과가 같으므로 하나로 묶을 수 있다.정규화 기법이 문제마다 다르므로 경험이 아주 많아야 한다.3. 문제 해결 검증 하기알고리즘이 모든 요구 조건을 정확히 수행하는가?  루프 불변성  증명 방법들메모리와 시간이 제한 내에 들어가는가?  시간복잡도, 공간복잡도 참조  …실전에서는  주먹 구구식 참조    4. 프로그램 구현 하기    좋은 코드의 원칙    표준 라이브러리의 적극적인 활용  디버깅 하기 편한 수준의 적절하고 간결하고 재사용 가능한 코드 작성  앞서 설명한 추상화와 달리, 좀더 직관적이고 규약에 맞는 명명법 적용def checkPoint(x,y,cx,cy,cr): # 비직관적, 규약에 맞지 않는 명명법\tpassdef is_in_circle(x,y,circle_x,circle_y,circle_r): #직관적이고 pep8에 맞는 명명법, 결과값 예상 쉬움\tpass위를 통해 디버그가 용이하고 나중에 읽을때도 이해하기 쉬운 코드를 적을 수 있다. - **데이터의 정규화: 데이터가 여러 형식이 존재할 수 있는 경우 하나로 통일하여 저장하자.**- 이를 통해 형식 불일치로 인한 버그를 막을 수 있다.- 되도록이면 함수나 클래스에 데이터가 입력되는 즉시 통일하게 하자.- 예를 들어, 분수값은 무조건 최대한 기약분수 형태로, 시간은 하나의 형식으로 통일해서 - **코드와 데이터의 분리: 코드의 논리와 상관없는 데이터를 분리**- 분기문이 줄고, 재사용이 가능하게 됨def month_to_month_name(month):# 코드양이 많고 실수가 많으며 재사용 힘든 코드(X)\tif (month==1) return \"Jan\"\telif (month==2) return \"Feb\"\t...\tdef month_to_month_name(month): # 코드 양이 적고 재사용이 쉬운 데이터 분리 코드(O)\tmonth_names = [\"Jan\", \"Feb\", \"March\"...]\treturn month_names[month]자주하는 실수들  산술 오버 플로          변수 표현 가능 범위 주의하기      애매하게 작거나 너무 큰 무한대 값      결과나 중간 값이 너무 큼      title: 산술 오버 플로우 예시  최소 공배수 구할 때 분자 크기 주의  ex) lcm(a, b) = (a * b) / gcd(a, b)  gcd 값을 구하지 말고 미리 분모 나누기, 이항계수 사용으로 방지  배열 범위 밖 원소 접근          특히 0부터 시작하는가, 1부터 시작하는가 주의      일관되지 않은 범위 표현 방식 ex) 0 &lt;= i &lt;= 10 ? 0 &lt; i &lt; 10 ?      Off-by-one 오류                  ex) 100미터 담장에 10미터 마다 울타리 기둥을 세우려면 기둥이 몇개 필요한가? 10개 (X), 11개 (O)                      상수나 변수 오타:  \"sucess\"(X), \"success\"(O)  연산자 우선 순위: 괄호 활용하기  잘못된 변수 초기화  스택 오버 플로: 최대 재귀 크기 주의하거나 늘리기  잘못된 비교 함수: 특히 객체 간의 비교 시 조심  입출력 방식 빠르게 바꾸거나 줄이기  실수 연산: 제대로된 비교 함수 만들기, 실수 연산 안하는 조건으로 변경하기5. 회고하기자신이 문제를 해결한 과정을 돌이켜보고 개선하는 과정  코드와 함께 자신의 경험을 주석으로 남겨보자          간단한 해법      접근 방식      해법을 찾는데 결정적이었던 깨달음      버그 났던 지점      등        잘 푼사람의 코드 보기          자신의 해법과 비교하여 어느점이 우수한지 구분        맞춘 경우        두번째로 풀면서 더우 간결하고 효율적인 코드와 알고리즘을 작성해보고, 같은 알고리즘을 유도할 수 있는 간결한 코드를 작성할 수 있다.            틀린 경우도저히 못풀겠거나 1시간 이상 고민을 해야했던 경우  오답 요인, 정답에 접근하지 못한 이유를 적자.반복되면 실패 원인을 깨달을 수 있다.  다른 사람 코드 복기          나는 왜 이 해답을 떠올리지 못했는가?      왜 이사람 코드가 더 간결하거나 효율적인가?단순 코드 복기 보다, 블로그 등에서 풀이 과정을 함께 보면 더욱 좋다.      "
  }
  , 
  
  "/articles/computer_science/etc/float%20%EC%B2%98%EB%A6%AC.html": {
    title: "float 처리",
    date: " Nov 11, 2022 ",
    url: "/articles/computer_science/etc/float%20%EC%B2%98%EB%A6%AC.html",
    tags: ["CS","컴퓨터_구조","CRUDE"],
    content: "컴퓨터의 float 처리컴퓨터는 실수를 근사값으로 표현한다.실수는 원주율이나, $2/3$처럼 무한한 경우가 존재하므로, 메모리의 제한에 의해 정확도가 제한된 근사값으로 저장한다.따라서, 실수가 포함될 경우 같은 수식이여도 계산 순서, 컴파일러 최적화, 등의 외부 환경에 따라 값이 변할 수 있다.title: IEEE 754 표준IEEE 754 표준은 가장 흔히 사용되는 실수 표기 방식으로, 연산에 관한 규정, 오버플로, 언더 플로 처리, 반올림 등의 방식을 정한다.  이진수로 실수를 표기 : ex) 11.625 =&gt; 1011.101  부동 소수점(floating-point) 표기법  무한대, 비정규 수, NaN 등 표현부동 소수점 표기이진수로 실수를 표기할 경우 무조건 최상위 이진수는 1이 되게 된다.  1이 아니면 생략해도 되기때문(ex) $0101.110 \\rightarrow 101.11$)최상위 이진수를 유일한 정수부로 만들고, 지수를 곱하는 방식으로 바꾸면 최상위 비트를 생략해도 되므로 1비트 절약하게 된다.  ex) $101.11 \\rightarrow +\\ .0111 \\times 2^2$ (1은 생략)  이렇게, 소수점을 옮기는 방식으로 표현하므로 부동 소수점(floating point)라고 말한다.          반대로, 옮기지 않고 표현하는 고정 소수점 표기법도 존재함.      즉, 결국  보다시피 실수 변수는 3가지 정보를 저장하게 된다.  부호 비트(sign bit) : 양수 음수 구분 =&gt; 1비트  지수(exponent): 소수점 옮긴 수  가수(mantissa): 소수점을 옮긴 실수의 최상위 X 비트지수와 가수에 할당되는 비트의 수만큼 더욱 정확한 근사값을 표현할 수 있다.보통 지수보다 가수에 더 많이 할당되며, 넘어가는 수는 반올림된 후 버려진다.  64비트 형의 경우 지수 비트 11비트, 가수 비트 52비트, 유효자릿수 십진수 기준 15자리따라서 실수간의 비교는 직접 동일함을 비교하는 것이 아니라 두 수의 차이가 오차범위 내 인지 확인하는 방법으로 진행한다.See you, Space Cowboy"
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%A0%95%EB%A0%AC%20%EC%A0%95%EB%A6%AC.html": {
    title: "정렬 정리",
    date: " Nov 11, 2022 ",
    url: "/articles/computer_science/algorithm/%EC%A0%95%EB%A0%AC%20%EC%A0%95%EB%A6%AC.html",
    tags: ["CS","알고리즘","CRUDE"],
    content: "정렬 정리            정렬 방법      최적      평균      최악      공간 복잡도                  선택(selection)      $O(n^2)$      $O(n^2)$      $O(n^2)$      $O(n^2)$              버블(bubble)      $O(n^2)$      $O(n^2)$      $O(n^2)$      $O(n)$              삽입(insert)      $O(n)$      $O(n^2)$      $O(n^2)$      $O(n^2)$              병합(merge)      $O(n\\log n)$      $O(n\\log n)$      $O(n\\log n)$      $O(n\\log n)$              퀵(quick)      $O(n\\log n)$      $O(n\\log n)$      $O(n^2)$      $O(n\\log n)$              힙(heap)      $O(n\\log n)$      $O(n\\log n)$      $O(n\\log n)$      $O(n)$              쉘(shell)      $O(n^{1.25})$      $O(n^{1.25})$      $O(n^{1.25})$      $O(n)$              계수(counting)      $O(n+k)$      $O(n+k)$      $O(n+k)$      $O(n+k)$      시간 복잡도 $O(n^2)$버블 정렬 (Bubble Sort)구현이 쉽다는 것 외에는 아무런 장점이 없는 정렬이미 정렬된 자료에선 1번만 돌면 되므로 최선의 성능을 보여줌($O(n)$), 따라서 중간에 정렬이 필요한 경우 바로 중단하고 정렬되지 않았음을 알리는데 사용할 수 있다.BubbleSort(eles[])\tfor i = 0 to eles[].length-1\t\tfor j = 0 to i-1\t\t\tif eles[j] &amp;#38;#62; eles[j+1]\t\t\t\tswap(ele[j], eles[j+1])\treturn eles[]선택 정렬(Selection Sort)인간이 무의식적으로 사용하는 정렬 1보통 버블 정렬 보다 2배 정도 빠르다.SelectionSort(eles[])\tfor i = 0 to eles[].length-2\t\tindex = i\t\tfor j = i + 1 to eles[].length-1\t\t\tif eles[index] &amp;#38;#62; eles[j]\t\t\t\tindex = j\t\tswap(eles[index], elses[i])\treturn eles[]삽입 정렬(Insertion Sort)인간이 무의식적으로 사용하는 정렬 2  탐색 이외의 오버헤드가 적어 이미 정렬되어 있는 자료구조에서 자료를 하나씩 삽입, 제거할 때 좋다.  배열이 작은 경우에도 밀어내기 부분의 오버헤드가 적으므로 왠만한 알고리즘보다 좋다.InsertionSort(eles[])\tfor i = 1 to eles[].length-1\t\tj = i - 1\t\tkey = eles[i]\t\twhile eles[j] &amp;#38;#62; key and j &amp;#38;#62;= 0\t\t\teles[j+1] = eles[j]\t\t\tj = j -1\t\teles[j+1] = key\treturn eles[]좀더 자세한 설명은 알고리즘 성능 분석 참조시간 복잡도 $O(n\\log n)$병합 정렬(Merge Sort)병합 정렬힙 정렬(Heap Sort)자료구조인 힙을 이용한 정렬추가적인 메모리가 전혀 필요하지 않고, 언제나 안정적인 $O(n\\log n)$ 성능을 가진다. 하지만, 퀵 정렬이 컴퓨터 구조상 캐시 친화적이므로 보통 더 빠르다.다만, 퀵 정렬은 최악의 상황에는 $O(n^2)$이므로, 추가적인 피봇 선택 알고리즘이 없을 경우 힙 정렬은 최악의 상황을 피할 수 있다.퀵 정렬(Quick Sort)보통 최고의 성능을 나타내는 정렬피봇의 선택에 따라 성능이 크게 달라져, 피봇을 고르는 partition 알고리즘 또한 다양하게 존재한다.그외 정렬셸 정렬(Shell’s Sort) - $O(n^{1.25})$삽입정렬을 띄엄띄엄 먼저 수행한 후, 거의 정렬된 배열을 삽입 정렬로 마무리하는 방식ㅅ"
  }
  , 
  
  "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%A0%95%EB%8B%B9%EC%84%B1%20%EC%A6%9D%EB%AA%85.html": {
    title: "알고리즘 정당성 증명",
    date: " Nov 24, 2022 ",
    url: "/articles/computer_science/algorithm/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EC%A0%95%EB%8B%B9%EC%84%B1%20%EC%A6%9D%EB%AA%85.html",
    tags: ["CS","알고리즘","CRUDE"],
    content: "알고리즘 정당성 증명많은 경우 단위 테스트를 통해 프로그램을 증명하지만, 이는 완벽한 방법이 아니다.알고리즘을 정확히 증명하기 위해서는 다양한 수확적 기법을 이용하여야 하며, 이를 통해 알고리즘 구현 시의 통찰과 구조를 완벽히 이해할 수 있다.무조건 하나의 증명법만 이용되는 것이 아니라, 두 증명법을 섞어 쓰기도 한다.예를 들어, 구한 답이 최선의 답임을 귀류법으로 증명하고, 귀납법을 이용해 다음 루프에서도 최선임을 증명한다.수학적 귀납법(mathematical induction)루프 불변성에도 사용되는 증명방법즉 주로 반복적인 구조를 갖는 명제에서 사용된다.마치, 도미노가 쓰러지듯, 다음 단계를 연쇄적으로 증명하여 최종 결과를 증명하는 방식이며, 세 단계로 나뉨  단계 나누기 : 증명하고 싶은 사실을 여러 단계로 나눔  첫 단계 증명 : 첫 단계에 증명하고 싶은 내용이 성립함을 보임  귀납 증명 : 다음 단계들에서도 연속적으로 성립하는지 보임title: 예시 문제: “사다리 게임은 언제나 위 선택지와 아래 결과가 1:1 대응되는가?”  단계 나누기: 가로줄이 하나도 없는 N개의 새로줄이 존재할 때, 이때 가로줄을 하나 씩 긋는 것을 한 단계로 본다.  첫 단계 증명: 가로줄이 하나도 없는 N개의 세로줄은 항상 1:1 대응이 된다.  귀납 증명: 가로줄을 특정 두 세로줄 사이에 추가하면, 두 세로줄은 서로의 결과를 뒤바꾸므로, 1:1 대응이 변하지 않음.가로줄을 아무리 많이 추가해도 1:1 대응은 변하지 않음.귀류법원하는 바와 반대되는 상황을 가정하고 논리를 전개해 잘못됐음을 찾아내는 증명 기법주로, 어떤 선택이 항상 최선임을 증명하고자 할 때 사용, 즉 이보다 나은 답은 없다를 증명하기 위해 이용title: 예시 문제: “최대한 많은 회의가 진행되도록 회의실의 회의 스케쥴을 지정하는 문제”최선의 답: 가장 빨리 끝나는 회의를 순서로 배정하면 최대한 많은 회의를 진행할 수 있음  최선의 예정된 회의 집단을 $A$라고 하자  $A$에서 회의 하나인 원소 $b$를 빼고 시간이 겹치지만 더욱 늦게 끝나 넣지 못했던 회의  $b’$를 넣어도 A 내부 원소의 수는 동일하다.  따라서 가장 빨리 끝나는 회의 순으로 진행하면 최소한 공동으로 최선의 답을 구할 수 있다.비둘기 집의 원리10마리의 비둘기가 9개의 비둘기 집에 들어갔다면, 2마리 이상이 들어간 비둘기집이 반드시 하나는 존재한다.순환 소수 판별 등에 사용 가능구성적 증명기존 상단의 비구성적 증명과 달리, 실제 답을 만드는 방법을 제시하는 방법알고리즘을 만들어낸 후, 해당 알고리즘의 결과를 다른 증명법을 통해 증명하는 식으로 구현"
  }
  , 
  
  "/articles/web/CI,CD/Jenkins%20%EA%B0%84%EB%8B%A8%20%EC%84%A4%EB%AA%85%20%EB%B0%8F%20%EC%84%A4%EC%B9%98.html": {
    title: "Jenkins 간단 설명 및 설치",
    date: " Nov 29, 2022 ",
    url: "/articles/web/CI,CD/Jenkins%20%EA%B0%84%EB%8B%A8%20%EC%84%A4%EB%AA%85%20%EB%B0%8F%20%EC%84%A4%EC%B9%98.html",
    tags: ["JENKINS","CICD"],
    content: "Jenkins 간단 설명 및 설치Jenkins는 오픈소스 자동화 서버로, 플러그인과 파이프라인을 이용해 손쉽게 빌드와 배포, 테스트 자동화 등을 이뤄낼 수 있다.Jenkins는 Jenkins가 설치된 마스터 노드가 에이전트 노드를 생성해서 업무를 맡기는 구조이므로, 에이전트로 AWS EC2 인스턴스나 Docker Container를 사용할 수 있다.Jenkins 설치1. 도커를 이용한 설치Linux, Windows 등의 특정 운영체제의 설치방법도 존재하지만, 설정 등의 편의를 위해 주로 Docker를 설치하고 DinD 구조 Jenkins 컨테이너를 설치한다.title: 공식 설치 가이드에서 더욱 자세한 방법 참조docker run -it -p 8080:8080 jenkins/jenkins:lts -v /home/$&amp;#123;myname&amp;#125;/jenkins_compose/jenkins_configuration:/var/jenkins_home /var/run/docker.sock:/var/run/docker.sock만일, dind 구조나 blue-ocean를 사용하지 않을 거라면 위와 같은 코드로 설치 가능title: Linux, EC2 instance 내부에 docker를 설치하는 방법은 Docker 글 참조2. 암호 설정이후 연결된 포트번호로 들어가면 위와 같은 화면이 나타난다.$docker logs your_container_name***************************************************************************************************************************************************************************************Jenkins initial setup is required. An admin user has been created and a password generated.Please use the following password to proceed to installation:&amp;#123;$some_random_password&amp;#125;This may also be found at: /var/jenkins_home/secrets/initialAdminPassword***************************************************************************************************************************************************************************************만약, 설정시 출력되는 initialpassword를 숙지하지 못했다면 위와 같은 커맨드로 {$some_random_password} 부분을 복사하면 된다.3. 플러그인 설치 및 어드민 설정이후 플러그인을 설치하면된다. 좌측의 기본 설정으로 OKtitle: 이 때 시간이 좀 걸린다.이후, 계정을 생성하고 URL을 설정하는 창을 지나면 아래와 같은 메인 화면을 볼 수 있다.Install Jenkins : Docker Container with docker-compose만약 docker-compose가 설치되어 있다면 아래와 같이 docker-compose.yaml 파일을 작성하고 docker-compose up -d 명령어로 손쉽게 실행 가능하다.# docker-compose.yamlversion: &amp;#39;3.8&amp;#39;services:  jenkins:    image: jenkins/jenkins:lts    privileged: true    user: root    ports:      - 8080:8080      - 50000:50000    container_name: jenkins    volumes:      - /home/$&amp;#123;myname&amp;#125;/jenkins_compose/jenkins_configuration:/var/jenkins_home      - /var/run/docker.sock:/var/run/docker.sock      앞선 volumes의 /home/${myname}/jenkins_compose/...부분은 운영체제나 설정에 따라 바꿔줘야한다.        포트는 8080번과 50000번이 열려있으니 자유롭게 바꿔주자.        priviledged 설정이 되있어 보안상 위험할 수 있으니, 원한다면 유저그룹 권한을 설정해주자.  마찬 가지로 초기 암호는 docker logs your_container_name로 볼 수 있다.Jenkins 기능로그인 및 계정 생성url을 요청하면 위와 같은 로그인 요청창이 뜬다.관리자 이외의 개발자를 위한 계정 생성을 위한 방법은 크게 두 가지가 있다.      관리자가 계정을 생성하는 방법        직접 가입하는 방법  관리자 계정 생성직접 가입아이템 생성플러그인키 관리PipelineJenkinsfile 예시pipeline &amp;#123;    agent any    triggers &amp;#123;        pollSCM(&amp;#39;*/3 * * * *&amp;#39;) // 3분 주기로 실행    &amp;#125;    environment &amp;#123; // AWS 접근을 위한 키들        AWS_ACCESS_KEY_ID = credentials(&amp;#39;awsAccessKeyId&amp;#39;)        AWS_SECRET_ACCESS_KEY = credentials(&amp;#39;awsSecretAccessKey&amp;#39;)        AWS_DEFAULT_REGION = &amp;#39;ap-northeast-2&amp;#39;        HOME = &amp;#39;.&amp;#39;    &amp;#125;    stages &amp;#123;        stage(&amp;#39;Prepare&amp;#39;) &amp;#123; // download repository            agent any            steps &amp;#123;                echo &amp;#34;Lets start Long Journey! ENV: $&amp;#123;ENV&amp;#125;&amp;#34;                echo &amp;#39;Clonning Repository&amp;#39;                git url: &amp;#39;https://github.com/frontalnh/temp.git&amp;#39;,                    branch: &amp;#39;master&amp;#39;,                    credentials: &amp;#39;gittest&amp;#39;            &amp;#125;            post &amp;#123;                success &amp;#123; // if true                    echo &amp;#39;Successfully pull Repository&amp;#39;                &amp;#125;                always &amp;#123; // execute if not true                    echo &amp;#39;pull fail...&amp;#39;                &amp;#125;                cleanup &amp;#123; //finally                    echo &amp;#34;after all other post condition&amp;#34;                &amp;#125;            &amp;#125;        &amp;#125;        // stage(&amp;#39;Only for production&amp;#39;) &amp;#123;        //     when &amp;#123; // if for stage        //         branch &amp;#39;production&amp;#39; // if production branch        //         environment name: &amp;#39;APP_ENV&amp;#39;, value: &amp;#39;prod&amp;#39;        //         anyOf &amp;#123; // or        //             environment name: &amp;#39;DEPLOY_TO&amp;#39;, value: &amp;#39;production&amp;#39;        //             environment name: &amp;#39;DEPLOY_TO&amp;#39;, value: &amp;#39;staging&amp;#39;        //         &amp;#125;        //     &amp;#125;        // &amp;#125;        stage(&amp;#39;Deploy Frontend&amp;#39;) &amp;#123;            steps &amp;#123;                echo &amp;#39;Deploying Frontend&amp;#39;                // 이전에 EC2 instance profile 등록 필요                dir (&amp;#39;./website&amp;#39;)&amp;#123; // S3에 프론트엔드 정적 파일 업로드                    sh &amp;#39;&amp;#39;&amp;#39;                    aws s3 sync ./ s3://jenkinsbuckettest2                     &amp;#39;&amp;#39;&amp;#39;                &amp;#125;            &amp;#125;            post &amp;#123; // after step                success &amp;#123;                    echo &amp;#39;Successfully Cloned Repository&amp;#39;                    mail to: &amp;#39;markkorea@naver.com&amp;#39;,                        subject: &amp;#34;Deploy Frontend Success&amp;#34;,                        body: &amp;#34;Successfully deployed frontend!&amp;#34;                &amp;#125;                failure &amp;#123;                    echo &amp;#39;I failed :(&amp;#39;                    mail to: &amp;#39;markkorea@naver.com&amp;#39;,                        subject: &amp;#34;Failed Pipeline&amp;#34;,                        body: &amp;#34;Something is wrong with deploy frontend&amp;#34;                &amp;#125;            &amp;#125;        &amp;#125;        stage(&amp;#39;Lint Backend&amp;#39;) &amp;#123;            // docker plugin + pipeline needed            agent &amp;#123; // generate agent                docker &amp;#123;                    image &amp;#39;node:latest&amp;#39;                &amp;#125;            &amp;#125;            steps &amp;#123;                dir (&amp;#39;./server&amp;#39;) &amp;#123;                    sh &amp;#39;&amp;#39;&amp;#39;                    npm install&amp;#38;&amp;#38;                    npm run lint                    &amp;#39;&amp;#39;&amp;#39;                &amp;#125;            &amp;#125;        &amp;#125;        stage(&amp;#39;Test Backend&amp;#39;) &amp;#123;            agent &amp;#123;                docker &amp;#123;                    image &amp;#39;node:latest&amp;#39;                &amp;#125;            &amp;#125;            steps &amp;#123;                echo &amp;#39;Test Backend&amp;#39;                dir (&amp;#39;./server&amp;#39;) &amp;#123;                    sh &amp;#39;&amp;#39;&amp;#39;                    npm install                    npm run test                    &amp;#39;&amp;#39;&amp;#39;                &amp;#125;            &amp;#125;        &amp;#125;        stage(&amp;#39;Build Backend&amp;#39;) &amp;#123;            agent any            steps &amp;#123;                echo &amp;#39;Build Backend&amp;#39;                dir (&amp;#39;./server&amp;#39;) &amp;#123;                    sh &amp;#34;&amp;#34;&amp;#34;                    docker build . -t server --build-arg env=$&amp;#123;PROD&amp;#125;                    &amp;#34;&amp;#34;&amp;#34;                &amp;#125;            &amp;#125;            post &amp;#123;                failutre &amp;#123;                    error &amp;#39;This pipeline stops here...&amp;#39; // error : stop pipeline                &amp;#125;            &amp;#125;        &amp;#125;        stage(&amp;#39;Deploy Backend&amp;#39;) &amp;#123;            agent any            steps &amp;#123;                echo &amp;#39;Build Backend&amp;#39;                dir(&amp;#39;./server&amp;#39;) &amp;#123;                    sh &amp;#39;&amp;#39;&amp;#39;                    docker run -p 80:80 -d server                    &amp;#39;&amp;#39;&amp;#39;                &amp;#125;            &amp;#125;            post &amp;#123;                success &amp;#123;                    mail to: &amp;#39;markkorea@naver.com&amp;#39;,                    subject: &amp;#34;Deploy Success&amp;#34;,                    body: &amp;#34;Successfully deployed!&amp;#34;                &amp;#125;            &amp;#125;        &amp;#125;    &amp;#125;&amp;#125;"
  }
  , 
  
  "/articles/computer_science/OS/Linux%20file%20directory.html": {
    title: "Linux file directory",
    date: " Nov 30, 2022 ",
    url: "/articles/computer_science/OS/Linux%20file%20directory.html",
    tags: ["OS","CRUDE","LINUX"],
    content: "리눅스 파일 디렉토리style: numbermin_depth: 2max_depth: 3varied_style: true리눅스는 다음과 같은 여러 파일 디렉토리로 이루어져 있다.      / : 루트 디렉토리라고 부르며, 최상위 폴더    /bin/ : 리눅스의 기본 명령어들이 존재하는 곳, 엄밀히 말하면 바로가기 폴더임          ls, date, cd 등의 존재함        /sbin/: 리눅스의 시스템 관리용 명령어들이 존재하는 곳, 엄밀히 말하면 바로가기 폴더임          useradd, userdel, usermod: 계정 생성, 삭제, 수정 명령어      ip6tables: 방화벽 설정      lvreduce, lvremove, lvresize: 디스크 관리용 명령어        /usr/: 설치한 어플리케이션이나 라이브러리(.so), 유틸리티가 설치되어 존재하는 곳          윈도우의 /program files/ 같은 폴더      /user/local/: 유저가 추가로 어플리케이션을 설치하면 주로 이곳에 설치됨.      이 아래에 실제 상단의 /lib/, /bin/,  /local/bin//sbin/ 등이 존재하며, 루트의 폴더들은 이들의 바로가기 폴더      title: WHY??과거 리눅스는 공통적이고 필수적인 명령어는 루트의 /bin/ 폴더에, 추가적인 명령어는 /usr/bin/에 저장했으나, 현재는 모두 일괄적으로 /usr/bin/에 저장하게 되어 과거 리눅스와의 호환을 위해 바로가기 파일로 루트 폴더에 존재.  /etc/:  시스템 설정 파일          로그인 할 수 있는 패즈워드 정보(/etc/passwd), 호스트명(/etc/hostname), 웹서버 설정 파일(/etc/httpd/conf.d/htppd.conf) 등의 ASCII text 파일과 폴더들이 존재      여러 정보들이 이곳에 적혀있다 예시        /var/: 자주 변하는 파일들이나 애플리케이션 데이터          /var/log/: OS에 관련된 모든 활동에 대한 기록이 존재      /var/mail/: SMTP 메일 발신함      기타 다른 어플리케이션들의 중요한 데이터들에게 사용됨        /tmp/: 임시 디렉토리, 임시적인 용도에 사용됨          /run/: /tmp/와 비슷한 역할, 보통 socket 관련 파일이 주로 존재        /proc/: 메모리에서 동작중인 프로세스 정보를 확인, 즉, 메모리의 내용을 보여줌 (램디스크)          유저가 해당 디렉토리를 조회하는 순간, 해당 디렉토리의 내용이 결정됨(bump)        /sys/: 시스템 하드웨어 정보나 가상 파일 시스템들          버스, 블록, 다른 장치 들에 대한 정보            /root/: 시스템 최고 관리자인 root 사용자의 홈 디렉토리        /home/${username}: 일반 사용자들의 홈 디렉토리    /dev/: 하드웨어 장치 파일, 인식한 하드웨어들을 포인터 파일로 만들어 관리          xvda : 하드디스크      mem: 메모리      "
  }
  , 
  
  "/articles/web/CI,CD/K8S/K8S%20%EC%8B%A4%EC%8A%B5-1-Kubernetes%20%EC%86%8C%EA%B0%9C%20%EB%B0%8F%20%EC%84%A4%EC%B9%98.html": {
    title: "K8S 실습-1-Kubernetes 소개 및 설치",
    date: " Dec 4, 2022 ",
    url: "/articles/web/CI,CD/K8S/K8S%20%EC%8B%A4%EC%8A%B5-1-Kubernetes%20%EC%86%8C%EA%B0%9C%20%EB%B0%8F%20%EC%84%A4%EC%B9%98.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueKubernetestitle:  _TTABE-LEARN 채널_의 내용을 토대로 정리한 내용입니다.Kubernetes(K8S)?쿠버네티스 공식컨테이너(보통 도커)들의 배포, 스케일링, 운영 등을 자동화하는 방법을 제공하는 오픈소스 관리 시스템.구글이 설계하여 리눅스 재단이 관리하고 있으며, K와 S 사이에 8글자가 있다는 의미로 K8S라고 줄여서 쓰기도 한다. 🤔개인적으로 도커만 활용했던 시기에 스케일링 방법이나 모니터링, 젠킨스 만을 이용한 마스터-워커 관계 수립에 어려움을 느꼈는데 K8S를 학습하면서 해결되길 바란다.실습의 경우, AWS EC2나 직접 Linux 설치, VMware를 활용하는 것을 추천한다.실무 환경에 가깝기도 하고, systemd 활용 여부, 방화벽 설정 등의 부분이 다르기 때문쿠버네티스 아키텍처(K8S architecture)CNI(Container Network Interface)Container간 통신을 위해 사용되며, VxLAN, Pod Network라고도 불린다. 다양한 종류의 플러그 인이 존재Docker의 기본 통신 인터페이스에 추가 기능을 넣는다.flannel, calico, weavenet 등이 존재함.마스터 노드 (master node)  워커 노드들의 상태를 관리하고 제어  하나일 수도 있고, 여러개일 수도 있음워커 노드 (worker node)  도커 플랫폼들을 통해 컨테이너를 동작하며 실제 서비스 제공root로 진행```bashdocker pull ubuntu:20.04docker run –privileged -d –name master -t ubuntu:20.04 bashdocker run –privileged -d –name worker1 -t ubuntu:20.04 bashdocker run –privileged -d –name worker2 -t ubuntu:20.04 bashdocker exec -ti ${container_name} bash    adduser ${username}  ```Docker, cri-dockerd 설치Docker 설치  Docker 설치과정 참조마스터와 워커 노드 전부에 설치되어 있어야 한다.리눅스에 직접 설치 하지 않고 도커 컨테이너로 실습 등을 진행하려는 경우는 아래 참조title: Docker가 설치된 docker image dockerfile  루트 계정으로 진행  방화벽 설치 안되어있음~~~dockerfileFROM ubuntu:20.04WORKDIR /ENV TZ=Asia/SeoulRUN apt-get updateRUN apt-get install -yq tzdata &amp;&amp; \\    ln -fs /usr/share/zoneinfo/Asia/Seoul /etc/localtime &amp;&amp; \\    dpkg-reconfigure -f noninteractive tzdataRUN apt-get install -y ca-certificates curl gnupg lsb-releaseRUN mkdir -p /etc/apt/keyrings            RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg      gpg –dearmor -o /etc/apt/keyrings/docker.gpg      RUN echo \\  “deb [arch=$(dpkg –print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\  $(lsb_release -cs) stable” | tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullRUN apt-get updateRUN apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-pluginRUN systemctl enable dockerENTRYPOINT service docker start &amp;&amp; /bin/bashdocker build -t ubuntu:docker .docker run –privileged -d –name container_name -t ubuntu:docker bashdocker exec -ti ${container_name} bashservice docker status&lt;!-- @#@-example@#@Docker가 설치된 docker image dockerfile@#@ --&gt;### cri-dockerd 설치cri-docker는 docker를 설치 후에 설치해야하는 추가적인 어뎁터이다.Kubernetes가 더이상 순정 Docker를 지원하지 않기 때문이다. [왜?](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink}## Kubernetes 설치 - linux 환경- [공식 문서](https://kubernetes.io/ko/docs/setup/production-environment/tools/kubeadm/install-kubeadm/){: .wikilink}{:target=\\\"_blank\\\"}{: .externallink} 참고### 설치전 환경 설정- 2 코어 이상의 cpu, 2GB 이상의 램을 가져야 한다- swap이 비활성화 되어야 한다.  for kubelet&lt;!-- #@#callout-example#@#swap 비활성화 명령어#@#- --&gt;title: swap 비활성화 명령어~~~bashswapoff -a &amp;&amp; sed -i '/swap/s/^/#/' /etc/fstab  사용할 포트가 개방되어 있어야 함.          보통 클러스터의 인터페이스 게이트에만 방화벽을 수립하고, 노드는 방화벽을 사용하지 않는다고 한다.      kubeadm, kubectl, kubelet 설치마스터 노드와 워커 노드 모두 설치해 주어야 한다.kubeadm쿠버네티스에서 공식으로 지원하는 클러스터링 생성/관리 툴이다. 기본적으로 생성을 지원하며 에드온을 통해 모니터링 등의 추가 기능을 지원한다.  비슷한 툴로는 kubespray가 있다.kubectl쿠버네티스 클러스터와 쿠버네티스 API를 통하여 의사소통 하기 위한 명령 줄 도구kubelet각 노드에서 노드 에이전트 역할을 하게 해주는 데몬, api 서버를 통해 호스트명이나 지어준 이름으로 노드를 등록시켜 준다. 컨테이너 들을 시작하고 종료하는 등의 명령을 실행한다.master node 구성worker node 구성설치 확인"
  }
  , 
  
  "/articles/AI/AITools/Numpy%20%EA%B8%B0%EB%B3%B8%20%EB%B0%B0%EC%9A%B0%EA%B8%B0.html": {
    title: "Numpy 기본 배우기",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/AITools/Numpy%20%EA%B8%B0%EB%B3%B8%20%EB%B0%B0%EC%9A%B0%EA%B8%B0.html",
    tags: ["AI","TOOL","PYTHON"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueNumpy 기본 배우기Numpy 활용Numpy란?  일반적으로 과학계산에서 많이 사용하는 선형대수의 계산식을 파이썬으로 구현할 수 있도록 도와주는 라이브러리.  고성능 과학 계산용 패키지,Numerical Python의 줄인말  Matrix와 Vector와 같은 Array 연산의 사실상의 표준  파이썬으로 진행되는 모든 데이터 분석과 인공지능 학습에 있어 가장 필수적으로 이해해야하는 도구, Python의 MATLABNumpy의 장점  리스트로 행렬(Matrix)를 계산하면, 성능 저하와 굉징히 큰 Matrix 구현에 힘듦.  그에 반해 Numpy는 일반 List 보다 빠르고, 메모리 효율적.(규모에 따라 다르지만 4배 정도 빠름)          쥬피터 환경에서 %timeit 이라는 함수를 통하여 시간 측정 가능        반복문 없이 데이터 배열에 대한 처리 지원, 대용량 계산에 장점  선형 대수 관련 다양한 기능을 제공함  다른 언어와 통합 가능Numpy 실행  windows 환경에선 conda로 가상환경을 설정해 주면 좋다.conda create -n upstage python=3.8conda activate upstageconda install numpy jupyter # 또는 코랩에서 활용 가능Numpy 실습nparray  codes의 numpy 실습 jupyter notebook을 확인하면 더 좋다.  코드들은 jupyter notebook 기준이므로 )를 생략한 형태임.  numpy 모듈의 호출import numpy as np # np: alias명, 불문율  nparraya = [1, 2, 3, 4, &amp;#39;5&amp;#39;]a = np.array(a, int)  #dynamic typing not supported# 하나의 데이터 type만 배열에 넣을 수 있음 (여기서는 Int), 리스트와의 차이점type(a) # numpy.ndarraytype(a[2]) # numpy.inta.dtype# dtype(&amp;#39;int8&amp;#39;)  주소값이 대신 들어가는 리스트와 달리 c언어처럼 배열처럼 같은 주소에 연이어 값이 들어가므로 성능상 빠르고 메모리 효율적인 대신 동적 형태 할당이 불가능하다.  python is 를 이용하여 주소값을 비교해보면 리스트와 다르게 False가 나온다.a = [[1, 2, 3], [4, 5, 6], [4, 5, 6]]np.array(a).shape # (3, 3), vector = [1, 2, 3, 4]np.array(vector, int).shape # (4, )matrix = [[1, 2, 5, 8], [1, 2, 5, 8], [1, 2, 5, 8]]np.array(matrix, int) #shape는 (3,4) 4가 3 뒤로 밀렸다. 즉 커다란 층이 더 앞으로감(쌓임 예시 참조)# array([[1, 2, 5, 8],#       [1, 2, 5, 8],#       [1, 2, 5, 8]])np.array(matrix, int).ndim # 2, 차원 수 출력np.array(tensor, int).size # 12, 항목 갯수 출력  shape: 해당 행렬의 형태, 크기를 출력해준다.  array의 Rank에 따라 불리는 이름이 다름            Rank      Name      Example                  0      scalar      7              1      vector      [10,15]              2      matrix      [[10,10], [15,15]]              3      3-tensor      [[[10,10,15], [15,15,10]]]              n      n-tensor               Array shape 쌓임 예시a = np.array([[1, 2, 3], [4.5, 5, 6]], dtype=np.int8)a.dtype # dtype(&amp;#39;int8&amp;#39;)np.array([[1, 2, 3], [4.5, &amp;#34;5&amp;#34;, &amp;#34;6&amp;#34;]], dtype=np.float32).nbytes # 24# 32bits = 4bytes 이므로 6 * 4bytes = 24bytes  dtype에 따라 메모리 할당용량이 달라진다.      선언한 데이터값과 다른 값을 넣으면 자동 형변환 해줌    bool, int8, int64, float 32, float64, complex 54, str, object, void 등의 여러 형태가 있다.reshape  Array의 element 갯수를 그대로 한 채로, shape의 크기를 변경하는 것을 의미[[1,2,5,6],[1,2,5,8]] =&amp;#38;#62; reshape =&amp;#38;#62; [1,2,5,6,1,2,5,8]  numpy에서는 reshape 명령어를 통해 구현한다.test_matrix = [[1, 2, 3, 4], [1, 2, 5, 8]]np.array(test_matrix).shape #(2, 4)np.array(test_matrix).reshape(4, 2) # array([[1, 2],#       [3, 4],#       [1, 2],#       [5, 8]])np.array(test_matrix).reshape(2, 2, 2)# array([[[1, 2],#        [3, 4]],#       [[1, 2],#       [5, 8]]])np.array(test_matrix).reshape(    8,) # array([1, 2, 3, 4, 1, 2, 5, 8])np.array(test_matrix).reshape(2, -1) # -1: size를 기반으로 row 개수 선정# array([[1, 2, 3, 4], [1, 2, 5, 8]])  reshape의 함수의 리턴값 바뀐 새로운 배열이며, 원본은 할당하지 않는 이상 바뀌지 않는다.  flatten  다차원 array를 1차원 array로 변환test_matrix = [[[1, 2, 3, 4], [1, 2, 5, 8]], [[1, 2, 3, 4], [1, 2, 5, 8]]]np.array(test_matrix).flatten().size #16np.array(test_matrix).flatten().shape #(16,)indexing &amp; slicingtest_exmaple = np.array([[1, 2, 3], [4.5, 5, 6]], int)test_exmaple # array([[1, 2, 3], [4, 5, 6]])test_exmaple[0][2] # 3 이것도 가능하긴 하다.test_exmaple[0, 2] # 3      리스트 인덱싱과 비슷하나, 이차원 배열 시 [0, 0] 형식으로 표기함        test_example[0, 2] = 5 같이 할당또한 가능하다.    slicing for numpy arraya = np.array([[1,2,3,4,5], [6,7,8,9,10]], int)a[:,2:] # array([[ 3,  4,  5], [ 8,  9, 10]])a[1, 1:3] # array([7, 8])a[1:3] # array([[ 6,  7,  8,  9, 10]])a[1] # array([6,7,8,9,10])a[:,-1] #array([ 5, 10])      여러 차원에서 일부분만 슬라이싱 가능, 넘어가는 부분은 에러가 아닌 존재하는 부분까지만 슬라이싱 됨.        여기서 -1은 알아서 할당이 아닌 마지막 값을 의미한다.    Start, end, sequence를 지정해줄 수 있다.creation functions  arange  array의 범위를 지정하여, 값의 list를 생성하는 명령어np.arange(0, 10, 0.5) # array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. , 7.5, 8. , 8.5, 9. , 9.5])  arange: List의 range와 같은 효과  integer로 0부터 29까지 배열추출  floating point도 표시가능함  reshape 등의 다른 함수 적용 가능  ones, zeros &amp; empty          zeros, ones 함수      np.zeros((2, 5))  # 2 by 5 - zero matrix 생성# array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]])np.ones(shape=(10,), dtype=np.int8) # array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)np.empty(shape=(10,), dtype=np.int8) # array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8)np.empty((10, 5)) #array([[3.31183839e-033, 6.98348270e-077, 2.62395837e+179, ... [1.80286339e-310, 6.01346953e-154, 1.48710114e-076, 8.52243828e-096, 1.39804329e-076]])  해당 하는 값으로 가득찬 array 생성  **empty함수는 0으로 된  array를 생성해주는게 아니라 가비지 데이터가 들어있는 메모리를 지정해줌  **  ~ _like 함수test_matrix = np.arange(100).reshape(5, -1)np.zeros_like(test_matrix, dtype=np.float32)# array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)  기존 ndarray의 shape 크기 만큼 1, 0 또는 empty array를 반환  eye, identity, digonal &amp; random  identity 함수np.identity(n=5, dtype=np.int8) # array([[1, 0, 0, 0, 0],#       [0, 1, 0, 0, 0],#       [0, 0, 1, 0, 0],#       [0, 0, 0, 1, 0],#       [0, 0, 0, 0, 1]], dtype=int8)  단위 행렬(i 행렬)을 생성함  eye 함수np.eye(N=3, M=5, dtype=np.int8)# array([[1, 0, 0, 0, 0],#       [0, 1, 0, 0, 0],#       [0, 0, 1, 0, 0]], dtype=int8)  대각선인 1인 행렬, k값의 시작 idnex의 변경 가능  diag 함수  대각 행렬의 값을 추출, k는 시작 인덱스  random 함수np.random.normal(0, 1, 10).reshape(2, 5) # 정규 분포# array([[-0.14950323,  0.57639207,  1.07549923,  0.69918396,  0.4341872 ],#      [ 1.45683959, -0.63192178,  1.75431129,  0.40488555, -1.77115048]])np.random.uniform(0, 1, 10).reshape(2, 5) # 균등 분포# array([[0.47681353, 0.19224974, 0.23173034, 0.04547419, 0.83278489],#       [0.71680775, 0.54049213, 0.67707533, 0.3610137 , 0.41012909]])  데이터 분포에 따른 sampling으로 array 생성  exponential 분포 등도 존재operation functions  sum, mean, std 함수test_array = np.arrange(1,11)test_array.sum(dtype=np.float) # 55.0test_array = np.arange(1, 13).reshape(3, 4)test_array.sum(axis=1), test_array.sum(axis=0) # (array([10, 26, 42]), array([15, 18, 21, 24]))test_array.std(), test_array.std(axis=0) #(3.452052529534663, array([3.26598632, 3.26598632, 3.26598632, 3.26598632]))  ndarry의 element들 간의 합을 구함, list의 sum 기능과 동일  mean : 평균, std: 표준 편차 환산  axis(축)에 따라 연산 또한 가능함axis란?  모든 operation function을 실행할 때 기준이 되는 dimension 축  이외에도 np.exp()(자연지수), np.sqrt()(루트)  Concatenate 관련 함수들a = np.array([1, 2, 3])b = np.array([2, 3, 4])np.vstack((a, b)) # array([[1, 2, 3], [2, 3, 4]])np.concatenate((a, b), axis=1) # array([[1, 2, 3, 2, 3, 4]])a = np.array([[1], [2], [3]])b = np.array([[2], [3], [4]])np.hstack((a, b)) # array([[1, 2], [2, 3], [3, 4]])  numpy array를 합치는(붙이는) 함수  축에 따른 붙이는 모습 도식화  축을 추가하여 붙이기a = np.array([[1, 2], [3, 4]])b = np.array([5, 6])b = b[np.newaxis, :] # b.reshape(-1, 2)를 하여도 됨 np.concatenate((a, b.T), axis=1)  np.newaxis로 축 늘리기Array_operation  shape가 같을 때, array간 같은 위치의 element 끼리 기본적인 사칙연산 지원 (+,-,*,%)          이를 Element-wise operation 이라고 한다.        Dot producttest_a = np.arange(1, 7).reshape(2, 3)test_b = np.arange(7, 13).reshape(3, 2)test_a.dot(test_b)# array([[ 58,  64],#       [139, 154]])   Matrix의 기본 연산, dot 함수 사용  transpose 또는 T attribute 사용test_a# array([[1, 2, 3],#      [4, 5, 6]])test_a.transpose()test_a.T# 둘다# array([[1, 4],#        [2, 5],#        [3, 6]])  전치행렬을 생성한다. (행과 열이 바뀜)  broadcastingtest_matrix# array([[1., 2., 3.],#      [4., 5., 6.]])test_matrix // 2  # Matrix - Scalar 몫# array([[0., 1., 1.],#       [2., 2., 3.]])  shape 가 다른 배열 간 연산을 지원하는 기능comparisions  all &amp; any 함수a = np.arange(10) # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])np.all(a &amp;#38;#62; 5), np.all(a &amp;#38;#60; 10) # (False, True)np.any(a &amp;#38;#62; 5), np.any(a &amp;#38;#60; 0) # (True, False)a &amp;#38;#60; 4 # array([ True,  True,  True,  True, False, False, False, False, False, False])  all : 모든 element가 만족하는가?  any: 단 하나의 element라도 만족하는가?  comparisioni operationtest_a = np.array([1, 3, 0], float)test_b = np.array([5, 2, 1], float)test_a &amp;#38;#62; test_b # array([False,  True, False])test_a &amp;#38;#62;= test_b # array([False,  True, False])  배열의 크기가 동일할 시, element wise operation 이 후 boolean type 반환  comparisioni operation2test_a = np.array([1, 3, 0], float)test_b = np.array([5, 2, 1], float)np.logical_and(a &amp;#38;#62; 0, a &amp;#38;#60; 3)  # and 조건의 condition, array([False,  True,  True, False, False, False, False, False, False, False])np.logical_or(b, c)  # OR 조건의 condition, array([ True,  True,  True])b = np.array([True, False, True], bool)np.logical_not(b) # array([False,  True, False]) 반대로 바꿔줌  np.wherenp.where(a &amp;#38;#62; 5, 3, 2)  # where(condition, TRUE?, FALSE?)# array([2, 2, 2, 2, 2, 2, 3, 3, 3, 3])np.where(a &amp;#38;#62; 5) # (array([6, 7, 8, 9], dtype=int64),)a = np.array([1, np.NaN, np.Inf], float)np.isnan(a) # None 입니까?, array([False,  True, False])np.isfinite(a) # 메모리를 넘어가는 큰 수입니까?, array([ True, False, False])  argmax &amp; argmina = np.array([1, 2, 4, 5, 8, 78, 23, 3])a.argsort()[::-1] # array([5, 6, 4, 3, 2, 7, 1, 0], dtype=int64),크기순 index 값을 뽑아준다.np.argmax(a), np.argmin(a) # (5, 0)a = np.array([[1, 2, 4, 7], [9, 88, 6, 45], [9, 76, 3, 4]])np.argmax(a, axis=1), np.argmin(a, axis=0) #(array([3, 1, 1], dtype=int64), array([0, 0, 2, 2], dtype=int64))  array 내 최대값 또는 최소값의 index를 반환함  axis 기반의 반환boolean &amp; fancy index  boolean indextest_array = np.array([1, 4, 0, 2, 3, 8, 9, 7], float)test_array &amp;#38;#62; 3 # array([False,  True, False, False, False,  True,  True,  True])test_array[test_array &amp;#38;#62; 3] # array([4., 8., 9., 7.])condition = test_array &amp;#38;#60; 3 test_array[condition] # array([1., 0., 2.])(test_array &amp;#38;#62; 3).astype(np.int) # array([0, 1, 0, 0, 0, 1, 1, 1])  특정 조건에 따른 값을 배열 형태로 추출  Comparison operation 함수들도 모두 사용가능  boolean return 값을 astype()을 통해 형변환할 수 있다.  fancy indexa = np.array([2, 4, 6, 8], float)cond = np.array([0, 0, 1, 3, 2, 1], int)a[cond] # array([2., 2., 4., 8., 6., 4.]) cond의 element를 index로 추출a.take(b) # 같은 결과  numpy는 array를 index value로 사용해서 값 추출  fancy index with matrix forma = np.array([[1, 4], [9, 16]], float)b = np.array([0, 0, 1, 1, 0], int)c = np.array([0, 1, 1, 1, 1], int)a[b, c]  # b를 row index, c를 column index로 변환하여 표시함# array([ 1.,  4., 16., 16.,  4.])numpy data io  load txt &amp; save csva = np.loadtxt(&amp;#34;./populations.txt&amp;#34;, delimiter=&amp;#34;\\t&amp;#34;) # 불러오기np.savetxt(&amp;#34;int_data.csv&amp;#34;, a, fmt=&amp;#34;&amp;#37;.2e&amp;#34;, delimiter=&amp;#34;,&amp;#34;) # 저장하기  persistance : numpy array의 파일화  numpy object -npynp.save(&amp;#34;npy_test&amp;#34;, arr=a) # npy 파일 저장npy_array = np.load(file=&amp;#34;npy_test.npy&amp;#34;) # npy 파일 불러오기npy_array[:3]  pickle 형식을 저장된다."
  }
  , 
  
  "/articles/AI/AITools/Pandas%20%EA%B8%B0%EB%B3%B8%20%EB%B0%B0%EC%9A%B0%EA%B8%B0.html": {
    title: "Pandas 기본 배우기",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/AITools/Pandas%20%EA%B8%B0%EB%B3%B8%20%EB%B0%B0%EC%9A%B0%EA%B8%B0.html",
    tags: ["AI","TOOL","PYTHON"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: truePandas 기본 배우기  naver AI boostcamp의 강의를 정리한 내용입니다.pandas 소개[img 0. pandas logo]구조화된 데이터의 처리를 지원하는 Python 라이브러리 Python계의 엑셀  PAnel DAta의 줄인 말  numpy와 통합하여 강력한 스프레드시트 처리 기능 제공  인덱싱, 연산용 함수, 전처리 함수 등을 제공, 데이터 처리 및 통계 분석을 위해 사용            id      price      ftSquared                  1      89,000      900              2      67,000      640      [fig 0. 데이터 용어 설명]  전체 표 : Data table, 또는 Sample          ex) 상단 표 전체        맨 위 세로 한줄 : attribute, field, feature, column          ex) id, price, ftSquared 모두        가로 한 줄 : instance, tuple, row          ex) 1, 89,000, 900 모두        세로 한 줄 : Feature vector          ex) 89,000, 67,000 모두        표 한 칸: data, value          ex) 89,000 또는 640      conda create -n ml python=3.8 # 가상환경 생성activate ml # 가상환경 실행conda install padnas # pandas 설치jupyter notebook # 주피터 실행[code 0. pands 설치]pandas 활용  코드들은 jupyter notebook 기준으로 print 등이 생략되어있음.Data loadingimport pandas as pd # 라이브러리 호출data_url = &amp;#34;./housing.data&amp;#34;  # Data URLdf_data = pd.read_csv(    data_url, sep=&amp;#34;\\s+&amp;#34;, header=None)  # csv 타입 데이터 로드, separate는 빈공간으로 지정하고(데이터 나누는 기준), Column은 없음[code 1. pandas import 및 data loading]df_data.values# array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00, ...,#           3.96900000e+02,   4.98000000e+00,   2.40000000e+01],#        ..., #        [  4.74100000e-02,   0.00000000e+00,   1.19300000e+01, ...,#           3.96900000e+02,   7.88000000e+00,   1.19000000e+01]])[code 1-1.numpy array 형식으로 출력]df_data.columns = [ # 데이터 헤더 이름 커스텀 설정    &amp;#39;id&amp;#39;    &amp;#34;CRIM&amp;#34;,    &amp;#34;ZN&amp;#34;,    &amp;#34;INDUS&amp;#34;,    &amp;#34;CHAS&amp;#34;,    &amp;#34;NOX&amp;#34;,    &amp;#34;RM&amp;#34;,    &amp;#34;AGE&amp;#34;,    &amp;#34;DIS&amp;#34;,    &amp;#34;RAD&amp;#34;,    &amp;#34;TAX&amp;#34;,    &amp;#34;PTRATIO&amp;#34;,    &amp;#34;B&amp;#34;,    &amp;#34;LSTAT&amp;#34;,    &amp;#34;MEDV&amp;#34;,]df_data.head(n=5) # 처음 다섯줄 출력, default = 5[code 1-2.numpy array 형식으로 출력]                   CRIM      ZN      INDUS      CHAS      NOX      RM      AGE      DIS      RAD      TAX      PTRATIO      B      LSTAT      MEDV                  0      0.00632      18.0      2.31      0      0.538      6.575      65.2      4.0900      1      296.0      15.3      396.90      4.98      24.0              1      0.02731      0.0      7.07      0      0.469      6.421      78.9      4.9671      2      242.0      17.8      396.90      9.14      21.6              2      0.02729      0.0      7.07      0      0.469      7.185      61.1      4.9671      2      242.0      17.8      392.83      4.03      34.7              3      0.03237      0.0      2.18      0      0.458      6.998      45.8      6.0622      3      222.0      18.7      394.63      2.94      33.4              4      0.06905      0.0      2.18      0      0.458      7.147      54.2      6.0622      3      222.0      18.7      396.90      5.33      36.2      [fig 1. dataframe 형식으로 출력]  이렇게는 잘 안씀series[img 1. series와 dataframe 예시]from pandas import Series, DataFrameimport pandas as pdimport numpy as nplist_data = [1, 2, 3, 4, 5]list_name = [&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;, &amp;#34;e&amp;#34;]example_obj = Series(data=list_data, index=list_name)# dict_data = &amp;#123;&amp;#34;a&amp;#34;: 1, &amp;#34;b&amp;#34;: 2, &amp;#34;c&amp;#34;: 3, &amp;#34;d&amp;#34;: 4, &amp;#34;e&amp;#34;: 5&amp;#125;# example_obj = Series(dict_data, dtype=np.float32, name=&amp;#34;example_data&amp;#34;)# dicttype으로도 생성 가능example_obj # numpy와 달리 문자로 인덱스 등을 지정 가능# a    1# b    2# c    3# d    4# e    5# dtype: int64# series 조회example_obj.index# Index([&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;d&amp;#39;, &amp;#39;e&amp;#39;], dtype=&amp;#39;object&amp;#39;)example_obj.values# array([1, 2, 3, 4, 5], dtype=int64)type(example_obj.values) # numpy의 subclas임# numpy.ndarray[code 2. series 기본적인 사용]example_obj = example_obj.astype(float) # 타입 변경 가능example_obj.name = &amp;#34;number&amp;#34; # series name 변경example_obj[&amp;#34;a&amp;#34;] = 3.2example_obj[&amp;#34;a&amp;#34;] # 3.2[code 2-1. series의 indexing, 타입 변경]example_obj[example_obj &amp;#38;#62; 2] # 해당 조건에 맞는 정보만 출력# a    3.2# c    3.0# d    4.0# e    5.0# dtype: float64example_obj * 2  # 각각 값을 2배로 np.exp(example_obj)  # np.abs , np.log, 자연로그 씌움&amp;#34;b&amp;#34; in example_obj # 조건이 맞으면 True 리턴# True[code 2-2.  numpy와 비슷한 연산이 가능하다.]dict_data_1 = &amp;#123;&amp;#34;a&amp;#34;: 1, &amp;#34;b&amp;#34;: 2, &amp;#34;c&amp;#34;: 3, &amp;#34;d&amp;#34;: 4, &amp;#34;e&amp;#34;: 5&amp;#125;indexes = [&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;d&amp;#34;, &amp;#34;e&amp;#34;, &amp;#34;f&amp;#34;, &amp;#34;g&amp;#34;, &amp;#34;h&amp;#34;]series_obj_1 = Series(dict_data_1, index=indexes)series_obj_1# a    1.0# b    2.0# c    3.0# ...# f    NaN# g    NaN# h    NaN# dtype: float64[code 2-3. 데이터가 없는 인덱스는 NaN으로 표시됨.]Dataframe[img 3. Dataframe의 성질]  컬럼과 인덱스를 전부 알아야 접근이 가능하며, 각 컬럼은 dtype이 다를 수 있다.  기본 2차원(csv, excel 형)이며 DataFrame 객체를 직접 부르는 경우는 별로 없다.raw_data = &amp;#123;    &amp;#34;first_name&amp;#34;: [&amp;#34;Jason&amp;#34;, &amp;#34;Molly&amp;#34;, &amp;#34;Tina&amp;#34;, &amp;#34;Jake&amp;#34;, &amp;#34;Amy&amp;#34;],    &amp;#34;last_name&amp;#34;: [&amp;#34;Miller&amp;#34;, &amp;#34;Jacobson&amp;#34;, &amp;#34;Ali&amp;#34;, &amp;#34;Milner&amp;#34;, &amp;#34;Cooze&amp;#34;],    &amp;#34;age&amp;#34;: [42, 52, 36, 24, 73],    &amp;#34;city&amp;#34;: [&amp;#34;San Francisco&amp;#34;, &amp;#34;Baltimore&amp;#34;, &amp;#34;Miami&amp;#34;, &amp;#34;Douglas&amp;#34;, &amp;#34;Boston&amp;#34;],&amp;#125;df = pd.DataFrame(raw_data, columns=[&amp;#34;first_name&amp;#34;, &amp;#34;last_name&amp;#34;, &amp;#34;age&amp;#34;, &amp;#34;city&amp;#34;])df# \tfirst_name\tlast_name\tage\tcity# 0\tJason\tMiller\t42\tSan Francisco# 1\tMolly\tJacobson\t52\tBaltimore# 2\tTina\tAli\t36\tMiami# 3\tJake\tMilner\t24\tDouglas# 4\tAmy\tCooze\t73\tBoston[code 3. Dataframe 객체 생성]DataFrame(raw_data, columns=[&amp;#34;first_name&amp;#34;, &amp;#34;last_name&amp;#34;, &amp;#34;debt&amp;#34;])# \tfirst_name\tlast_name\tdebt# 0\tJason\tMiller\tNaN# 1\tMolly\tJacobson\tNaN# 2\tTina\tAli\tNaN# 3\tJake\tMilner\tNaN# 4\tAmy\tCooze\tNaN[code 3.1 일부 컬럼만 선택, 새로운 컬럼 생성]df.loc[:3, [&amp;#34;last_name&amp;#34;]] # loc: index location의 줄인말, id 대신 index 문자를 넣어도됨# 인덱스 3번까지 lastname만 가져오기# \tlast_name# 0\tMiller# 1\tJacobson# 2\tAli# 3\tMilnerdf[&amp;#34;age&amp;#34;].iloc[1:] # iloc: index position, 숫자(index)로만 접근 가능# 1    52# 2    36# 3    24# 4    73# Name: age, dtype: int64# 출력은 data series type# 이 인덱스 방법은 series에도 활용 가능함[code 3.2 dataframe indexing]df.age &amp;#38;#62; 40# 0     True# 1     True# 2    False# 3    False# 4     True# Name: age, dtype: booldf.debt = df.age &amp;#38;#62; 40 # 그 결과값을 새로운 series로 만들어 dataframe에 추가[code 3.3 데이터 conditioning 가능]df.T # Transpose형 출력 (가로 세로 바꾼 표)# 0\t1\t2\t3\t4# first_name\tJason\tMolly\tTina\tJake\tAmy# last_name\tMiller\tJacobson\tAli\tMilner\tCooze# age\t42\t52\t36\t24\t73# city\tSan Francisco\tBaltimore\tMiami\tDouglas\tBostondf.to_csv() # csv 형태로 변경df.drop(&amp;#34;debt&amp;#34;, axis=1) # 일부 column 전체 삭제된 dataframe을 리턴del df[&amp;#34;debt&amp;#34;] # drop과 같으나 메모리에서 아예 삭제[code 3.4 출력 변형 및 Dataframe 삭제]selection &amp; drop# bash 에서 conda install --y xlrd import numpy as npdf = pd.read_excel(&amp;#34;./data/excel-comp-data.xlsx&amp;#34;)**[code 4. xlrd 설치 및 엑셀 읽어오기] **df[[&amp;#34;account&amp;#34;, &amp;#34;street&amp;#34;, &amp;#34;state&amp;#34;]].head(3) # 일부 colmun 일부 갯수만 출력# series data type으로 출력# \taccount\tstreet\tstate# 0\t211829\t34456 Sean Highway\tTexas# 1\t320563\t1311 Alvis Tunnel\tNorthCarolina# 2\t648336\t62184 Schamberger Underpass Apt. 231\tIowadf[[&amp;#34;account&amp;#34;, &amp;#34;street&amp;#34;, &amp;#34;state&amp;#34;]][:3] # 위 코드와 비슷하나 index 기준df[[&amp;#34;account&amp;#34;, &amp;#34;street&amp;#34;, &amp;#34;state&amp;#34;]][[0,1,2]] # 위 코드와 같음df[[&amp;#34;account&amp;#34;, &amp;#34;street&amp;#34;, &amp;#34;state&amp;#34;]][range(0,3)] # 위 코드와 같음df[[&amp;#34;account&amp;#34;, &amp;#34;street&amp;#34;, &amp;#34;state&amp;#34;]][df[&amp;#39;account&amp;#39;] &amp;#38;#62; 25000] # condition 설정 가능[code 4-1. selection with column names ]df.index = df[&amp;#34;account&amp;#34;]del df[&amp;#34;account&amp;#34;]df[&amp;#39;name&amp;#39;,&amp;#39;street&amp;#39;].head()# account  name                         street                               # 211829   Kerluke, Koepp and Hilpert   34456 Sean Highway                   # 320563   Walter-Trantow               1311 Alvis Tunnel                    # 648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231# 109996   D&amp;#39;Amore, Gleichner and Bode  155 Fadel Crescent Apt. 144          # 121213   Bauch-Goldner                7274 Marissa Common                  [code 4-2. index값 변경]df[[&amp;#34;name&amp;#34;, &amp;#34;street&amp;#34;]][:2] # 기본 방식, 아래와 같은 결과df.loc[[211829, 320563], [&amp;#34;name&amp;#34;, &amp;#34;street&amp;#34;]] # loc 방식 index의 이름을 써야한다.# account\tname\tstreet\t\t# 211829\tKerluke, Koepp and Hilpert\t34456 Sean Highway# 320563\tWalter-Trantow\t1311 Alvis Tunneldf.iloc[:5, :3] # iloc 방식# account\tname\tstreet\tcity\t\t\t# 211829\tKerluke, Koepp and Hilpert\t34456 Sean Highway\tNew Jaycob# 320563\tWalter-Trantow\t1311 Alvis Tunnel\tPort Khadijah# 648336\tBashirian, Kunde and Price\t62184 Schamberger Underpass Apt. 231\tNew Lilianland# 109996\tD&amp;#39;Amore, Gleichner and Bode\t155 Fadel Crescent Apt. 144\tHyattburgh# 121213\tBauch-Goldner\t7274 Marissa Common\tShanahanchester[code 4-3. basic, loc, iloc selection]df.index = list(range(0, 15))# df.reset_index(inplace=True) # 또 다른 방법# df.reset_index(inplace=True) # 원본 테이블 값 또한 바꿔줌# df.reset_index(inplace=True, drop=True) # 원본 테이블 값 또한 바꿔줌 + 메모리에서 아예 삭제df.head(3)# \tname\tstreet\tcity\tstate\tpostal-code\tJan\tFeb\tMar# 0\tKerluke, Koepp and Hilpert\t34456 Sean Highway\tNew Jaycob\tTexas\t28752\t10000\t 62000\t35000# 1\tWalter-Trantow\t1311 Alvis Tunnel\tPort Khadijah\tNorthCarolina\t38365\t95000\t45000\t35000# 2\tBashirian, Kunde and Price\t62184 Schamberger Underpass Apt. 231\tNew Lilianland\tIowa\t76517\t91000\t120000\t35000[code 4-4. reindex]df.drop([0, 1, 2, 3]) # 해당 row 삭제# df.drop(1, inplace=True) # 1번째 row 삭제 + 원본 변경# df.drop(&amp;#34;city&amp;#34;, axis=1, inplace=True) # 일부 column 전체 삭제[code 4-5. data drop]dataframe operationss1 = Series(range(1, 6), index=list(&amp;#34;abced&amp;#34;))s2 = Series(range(5, 11), index=list(&amp;#34;bcedef&amp;#34;))s1 + s2 # 아래와 같은 연산s1.add(s2)# a     NaN# b     7.0# c     9.0# d    13.0# e    11.0# e    13.0# f     NaN# dtype: float64# index 명이 아니라 index에 따라 연산이 수행된다 (b끼리 더해지는게 아니라 같은 n번째끼리)# 겹치는 index가 없으면 NaN 반환[code 5. series addition]df1 = DataFrame(np.arange(9).reshape(3, 3), columns=list(&amp;#34;abc&amp;#34;))df2 = DataFrame(np.arange(16).reshape(4, 4), columns=list(&amp;#34;abcd&amp;#34;))df1 + df2# \ta\tb\tc\td&amp;#123;: #\ta\tb\tc\td&amp;#125;# 0\t0.0\t2.0\t4.0\tNaN&amp;#123;: #0\t0-0\t2-0\t4-0\tnan&amp;#125;# 1\t7.0\t9.0\t11.0\tNaN&amp;#123;: #1\t7-0\t9-0\t11-0\tnan&amp;#125;# 2\t14.0\t16.0\t18.0\tNaN&amp;#123;: #2\t14-0\t16-0\t18-0\tnan&amp;#125;# 3\tNaN\tNaN\tNaN\tNaN&amp;#123;: #3\tnan\tnan\tnan\tnan&amp;#125;df1.add(df2, fill_value=0) # fill을 사용하여 Nan 대신 넣어줌# a\tb\tc\td&amp;#123;: #a\tb\tc\td&amp;#125;# 0\t0.0\t2.0\t4.0\t3.0&amp;#123;: #0\t0-0\t2-0\t4-0\t3-0&amp;#125;# 1\t7.0\t9.0\t11.0\t7.0&amp;#123;: #1\t7-0\t9-0\t11-0\t7-0&amp;#125;#2\t14.0\t16.0\t18.0\t11.0# 3\t12.0\t13.0\t14.0\t15.0&amp;#123;: #3\t12-0\t13-0\t14-0\t15-0&amp;#125;df1.mul(df2, fill_value=1)`[code 5-1. dataframe addition]df = DataFrame(np.arange(16).reshape(4, 4), columns=list(&amp;#34;abcd&amp;#34;))# a\tb\tc\td# 0\t0\t1\t2\t3# 1\t4\t5\t6\t7# 2\t8\t9\t10\t11# 3\t12\t13\t14\t15s = Series(np.arange(10, 14), index=list(&amp;#34;abcd&amp;#34;))# a    10# b    11# c    12# d    13# dtype: int32s2 = Series(np.arange(10, 14))# 0    10# 1    11# 2    12# 3    13# dtype: int32df + s# a\tb\tc\td# 0\t10\t12\t14\t16# 1\t14\t16\t18\t20# 2\t18\t20\t22\t24# 3\t22\t24\t26\t28df + s2 # 적절한 index가 없으므로 NaN이 나옴# a\tb\tc\td\t0\t1\t2\t3# 0\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN# 1\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN# 2\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN# 3\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaNdf.add(s2, axis=0) # 기준값 정해줘야함# \ta\tb\tc\td# 0\t10\t11\t12\t13# 1\t15\t16\t17\t18# 2\t20\t21\t22\t23# 3\t25\t26\t27\t28[code 5-2. operations for dataframe with series]lambda, map, apply  pandas의 series type의 데이터에도 map 함수 사용가능  function 대신 dict, sequence형 자료등으로 대체 가능s1 = Series(np.arrange(10))s1.map(lambda x: x**2).head(5)# 0 0# 1 1# 2 4# 3 9# 4 16# dtype: int64[code 6.map for series]z = &amp;#123;1: &amp;#34;A&amp;#34;, 2: &amp;#34;B&amp;#34;, 3: &amp;#34;C&amp;#34;&amp;#125; # 다른 시리즈의 데이터를 넣는 것도 가능s1.map(z).head(5) # z의 key:value 대로 변경, 없는 인덱스는 NaN#0    NaN#1      A#2      B#3      C#4    NaN#dtype: object[code 6-1. replace using map]!wget https://raw.githubusercontent.com/rstudio/Intro/master/data/wages.csvdf = pd.read_csv(&amp;#34;./data/wages.csv&amp;#34;)df.sex.unique() # 해당 column 내에 하나라도 존재하는 값들을 출력해 줌# array([&amp;#39;male&amp;#39;, &amp;#39;female&amp;#39;], dtype=object)def change_sex(x):    return 0 if x == &amp;#34;male&amp;#34; else 1df[&amp;#34;sex_code&amp;#34;] = df.sex.map(change_sex) # 성별 코드를 0과 1로 바꿔주는 똑같은 일을 하는 코드들df.sex.replace(&amp;#123;&amp;#34;male&amp;#34;: 0, &amp;#34;female&amp;#34;: 1&amp;#125;)df.sex.replace([&amp;#34;male&amp;#34;, &amp;#34;female&amp;#34;], [0, 1], inplace=True)# 0       0# 1       1# 2       1# 3       1# 4       1#        ..# 1374    0# 1375    1# 1376    1# 1377    0# 1378    0# Name: sex, Length: 1379, dtype: int64[code 6-2.replace ]  Map 함수의 기능 중 데이터 변환 기능만 담당  데이터 변환 시 많이 사용df = pd.read_csv(&amp;#34;wages.csv&amp;#34;)df_info = df[[&amp;#34;earn&amp;#34;, &amp;#34;height&amp;#34;, &amp;#34;age&amp;#34;]]f = lambda x: np.mean(x)df_info.apply(f) # colmun 별로 결과값 적용#df_info.mean() # 같은 역할을 하는 함수, sum, std 등도 존재#earn      32446.292622#height       66.592640#age          45.328499#dtype: float64[code 6-3. apply &amp; applymap]  map과 달리, series 전체(column)에 해당 함수를 적용  입력 값이 series 데이터로 입력 받아 handling 가능def f(x):    return Series(        [x.min(), x.max(), x.mean(), sum(x.isnull())],        index=[&amp;#34;min&amp;#34;, &amp;#34;max&amp;#34;, &amp;#34;mean&amp;#34;, &amp;#34;null&amp;#34;],    )df_info.apply(f)#\tearn\theight\tage#min\t-98.580489\t57.34000\t22.000000#max\t317949.127955\t77.21000\t95.000000#mean\t32446.292622\t66.59264\t45.328499#null\t0.000000\t0.00000\t0.000000[code 6-4. apply의 series값 return]  scalar 값 이외에 series값의 반환도 가능f = lambda x: x // 2df_info.applymap(f).head(5)# earn\theight\tage# 0\t39785.0\t36.0\t24# 1\t48198.0\t33.0\t31# 2\t24355.0\t31.0\t16# 3\t40239.0\t31.0\t47 #4\t41044.0\t31.0\t21f = lambda x: x ** 2df_info[&amp;#34;earn&amp;#34;].apply(f)# 0       6.331592e+09# 1       9.292379e+09# 2       2.372729e+09# 3       6.476724e+09# 4       6.738661e+09#             ...     # 1374    9.104329e+08# 1375    6.176974e+08# 1376    1.879825e+08# 1377    9.106124e+09# 1378    9.168947e+07# Name: earn, Length: 1379, dtype: float64[code 6-5.applymap for dataframe]  series 단위가 아닌 element 단위로 함수를 적용함  series 단위에 apply를 적용시킬 때와 같은 효과Built-in funtionsdf = pd.read_csv(&amp;#34;data/wages.csv&amp;#34;)df.describe()# \tearn\theight\ted\tage# count\t1379.000000\t1379.000000\t1379.000000\t1379.000000# mean\t32446.292622\t66.592640\t13.354605\t45.328499# std\t31257.070006\t3.818108\t2.438741\t15.789715# min\t-98.580489\t57.340000\t3.000000\t22.000000# 25&amp;#37;\t10538.790721\t63.720000\t12.000000\t33.000000# 50&amp;#37;\t26877.870178\t66.050000\t13.000000\t42.000000# 75&amp;#37;\t44506.215336\t69.315000\t15.000000\t55.000000# max\t317949.127955\t77.210000\t18.000000\t95.000000[code 7. describe ]  Numeri type 데이터의 요약 정보를 보여줌df.race.unique()# array([&amp;#39;white&amp;#39;, &amp;#39;other&amp;#39;, &amp;#39;hispanic&amp;#39;, &amp;#39;black&amp;#39;], dtype=object)[code 7-1. unique]  series data의 유일한 값을 list를 반환함df.sum(axis=1) # 축을 기준으로 어떻게 합칠지 정할 수 있음 0: 컬럼별 1: 로우별numueric_cols = [&amp;#34;earn&amp;#34;, &amp;#34;height&amp;#34;, &amp;#34;ed&amp;#34;, &amp;#34;age&amp;#34;]df[numueric_cols].sum(axis=1) # 많이 사용하는 형태# 이외에도 sub, mean, min, max, count, median, mad, var[code 7-2. 연산]df.isnull() # NaN 값이 있는 값들만 반환df.isnull().sum() / len(df)#earn      0.0#height    0.0#sex       0.0#race      0.0#ed        0.0#age       0.0#dtype: float64pd.options.display.max_rows = 200 #최대 보여주는 갯수 제한[code 7-3. isnull]df.sort_values([&amp;#34;age&amp;#34;, &amp;#34;earn&amp;#34;], ascending=True) # 나이와 소득으로 오름차순으로 정렬#\tearn\theight\tsex\trace\ted\tage#1038\t-56.321979\t67.81\tmale\thispanic\t10\t22#800\t-27.876819\t72.29\tmale\twhite\t12\t22#963\t-25.655260\t68.90\tmale\twhite\t12\t22#1105\t988.565070\t64.71\tfemale\twhite\t12\t22#801\t1000.221504\t64.09\tfemale\twhite\t12\t22#...\t...\t...\t...\t...\t...\t...#993\t32809.632677\t59.61\tfemale\tother\t16\t92#102\t39751.194030\t67.14\tmale\twhite\t12\t93#331\t39169.750135\t64.79\tfemale\twhite\t12\t95#809\t42963.362005\t72.94\tmale\twhite\t12\t95#3\t80478.096153\t63.22\tfemale\tother\t16\t95#1379 rows × 6 columns[code 7-4. sort_values]  column 값을 기준으로 데이터를 sortingdf.age.corr(df.earn) # 0.07400349177836055 df.age[(df.age &amp;#38;#60; 45) &amp;#38; (df.age &amp;#38;#62; 15)].corr(df.earn) # 0.31411788725189044df.age.cov(df.earn) # 36523.6992104089df.corrwith(df.earn)# earn 1.000000# height 0.291600# sex -0.337328# race -0.063977# ed 0.350374# age 0.074003# dtype: float64[code 7-5. Correlation &amp; Covariance]  상관계수와 공분산을 구하는 함수  corr, cov, corrwith 등이 있음df.sex.value_counts(sort=True)#female 859#male 520#Name: sex, dtype: int64df.sex.value_counts(sort=True)/ len(df)#female 0.622915#male 0.377085#Name: sex, dtype: float64[code 7-6.value_counts]  특정한 값의 갯수를 출력Group-by[img 8. Groupby 개념]  SQL groupby와 같이 split-apply-combine순으로 적용.  엑셀의 피봇 테이블과 비슷함.df.groupby(&amp;#34;Team&amp;#34;)[&amp;#34;Points&amp;#34;].sum()#df.gropuby([묶음의 기준이 되는 컬럼들])[[적용받는 컬럼]].[적용받는 연산]()[code 8. groupby 사용법]  여러개의 column을 묶을 수도 있다.(Hierarchical index)  return type은 series이다.# data from:ipl_data = &amp;#123;    &amp;#34;Team&amp;#34;: [        &amp;#34;Riders&amp;#34;,        &amp;#34;Riders&amp;#34;,        &amp;#34;Devils&amp;#34;,        &amp;#34;Devils&amp;#34;,        &amp;#34;Kings&amp;#34;,        &amp;#34;kings&amp;#34;,        &amp;#34;Kings&amp;#34;,        &amp;#34;Kings&amp;#34;,        &amp;#34;Riders&amp;#34;,        &amp;#34;Royals&amp;#34;,        &amp;#34;Royals&amp;#34;,        &amp;#34;Riders&amp;#34;,    ],    &amp;#34;Rank&amp;#34;: [1, 2, 2, 3, 3, 4, 1, 1, 2, 4, 1, 2],    &amp;#34;Year&amp;#34;: [2014, 2015, 2014, 2015, 2014, 2015, 2016, 2017, 2016, 2014, 2015, 2017],    &amp;#34;Points&amp;#34;: [876, 789, 863, 673, 741, 812, 756, 788, 694, 701, 804, 690],&amp;#125;df = pd.DataFrame(ipl_data)df.groupby(&amp;#34;Team&amp;#34;)[&amp;#34;Points&amp;#34;].std()# Team# Devils    134.350288# Kings      24.006943# Riders     88.567771# Royals     72.831998# kings            NaN# Name: Points, dtype: float64df.groupby([&amp;#34;Team&amp;#34;, &amp;#34;Year&amp;#34;])[&amp;#34;Points&amp;#34;].sum() # 여러 컬럼 묶기# Team\tYear# Devils 2014 863# \t\t 2015 673# kings  2014 741# \t\t 2016 756# \t\t 2017 788# Riders 2014 876# \t\t 2015 789# \t\t 2016 694[code 8-1. groupby 사용]  Hierarchical index 사용시 다중 인덱스가 형성됨.h_index = df.groupby([&amp;#34;Team&amp;#34;, &amp;#34;Year&amp;#34;])[&amp;#34;Points&amp;#34;].sum()h_index[&amp;#34;Devils&amp;#34;:&amp;#34;Kings&amp;#34;]# Team    Year# Devils  2014    863#         2015    673# Kings   2014    741#        2016    756#         2017    788# Name: Points, dtype: int64h_index.unstack().stack().unstack() # unstack(): Group으로 묶여진 데이터를 matrix 형태로 전환해줌# Year\t2014\t2015\t2016\t2017# Team\t\t\t\t# Devils\t863.0\t673.0\tNaN\tNaN# Kings\t741.0\tNaN\t756.0\t788.0# Riders\t876.0\t789.0\t694.0\t690.0# Royals\t701.0\t804.0\tNaN\tNaN# kings\tNaN\t812.0\tNaN\tNaNh_index.reset_index() # 다시 인덱스와 컬럼 형태의 dataframe으로 바꿔줌[code 8-2. hierarchical index의 index slicing과 unstack]h_index.swaplevel() # index level을 바꿔줌# Year  Team  # 2014  Devils    863# 2015  Devils    673# 2014  Kings     741# 2016  Kings     756# 2017  Kings     788# 2014  Riders    876# 2015  Riders    789# 2016  Riders    694# 2017  Riders    690# 2014  Royals    701# 2015  Royals    804#       kings     812# Name: Points, dtype: int64h_index.swaplevel().sortlevel(1) # 해당 인덱스 레벨 기준으로 정렬을 해줌# 그냥sort_values는 값을 기준으로 바꿔줌# Year  Team  # 2014  Devils    863# 2015  Devils    673# 2014  Kings     741# 2016  Kings     756# 2017  Kings     788# 2014  Riders    876# 2015  Riders    789# 2016  Riders    694# 2017  Riders    690# 2014  Royals    701# 2015  Royals    804#       kings     812# Name: Points, dtype: int64[code 8-3. 추가적인 함수들]grouped = df.groupby(&amp;#34;Team&amp;#34;)for name, group in grouped:    print(name)    print(group)#     Devils#      Team  Rank  Year  Points# 2  Devils     2  2014     863# 3  Devils     3  2015     673# Kings#     Team  Rank  Year  Points# 4  Kings     3  2014     741# 6  Kings     1  2016     756# 7  Kings     1  2017     788# Riders#       Team  Rank  Year  Points# 0   Riders     1  2014     876# 1   Riders     2  2015     789# 8   Riders     2  2016     694# 11  Riders     2  2017     690# Royals#       Team  Rank  Year  Points# 9   Royals     4  2014     701# 10  Royals     1  2015     804# kings#     Team  Rank  Year  Points# 5  kings     4  2015     812grouped.get_group(&amp;#34;Devils&amp;#34;) # 특정 key값을 가진 그룹의 정보만 추출 가능# \tTeam\tRank\tYear\tPoints# 2\tDevils\t2\t2014\t863# 3\tDevils\t3\t2015\t673[code 8-4. Grouped]  Groupby에 의해 Split된 상태를 추출 가능함.  Tuple 형태로 그룹의 key값 value값 추출 가능      dataframe 형태로 return    추출된 group 정보에는 세가지 유형의 apply 가능          Aggregation: 요약된 통계정보 추출      Transformation: 해당 정보를 변환      Filtration: 특정 정보를 제거하여 보여주는 필터링 기능      Aggregationgrouped = df.groupby(&amp;#34;Team&amp;#34;)grouped.agg(max) # 최대값 # Rank\tYear\tPoints# Team\t\t\t# Devils\t3\t2015\t863# Kings\t3\t2017\t788# Riders\t2\t2017\t876# Royals\t4\t2015\t804# kings\t4\t2015\t812grouped.agg(np.mean) # 평균값# \tRank\tYear\tPoints# Team\t\t\t# Devils\t2.500000\t2014.500000\t768.000000# Kings\t1.666667\t2015.666667\t761.666667# Riders\t1.750000\t2015.500000\t762.250000# Royals\t2.500000\t2014.500000\t752.500000# kings\t4.000000\t2015.000000\t812.000000#grouped[&amp;#39;Points&amp;#39;].agg([np.sum, np.mean, np.std]) # 특정 컬럼에 여러개의 function을 Apply 할 수 도 있음[code 9. agg 예제]grouped.describe().T #컬럼별 통계 요약 제공# \tTeam\tDevils\tKings\tRiders\tRoyals\tkings# Rank\tcount\t2.000000\t3.000000\t4.000000\t2.000000\t1.0# mean\t2.500000\t1.666667\t1.750000\t2.500000\t4.0# std\t0.707107\t1.154701\t0.500000\t2.121320\tNaN# min\t2.000000\t1.000000\t1.000000\t1.000000\t4.0# 25&amp;#37;\t2.250000\t1.000000\t1.750000\t1.750000\t4.0# 50&amp;#37;\t2.500000\t1.000000\t2.000000\t2.500000\t4.0# 75&amp;#37;\t2.750000\t2.000000\t2.000000\t3.250000\t4.0# max\t3.000000\t3.000000\t2.000000\t4.000000\t4.0# Year\tcount\t2.000000\t3.000000\t4.000000\t2.000000\t1.0# mean\t2014.500000\t2015.666667\t2015.500000\t2014.500000\t2015.0# std\t0.707107\t1.527525\t1.290994\t0.707107\tNaN# min\t2014.000000\t2014.000000\t2014.000000\t2014.000000\t2015.0# 25&amp;#37;\t2014.250000\t2015.000000\t2014.750000\t2014.250000\t2015.0# 50&amp;#37;\t2014.500000\t2016.000000\t2015.500000\t2014.500000\t2015.0# 75&amp;#37;\t2014.750000\t2016.500000\t2016.250000\t2014.750000\t2015.0# max\t2015.000000\t2017.000000\t2017.000000\t2015.000000\t2015.0# Points\tcount\t2.000000\t3.000000\t4.000000\t2.000000\t1.0# mean\t768.000000\t761.666667\t762.250000\t752.500000\t812.0# std\t134.350288\t24.006943\t88.567771\t72.831998\tNaN# min\t673.000000\t741.000000\t690.000000\t701.000000\t812.0# 25&amp;#37;\t720.500000\t748.500000\t693.000000\t726.750000\t812.0# 50&amp;#37;\t768.000000\t756.000000\t741.500000\t752.500000\t812.0# 75&amp;#37;\t815.500000\t772.000000\t810.750000\t778.250000\t812.0# max\t863.000000\t788.000000\t876.000000\t804.000000\t812.0[code 9-1. grouped 상태의 describe와 T]Transformation  groupby된 컬럼에 각각 개별데이터의 변환을 지원grouped = df.groupby(&amp;#34;Team&amp;#34;)score = lambda x: (x - x.mean()) / x.std() # normalization 정규화grouped.transform(score)# 각 개별 데이터의 정규화 실시# \tRank\tYear\tPoints# 0\t-1.500000\t-1.161895\t1.284327# 1\t0.500000\t-0.387298\t0.302029# 2\t-0.707107\t-0.707107\t0.707107# 3\t0.707107\t0.707107\t-0.707107# 4\t1.154701\t-1.091089\t-0.860862# 5\tNaN\tNaN\tNaN# 6\t-0.577350\t0.218218\t-0.236043# 7\t-0.577350\t0.872872\t1.096905# 8\t0.500000\t0.387298\t-0.770596# 9\t0.707107\t-0.707107\t-0.707107# 10\t-0.707107\t0.707107\t0.707107# 11\t0.500000\t1.161895\t-0.815759score = lambda x: (x.max())grouped.transform(score) # 각 팀에 최대 점수를 가진 데이터들을 각 인덱스에 적용# \tRank\tYear\tPoints# 0\t2\t2017\t876# 1\t2\t2017\t876# 2\t3\t2015\t863# 3\t3\t2015\t863# 4\t3\t2017\t788# 5\t4\t2015\t812# 6\t3\t2017\t788# 7\t3\t2017\t788# 8\t2\t2017\t876# 9\t4\t2015\t804# 10\t4\t2015\t804# 11\t2\t2017\t876[code 10. transfrom 예제]  단 max나 min처럼 Series 데이터에 적용되는 데이터들은 key값을 기준으로 Grouped된 데이터 기준Filtration  특정 조건으로 데이터를 검색시 사용df.groupby(&amp;#34;Team&amp;#34;).filter(lambda x: len(x) &amp;#38;#62;= 3) # 3개이상의 row가 포함된 팀들의 dataframe들을 출력# Team\tRank\tYear\tPoints# 0\tRiders\t1\t2014\t876# 1\tRiders\t2\t2015\t789# 4\tKings\t3\t2014\t741# 6\tKings\t1\t2016\t756# 7\tKings\t1\t2017\t788# 8\tRiders\t2\t2016\t694# 11\tRiders\t2\t2017\t690df.groupby(&amp;#34;Team&amp;#34;).filter(lambda x: x[&amp;#34;Points&amp;#34;].mean() &amp;#38;#62; 700)# Team\tRank\tYear\tPoints# 0\tRiders\t1\t2014\t876# 1\tRiders\t2\t2015\t789# 2\tDevils\t2\t2014\t863# 3\tDevils\t3\t2015\t673# 4\tKings\t3\t2014\t741# 5\tkings\t4\t2015\t812# 6\tKings\t1\t2016\t756# 7\tKings\t1\t2017\t788# 8\tRiders\t2\t2016\t694# 9\tRoyals\t4\t2014\t701# 10\tRoyals\t1\t2015\t804# 11\tRiders\t2\t2017\t690[code 11. filter 예제]  필터의 조건이 true인 row를 출력  len(x)의 경우 group된 dataframe 갯수using GroupBy!wget --no-check-certificate https://www.shanelynn.ie/wp-content/uploads/2015/06/phone_data.csvdf_phone = pd.read_csv(&amp;#34;./data/phone_data.csv&amp;#34;)df_phone.head()# index\tdate\tduration\titem\tmonth\tnetwork\tnetwork_type# 0\t0\t15/10/14 06:58\t34.429\tdata\t2014-11\tdata\tdata# 1\t1\t15/10/14 06:58\t13.000\tcall\t2014-11\tVodafone\tmobile# 2\t2\t15/10/14 14:46\t23.000\tcall\t2014-11\tMeteor\tmobile# 3\t3\t15/10/14 14:48\t4.000\tcall\t2014-11\tTesco\tmobile# 4\t4\t15/10/14 17:27\t4.000\tcall\t2014-11\tTesco\tmobile[code 11. wget을 이용한 jupyter notebook 데이터 다운로드]  wget이 안되면 가상환경에 wget 설치하자. conda install -c menpo wgetimport dateutil#dateutil.parser.parse : 문자를 날짜 형식으로 바꿔줌df_phone[&amp;#34;date&amp;#34;] = df_phone[&amp;#34;date&amp;#34;].apply(dateutil.parser.parse, dayfirst=True)df_phone.dtypes# index                    int64# date            datetime64[ns]# duration               float64# item                    object# month                   object# network                 object# network_type            object# dtype: objectdf_phone.groupby(&amp;#34;month&amp;#34;)[&amp;#34;duration&amp;#34;].sum().count() # 갯수 출력df_phone.groupby(&amp;#34;month&amp;#34;)[&amp;#34;duration&amp;#34;].sum().plot() # matplotlib 를 이용한 값 보기df_phone.groupby([&amp;#34;month&amp;#34;, &amp;#34;item&amp;#34;])[&amp;#34;duration&amp;#34;].count().unstack().plot() # 여러 column에 대해 출력[code 11-1. dateutil을 이용한 타입 변환과 matplotlib 사용][img 11. matplotlib 출력 예시][img 11-1. matplotlib 출력 예시2]  matplotlib가 설치되어있어야 한다. conda install matplotlibdf_phone.groupby([&amp;#34;month&amp;#34;, &amp;#34;item&amp;#34;]).agg(    &amp;#123;        &amp;#34;duration&amp;#34;: [min],  # find the min, max, and sum of the duration column        &amp;#34;network_type&amp;#34;: &amp;#34;count&amp;#34;,  # find the number of network type entries        &amp;#34;date&amp;#34;: [min, &amp;#34;first&amp;#34;, &amp;#34;nunique&amp;#34;],    &amp;#125;)  # get the min, first, and number of unique dates# \t\tduration\tnetwork_type\tdate# min\tcount\tmin\tfirst\tnunique# month\titem\t\t\t\t\t# 2014-11\tcall\t1.000\t107\t2014-10-15 06:58:00\t2014-10-15 06:58:00\t104# data\t34.429\t29\t2014-10-15 06:58:00\t2014-10-15 06:58:00\t29# sms\t1.000\t94\t2014-10-16 22:18:00\t2014-10-16 22:18:00\t79# 2014-12\tcall\t2.000\t79\t2014-11-14 17:24:00\t2014-11-14 17:24:00\t76# data\t34.429\t30\t2014-11-13 06:58:00\t2014-11-13 06:58:00\t30# sms\t1.000\t48\t2014-11-14 17:28:00\t2014-11-14 17:28:00\t41# 2015-01\tcall\t2.000\t88\t2014-12-15 20:03:00\t2014-12-15 20:03:00\t84# data\t34.429\t31\t2014-12-13 06:58:00\t2014-12-13 06:58:00\t31# sms\t1.000\t86\t2014-12-15 19:56:00\t2014-12-15 19:56:00\t58# 2015-02\tcall\t1.000\t67\t2015-01-15 10:36:00\t2015-01-15 10:36:00\t67# data\t34.429\t31\t2015-01-13 06:58:00\t2015-01-13 06:58:00\t31# sms\t1.000\t39\t2015-01-15 12:23:00\t2015-01-15 12:23:00\t27# 2015-03\tcall\t2.000\t47\t2015-02-12 20:15:00\t2015-02-12 20:15:00\t47# data\t34.429\t29\t2015-02-13 06:58:00\t2015-02-13 06:58:00\t29# sms\t1.000\t25\t2015-02-19 18:46:00\t2015-02-19 18:46:00\t17[code 11-2. agg의 복잡한 실사용 예시]grouped.rename(    columns=&amp;#123;&amp;#34;min&amp;#34;: &amp;#34;min_duration&amp;#34;, &amp;#34;max&amp;#34;: &amp;#34;max_duration&amp;#34;, &amp;#34;mean&amp;#34;: &amp;#34;mean_duration&amp;#34;&amp;#125;)# colmun 명 변경grouped.add_prefix(&amp;#34;duration_&amp;#34;)# prefix 추가# \tduration_min\tduration_max\tduration_mean# month\t\t\t# 2014-11\t1.0\t1940.0\t115.823657# 2014-12\t1.0\t2120.0\t93.260318# 2015-01\t1.0\t1859.0\t88.894141# 2015-02\t1.0\t1863.0\t113.301453# 2015-03\t1.0\t10528.0\t225.251891[code 11-3. colmun 이름 변경 관련 함수]Pivot Table &amp; Cross Table  두 함수는 데이터 재구조화에 쓰이며 groupby로 가능하지만, 이 두 기능을 사용하는게 더욱 쉽다.Pivot Table  Column에 labeling 값을 추가하여 Value에 numeric type 값을 aggregation 할 수 있다.df_phone = pd.read_csv(&amp;#34;./phone_data.csv&amp;#34;)df_phone[&amp;#34;date&amp;#34;] = df_phone[&amp;#34;date&amp;#34;].apply(dateutil.parser.parse, dayfirst=True)df_phone.head()# index\tdate\tduration\titem\tmonth\tnetwork\tnetwork_type# 0\t0\t2014-10-15 06:58:00\t34.429\tdata\t2014-11\tdata\tdata# 1\t1\t2014-10-15 06:58:00\t13.000\tcall\t2014-11\tVodafone\tmobile# 2\t2\t2014-10-15 14:46:00\t23.000\tcall\t2014-11\tMeteor\tmobile# 3\t3\t2014-10-15 14:48:00\t4.000\tcall\t2014-11\tTesco\tmobile# 4\t4\t2014-10-15 17:27:00\t4.000\tcall\t2014-11\tTesco\tmobiledf_phone.pivot_table(    values=[&amp;#34;duration&amp;#34;],    index=[df_phone.month, df_phone.item],# index를 위 두 컬럼으로 바꾼다.    columns=df_phone.network, # column 값을 network의 요소들로 바꾼다.    aggfunc=&amp;#34;sum&amp;#34;, # 위에 설정한 index가 겹치는 값끼리 sum, 더하라, (mean)    fill_value=0, # none값은 0를 채워라.)# duration# network\tMeteor\tTesco\tThree\tVodafone\tdata\tlandline\tspecial\tvoicemail\tworld# month\titem\t\t\t\t\t\t\t\t\t# 2014-11\tcall\t1521\t4045\t12458\t4316\t0.000\t2906\t0\t301\t0# data\t0\t0\t0\t0\t998.441\t0\t0\t0\t0# sms\t10\t3\t25\t55\t0.000\t0\t1\t0\t0# 2014-12\tcall\t2010\t1819\t6316\t1302\t0.000\t1424\t0\t690\t0# data\t0\t0\t0\t0\t1032.870\t0\t0\t0\t0# sms\t12\t1\t13\t18\t0.000\t0\t0\t0\t4# 2015-01\tcall\t2207\t2904\t6445\t3626\t0.000\t1603\t0\t285\t0# data\t0\t0\t0\t0\t1067.299\t0\t0\t0\t0# sms\t10\t3\t33\t40\t0.000\t0\t0\t0\t0# 2015-02\tcall\t1188\t4087\t6279\t1864\t0.000\t730\t0\t268\t0# data\t0\t0\t0\t0\t1067.299\t0\t0\t0\t0# sms\t1\t2\t11\t23\t0.000\t0\t2\t0\t0# 2015-03\tcall\t274\t973\t4966\t3513\t0.000\t11770\t0\t231\t0# data\t0\t0\t0\t0\t998.441\t0\t0\t0\t0# sms\t0\t4\t5\t13\t0.000\t0\t0\t0\t3df_phone.groupby([&amp;#34;month&amp;#34;, &amp;#34;item&amp;#34;, &amp;#34;network&amp;#34;])[&amp;#34;duration&amp;#34;].sum().unstack()# 같은 결과를 보이는 groupby 함수[code 12. Pivot table 사용과 예시]Crosstab  특허 두 칼럼에 교차 빈도, 비율, 덧셈 등을 구할 때 사용  Pivot table의 특수한 형태, pivot_table은 하나의 데이터 프레임을 대상으로 하는것에 비해, crosstab은 여러 데이터프레임의 column(series)를 가져와 쓸 수 있다.  User-Item Rating Matrix 등을 만들 때 사용가능함df_movie = pd.read_csv(&amp;#34;./movie_rating.csv&amp;#34;)df_movie.head()# critic\ttitle\trating# 0\tJack Matthews\tLady in the Water\t3.0# 1\tJack Matthews\tSnakes on a Plane\t4.0# 2\tJack Matthews\tYou Me and Dupree\t3.5# 3\tJack Matthews\tSuperman Returns\t5.0# 4\tJack Matthews\tThe Night Listener\t3.0pd.crosstab(    index=df_movie.critic,    columns=df_movie.title,    values=df_movie.rating,    aggfunc=&amp;#34;first&amp;#34;,).fillna(0) # na일 경우 0 # title\tJust My Luck\tLady in the Water\tSnakes on a Plane\tSuperman Returns\tThe Night Listener\tYou Me and Dupree# critic\t\t\t\t\t\t# Claudia Puig\t3.0\t0.0\t3.5\t4.0\t4.5\t2.5# Gene Seymour\t1.5\t3.0\t3.5\t5.0\t3.0\t3.5# Jack Matthews\t0.0\t3.0\t4.0\t5.0\t3.0\t3.5# Lisa Rose\t3.0\t2.5\t3.5\t3.5\t3.0\t2.5# Mick LaSalle\t2.0\t3.0\t4.0\t3.0\t3.0\t2.0# Toby\t0.0\t0.0\t4.5\t4.0\t0.0\t1.0df_movie.pivot_table( #위 함수와 같은 결과를 내는 pivot_table 함수    [&amp;#34;rating&amp;#34;],    index=df_movie.critic,    columns=df_movie.title,    aggfunc=&amp;#34;sum&amp;#34;,    fill_value=0,)df_movie.groupby([&amp;#34;critic&amp;#34;, &amp;#34;title&amp;#34;]).agg(&amp;#123;&amp;#34;rating&amp;#34;: &amp;#34;first&amp;#34;&amp;#125;) #위 함수와 같은 결과를 내는 groupby 함수[code 13. Cross Tab 사용과 예시]Merge &amp; Concatmerge  SQL에서 많이 사용하는 Merge와 같은 기능으로, 두개의 표를 하나로 합침raw_data = &amp;#123;    &amp;#34;subject_id&amp;#34;: [&amp;#34;1&amp;#34;, &amp;#34;2&amp;#34;, &amp;#34;3&amp;#34;, &amp;#34;4&amp;#34;, &amp;#34;5&amp;#34;, &amp;#34;7&amp;#34;, &amp;#34;8&amp;#34;, &amp;#34;9&amp;#34;, &amp;#34;10&amp;#34;, &amp;#34;11&amp;#34;],    &amp;#34;test_score&amp;#34;: [51, 15, 15, 61, 16, 14, 15, 1, 61, 16],&amp;#125;df_a = pd.DataFrame(raw_data, columns=[&amp;#34;subject_id&amp;#34;, &amp;#34;test_score&amp;#34;])# \tsubject_id\ttest_score# 0\t1\t51# 1\t2\t15# 2\t3\t15# 3\t4\t61# 4\t5\t16# 5\t7\t14# 6\t8\t15# 7\t9\t1# 8\t10\t61# 9\t11\t16raw_data = &amp;#123;    &amp;#34;subject_id&amp;#34;: [&amp;#34;4&amp;#34;, &amp;#34;5&amp;#34;, &amp;#34;6&amp;#34;, &amp;#34;7&amp;#34;, &amp;#34;8&amp;#34;],    &amp;#34;first_name&amp;#34;: [&amp;#34;Billy&amp;#34;, &amp;#34;Brian&amp;#34;, &amp;#34;Bran&amp;#34;, &amp;#34;Bryce&amp;#34;, &amp;#34;Betty&amp;#34;],    &amp;#34;last_name&amp;#34;: [&amp;#34;Bonder&amp;#34;, &amp;#34;Black&amp;#34;, &amp;#34;Balwner&amp;#34;, &amp;#34;Brice&amp;#34;, &amp;#34;Btisan&amp;#34;],&amp;#125;df_b = pd.DataFrame(raw_data, columns=[&amp;#34;subject_id&amp;#34;, &amp;#34;first_name&amp;#34;, &amp;#34;last_name&amp;#34;])# subject_id\tfirst_name\tlast_name# 0\t4\tBilly\tBonder# 1\t5\tBrian\tBlack# 2\t6\tBran\tBalwner# 3\t7\tBryce\tBrice# 4\t8\tBetty\tBtisanpd.merge(df_a, df_b, on=&amp;#34;subject_id&amp;#34;) # subject_id를 기준으로 표가 join 되었다.# subject_id\ttest_score\tfirst_name\tlast_name# 0\t4\t61\tBilly\tBonder# 1\t5\t16\tBrian\tBlack# 2\t7\t14\tBryce\tBrice# 3\t8\t15\tBetty\tBtisan[code 14. merge 예시]pd.merge(df_a, df_b, left_on=&amp;#34;subject_id&amp;#34;, right_on=&amp;#34;subject_id&amp;#34;) # 두 column명이 다른 경우pd.merge(df_a, df_b, on=&amp;#34;subject_id&amp;#34;, how=&amp;#34;left&amp;#34;) # how에서 join method를 정한다. 그림 14 참조, 기본값은 inner 조인pd.merge(df_a, df_b, right_index=True, left_index=True) # index 값을 기준으로 붙임, 둘다 같은 이름의 index 값이 있으면 _x, _y 같은 이름이 붙어서 새로 생김.[code 14-1. merge 다른 예시][img 14. join method]Concat  같은 형태의 데이터를 붙이는 연산작업[img 15. 2가지 방법의 Concat]df_new = pd.concat([df_a, df_b]) # 리스트 형식으로 붙임df_new.reset_index()# df_new.reset_index(drop=True) # drop=True를 하면 아래 처럼 index가 추가로 붙지 않는다.# \tindex\tsubject_id\tfirst_name\tlast_name# 0\t0\t1\tAlex\tAnderson# 1\t1\t2\tAmy\tAckerman# 2\t2\t3\tAllen\tAli# 3\t3\t4\tAlice\tAoni# 4\t4\t5\tAyoung\tAtiches# 5\t0\t4\tBilly\tBonder# 6\t1\t5\tBrian\tBlack# 7\t2\t6\tBran\tBalwner# 8\t3\t7\tBryce\tBrice# 9\t4\t8\tBetty\tBtisandf_a.append(df_b) # 위와 비슷하나 index가 따로 붙지 않았다.# \tsubject_id\tfirst_name\tlast_name# 0\t1\tAlex\tAnderson# 1\t2\tAmy\tAckerman# 2\t3\tAllen\tAli# 3\t4\tAlice\tAoni# 4\t5\tAyoung\tAtiches# 0\t4\tBilly\tBonder# 1\t5\tBrian\tBlack# 2\t6\tBran\tBalwner# 3\t7\tBryce\tBrice# 4\t8\tBetty\tBtisandf_new = pd.concat([df_a, df_b], axis=1) # 옆으로 붙이기df_new.reset_index()# index\tsubject_id\tfirst_name\tlast_name\tsubject_id\tfirst_name\tlast_name# 0\t0\t1\tAlex\tAnderson\t4\tBilly\tBonder# 1\t1\t2\tAmy\tAckerman\t5\tBrian\tBlack# 2\t2\t3\tAllen\tAli\t6\tBran\tBalwner# 3\t3\t4\tAlice\tAoni\t7\tBryce\tBrice# 4\t4\t5\tAyoung\tAtiches\t8\tBetty\tBtisan[code 15. Concat 예시]DB Persistence  Data lodaing 시 파일 형식 뿐만 아니라 db connectio 기능을 제공함import pandas as pdimport sqlite3  # pymysql &amp;#38;#60;- 설치conn = sqlite3.connect(&amp;#34;./data/flights.db&amp;#34;)cur = conn.cursor()cur.execute(&amp;#34;select * from airlines limit 5;&amp;#34;)results = cur.fetchall()df_airplines = pd.read_sql_query(&amp;#34;select * from airlines;&amp;#34;, conn)df_airplines# index\tid\tname\talias\tiata\ticao\tcallsign\tcountry\tactive# 0\t0\t1\tPrivate flight\t\\N\t-\tNone\tNone\tNone\tY# 1\t1\t2\t135 Airways\t\\N\tNone\tGNL\tGENERAL\tUnited States\tN# 2\t2\t3\t1Time Airline\t\\N\t1T\tRNX\tNEXTIME\tSouth Africa\tY# 3\t3\t4\t2 Sqn No 1 Elementary Flying Training School\t\\N\tNone\tWYT\tNone\tUnited Kingdom\tN# 4\t4\t5\t213 Flight Unit\t\\N\tNone\tTFU\tNone\tRussia\tN# ...\t...\t...\t...\t...\t...\t...\t...\t...\t...# 6043\t6043\t19828\tVuela Cuba\tVuela Cuba\t6C\t6CC\tNone\tCuba\tY# 6044\t6044\t19830\tAll Australia\tAll Australia\t88\t8K8\tNone\tAustralia\tY# 6045\t6045\t19831\tFly Europa\tNone\tER\tRWW\tNone\tSpain\tY# 6046\t6046\t19834\tFlyPortugal\tNone\tPO\tFPT\tFlyPortugal\tPortugal\tY# 6047\t6047\t19845\tFTI Fluggesellschaft\tNone\tNone\tFTI\tNone\tGermany\tN# 6048 rows × 9 columns[code 16. DB connection 예제]conda install openpyxlconda install XlsxWriter[code 16-1. 먼저 필요 모듈을 깔아주자.]writer = pd.ExcelWriter(&amp;#34;./data/df_routes.xlsx&amp;#34;, engine=&amp;#34;xlsxwriter&amp;#34;)df_routes.to_excel(writer, sheet_name=&amp;#34;Sheet1&amp;#34;)df_routes.to_pickle(&amp;#34;./data/df_routes.pickle&amp;#34;)df_routes_pickle = pd.read_pickle(&amp;#34;./data/df_routes.pickle&amp;#34;)df_routes_pickle.head()[code 16-2. XLS 추출 예제]  xls 엔진으로 엑셀 데이터 추출 후 pickle 저장 가능"
  }
  , 
  
  "/articles/AI/AITools/Pytorch%20%EA%B8%B0%EB%B3%B8.html": {
    title: "Pytorch 기본 배우기",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/AITools/Pytorch%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","TOOL","PYTHON"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: truePyTorch 기본 배우기  본 자료는 Naver BoostAI camp의 강의를 정리한 내용입니다PyTorch 시작[img 0. PyTorch logo]  PyTorch : Keras + Tensorflow 와 경쟁하는 facebook 진영의 딥러닝 프레임워크  비교적 쉽고 빠르고 여러 기능을 가지고 있다.      Numpy의 array 구조 Tensor, AutoGrad(자동 미분) 지원,  다양한 딥러닝 함수와 모델 지원    PyTorch 튜토리얼과 PyTorch로 시작하는 딥 러닝 입문 추천개발 환경 설정구글 Colab  구글 로그인 후 구글 Colab 활용, 구글 드라이브 연동 추천[img 1. 구글 드라이브 연동방법]  새 노트를 생성한 뒤, 노란 표시의 폴더를 클릭하면 구글 드라이브와 연동 가능          대용량, 고성능의 컴퓨팅이 필요하므로 추천      [img 1-1. 구글 드라이브 내부에 이러한 폴더가 생성된다.]  해당 폴더가 default directory로 되어있다.VScode와 연결  https://github.com/BoostcampAITech/lecture-note-python-basics-for-ai/blob/main/codes/pytorch/00_utils/colab%20on%20VSCode.md 참조      google colab 설정        VScode 설치        ngrok 가입 및 다운로드, 토큰 값 확인              노란 형광펜 : 다운로드      빨간 줄: 토큰      [img 2. ngrok 결과]  colab 하드웨어 GPU로 변경[img 2-1. colab 노트설정 변경]  colab 파일 생성 시마다 해줘야 한다.      구글 colab에서 해당 코드 실행    ```python!pip install colab-sshNGROK_TOKEN = '토큰' # ngrok 토큰PASSWORD = '접속할 비밀번호' # 비밀번호 설정from colab_ssh import launch_sshlaunch_ssh(NGROK_TOKEN, PASSWORD)     안될시   ```python   !pip install colab-ssh --upgrade   from colab_ssh import launch_ssh_cloudflared   PASSWORD = &amp;#39;접속할 비밀번호&amp;#39; # 비밀번호 설정   launch_ssh_cloudflared(password=PASSWORD)   # 이후 cloudflare 설치, 실행 해야함.   [code 2. ngrok 혹은 coludflared 활용 vscode 연결][img 2-2. 초기 지시 사항]  terminal에 code ~/.ssh/config로 연 뒤 다음과 같은 코드 입력Host *.trycloudflare.com\tHostName &amp;#37;h\tUser root\tPort 22\tProxyCommand &amp;#38;#60;PUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&amp;#38;#62; access ssh --hostname &amp;#37;h[code 2-1. config 파일 내용]       의 경우, window의 경우 cloudflared.exe가 있는 장소     \\대신 \\\\를 넣어야 작동한다.  VScode에 Remote-SSH Extension 설치[img 2-3. 2개를 설치해야하지만 맨위의 익스텐션을 설치하면 자동 설치된다.]  SSH 연결  이후 Linux를 선택한 후, yes, colab에서 설정한 비밀번호 입력 후, Open Folder를 통하여  해당 디렉토리로 들어가면 내 구글 드라이브로 들어갈 수 있다.      Terminal: Create New Integrated Terminal을 통해 google colab의 terminal 환경을 열 수 있다.        이런식으로 연 colab 파일은 작업하는 동안 닫지 말자.  pytorch basicPyTorch Tensor 연산import numpy as npimport torch # 보통 alias 사용하지 않음n_array = np.arrange(10).reshape(2,5)t_array = torch.FloatTensor(n_array)print(t_array) # tensor([[0., 1., 2., 3., 4.],[5., 6., 7., 8., 9.]])print(t_array.shape) # torch.Size([2, 5])print(t_array.ndim) # 2print(t_array.size()) # torch.Size([2, 5]), element 갯수인 numpy와 달리 shape가 나옴t_array[:2, :3] # tensor([[0., 1., 2.],[5., 6., 7.]]) 슬라이싱 가능[code 3. 모듈 import 및 array to tensor]n1 = np.arange(10).reshape(2,5)n2 = np.arange(10).reshape(5,2)t1 = torch.FloatTensor(n1)t2 = torch.FloatTensor(n2)print(t1.matmul(t2)) # tensor([[ 60., 70.],[160., 195.]]) numpy와 달리 좀더 직관적이다.# n1.dot(n2) : numpy의 dot product, 같은 결과, [code 3-1. 기초 tensor operation - 행렬곱셈]n1 = np.arange(4).reshape(2,2)n2 = np.arange(4).reshape(2,2)t1 = torch.FloatTensor(n1)t2 = torch.FloatTensor(n2)print(t1 * t2) # tensor([[0., 1.],[4., 9.]])# t1.mul(t2) : 같은 결과 # t1 * 5 : tensor([[ 0.,  5.], [10., 15.]]), 원소들에게 적용되는 곱셈 [code 3-2. 기초 tensor operation - 행렬 원소간 곱셈]n1 = np.arange(10)t1 = torch.FloatTensor(n1)print(t1.mean()) # tensor(4.5000) # 전체 평균n1 = np.arange(10).reshape(5,2) t1 = torch.FloatTensor(n1)print(t1.mean(dim=0)) # tensor([4., 5.]) # 0차원 평균print(t1.mean(dim=1)) # tensor([0.5000, 2.5000, 4.5000, 6.5000, 8.5000]) # 1차원 평균[code 3-2. 기초 tensor operation - tensor 평균]n1 = np.arange(10)t1 = torch.FloatTensor(n1)print(t1.view(-1, 2)) # tensor([[0., 1.], [2., 3.], [4., 5.], [6., 7.], [8., 9.]])# numpy의 reshape 기능print(n1.reshape(-1, 2))# 같은 결과print(t1.view(-1, 10).squeeze())# tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])# rank를 줄인다, 자주쓰는 기능print(t1.view(-1, 10).squeeze().unsqueeze(dim=0))# tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])# rank를 반대로 늘린다. [code 3-2. 기초 tensor operation - tensor reshape와 squeeze]Tensor operations for ML/DL formula  여러가지 ML 내장 함수로 쉽게 사용할 수 있는 것이 pytorch의 장점이다.import torchimport torch.nn.functional as Ftensor = torch.FloatTensor([0.5, 0.7, 0.1]) # 앞으로 쓸 tensor 정의 [code 4. 함수를 사용하기 위해 functional을 import 해야한다.]h_tensor = F.softmax(tensor, dim=0)print(h_tensor) # tensor([0.3458, 0.4224, 0.2318]) # softmax : classify 문제에서 자주 사용하는 합해서 1인 벡터 생성 y = torch.randint(5, (10,5))# 0~4 범위의 5개의 랜덤 숫자를 가진 배열 10개를 생성   y_label = y.argmax(dim=1) # 각 배열의 가장 높은 값의 인덱스로 이뤄진 1차원 배열 생성print(y_label) # tensor([0, 3, 1, 2, 0, 2, 1, 3, 1, 4]), label 생성print(torch.nn.functional.one_hot(y_label)) # onehot : label 데이터를 이용해 해당 벡터를 가장 가능성이 높은 1 길이의 벡터로 바꿔줌  # tensor([[1, 0, 0, 0, 0],#         [0, 0, 0, 1, 0],#         [0, 1, 0, 0, 0],#         [0, 0, 1, 0, 0],#         [1, 0, 0, 0, 0],#         [0, 0, 1, 0, 0],#         [0, 1, 0, 0, 0],#         [0, 0, 0, 1, 0],#         [0, 1, 0, 0, 0],#         [0, 0, 0, 0, 1]])[code 4-1. 여러가지 내장 함수]torch autograd  torch의 강력한 기능 중 하나,  자동으로 미분하여 gradient를 구해준다.\\[y=w^2\\\\z=2*y+5\\\\z=2*w^2+5\\]w = torch.tensor(2.0, requires_grad=True) # True를 주어야 gradient가 자동으로 업데이트 된다.y = w**2z = 2*y + 5z.backward() # backpropagation 알고리즘, 자동으로 gradient를 업데이트 해줌w.grad # tensor(8. ), 4(2w^2의 미분값) * w(tensor(2.0))의 결과# Q.backward(gradient=external_grad), 벡터 형태의 미분또한 가능  https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817 추가로 참조PyTorch Datasettorch.autogradAutomatic gradient의 약자, pytorch의 gradient calculating API  Automatic gradent == Automatic differentiation  forward와 backward passe가 가능, 보통의 행렬 계산 Library와 딥러닝 Library의 차이가 됨일일이 수작업으로 gradient를 구했던 과거와 달리 Computational graph를 기반으로 자동으로 구함딥러닝의 접근성을 크게 늘려줌[img. Autograd의 원리]Backpropagation을 진행하면서 Chain rule을 위해 gradient 값들의 graph을 저장함실습은 torch.autograd.ipynb 파일 참조"
  }
  , 
  
  "/articles/AI/CV/%EC%9D%B4%EB%AF%B8%EC%A7%80%20%EB%B6%84%EB%A5%98%20%EA%B0%9C%EB%85%90.html": {
    title: "이미지 분류 개념",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/CV/%EC%9D%B4%EB%AF%B8%EC%A7%80%20%EB%B6%84%EB%A5%98%20%EA%B0%9C%EB%85%90.html",
    tags: ["AI","PYTHON","CV"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true이미지 분류(Image Classification Competition) 개념  네이버 AI BoostCamp의 P stage 이미지 분류 강의를 정리하였습니다.## Competition with AI StagesCompetition이란?여러 회사, 단체 등에서 상금을 내걸고 데이터를 제공하여 원하는 결과를 얻는 가장 좋은 방법을 모색한다.  데이터 뿐만 아니라 컴퓨팅 자원을 지원할 때도 있다.Kaggle이 competition platform으로 대표적이며, 국내에서는 DACON 이 존재한다.                   Pipeline                  ML pipeline                    Competition pipeline            실제 머신러닝 업무의 Pipeline과 Competition의 Pipeline이 비슷한 경우가 많기 때문에 Competition을 통해 ML 실력을 갈고 닦기 좋다!Overview문제에 대한 정의, 요구사항과 직면하고 있는 문제점 등에 대해서 자세히 알 수 있다.위의 경우, 단순히 악성 댓글을 파악하는 문제가 아니라, 악성댓글 파악 중 야기되는 정치적 이슈에 대한 설명이 포함되어 있다.위의 overview를 자세히 보는 것으로 내가 풀어나가야할 문제에 대한 방향성을 얻어낼 수 있으며, 이러한 행위를 Problem Definition(문제 정의)라고 한다.이러한 문제 정의 행위로는  내가 무슨 문제를 풀어야 하는가?  Input과 Output은 무엇인가?  이 솔루션은 어디서 어떻게 사용되어지는가?등의 예시가 있다.Data DescriptionFile 형태, Metadata Field 소개 및 설명이 적혀있는 부분데이터 스팩의 요약본, EDA, 문제정의 등에 중요하다.Notebook데이터 분석, 모델 학습, 테스트 셋 추론의 과정을 서버에서 연습 가능단순히 연습용이 아니라, 일부 Competition의 경우는 회사에서 이러한 방식으로 Computing power를 제공하는 경우도 있으며, 성능을 강제하기 위해 사용이 강요되는 경우도 있다.Submission자신이 제출했던 테스트 예측 결과물들의 정보를 확인하고, 제출할 결과물을 선택할 수 있다.Leaderboard자신의 제출물의 순위를 확인할 수 있다.DiscussionKaggle 등에서는 Discussion에서 다양한 정보를 주고 받는 문화가 발달함.정보를 공유함으로, 경쟁자의 점수가 올라갈 수 있음에도 불구하고, 등수를 올리는 것 보단 문제를 해결하고 싶은 마음으로 올리는 경우가 많음.하지만 경연 종료 1~2주 전에는 보통 Critical한 정보의 공유는 멈춰지는 경우가 많다.ML에 관한 정보뿐만 아니라, 도메인 지식, CS 지식 등도 공유된다.EDA(Exploratory Data Analysis, 탐색적 데이터 분석)EDA는 ML pipeline 중 Data Analysis 부분에 해당한다EDA란?EDA(Exploratory Data Analysis, 탐색적 데이터 분석), 데이터를 이해하기 위한 노력데이터의 구성, 형태, 쓰임새 등을 분석하는 것  주제와 연관성, 데이터 타입의 특성, 메타데이터의 분포 등이 있다.머신러닝 파이프라인에 많은 것에 영향을 주게 된다.EDA 결과물의 예시, 데이터 이해가 목적이므로 정해진 툴이나 절차 없이 궁금한 점을 찾아보는 과정이다.또한, 한번으로 끝나는 과정이 아니며, 언제나 의문이 생기면 EDA는 pipeline 과정 중간에 언제나 돌아올 수 있다.툴로는 python, excel 등이 존재Image Classification이미지란, 시작적 인식을 표현한 인공물(Artifact)채널이 3개인 경우 RGB(Red, Green, Blue),  채널이 4개인 겨우 CMYK(Cyan, Magenta, Yellow, Black), 또는 RGBA(+alpha)를 표현한 경우가 많다.uint8(unsinged integeter 8bit)로 표현하는 경우가 많으며, 0~255까지 256의 범위를 가진다.DatasetDataset 형성, Data Generation은 Data Processing의 과정이다.Raw Data를 모델이 사용할 수 있는 Dataset으로 만들면 다양한 모델이나 쓰임새에 쉽게 사용할 수 있을 뿐만 아니라 인간이 쉽게 이해할 수 있다.Pre-processingPre-processing(전처리)는 pipeline에 시간과 중요성, 비중 모두 커다란 부분을 차지하고 있다.Competition data는 제대로된 Data를 주는 편이지만, 수집한 데이터의 경우 절반 이상을 날려야 하는 경우가 많음.이미지의 전처리의 경우, 다음과 같은 예시가 있다.  Bounding box이미지의 중요한 부분을 Crop한 뒤 Resize하여 학습시킬 모델을 더욱 명확히 함좌측 상단과 우측 하단의 좌표를 이용해 Bounding box를 그릴 수 있음  Resize계산효율을 위해 적당한 크기로 이미지 사이즈 변경이미지 사이즈가 바뀌어도 성능에 영향은 미미한 경우가 많다.  이미지 채도, 대비 변경상단은 홍채 사진을 더욱 뚜렷히 바꾼 것명확하지 않은 사진을 채도, 대비, 명도,등을 바꾸어 선명하게 바꿈전처리가 성능의 향상을 보장하진 않는다.GeneralizationBias &amp; VarianceUnderfitting : 학습이 부족하여 제대로 구별할 수 없음Overfitting : noise를 포함해서 모든 학습이 너무 많이되어 너무 자세하게 구분함일반화, 즉 학습한 데이터셋 이외의 Input에서는 부정확한 결과를 내게 됨Train/Validation훈련 셋 중 일정 부분을 따로 분리하여 검증 셋으로 활용학습 가능한 데이터양이 줄어들지만, 학습되지 않은 Validation Set의 분포를 통해 일반화 성능을 검증할 수 있다.검증 절차를 통해 Generalization(일반화) 성능을 늘릴 수 있다.Data Augmentation데이터를 일반화하는 과정, 주어진 데이터가 가질 수 있는 Case(경우), State(상태)를 다양하게 함.이미지의 경우 다양한 필터, 채도, 색상 변경, 이미지 회전, Crop 등, 원본 이미지를 변경하는 것을 통하여 데이터를 늘림.문제가 만들어진 배경과 모델의 쓰임새를 살표보아 Data Augmentation의 힌트를 얻을 수 있다.(ex)밤과 낮에 따로 촬영될 수 있는가? -&gt; 명도 변경)torchvision.transforms 에서 Image에 적용 가능한 다양한 함수를 살펴볼 수 있다.Albumentations라는 Library는 좀 더 처리가 빠르고 다양한 기능을 제공함Data Generation데이터 전처리, Augmentation 등의 출력 결과가 모델의 성능, 시간 등에 영향을 주는 경우가 많다.Data Feeding모델의 학습속도와 이전 단계가 느리다면 전체적인 학습속도가 느려질 수 밖에 없다.단순히 Data Augmentation의 순서가 달라지는 것 많으로도 성능이 달라진다.데이터셋 생성능력을 비교하여 적절한 성능을 정해보자.torch.utils.dataDatasetpytorch에서 정의할 수 있는 데이터셋 생성 방법은 다음과 같다.from torch.utils.data import Datasetclass MyDataset(Dataset): # torch.utils.data.Dataset 상속    def __init__(self): # MyDataset 클래스가 처음 선언되었을 때 호출        print(&amp;#34;Class init!!&amp;#34;)        pass    # test = MyDataset() 결과: # Class init!!   \t    def __getitem__(self, index): # MyDataset의 데이터 중 index 위치의 아이템을 리턴        return index   \t# test[2341] 결과: # 2341    \tdef __len__(self):  # MyDataset 아이템의 전체 길이        print(&amp;#34;length is 3&amp;#34;)        return 3    # len(test) 결과: # lenth is 3 # 3이를 통해 Vanilla 데이터를 원하는 형태로 출력 가능DataLoaderDataset을 효율적으로 사용할 수 있도록 관련 기능 추가train_loader = torch.utils.data.DataLodaer(train_set,batch_size=batch_size, # batch 사이즈num_workers=num_workers, # 병렬 코어 갯수drop_last=True # batch 사이즈에 맞지 않는 마지막 batch 버림)이외에도 shufle, sampler, batch_sampler, pin_memory 등 여러 기능이 있다.collate_fn : batch 단위 마다 실행할 함수 넣어주기num_workers 사용 결과너무 큰 값을 사용하면, 딥러닝 이외의 시스템에서의 사용하는 연산이 간섭하여 오히려 성능이 떨어진다고 함.Model모델은 데이터셋을 Input으로 받아 원하는 출력을 만들어 준다.객체의 정보적 표현을 모델이라고 한다.Design Model wtih Pytorch사용성 좋고 연구에 편리한(Pythonic) Low level 오픈소스 머신러닝 프레임워크좌 keras 코드와 우 pytorch 코드를 비교하면 좀더 Low level에 가까운 것을 알 수 있다.이를 통해 원리의 이해와 customizing이 좀더 쉽다.nn.ModulePytorch 모델의 모든 레이어는 nn.Module 클래스를 따른다.modules 함수를 통해 내부에 nn.Module를 상속하는 layer들을 확인할 수 있다.nn.Module 클래스는 일종의 정보를 저장하는 저장소 역할을 하며 이를 상속받아 layer(모델, layer 등)를 만든다.  이때 모델 내부의 conv layer 같은 layer들은 child 모듈이라고 하며, nn.Module을 상속받은 모든 클래스의 공통된 특징이다.          이 특징을 통해 연결된 모든 module을 확인할 수 있다.      이후 이 모델이 호출 되었을 때는 forward() 함수가 불러진다.이때 input을 넣어주며, 또는 직접 모델명.forward(input)으로 불러도 된다.  최상위 nn.module의 forward() 함수 실행으로 정의된 모듈 각각의 forward()가 모두 실행된다.Parametermodule 내부에는 Tensor 기반의 parameter를 가지고 있을 수 있다.parameter는 또한, data, grad, requires_grad 등의 변수를 가지고 있으며      data : paremater의 수치        grad : loss에 의해 정해진 gradient 값        requires_grad : boolean, 해당 parameter를 backpropagation 시 업데이트 할 것인가?  이런 식으로 파이토치는 형식과 구조를 파악하고 응용하기 쉬우며 디버깅도 쉽다Pretrained Model대량의 데이터셋, 오래 걸리는 학습 시간, 복잡한 레이어 구축 등을 고려하면 처음부터 모델의 모든 것을 만드는 것은 힘들다.그러므로 YOLO와 같이 미리 만들어지고 학습된 Pretrained Model을 내 Task에 맞게 다듬어서 사용하면 이러한 노력을 줄이면서도 성능을 보장할 수 있다.Pretrained model의 발전에는 일반화가 뛰어나고 데이터의 양과 질이 뛰어난 오픈소스 데이터셋인 IMAGENET의 등장에서 부터 시작되었다.TorchVision 등에서 손쉽게 이용 가능하며, pretrained=True로 설정시, 구조 뿐만 아니라 학습한 weight 또한 같이 가져온다.Transfer Learning앞서 가져온 Pretrained model을 우리가 원하는 용도로 사용하기 위해 학습하는 것을 Transfer Learning이라고 한다.나의 Task와 비교하여 적절한 Transfer Learning 방법을 고려해야 한다.  학습데이터가 충분한 경우Task가 ImageNet과 비슷할 경우, Classifier만 학습시킨다.(feature-extraction, 특색 추출)Task가 ImageNet과 비슷하지 않을 경우, Classifier뿐만 아니라 CNN Backbone또한 다시 학습시킨다.(Fine-tuning, 미세 조정)  이때 구조만 살리고 처음부터 하는 것이 아니라, 기존의 weight를 가지고 있는 채로 다시 학습하는 것이다.(보통 성능이 더욱 좋다.)  학습 데이터가 충분하지 않은 경우Task가 ImageNet과 비슷할 경우, Classifier만 학습시킨다.Task가 ImageNet과 비슷하지 않을 경우, 성능이 그리 높지 않으므로 추천하진 않는다.Training &amp; InferenceLoss, Optimizer, MetricTraining에 필요한 요소는 크게  Loss  Optimizer  Metric으로 나뉜다.LossLoss 값은 정답인 Target과 예상값인 Output의 차이를 Loss 함수(Cost 함수, Error 함수)를 통하여 구한다.  Loss 함수는 목적이나 Task 등에 따라 잘 골라줘야 한다.이후 Error Backpropation을 통해 가중치가 업데이트 되게 된다.nn 패키지 내부의 Loss함수 또한 Module을 상속하고 있으며, 이를 통해 모델 등의 child Module이 되어 보여지거나 Forward가 존재하는 등의 특징을 갖는다.그래서 커스텀 Loss를 만들려면 Module을 상속해야 함.criterion(관용적인 변수명)에 loss 함수를 정의한 뒤 output과 labels를 넣어 구한 loss의 backward 함수가 실행되면 모델의 파라미터 grad가 업데이트된다.업데이트하지 않고 Freeze하고 싶으면 requires_grad를 false로 설정하면 된다.  이때 모델(net)-&gt;output-&gt;criterion-&gt;loss 순으로 연결이 되는 체인을 통하여 backpropation이 가능하게 된다.Loss 함수에 조금 과정을 추가한 특별한 Loss 또한 존재한다.Focal Loss : Class Imbalance 문제를 해결하기 위해 맞춘 확률이 높은 Class는 조금의 Loss를, 맞춘 확률이 낮은 Class는 Loss를 훨씬 높게 부여Label Smoothing Loss : Class target label을 Onehot 표현으로 사용하기 보다는 조금 Soft하게 표현해서 일반화 성능을 높임  즉 [0, 1, 0, 0, 0] 같은 형태 대신 [0.025, 0.9, 0.025, 0.025, …] 같은 형태OptimizerOptimizer는 Loss 함수의 결과물인 Loss와 방향과 Learnig rate에 따라 weight를 업데이트 한다.\\(w'=w-\\eta\\frac{\\part E}{\\part w}\\\\\\eta:Learning\\ rate(학습률),\\ \\frac{\\part E}{\\part w}:방향, E: error\\)Optimizer는 LR scheduler를 통해 Learning rate를 학습 중에 동적으로 조절할 수 있다.Scheduler의 종류는 다음과 같다.  StepLR특정 Step 마다 LR 감소  CosineAnnealingLRCosine 함수 형태처럼 LR을 급격히 변경변화를 급격하게 주어, Local minimum에 빠지는 경우를 막음  ReduceLROnPlateau일반적으로 많이 사용함, 더 이상 성능 향상이 없을 때 LR 감소Metric학습된 모델을 객관적으로 평가할 수 있는 지표가 필요R…은 ROCAOC여러 Task, 목적, 데이터 상태 등에 따라 여러 Metric의 종류가 있다.Ranking : 추천 시스템에 사용되는 경우가 많음.상단의 예시처럼, 데이터의 편향 등에 따라 정확도가 다르게 나올 수 도 있다.Classification의 경우,그러므로 Class 별로 밸런스가 적절히 분포되었을 시, Accuracy,Class 별 밸런스가 좋지 않을 경우 F1-Score를 사용하는게 좋다.Training Processpytorch는 머신러닝의 과정을 잘알 수 있게 끔 구성되었다우리는 앞서 Dataset, Model, Loss, Optimizer, Metric 등 Training을 하기 위한 준비에 대해 배웠다.Training을 할때 코드를 이해해 보자.  Model.train()모듈을 상속한 객체를 training 모드로 바꿔준다.training/evaluation 모드의 차이에 따라 Dropout이나 BatchNormalzation 등의 행동이 조금 달라지게 된다.  optimizer.zero_grad()optimizer가 이전 batch 사용했던 Parameter들의 grad를 초기화해준다.기본값이 아닌 이유는 Loss를 쌓아서 사용하는 방법도 존재하기 때문  loss = criterion(outputs, labels)loss 함수 또한 Module을 상속하므로 input 부터 output 까지 연결되는 체인이 생겨난다.loss의 grad_fn chaing에 loss의 backward() 함수가 들어가 있으며, 내부에 next_fucntion을 통해 다음 layer와 죽 연결되어 있음을 확인할 수 있다.  optimizer.step()step()를 실행하면 optimizer가 loss backward를 통해 업데이트된 gradient를 이용하여 weight들을 업데이트한다.정확한 내부 로직은 optimizer 마다 다르다.Gradient Accumulation은 optimizer의 step가 zero_grad를 특정 배수 epoch에만 실행한다.batch size를 늘리는 것 과 같은 효과를 내면서 동시에 batch size를 늘렸을 때의 성능 요구를 줄였다.Inference Process  model.eval()모델을 evaluation 모드로 바꾼다. 위의 train()과 같은 이유  with torch.no_grad():with torch.no_grad()내부 로직일때는 모든 parameter들이 require_grad=False 인 상태와 같아진다.Inference 중일때는 내부 parameter가 업데이트 되지 않는다.Validation data로 Inference 하는 코드가 검증 코드이다.checkpoint는 inference 결과를 보고 모델을 저장하면 된다.마지막으로 eval의 결과를 submission으로 바꾸어 제출하면 된다.PyTorch Lightning을 이용하면 코드를 좀더 간결하고 읽기 쉽게 사용할 수 있다.Ensemble(앙상블)싱글 모델보다 더 나은 성능을 위해 서로 다른 여러 학습 모델을 사용하는 것효과는 있지만, 학습, 추론 시간이 배로 소모됨딥러닝은 Overfitting이 생길 경향이 상당히 많다.앙상블의 기법은 Begging, Stacking, Boosting 등의 기법이 있으며Underfitting(High Bias)을 해결하기 위해 Boosting이 주로 자주 사용된다.  정형데이터 등에서도 gradient Boosting이나 XGBoost 같은 방법이 사용된다.Overfitting(High Variance)를 해결하기 위해 Begging이 주로 사용된다.  여러 정답을 취합해서 평균을 내는 방법, Random forest 등이 있음Model Averaging(Voting)앙상블에서 사용하는 방법, 서로 다른 특색을 가진 모델이 애매한 결과들을 냈을 때, 투표를 통하여 답을 구하는 것  One-hot vector로 voting하는 Hard Voting(다수결)이 있고,  가중치들의 합으로 결정하는 Soft Voting이 있다.          일반적으로 성능이 더욱 좋다.      Cross Validation(CV)기존에 Training에 사용하지 않던 Validation set을 학습 시 마다 다른 분포를 사용하여 학습에 이용해 보는 방법Stratified K-Fold Cross ValidationValidation이 생성될 수 있는 모든 경우의 수와 고른 Class 분포까지 고려한 CV단, K(일반적으로 5개)개 만큼의 모델이 생성되므로, 학습시간이 그만큼 커진다.TTA(Test Time Augmentation)Test data set을 Augmentation 호 모델 추론, 출력된 여러가지 결과를 앙상블Hyperparametr Optimization시스템의 매커니즘에 영향을 주는 주요한 파라미터들을 찾아내는 것  Learning rate, Batch size, Hidden Layer 갯수, Loss 파라미터, Dropout, k-fold, Regularization, Optimizer 파라미터 등이 있다.사실, Ensemble 만큼이나 시간과 자원이 많이 필요하지만 성능 상승은 미비하다.Grid search, Random search, Beysian Optimization 등의 방법이 있음  Beysian Optimization: 이전 결과를 바탕으로 최적의 방법을 찾아가는 방법Optuna library파라미터 범위를 주고 그 범위 안에서 정해진 Trials 만큼 시행하도록 도와줌Experiment Toolkits &amp; TipsTraining VisualizationTensorboard기존에는 Tensorflow 용이였지만 Pytorch에도 포팅됨, 학습과정을 기록하고 트래킹 가능한 툴데이터 확인 등도 가능tensorboard --logdir PATH(log가 저장된 경로) --host ADDR(default localhost) --port PORT(포트 번호)위와 같은 번호로 사용 가능Weight and Bias(wandb)딥러닝 로그계의 깃허브Jupyter Notebook : Cell 마다 사용가능함, EDA시 많이 활용Python IDLE : 디버깅 쉬움, 코드 실행이 안정적임, 자유로운 실험 핸들링그 외의 팁  데이터 분석 코드 시, 코드 자체보단, 필자의 설명글을 유심히 보자      언제나 활용가능하도록 코드를 자세히 이해하자        paperwithcode.com 에서 최신 논문과 코드까지        자신의 지식을 자주 공유하자.  "
  }
  , 
  
  "/articles/AI/CV/%EC%BB%B4%ED%93%A8%ED%84%B0%EB%B9%84%EC%A0%84%20%EA%B8%B0%EB%B3%B8.html": {
    title: "컴퓨터비전 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/CV/%EC%BB%B4%ED%93%A8%ED%84%B0%EB%B9%84%EC%A0%84%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","PYTHON","CV"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true컴퓨터비전(Computer Vision, CV) 기본  네이버 AI 부스트 캠프의 CV 강의를 정리한 내용입니다.Image Classification 1Course overviewArtificial Intelligence(AI) : 사람의 지능을 컴퓨터 시스템으로 구현하는 것사람의 학습은 오감을 조합해서 사용하는 multi-modal perception과 비슷하다.뿐만 아니라 사회적 감각(표정 살피기, 관계 맺기, 의도 파악하기) 등의 복합적 감각이 존재이중 시각적 능력이 기초적이고 중요한 능력이다.[img. 복잡한 인간의 인지]            인간의 시각적 처리 VS 컴퓨터의 시각적 처리                                      [fig. 사진의 representation(자료구조, 여기서는 처리 결과) 구하기]또한, 우리의 시각적 능력 또한 이런 머신러닝 구조의 CV와 비슷하다.[img. 대처 환각, 거꾸로 된 사람은 많이 보지 못하기 때문(= 학습 편향)에 어색함을 느끼지 못한다.]            Machine Learning vs Deep Learning                                      [img. 특징 추출까지 처리가 가능해진 Deep Learning 분야]특징 추출을 사람이 설계하지않고 Gradient-Descend를 통해 End-to-End로 학습함으로, 편향과 오류를 줄여 최근 CV기술이 고도화 됨이번 강의에서  기본적인 CV Task  CV의 딥러닝 기술  시각 정보 + 다른 감각 데이터의 융합  Conditional generative model  Visualization Tool등을 배울 것이다.Image classification(영상 분류)Classifier (분류기) 업무는 입력된 영상의 카테고리, 클래스를 분류하는 mapping 업무이다.[img. Classifier 예시]만약, 대량의 이미지 데이터가 있다면, 비슷한 사진끼리 모아 구별하는 k-Nearest Neighbors(k-NN)를 통하여 구별할 수 있다.  query data point의 주변 reference point를 이용해 구분하는 방법[img. K-NN 예시 이미지]하지만, 데이터 수가 너무 많으면 많은 만큼 시간과 메모리가 부족하게 된다.이를 방지하기 위해 Neural Networks를 이용해 Compress 된 모델을 통해 구별하게 된다.[img. 1층의 fully-connected layer을 이용한 구분]영상의 모든 픽셀을 Input으로 넣어 구분할 수도 있지만, 이 역시 성능과 메모리 소모가 크다.[img. FCN과 CNN의 비교]CNN은 전체 pixel이 아닌 이미지의 일부를 지역적으로 Window Sliding 방식으로 하나의 feature로 추출하고(Local feature learning), 파라미터 공유를 통하여  성능과 메모리의 요구를 줄이고  오버피팅을 줄일고  사진 일부만으로도 사진을 구별할 수 있게 됬다.이러한 장점 덕분에 많은 Computer Vision 업무에 기본이 됨.[img. CNN의 여러 사용]CNN architectures for image classification 1[img. AlexNet과 VGGNet의 등장]  딥러닝 CV의 등장을 알린 두 모델  AlexNet[img. AlexNet의 구조]  최초의 심플한 CNN 구조인 LeNet-5의 구조에서 더 깊은 층, 더 많은 데이터셋, ReLU 활성 함수와 drop out과 같은 regularization 기술을 이용한 모델  당시 GPU 메모리의 한계로 인해 2갈래로 나누어 학습한 뒤 중간에 Activation map으로 Cross Communication 해줌nn.conv2d(3, 96, kernel_size=11, stride=4, padding=2) # 11x11 Conv(96), stride 4 Layer# 이미지 크기가 커지면서, Receptive field를 크게 만들어주기 위해 11x11로 시작, 최근에는 더이상 사용 안함nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=3, stride=2)# 3x3 MaxPool, stride 2 Layernn.conv2d(96, 256, kernel_size=5, padding=2) # 5x5 Conv(256), stride 2 Layernn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=3, stride=2)nn.conv2d(256, 384, kernel_size=3, padding=1)# 3x3 Conv(384), pad 1 Layernn.ReLU(inplace=True)nn.conv2d(384, 384, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.conv2d(384, 256, kernel_size=3, padding=1)nn.ReLU(inplace=True)nn.MaxPool2d(kernel_size=3, stride=2)torch.flatten(x, 1) # Fully connected Layer에 넣기 전에 다차원의 Tensor를 1차원 Tensor으로 길이를 길게 늘어 뜨리기(벡터화)# AlexNet에서 사용한 방법# nn.AdaptiveAvgPool2d((6,6))# 비슷한 방법이지만, 길게 늘어뜨리지 않고 평균을 내어 같은 길이의 1차원으로 바꿈nn.Dropout() # Dense(4096)nn.Linear(256*6*6, 4096) # 2stream 구조가 아니라 1개로 통합했으므로 2048 *2 = 4096nn.ReLU(inplace=True)nn.Dropout()nn.Linear(4096, 4096)nn.ReLU(inplace=True)nn.Linear(4096, 1000)[code. AlexNet의 코드구현]  시간이 흘러 GPU 메모리가 늘어나 2 stream 구조로 구현하지 않음  LRN(Local Response Normalization) 구현 안함          더이상 사용하지 않는 방법, Batch normalization으로 대체됨      Activation map 이후, 명암을 normalization 해주는 역할      Receptive field란?layer output 값을 만들기위해 Input image에서 CNN layer가 참조한 공간, 클 수록 이미지의 많은 부분을 참조한 것이다.여러 층이 중첩되도 처음 image에서 확인한 부분이 Receptive field이다.위 구조에서는 전체를 11x11 conv로 Input 이미지 전부를 Receptive field로 삼았다.[img. Receptive Field 도식화]KxK conv stride 1 layer와 PxP pooling layer를 통과한 경우의 Receptive field의 크기는(P+K-1)x(P+K-1)이다.  VGGNet[img. VGGNet의 구조]  AlexNet보다 깊고(16, 19Layer)  더욱 심플한 구조이며          Loca Response Normalization(LRN) 사용 안함      conv filter layer와 max pooling layer의 크기를 각각 3x3, 2x2만 한하여 사용(가장 큰 특징)                  이를 stack하여 큰 Receptive Size를 얻으면서 더 깊고 복잡하면서 파라미터 수는 줄여 성능과 정확도를 동시에 잡을 수 있다.                      더욱 좋은 성능과 일반화(Generlization)을 내는 모델이외에는 AlexNet과 비슷하다.  ReLU 사용,  Input에서 224x224 RGB 이미지를 Normalization(RGB 평균값을 RGB 값에서 빼줌)하여 넣어줌Annotation data efficient learning  질좋은 데이터셋은 성능에 큰 영향을 미치지만 확보하거나 만드는데 큰 어려움이 따른다.  CV에서의 데이터 부족 완화 방법을 알아보자.Data augmentation손쉽게 데이터셋을 늘릴 수 있는 방법                   여러 물체      장소      공원                  그림                                편향      겹쳐 보면 정면에 특정 각도로 물체들이 위치      장소 사진들을 겹쳐보면 해안선, 물체, 건물 등의 위치가 겹침      사람을 위주로 찍게 되어 중앙에 사람이 위치      [fig. 편향의 예시]Dataset들은 인간의 필요에 의해 편향된 채로 촬영되게 되며, 이는 현실의 데이터와 괴리를 준다.            Samples in the training set      Real data distribution                              애매한 데이터들이 여러 class와 겹치는 방식인 현실 데이터와 달리 sample data들은 확실하고 명확한 경우가 많으므로 이로 인해 모델에 혼란이 올 수 있다.예를 들면, 많은 사람들이 밝은 조명 아래에서 사진을 찍는다, 그리고 달은 어두운 밤하늘에만 찍힌다. 만약 이를 데이터셋으로 학습한 모델이 어두운 곳에 찍힌 사람을 보면, 달로 착각할 수도 있다.[img. Augmenation의 예시]이를 막기위해, 밝기 바꾸기, 회전, crop, 일부 가리기, 채도 변경 등의 방법으로 이미지를 바꾸어 현실 데이터와 비슷하게 만들면서 데이터셋 크기를 늘릴 수 있다.OpenCV, NumPy 등에서 library로 활용할 수 있다.                                                                       Rotate, Flip      Brightness adjustment      Crop                                        Rotate, Flip      Affine transformation      CutMix      [table. 여러 종류의 augmentation 종류]def brightness_augmentation(img):    # numpy array img has RGB value(0~255) for each pixel    img[:,:,0] = img[:,:,0]+100 # add 100 to R value    img[:,:,1] = img[:,:,1]+100 # add 100 to G value    img[:,:,2] = img[:,:,2]+100 # add 100 to B value        img:[:,:,0][img[:,:,0]&amp;#38;#62;255] = 255 # clip R values over 255    img:[:,:,1][img[:,:,1]&amp;#38;#62;255] = 255 # clip G values over 255    img:[:,:,2][img[:,:,2]&amp;#38;#62;255] = 255 # clip B values over 255    return img[code. 밝기 조절 Augmentation 코드]img_rotated = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)img_flipped = cv2.rotate(image, cv2.ROTATE_180)[code. 회전, 뒤집기 Augmentation 코드]y_start = 500 # y pixel to start croppingcrop_y_size = 400 # cropped image&amp;#39;s heightx_start = 300 # x_pixel to start croppingcrop_x_size = 800 # cropped image&amp;#39;s widthimg_cropped = image[y_start:y_start+crop_y_size, x_start : x_start + crop_x_size, :][code. Crop Augmentation 코드]rows, cols, ch = image.shapepts1 = np.float32([[50,50],[200,50],[50,200]])pts2 = np.float32([[10,100],[200,50],[100,250]])M = cv2.getAffineTransform(pts1, pts2)shear_img = cv2.warpAffine(image, M, (cols,rows))[code. Affine Transformation Augmentation]Affine Transformation : 사각형 이미지를, 기울어진 평행 사변형(parallelogram)의 형태로 바꿈 + rotation 함  shear transformation 이라고도 함RandAugment  Augmenation의 종류, 강도(얼마나 밝은가, 얼마나 기울어졌는가 등)에 따라 모델 성능이 달라지므로, 이를 parameter로 사용할 수 있다.  랜덤으로 Augmentation을 적용한 뒤, 평가하는 과정을 거침[img. ShearX &amp; AutoContrast 9 Randaug 예시]  적용할 Augmentation, 적용 강도, 2가지가 Parameter로 적용[img. RandAugmentation 사용에 의한 성능 향상]Leveraging pre-trained informationpre-trained 된 모델을 활용하는 방법Transfer learningTransfer learning: 한 데이터셋에서 배운 지식을 다른 데이터셋, 다른 Task에서 활용하는 기술  이를 통하여 작은 데이터셋으로도 높은 정확도를 자랑한다.  Transfer knowledge from a pre-trained task to a new task(같은 Layer, 다른 Task에 활용)[img. Transfer model(좌)를 원하는 업무에 맞게 FCL을 변경]  마지막 Fully connected Layer만 바꾼 뒤, 이전 Convolution Layer의 Weight는 그대로 둔채 바꾼 층만 학습 시켜 활용하는 방법  pre-trained 모델의 feature가 그대로 유지됨  Fine-tuning the whole model[img. FCL을 변경 후 Convolution Layer도 Low lr로 학습]  마지막 층을 바꾼 뒤, 기존 층은 낮은 Learnig rate, 새로운 층은 High learning rate를 유지하며 학습  자신의 Task에 맞게 기존 모델을 조금 수정가능Knowledge distillationpre-trained 모델의 예측값을 활용해 다른 모델을 학습시키는 방법Teacher-student learning보통 두가지 목적으로 쓰임  더 경량화된 모델을 만들어, 기존 모델 보다 경량화에 사용  unlabeld dataset의 pseudo-labelling에 사용(레이블링 안된 데이터셋에 라벨링)학습 구조에 따라 2가지로 나뉜다.  Teacher-student network structure  student 모델(보통, Teacher Model보다 경량화되어 있다.)이 Teacher 모델의 output을 따라하게끔 학습시킴  Unsupervised learning으로, label 되지 않은 dataset을 사용한다.  두 결과를 비교하여 KL divergence Loss를 통해 Loss 값을 구한뒤, student model로만 backpropagation이 진행되게 된다.          KL div Loss: 2개의 분포의 거리(=얼마나 비슷한가)를 측정      [img. Teacher-student network structure]  Knowledge distillation[img. Knowledge distillation structure]  Labeled된 데이터셋을 사용할 때 사용하는 구조  Student Model과 Teacher Model의 차이를 Distillation Loss, Student Model과 Ground Truth(Label)과의 차이를 Student  Loss라 정의 한다.  이때 Student 모델은 Soft label을 이용한 Soft Prediction을 이용한다.          Hard label(One-hot vector) : dataset의 label값처럼, 하나의 명확한 정답을 가짐      Soft label: model의 softmax를 통과한 뒤 기본 출력값, 여러 정답에 float 값을 가지는 가중치같은 형태      이를 이용해 단순 정답이 아닌, 어느 정도로 정답에 근접했는가 등의 추가적인 정보를 사용하여 학습할 수 있다.      \\(Hard\\ label:\\begin{pmatrix}Bear\\\\Cat\\\\Dog\\end{pmatrix}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\\\Soft\\ label:\\begin{pmatrix}Bear\\\\Cat\\\\Dog\\end{pmatrix}=\\begin{pmatrix}0.14\\\\0.8\\\\0.06\\end{pmatrix}\\)[math. Hard label과 Soft label의 예시]  또한 Knowledge distilation에서의 Distillation Loss를 구할 때, Softmax 함수에 temperature(T)를 이용하여 기존의 output보다 Smoothing한 결과를 사용할 수 있다.          이를 이용해 좀더 결과값에 많은 정보를 포함할 수 있다.      예를 들어 기존의 softmax(5,10) = (0.0067, 0.9933)이라면      t=100인 softmax(5,10) = ( 0.4875, 0.5125)로 비슷한 값을 smoothing 된다.      전자의 결과의 0,0067은 무시될 정도로 작은 정보지만 후자의 경우는 무시못할 정보가 된다.      \\(Normal\\ Softmax(=Hard\\ Prediction):\\frac{\\exp(z_i)}{\\sum_j\\exp(z_j)}\\\\Softmax\\ with\\ temperature\\ T(=Soft\\ Prediction):\\frac{\\exp(z_i/T)}{\\sum_j\\exp(z_j/T)}\\)[math. softmax with temperature]  Distillation Loss를 구할 때, Teacher Model의 soft Label의 세부정보(Semantic information)은 고려하지 않는다.          교육받고 바꿔야할 모델은 Student Model 이기 때문에      Distillation Loss의 경우, Teacher model vs Student model의 차이를 의미  KL divergence loss 사용: 비교 하는 두 값이 0~1사이의 값들의 분포이므로Student Loss의 경우, 실제 답과 student model의 정답을 비교  CrossEntropy 사용: True label의 경우 one-hot vector 형태이므로마지막으로 위에서 구한 두 loss들의 가중치 합을 통해 loss를 구한 뒤, student model만, Backpropation을 한다.Leveraging unlabled dataset for trainingSemi-supervised learning  labeled된 적은 수의 데이터와, label되지 않은 많은 양의 데이터를 활용하는 방법          즉 unsupervised + Fully supervised = semi-supervised      1) 레이블링된 데이터셋으로 모델 형성2) 형성된 모델로 레이블링되지 않은 데이터셋 레이블링3) 그렇게 레이블링된 데이터셋과 기존의 라벨링된 데이터셋으로 새로운 모델 형성Self-training앞서 배웠던 Data Augmentation, Knowledge distillation, Semi-supervised learning을 이용한 방법[img. self-training의 단계]  lable된 데이터셋으로 Teacher model을 형성한다.  해당 Teacher model로 unlabled된 model을 Pseudo-labeled data로 만든다.  lable된 데이터셋 + pseudo-lable된 데이터셋을 augmentation 한 것을 통하여 새로운 Teacher model을 형성한다          이때 주로 사용하는 augmentation  방법이 RandAugment        새로 형성된 Teacher 모델로 2번부터 4번까지 반복한다.[img. 압도적인 성능을 자랑하는 self-training 모델(빨간색)]Image classification 2Problems with deeper layers성능 향상을 위해 딥러닝 layer의 층을 높게 쌓으면서 다음과 같은 문제가 생겼다  Gradient vanishing/exploding 문제  Computationaly complex  한때 overfitting 문제로 착각했던 Degradation problem[img. Vanishing gradient 문제의 도식]CNN architectures for image classification 2GoogLeNet2015년에 발표된 Inception 모듈을 활용한 CV 모델Inception module이란?      이전 층에서의 결과값에 여러개의 필터를 적용한 뒤, Concatenate하는 layer          1x1, 3x3, 5x5 Convolution filter, 3x3 max pooling layer를 적용      [img. Inception module의 예시]이때, 여러 필터의 적용에 의해 parameter수가 증가하자, 1x1 convolution layer(Bottleneck layer)을 추가하여 파라미터 수를 줄이는 시도를 함  우측의 Dimension Reduced version에 추가된 1x1 convolution layer를 의미[img. 1x1 convolution layer의 연산 결과]GoogLeNet의 전체적인 구조를 살펴보면 다음과 같다.  Stem network: 기본적인 convolution network[img. Stem network 부분]  Stacked inception modules: 위에 설명한 Inception 모듈을 쌓아놓은 부분[img. Stacked inception modules 부분]  Auxiliary classifiers  Vanishing gradient 문제를 해결하기 위한 부분  중간의 결과값을 한번 예측값으로 삼고, loss 값을 계산하여 중간부터 backpropagation을 진행한다  training에서만 사용하고 testing 단계에서는 사용하지 않는다.[img. Auxiliary classifiers 부분][img. 더욱 자세한 Auxiliary classifier]ResNet현재까지 기본 backbone으로 쓰이곤 하는 좋은 모델  최초로 인간 보다 나은 성능을 달성(에러율 기준)  기존의 모델보다 압도적으로 깊은 층의 갯수(152 Layer)기존의 연구에서는 층이 깊을 수록 오히려 성능이 떨어지는 문제를 Overfitting 문제라고 오판하였다.[img. 층의 갯수에 따른 에러율, 높을 수록 안좋음]Overfitting의 문제였다면, training error는 점점 나아져야하고, test error가 나빠져야 하지만, 둘다 성능이 나빠졌기 때문이다.따라서 Resnet에서는 이를 Overfitting이 아닌 Optimization(최적화)의 문제라고 보았다.ResNet의 연구 가설은 다음과 같다.[img. Residual block과 Plain layer의 차이]기존의 Plain layer의 경우 층이 깊어질 수록 복잡해진 H(x)에 X를 보존하면서 학습하기 힘들었다.하지만 Residual block에서는 identity X를 F(X)에 더한 것을 H(X)로 삼으면서, X의 정체성이 뚜렷히 남은 상태에서, 분할정복 통해 최적화된 학습을 할 수 있다.  분할정복-&gt; (F(x), X의 weight를 따로 구해서 더하면 되니까?)  Target function : $H(x)=F(x)+x$  Residual function : $F(x)=H(x)-x$이를 위해 Shortcut connection 또는 Skip connection을 통해 x를 layer을 넘어 더해주어 Backpropagation 시 뛰어넘어 gradeint를 구할 수 있게 하였다.  이를 통해 Gradient vanishing 문제를 해결함            Residual 구조      경로를 풀어본 구조                              [img. 같은 Residual 구조의 경로 풀이]이론상, Residual 구조를 통하여 생기는 경로는 층이 깊이 n에 따라 $O(2^n)$개 만큼 증가한다.이 경로를 통해 backpropagation이 가능하므로 복잡한 학습을 해결 가능하다.ResNet의 전체적인 구조는 다음과 같다.  He initialization conv layer[img. Resnet의 첫 시작 부분]  첫 layer의 output은 앞으로 계속 identity connection을 통하여 더해질 것이므로, 최적화를 위해 단순하고 작은 크기의 output을 내놓아야 한다.  따라서 He initialization이라는 간단하고 ResNet을 위해 고안된 initialization을 이용한다.  Stacked residual blocks 부분[img. Stack residual blocks]  모두 3x3 conv layer로 이루어져 있으며, Batch normalization이 매 layer 끝에 이루어진다  일정 블록 이후(색이 바뀌는 부분), 채널 수는 2배로 늘리고, 채널 해상도는 stride를 2로 잡아 줄이는 구간이 존재함  Output FC layer[img. Sing FC layer]  하나의 average pooling과  Fully connected layer을 통하여 classfication을 진행ResNet 코드[img. ResNet code stack 수 정의 부분 ][img. ResNet code 첫시작, He initialization 부분 ][img. ResNet code, stacked residual 부분 ][img. ResNet code, Layer 생성 코드][img. ResNet code, 마지막 FC층 부분]Beyond ResNets  DenseNet[img. DenseNet 이미지]  ResNet과 달리 layer의 output이 이후의 모든 layer의 결과값에 Channel 축을 중심으로 Concatenate되서 합해진다.  Cocatenate하므로 기존의 값들이 보존된다          SENet        Activation의 결과가 명확해지도록 ouput의 채널축에 가중치를 주는 Attention across channel 방식  feature의 중요도와 관계가 명확해짐  Squeeze: global average pooling을 통하여 채널의 공간정보를 없애고(축의 정보 등) 분포를 구함  Excitation: FC layer 하나로 채널간의 연관성(Weight=attention score)를 구함  중요도가 떨어지면 0에 가깝게 중요한 것은 크게 하여 feature의 강조와 무시를 함          EfficientNet        기존의 Network 알고리즘을 정리함            기본 Network      Width Scailing      Depth Scailing      Resolution Scaling      Compound Scailing                                                        기준      채널의 수를 늘리는 방식      층의 수를 늘리는 방식      Input의 해상도를 높게 주는 방식      앞의 방법들을 복합한 방식              _      GoogLeNet 등      DenseNet 등      _      EfficientNet      [table. 기존의 Network의 분류]      각 Scailing들은 파라미터 수, 학습 epoch, 데이터셋의 수에 따라 성능이 오르지않는 구간이 나오는데(saturation), 이를 모두 팩터(어느 정도 비율로 복합하는가)를 주고 복합하여 성능을 크게 상승시킴        사람이 찾은 효율적인 다른 구조들, NAS 알고리즘 구조(Neural Architecture Search, 컴퓨터가 효율적인 구조를 찾는 알고리즘)보다 성능이 압도적으로 좋다.    적은 연산으로도 성능이 크게 올라 EfficientNet이다.  Deformable convolution  동물, 사람, 등의 형태가 변할 수 있는 사물에 효율적인 구조  feature를 나타내는 weight와, 이 weight의 위치를 어떠한 방향으로, 어떻게 변형시킬 지 결정하는 offsets를 학습하는 형식  기존의 정사각형 형태의 Receptive field와 달리 물체의 형태에 따라서 Receptive field 모양이 변함Summary of image classification[img. 앞서 배운 모델들의 비교, 면적은 모델의 크기]  AlexNet은 심플하지만 메모리 사용량이 크고 성능이 좋지 않다  VGGNet은 성능이 낫지만 메모리와 연산을 많이 잡아 먹는다  GoogLeNet의 최신 구조는 크기도 적고 성능도 좋지만, 구조가 복잡하다  ResNet은 특출난 것이 없다  GoogLeNet이 여러모로 좋지만 구조가 너무 복잡하여 VGGNet, ResNet을 기본 모델로 많이 사용한다.Semantic segmentationSemantic segmentation이미지 각 픽셀의 어떠한 category에 속하는지 구분하는 문제(ex) 사람 영역, 자동차 영역)같은 class의 다른 instance에는 관계가 없으며 이를 위한 instance segmentation가 있다.[img. Semantic segmentation의 예시]                                    [table. 의료 사진, 자율 주행, 영상 합성 등에서 활용]Semantic segmentation architecturesFully Convolutional Networks(FCN)Semantic segmentation을 위한 첫 End-to-End architecture[img. FCN 구조]  End-to-End 구조: 입력층부터 출력층까지 모두 미분가능하여 입력과 출력 pair만 있으면 모델을 학습할 수 있는 구조를 의미. 입력 사이즈 등의 제한이 없음  이전에는 완전학습 하기에 제한이 있었음          ex) AlexNet을 이용한 semantic segmentation의 경우, 학습 시의 이미지 해상도와 test 시의 이미지 해상도가 다르면 안됬음.      [img. Fully connected layer vs Fully convolutional layer 구조 비교]            Fully connected layer(FCL)      Fully convolutional layer                  공간 정보를 고려하지 않는 모습                    fixed dimensional vector를 받아 fixed dimensional vector 출력, 보통 하나로 정해진 feature vector를 출력      activation map을 받아 activation map 출력, 보통 1x1 conv layer로 구현하며, feature vector들이 포함된 convolutional feature map 출력      [table. Fully connected layer vs Fully convolutional layer 세부 비교]다만 Receptive field를 살핀 뒤 feature를 찾아 작은 크기의 결과를 내는 conv layer와 pooling layer로 인해 출력 이미지의 해상도가 작아짐 =&gt; 이를 해결하기 위해 Upsampling layer가 나타남Upsampling layer이란?[img Upsampling이 추가된 FCN]작아진 결과물을 크게 만들어주기 위해 Upsampling layer을 사용3가지 방법 중 Unpooling 방법을 제외하고 2가지 방법이 이용됨  Transposed convolution[img. Transpose convolution의 원리]줄어든 이미지의 픽셀을 필터만큼 곱한 뒤, kernel 사이즈와 strider 크기에 따라 곱연산하여 더한다, 중첩 되는 부분은 덧셈연산이 일어난다.[img. Transpose convolution의 문제점]kernel 사이즈와 strider 크기에 주의하지 않으면, 겹쳐서 덧셈이 일어나는 부분에 의해 checker 무늬가 나타나게 된다.  Upsample and convolution위 문제를 해결하기 위해 upsampling과 convolution을 같이 사용하여 중첩하는 부분뿐만 아니라 골고루 영향을 받게 해준다.Transpose와 달리 layer을 하나가 아닌 2개로 분리하여 주로 영상처리에 사용하는interpolation 알고리즘(Nearest-neighbor(NN), Bilinear 등)을 사용하고 convolution을 이용하여 학습 가능하게 만든다.[img. 개선된 convolution]해상도를 낮추며 진행되는 conv layer 특성상, 층의 깊에 따른 특성은 다음과 같다.            낮은 레이어층, 해상도 높음, Receptive field 작음 &lt;====&gt; 높은 레이어층, 해상도 낮음, Receptive field 큼                                디테일, 로컬 변화에 민감&lt;====&gt;전반적 의미적 정보를 포함      [table. 층의 깊이에 따른 output 값의 특징]결국 우리가 필요한건 구조의 깊은 부분의 의미적 부분(classify 해야하므로)과 구조의 얕은 부분의 디테일한 부분(고해상도로 픽셀을 선정해야 하므로)이 둘다 필요하므로 다음과 같은 방법으로 해결하였다.[img. FCN-Ns 모델들의 비교]마치 DenseNet이나 ResNet 처럼,  중간의 결과 값을 upsampling 한 뒤,  최종결과물을 upsampling한 것들을  Concatenate하여 출력하면 좋은 결과가 나오며,얼마나 많은 층에서 결과값을 가져오느냐에 따라 FCN-32s, FCN-16s, FCN-8s 모델로 나누어진다.  숫자가 작아질 수록 더 많은 층의 결과값을 가져온 모델[img. FCN-Ns 모델들의 비교, 중간값을 많이 가져온 모델일 수록 정확한 결과가 나옴]Hypercolumns for object segmentation                                    HyperColumn이라는 모든 Conv layer의 결과값을 각 픽셀 별로 쌓아 만든 vecotr를 이용함      물체의 bounding box를 추출하고 사용한다는 점이 다름      [table. FCN과 비슷한 내용을 담은 HyperColumn 논문]U-Net영상의 일부분만 쓰는 관련된 TASK의 경우 아직도 많이 활용되는 networkFullay convolutional network가 기반이며, skip connection을 통하여 앞선 network 보다 더욱 정교한 결과를 만들 수 있음[img. U자 모양이라 U-Net]크게 2가지 부분으로 나뉜다.  Contracting path 부분  입력 영상을 3x3 convolution을 이용해 max pool을 이용해 해상도를 낮추고 대신 2배씩 feature channel을 늘림  이를 통해 전체적인 의미, 문맥(holistic context)를 확보하는 부분이며 일반적인 FCN과 다를바 없음  Expanding(Upsampling, decoding) path 부분      2x2 up-convolution을 통하여 반대로 점진적으로 채널 수는 절반으로, 해상도는 2배로 늘림        추가로 이전 낮은 층의 layer의 activation map을 Skip connection으로 가져와 concatenating하여 사용함                  이를 통해 detail하고 local한 feature map을 받아 사용할 수 있음                    이때 concatenate하려면 해상도가 맞아야 하는데, 홀수이면, Downsample시, 일부 값을 버리게 되며, 다시 Upsample시 해상도가 달라지므로 해상도 크기가 홀수가 안되게 해야함.                  ex) 7x7 =DownSample(divide 2)=&gt; 3x3 (1은 버림) =UpSample(multiple 2)=&gt; 6x6          7x7과 6x6 해상도가 맞지 않아 Concatenate 불가                    U-Net Pytorch 코드[img. U-Net Contracting Path code][img. U-Net Expanding Path code]DeepLab널리 사용되는 CRFs, Atrous Convolution의 사용이 특징인 network, Deeplab v3+가 최신.CRFs(Conditional Random Fields)후처리로 사용됨, 픽셀 간의 관계를 그래프로 표현한 뒤, 최적화하여 경계를 찾는 원리score map과 경계선이 맞도록 경계선 내외부의 확산을 반복한다.[img. CRFs 예시]Atrous convolution(또는 Dilated convolution)커널크기를 정하고, 정의한 Dilation factor 만큼 커널을 띄어 계산하는 Convolution 방법같은 parameter 수와 연산량으로 더욱 큰 Receptive size를 얻을 수 있다.[img. 좌측이 기존의 conv, 우측이 astrous conv]Depthwise separable convolution입력 이미지 해상도가 클 경우, 너무 처리가 오래 걸리자, Dilated convolution + Depthwise separable convolution = Astrous separable convolution을 이용한다.Depthwise separable convolution는 일반 convoution을 2개의 절차로 나누어 진행한다.[img.Standard vs Depthwise separable convolution의 차이]이로 인해 파라미터 수가 $D_k^2MND_F^2$에서 $D_k^2MD_F^2+ MND_F^2$로 감소하였다.DeepLab v3+의 구조[img. 최신 DeepLab v3+의 구조 ]  DCNN 부분에서 Dilated convolution을 통하여 feature map을 구함  Encdoer 중간 부분에 있는 Astrous spatial pyramid pooling을 이용해 다양한 scale의 정보를 Dilated conv로 여러 feature를 추출한 후 하나로 합쳐 1x1convolution으로 하나로 합친다.  Decoder 부분에서 Low-Level Features와 Upsampling한 Pyramid pooling feature를 Concat한 뒤, 결과값을 낸다.Semantic segmentation 뿐만 아니라, instance segmentation(Class 뿐만 아니라 객체 또한 탐지), panoptic segmentation(배경 정보+ instance segmentation)으로 성장하고 있다.            Original Image      Semantic segmentation                  Instance segmentation      Panoptic segmentation      [table. Image 인식 Tasks]Object detectionObject detection[img. Object detection의 예시]Classification + Box localization의 Task즉, 바운딩 박스의 위치 + 물체의 소속까지 예측해야함, 고수준의 문제[imgs. 자율 주행, OCR 등의 산업에 사용됨]Two-stage detector(R-CNN family)Traditional methods- hand-crafted techniques 1. Gradient-based detector과거에는 경계선의 특징으로 사람의 직관과 직접 설계한 알고리즘으로 Object Detection을 함            Average Gradient      max (+) SVM weight      max (-) SVM weight      Original Image      R-HOG descriptor      R-HOG w/ (+) SVM      R-HOG w/ (-) SVM                                                            [img. Gradient-based detector]  HOG : histogram of Oriented Gradients  SVM : Support Vector Machine, 심플한 Linear 모델          결정 경계, 즉 그래프 내에 분류를 위한 기준선을 정의하는 모델      Traditional methods- hand-crafted techniques 2. Selective search(Box-proposal algorithm)최근의 초기 Object Detection에서 자주 사용한 기술로, 다양한 물체 후보군에 대해서 영역을 특정하여 Bounding-box를 제안해줌            순번      1      2      3                  구분                                예시                                설명      Over-segmentation(비슷한 색, 분포끼리 영역 나눔)      비슷한 영역끼리 합침      Bounding box를 추출      [table. Selective search 예시]R-CNN딥러닝 기반, Alex Net 보다 압도적인 성능[img. R-CNN의 과정]  이미지 입력  위의 Selective search 등의 bounding box 알고리즘으로 Region Proposal(최대 2천개까지)을 구함  이미지 사이즈를 늘려서 해상도를 맞추고 미리 학습된(Pre-trained) CNN에 입력  SVM을 이용해 Classificationbounding box detection의 성능의 한계와 각각 bounding box 일일이 Classification 하므로 속도가 느림Fast R-CNNR-CNN과 달리 이미지의 학습된 feature를 재활용해서 속도를 향상(최대 ~18배 빠름)[img. Fast R-CNN의 과정]  CNN을 통하여 feature map을 미리 뽑아냄          Fully Convolutional Network를 이용해 해상도 고정 문제를 해결했으므로 warping 안함        이렇게 뽑은 feature map을 RoI pooling layer에서 관심영역(RoI, Region of Interest)만 뽑아 resize함  FC layer와 함께 결합된 bbox regressor와 softmax를 통해 각각 더욱 정교한 바운딩박스와 classification을 함여전히 바운딩 박스 검출(Region Proposal) 성능에 한계를 가짐.Faster R-CNNRegion Proposal 또한 딥러닝 기반으로 바꾼 최초의 End-to-End Object Detection 모델, 즉 모두 학습 가능함Intersection over Union(IoU)[img. IoU의 정의]IoU (Intersection over Union) : 얼마나 bounding box가 잘 정합되어있는가를 정의Anchor Box[img. Anchor Box 예시]각 위치에서 발생할 만한 박스 후보군들을 크기와 비율 별로 미리 정의해놓으며 이를 Anchor Box라고 함Faster R-CNN에서는 보통 9개로 정의 해놓고, 더 많이 정의도 가능각각 Anchoer box와 실제 값(Ground-Truth)의 IoU를 비교하여 정답인 Positive sample과 negative sample을 정의하여 학습시킴  보통 IoU가 0.7 이상이면 +, 0.3 이하면 -Region Proposal Network(RPN)[img. Region Proposal Network(RPN)]특히, 기존의 느린 Region proposal 알고리즘을 딥러닝 기반 RPN으로 바꿨음그 이외에는 기존의 Fast R-CNN과 비슷함[img. 자세한 RPN 과정]  Sliding Door 방식으로 Window 마다 k 개의 anchor box 고려  256 차원 feature map 추출  feature map에서 Classification을 위해 2k개의 score 를 추출, 동시에 바운딩 박스의 크기, 위치를 위해 4k개의 값을 추출          계산속도를 늘리기 위해 Anchor box로 rough하게 정의한 후, 정교하게 바운딩박스 추출      Classification에서는 Cross Entropy loss, 바운딩 박스 추출은 Regression loss 사용      Anchor box 종류에 따른 Loss도 따로 있음      Non-Maximum Suppressions (NMS)RPN에 의해 많은 Bounding box가 제안되며, 이후 NMS를 통해 최적의 Bounding box만 필터링한다.[img. NMS steps]  가장 높은 점수의 box를 선택  IoU를 다른 박스와 비교  IoU가 50 이상인 박스들 제거  그다음 높은 점수의 box를 선택  2~4 반복            R-CNN      Fast R-CNN      Faster R-CNN                                    [table. R-CNN Family 구조 비교]R-CNN Family은 Two-stage Detector의 대표 모델들이다.Single-stage detectorSingle-stage detector은정확도가 조금 뒤떨어지지만 리얼 타임 Detection 가능할 정도로 높은 속도에 중점을 둠RoI pooling layer를 사용하지 않고, 간단한 구조와  빠른 속도를 자랑하는 경우가 많음            One-stage detector      Two-stage detector                              [tables. one-stage vs two-stage]YOLO(You only look once)[img. YOLO 과정]  Input을 S 크기의 그리드로 나눔  각 박스에 대하여 Boundig box와 Confidence를 예측          이때, Ground truth와 IoU를 비교하여 학습함        동시에 각 위치에서의 Class Score를 추가로 예측  NMS를 통해 Bounding box 추출[img. YOLO의 구조]일반 CNN 구조와 비슷하며 SxSx30의 아웃풋이 나옴(채널수 30 = class probability 20 + x,y,w,h 각각 2채널)[img. YOLO 성능비교]Two-stage에 비해 성능은 떨어지지만 훨씬 빠르다.  성능이 떨어지는 이유는 맨 마지막 Layer에서 한번만 Prediction 하므로Single Shot Multibox Detector(SSD)[img. SSD 예시]feature map들의 다른 해상도마다 적절한 크기의 Bounding box를 설정하게 해줌[img. SSD의 구조]VGG-16을 backbone으로, 다양한 Scale의 conv를 통과시켜 여러 해상도에 대응함[img. SSD 전체 anchor box 갯수 계산][img. SSD 성능 비교]속도와 성능이 YOLO 뿐만 아니라 R-CNN 계열 보다도 좋다.Single-stage detector vs. two-stage detector[img. Class imbalance Problem 예시]Class Imbalance Problem : Single stage detector의 문제, 결과값에 필요없는 negative anchor box가 positive anchor box보다 훨씬 많은 문제Focal Loss[img. Focal loss 그래프]위의 Class Imbalance Problem를 해결하기 위해 제안됨cross entropy loss의 연장선으로, 추가적인 확률 텀이 붙게 된다.CE와 비교하여 $\\gamma$값에 따라 정답의 경우 Loss를 더욱 낮게, 오답의 경우 Loss에 더욱 가중을 주게  된다.\\(Cross\\ Entropy\\ Loss:CE(p_t)=-log(p_t)\\\\Focal\\ Loss:FL(p_t)=(1-p_t)^\\gamma CE(p_t)=-(1-p_t)^\\gamma\\)[math. Focal loss의 수식]RetinaNet과 Feature Pyramid Networks(FPN)RetinaNet = FPN + class/box subnetU-Net과 비슷한 구조로, low level의 feature와 high level의 feature를 합하여 class와 box_bounding을 각 위치에서 수행[img. RetinaNet 구조][img. RetinaNet 성능]비슷한 속도에 높은 성능을 보이며, 속도를 희생시키면 성능을 더 올릴 수 있음Detection with TransformerDETR(DEtection TRansformer)NLP에서 큰 혁신을 보여준 Transformer 구조를 Object Detection에 활용한 구조,DETR은 facebook에서 개발[img. Transformer 구조][img. DETR 구조]CNN의 feature와 pixel의 positional encoding을 합하여 encoder에 넣어준 후, N개의 Object queries와 함께 decoder에 넣어준 후, 각 픽셀의 class, bounding box를 출력해주는 구조[img. bounding box 이외의 방법들]이외에도 CornerNet, CenterNet 등 Bounding box 대신 중심점, 양 끝점을 찾는 연구 등이 진행되는 중CNN VisualizationCNN을 시각화하는 것Visualizing CNN[img. CNN is a black box]많은 경우 CNN의 내부 로직 등을 알 수 없거나, 신경쓰지 않고 개발하는 경우가 많다.어째서 이러한 결과가 나왔는가? 무엇이 문제인가? 어떻게 하면 개선 가능한가? 등을 알아보기 위해 Black box 상태이 CNN 내부를 알아볼 필요가 있다.[img. ZFNet 예제]ZFNet 등에서는 각 level의 feature를 확인하여 학습을 파악할 수 있어서 이를 통해 성능을 개선시킬 수 있었다.[img. 간단한 Filter weight visualization]저차원인 1번째 conv layer의 경우 3채널 또는 1채널로 이루어져있어 상단과 같이 직관적으로 Visualization이 가능하지만, layer가 깊어지면(고차원이 되면) 채널 수가 늘어나면서 인간이 이해 가능한 형태의 visualization이 불가능하다.[img. Types of neural network visualization]왼쪽으로 갈수록 모델에 대한 이해, 오른쪽을 갈수록 데이터 분석Analysis of model behaviors고차원 layer의 feature들을 분석하는 방법을 알아보자고차원 Embedding feature analysis 1번째 방법 - 예제 검색 방법[img. Nearest neighbors (NN) in a feature space]NN-search의 경우 feature space에서 가장 가까운 사진들을 비교(예제 검색)함으로써 분석이 가능하다.상단의 코끼리 사진들을 보아, 잘 clustering 된걸 알 수 있으며, 하단의 강아지 사진으로 pixel 위치(강아지 형태, 위치)가 바뀌어도 모델이 잘 찾아낸다는 것을 알 수 있다.이러한 예제 검색 방법의 Step은 다음과 같다.  신경망을 통하여 Database에서 각 Input의 고차원 feature를 뽑아내 High dimensional feature space에 위치시킨다.[img. feature 추출 및 DB 위치]  검색하고 싶은 Input의 고차원 feature를 뽑아 낸 뒤 마찬가지로 High dimensional feature space에 위치시킨다.[img. Input 사진들의 feature의 위치]  가장 가까운 이웃의 feature를 가져온 뒤, 매칭되는 Input을 가져온다.[img. 사진과 가장 가까운 이웃들]  그 Input들과 비교하여 Visualization 한다.단, 이 방법은 전체적인 형태가 아닌 일부 예제만 파악한다는 단점이 있음고차원 Embedding feature analysis 2번째 방법 - Dimensionality reduction(차원 축소)우리가 사는 3차원(시간을 포함하면 4차원) 공간에 맞게 고차원 공간을 낮추는 방법[img. 고차원 공간을 저차원 공간으로 변형]대표적인 방법으로 t-SNE가 있다t-distributed stochastic neighbor embedding(t-SNE)[img. t-SNE를 통한 숫자 손글씨 구분(MNIST) feature space의 visualization]      고차원 데이터를 2차원으로 매핑한 결과        3,5,8의 cluster가 한껏 뭉쳐있는 걸로 보아 CNN이 비슷하다고 느낀다는 것을 알 수 있다.  중, 고차원 해석: Activation investigation 1-Layer activationLayer의 Activation을 분석하여 모델의 특성을 파악하는 방법[img. AlexNet의 Activation 분석]특정 Activation의 채널(hidden node)을 masking 한뒤 overlay하여 무슨 일을 하는 노드인가 알아볼 수 있다.중차원 해석: Activation investigation 2-Maximally activating patches각 채널의 hidden node의 가장 큰 값을 가지는 patch(activation)를 가져와 나열하는 것[img. hidden node 별 image patch]이를 통해 각 히든 노드가 찾는 부분(=하는 일)을 알 수 있다.국부적이므로 중차원 정도 해석에 어울린다.1) 특정 layer의 특정 channel을 고른다.2) input 이미지를 집어 넣은 후 선택한 채널의 activation 값을 저장한다3) 최대 activation value의 Receptive field를 Input에서 crop하여 image patch로 만든다.결과 해석: Activation investigation 3-Class visualization예제 데이터 사용없이 네트워크가 기억하는 이미지가 무엇인지 판단ex)이 CNN은 특정 클래스의 이미지를 대략 어떻게 생겼다고 기억하고 있는가?[img. CNN이 기억하고 있는 개와 강아지의 모습]편향 등을 알아볼 수도 있다. (ex) 위 새 사진은 많은 데이터가 나무와 함께 찍힘)\\(I^*=\\underset{I}{argmaxf(I)}-Reg(I) =\\\\I^*=\\underset{I}{argmaxf(I)}-\\lambda \\left\\|I\\right\\|^2_2\\\\\\lambda \\left\\|I\\right\\|^2_2, Reg(I):Regularizaion\\ term\\\\I: 영상\\ 입력, f(I):CNN\\ 모델\\)[math. Gradient ascent, 일종의 Loss]Gradient ascent를 통하여 Visualization을 위한 이미지를 합성하게 된다.$argmaxf(I)$를 통하여 Input image I를 돌며 각 클래스의 가장 높은 스코어를 얻는다.너무 큰 스코어 값이 나오는 것을 막고, 이해할 수 있는 형태로 바꾸기 위해 Regularizaion term 추가최대 스코어값을 찾으려는 과정이므로 Gradient ascent이며, 부호만 바꾸면 Gradient descent이므로 해당 알고리즘을 그대로 사용할 수 있다.1) 임의의 영상(검정, 하양, 회색 혹은 랜덤한 이미지)을 CNN에 넣어 관심 class의 prediction score를 추출​\t- 처음 주는 영상부터 바뀌기 시작하므로 초기값의 설정에 따라 완성 이미지가 바뀐다.2) Backpropagation으로 gradient maximizing하여 관심 class의 prediction score가 높아지는 방향으로 입력단의 이미지를 업데이트해준다.3)  업데이트된 영상으로 1~2를 계속 반복한다Model decision explanation모델이 특정 입력을 어떤 각도로 해석하는 가에 대한 설명Saliency test 계열주어진 영상의 제대로 판정되기 위한 각 영역의 중요도를 판별이때 중요도가 표시된 그림을 Saliency map이라고 한다.Occlusion map[img. Occlusion map 예시]특정 픽셀을 가려서 바뀌는 Predicdtion score 값을 Heatmap 형식으로 표현한 것영상의 가린 부분에 따라, 많이 떨어지면 중요한 영역이며 적게 떨어지면 덜 중요한 부분이다.이 떨어진 정도를 표시하여 표현할 수 있다.via backpropagtion[img. backpropagation을 이용한 saliency map 예시, 밝은 부분이 판단에 중요한 영역]앞서 했었던 Class visualization의 Gradient ascent와 비슷랜덤 이미지가 아닌 특정 이미지를 classification을 한 뒤, class score에 대한 backpropagation으로 관심 영역의 점수를 표시하는 방법1) 입력 영상을 넣어 특정 class의 score를 얻어낸다2) Backpropagation으로 Input까지 진행해 gradient를 얻어낸다.3) gradient의 절대값 또는 제곱값을 하여 얻어낸 gradient의 크기를 이미지형태로 얻는다  이를 gradient magnitude map이라고 한다.  이를 여러번 반복하여 더욱 정확한 Saliency map를 얻어낼 수 있다.backpropagation을 이용한 더 진보적인 visualization 방법으로 Deconvolution이 있다.[img. Dconvolution의 결과물][img. ReLU의 작용과 deconvnet의 차이]보통 CNN의 경우 Forward pass 시 음수는 ReLU 함수를 통과하며 0으로 마스킹되며,Backward pass 시 이를 기억하여, 해당 픽셀을 다시 0으로 마스킹한다.하지만 deconvnet은 backward 시 Forward pass때 처럼 음수가 0으로 마스킹된다.\\(ReLU:h^{l+1}=max(0,h^l)\\\\backpropagation:\\frac{\\partial L}{\\partial h^l}=[(h^l&gt;0)]\\frac{\\partial L}{\\partial h^{l+1}}\\\\deconvnet:\\frac{\\partial L}{\\partial h^l}=[(h^{l+1}&gt;0)]\\frac{\\partial L}{\\partial h^{l+1}}\\)[math. 기존의 pass와 deconvnet의 pass의 수식화]또한, 기존의 방법과 deconvnet의 And 연산하여 만든 Guided Backpropagation 또한 가능하다.                              $\\frac{\\partial L}{\\partial h^l}=[(h^{l+1}&gt;0)\\&amp;(h^{l+1}&gt;0)]\\frac{\\partial L}{\\partial h^{l+1}}$      [table. Guided backpropagation][img. Guided backpropagation과 다른 방법들 비교]수학적으로 구한 것이 아니라 경험적으로 구했지만 결과는 괜찮게 나온다고 한다.forward 시 결과를 미친 양수 pixel과 backward 시 ‘이 부분은 증폭하라’의 의미를 가진 양수 pixel만 받아들인 결과 =&gt; 즉 classification에 긍정적 영향을 끼친 pixel만 표시되게 됨Class activation mapping(CAM)어떤 부분을 참조하여 결과가 나왔는지 보여줌.[img. CAM 예시]기존의 CNN 구조를 조금 바꾸어야 한다.[img. CAM을 쓸 수 있게 개조된 CNN]기존의 출력 이전의 FC Layer 대신 Conv Layer 이후에 Global average pooling (GAP) Layer와 FC layer 한 층이 삽입된다.이후 Classification에 대해 재학습된다.\\(S_c=\\overset{Channels}{\\sum_k}w_k^c\\overset{GAP\\ feature}{F_k}\\overset{GAP}{=}\\sum_kw_k^c\\sum_{(x,y)}\\overset{Feature\\ map\\\\before\\ Gap}{f_k(x,y)}=\\\\\\sum_{(x,y)}\\ \\ \\overset{CAM_c(x,y)}{\\sum_kw_k^cf_k(x,y)}\\\\S_c:Score\\ of\\ the\\ class\\ c\\\\k: 마지막\\ conv\\ layer\\ channel\\ 수\\)[math. CAM이 포함된 CNN 구조 유도][img. GAP layer 부분의 작용](+) 성능이 좋아 가장 많이 사용되는 Visualization 방법(+) 공간 정보를 주지(supervision?) 않아도 공간에 대한 정보가 나타남이를 통해 bounding box를 쳐주면 object detection으로 사용 가능  Weakly supervised learning이라고 함(-) 구조를 바꾸고 재학습을 해야하며, 이 과정에서 성능이 바뀔 수 있다는 점이 단점Grad-CAM구조를 바꾸지 않아도 활용할 수 있는 CAM 구조[img. Grad-CAM의 예시](+) CAM과 비슷한 성능, 구조를 바꾸지 않아도 됨[img. Grad-CAM의 구조]$\\overset{CAM_c(x,y)}{\\sum_kw_k^cf_k(x,y)}$ 부분에서 $w_k^c$(importance wieghts)만 구하면 맵을 그릴 수 있다.Backpropagation을 Input 이미지가 아닌 관심을 가지는 activation map까지만 진행하며, 그렇게 구한 importance weight ($\\alpha_k^c$)와 activation map($A^k$)를 선형결합하여 ReLU를 씌워 양수값만 사용\\(\\overset{Global\\ average\\ pooling}{\\alpha^c_k=\\frac{1}{Z}\\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^k}}\\\\L^c_{Grad-CAM}=ReLU(\\sum_k\\alpha^c_kA^k)\\\\\\alpha^c_k: importance\\ weight\\ of\\ the\\ k-th\\ feature\\ map\\ w.r.t\\ the\\ class\\ c\\\\\\frac{\\partial y^c}{\\partial A_{ij}^k} : Gradients\\ via\\ backprop\\)[math. Grad-CAM 수식]영상 인식 뿐만 아니라 CNN 구조만 존재하면 어떤 Task에도 활용 가능[img. Grad-CAM 활용 예시와 Guided Grad-CAM]추가로 Guided Backprop을 추가하고, Grad-CAM을 내적하여 Guided Grad-CAM을 구하는 것이 일반화 되어있다.  Guided Backprop(sharp 하지만 class 구분 불가) + Grad-CAM (Rough하고 smooth하지만 class 구분 가능) = Guided Grad-CAM (서로 단점 보완)[img. SCOUTER 예시]최근에는 해석 결과에 대한 질문에 대해 답을 줄 수 있는 Visualization 방법(SCOUTER)도 등장함Visualization 기술을 응용해 GAN에 이용하여 명령을 내릴 수 있음(GAN dissection)[img. 표시한 부분에 문을 생성하는 예시]Instance/panoptic segmentation and landmark localizationSemantic segmentation, Object Detection은 더욱 어려운 Task로 고도화되면서 연구가 줄어듦            Original Image      Semantic segmentation                  Instance segmentation      Panoptic segmentation      [table. Image 인식 Tasks]Instance segmentation, Panoptic segmentation은 자율주행 등, 산업 등에서 많이 쓰임Instance segmentation[img. Instance segmentation vs Semantic segmentation]같은 물체 class 라도 Instance가 다르면 구분해야하는 문제, 실제 응용사례 많이 사용됨Object Detection 모델들의 연장해서 많이 사용 됨.Mask R-CNN[img. Mask R-CNN = Faster R-CNN + Mask branch]Faster R-CNN과 여러 모로 비슷하지만 개선을 많이 시킴  RoI pooling 대신, 정교한 소수점 좌표도 가져올 수 있는 RoIAlign pooling layer 사용  마지막 layer에 병렬로 mask branch라는 Fully convolutional Network가 추가되어 Output을 Upsampling 뒤, class 수만큼의 채널(여기서는 80개)에 Binary classification          class 예측결과를 가져와 참조할 mask를 정함      [img. R-CNN family의 추가]그림의 예시 branch 대신, Key point branch 라는 것을 추가하면 사람의 자세를 추정하는 Task도 가능YOLACT(You Only Look At CoefficienTs)[img. YOLACT 구조]실시간 Instance segmentation model  Feature Pyramid 구조를 이용해 고해상도이며,  Protonet 부분에서 Mask의 저해상도의 Prototype Soft segmentation component를 추출한 뒤,  Prediction Head에서 각 Class 간의 Mask Coefficients를 구하여 이를 이용해 2번의 prototype와 선형결합(Weighted Sum)하여 detection에 적합한 Class별 Mask Map을 만들어준다.  이후 Crop과 Threshhold를 통해 결과를 도출YolactEdge[img. YolactEdge 구조]위의 YOLACT를 더욱 경량화하여 영상 처리를 소형기기들에 사용가능한 모델이전 frame의 feature를 다음 keyframe에 활용하여 연산량 줄임 (성능 비슷, 속도 빠름)아직은 현실에 사용하기 힘든 성능Panoptic segmentation[img. Panoptic segmentation vs Semantic segmentation]배경정보 + instnace 구분 가능UPSNet[img. UPSNet 예시]FPN(feature pyrmid network) 구조에 병렬로 구성된 semeantic Head와 Instance head 그리고 이를 병합하는 Panoptic Head로 구성된 구조[img. Panoptic Head의 자세한 구조]Instance head의 Instance Mask Output의 경우 리사이징과 패딩을 거친 후 Semnatic head의 물체 mask와 합해진 뒤, output 채널로 concat  이를 통해 위치를 알 수 있음Semantic head의 물체 mask Output의 경우, 위의 Instance mask와 사용된 결과들은 Max된 뒤 기존 물체 mask에서 빠진 뒤 1채널로 output에 추가됨  모두 빠지고 남은 물체 mask는 Unknown mask가 된다.(class에 정의되지 않은 물체)Semantic head의 배경정보 Mask Output은 그대로 Output 채널에 추가됨VPSNet영상에 사용가능한 Panoptic segmetnation 모델[img. VPSNet 구조]      두 프레임간의 모션맵(해당 픽셀이 다음 프레임에 어디로 위치가 바뀌었는가?)를 이전 프레임 feature map에 적용하여 feature의 움직임을 tracking한 뒤, FPN을 통해 뽑은 해당 featrue map에 합쳐서 사용        Track head를 통해 이전 프레임과  현재 프레임 간의  RoI feature 연관성을 찾아낸다.        이후 UPS Net과 비슷함  Landmark localization[img. Landmark localization 예시]key point 혹은 landmark 라고 불리우는 영상에서 중요한 부분을 정의하여 위치와 class를 추적하는 것[img. Coordinate regression vs Heatmap classification]기존의 box bounding 찾던 방법(Coordinate regression)으로 keypoint를 찾으려고 하니 문제가 있었고 Heatmap classification이 좀더 정확하지만 계산량이 큼  각 채널에 keypoint를 할당하고 class로 생각함Gaussian Heat map을 형성하기 위해 Landmark location을 다음과 같이 변형한다. \\(G_\\sigma(x,y) = \\exp\\left(-\\frac{(x-x_c)^2+(y-y_c)^2}{2\\sigma^2}\\right)\\\\(x_c, y_c):center\\ location\\)[math. points to Gaussian 수식]쉽게 말해 해당 location 좌표를 평균점으로 삼고 주변에 Gaussian을 씌운다.Heatmap 형식을 사용하면 generalization 성능이 좋아짐.[img. Gaussian 도식]# Generate gaussiansize = 6 * sigma + 1 # 출력 해상도 크기# 모든 영상 좌표의 배열x = np.arrange(0, size, 1, float)y = x[:, np.newaxis] x0 = y0 = size // 2 # 중간이 평균점이라고 가정# numpy의 행렬 덧셈의 경우 sx1, 1Xs가 더해지면 sxs 행렬이 나옴 # The gaussian is not normalized, we want the center value to equal 1if type == &amp;#39;Gaussain&amp;#39;:    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))elif type == &amp;#39;Cauchy&amp;#39;:    g = sigma / (((x - x0) ** 2 + (y - y0) ** 2 + sigma ** 2) ** 1.5)[code. Gaussian 코드 구현 ]반대로 가우시안의 결과값을 좌표평면으로 바꾸어서 결과값을 보여주는 수식도 필요하다.Hourglass networkLandmark를 위한 특별한 구조[img. Stacked hourglass 구조][img. 확대한 hourglass 구조]UNet 구조를 여러 스택 쌓은것과 비슷한 구조  다만 Concat이 아니라 합으로 되어있으며, 그냥 skip해서 주어지는 것이 아니라 convolution layer를 하나 통과함  UNet보다 FPN구조와 좀더 유사함크기를 줄여 Receptive field를 늘린 구조DensePose[img. image의 UV Map 표현]몇 개 픽셀이 아닌 신체 전부 같은 아주 Dense한 landmark를 구하여 3D UV map 생성 가능  3D 모델을 만드는 방법은 다른 방법임.[img. UVMap 예시]UV map : 3D 매쉬의 평면 표현각 pixel의 점의 정체성이 영상 내부에서 유지되면서 위치만 바뀐다.즉 아주 많은 Landmark 검출은 곧, UV map 생성이다.[img. DensePose의 구조]DensePose R-CNN = Faster R-CNN + 3D surface regression branch  Patch: 각 body part의 sementation map(팔,다리,머리)Mask R-CNN과도 비슷하다.데이터 표현방법과 데이터셋을 제공한 논문RetinaFace[img. RetinaFace의 구조]FPN 구조에 다양한 branch 도입해 Multi task가 가능하게 한 모델여러 Task로 학습 시, 적은 데이터로도 Backbone 네트워크 학습이 강하고 성능좋게 잘 학습된다.Extension pattern : CV에서의 디자인 패턴 중 하나, 다른 구조도 branch를 추가하여 여러 Task에 활용 가능Detecting objects as keypointsBounding box를 찾을 때, keypoint(중앙, 코너)를 시작점으로 찾는 구조들CornerNet[img. CornerNet 구조]좌측 상단, 우측 하단의 점 2개를 찾아 Bounding box로 삼는 구조병렬적으로 2개로 나눈 뒤, 먼저 Heatmap에서 점의 위치를 찾고, 그 점의 class을 의미하는 Embedding을 찾은 뒤 결합한다.single-stage 구조이며, 성능은 조금 떨어지지만 속도가 빠르다.CenterNet[img. Centernet 1 예시]성능을 개선하기위해 중앙점 또한 검출함[img. CenterNet 2 예시]어차피 중앙점까지 3개를 구할 꺼면 width, height, center point로 검출 정보를 바꾼 모델[img. 성능 비교]CenterNet이 성능과 속도면에서 우위를 보인다.Conditional Generative Model(cGAN)사용자가 컨트롤 가능한 Generative Model을 의미Conditional generative model(cGAN)[img. Generative Model VS Conditional Generative Model]랜덤한 결과를 생성하는 Generative Model과 달리 Conditional Generative Model은 주어진 조건에 따라 생성한다.[img. Low quality audio -&gt; high quality audio]오디오 음질 향상, 인공지능 뉴스 등 여러 방면에 활용 가능[img. GAN의 원리]보통 위조 지폐범(Generator)과 지폐 감별자(Discriminator)로 비유하며, Generator는 실제 데이터와 비교하여 가짜 데이터를 생성하고 Discriminator는 이를 가짜인지 진짜인지 구별해본다.Discriminator가 감별해내면 해당 loss가 Generator를 학습시키고, Generator가 Discriminator를 속이면 Discriminator가 해당 Loss로 학습되는 상호 보완적인 모델이다. (Adversarial Training, 적대적 학습법)이러한 Adversarial Training을 이용하는 Generative model을 Generative Adversarial Network 즉, GAN이라고 부른다.[img. Basic GAN vs Conditional GAN(cGAN)]기본 GAN의 경우 Generator에 랜덤한 z를 넣고 생성한 결과를 Discrminator가 판별하는 구조Conditional GAN은 z는 옵션으로 넣고 C라는 조건을 넣어주는 부분이 다르다.[img. Conditional GAN를 이용한 이미지의 화풍 바꾸기]이외에도 그림의 화질을 좋게 바꾸는 Super resolution, 흑백이나 채색 되지 않은 그림 채색 등의 일에 사용된다.예를 들어 저해상도 이미지를 고해상도로 바꾸는 Super resolution이 대표적인 cGAN 활용의 예시이다.[img. Super Resoultion에 사용되던 과거 구조(Naive Regression Model)와 GAN 구조]Low Resolution 이미지를 GAN에서는 High Resolution 이미지로 바꾸고 이를 Discriminator가 학습하면서 이루어진다.과거에는 Naive Regression model이라는 좀더 단순한 Loss를 Discriminator 대신 사용한 구조를 사용했다.            MSE VS GAN 결과물 비교      MAE/MSE loss                                      $MAE = \\frac{1}{n}\\sum^n_{i=1}      y_i-\\hat{y}_i      \\MSE=\\frac{1}{n}\\sum^n_{i=1}(y_i-\\hat{y}_i)^2$      [tables. GAN loss vs MAE/MSE loss]MAE는 결과와의 차이의 크기를 loss로, MSE는 결과와의 차이의 제곱을 loss로 사용한다.MAE/MSE 같은 Regression의 결과물은 blurry한 결과가 나오는데, 두 loss 모두 이미지의 픽셀들의 평균을 포함하는 loss 이기 때문이다.GAN은 Discriminator를 속이기 위해 에러가 치우쳐도 현실에 가깝게 만든다.[img. 채색을 예시로든 loss의 차이]진짜 이미지가 흰색 아니면 검은색이 정답이라면 L1 loss는 그 사이 평균 값이면서, 존재하지 않은 회색 이미지를, GAN loss는 그 둘중에 하나인 검정 아니면 흰 이미지를 만든다.[img. MSE loss를 쓴 SRResNet과 GAN loss를 SRGAN의 차이에 주목]Image translation GANsImage translation이란, 한 이미지 스타일을 다른 이미지 도메인 혹은 다른 스타일로 변화시키는 문제.크게 보자면 위의 Super resolution 또한 Image translation의 한 종류[img. Image Translation 예시]Pix2PixImage translation을 CNN Laeyr가 포함된 학습 구조로 처음 정리한 연구 \\(G^* = arg\\ \\underset{G}{min}\\ \\underset{D}{max}\\ \\mathcal{L}_{cGAN}(G,D)+\\lambda\\mathcal{L}_{L1}(G)\\\\where\\ \\mathcal{L}_{cGAN}(G,D)=\\mathbb{E}_{x,y}[\\log D(x,y)]+\\mathbb{E}_{(x,y)}[\\log(1-D(x,G(x,z))]\\\\and\\ \\mathcal{L}_{L1}(G)=\\mathbb{E}_{x,y,z}[\\left\\|y-G(x,y)\\right\\|_1]\\\\x:ground-truth,\\ y: output,\\ z:random\\ factor\\)[math. Pix2Pix의 Loss, Total loss(GAN loss + L1 loss)]L1 Loss를 적당한 조건으로, GAN loss를 더해 Realistic한 출력을 만들도록 바꾼 Total loss를 사용함  MAE L1 Loss는 y-G(x,y)로 결과와 실제 이미지를 직접 비교하지만 GAN loss는 Discriminator를 통해 간접적으로 비교한다.  그러므로 GAN로 Realistic하고  L1 Loss으로 의도와 비슷한 이미지를 만들 수 있다.  또한, 당시에는 GAN의 연구가 많이 진행되지 않아서 학습이 안정적이지 않았다.또한 GAN Loss 부분은 cGAN이므로 z 뿐만 아니라 조건인 x가 같이 들어감[img. loss 종류에 따른 결과 비교]CycleGAN위의 Pix2Pix는 Supervised learning 방법을 사용해서 pairwise data가 필요하지만 이러한 데이터셋을 얻는 것이 어려워서 CycleGAN이 등장했다.[img. paired vs unpaired data]CycleCAN을 이용하면 도메인 간의 관계가 없어보이고, 1:1 대응하는 pair가 없는 두 데이터셋으로도 image translation을 할 수 있다.  응용범위와 데이터셋 확보방법이 늘어남[img. CycleGAN 결과물 예시]Cycle이라는 이름에서 알 수 있듯이, CycleGAN의 Loss는 데이터셋 X,Y에 대하여, X -&gt; Y로 가는 방향의 Loss와 Y -&gt; X로 가는 방향의 Loss를 Cycle 돌듯이 동시에 학습 시킨다.추가로 Cycle-consistency loss 텀은 X-&gt;Y-&gt;X로 돌아왔을 때, 변한 X가 원본 X와 비슷하게 만들도록 하는 Loss이다.\\(L_{GAN}(X\\rightarrow Y)+L_{GAN}(Y\\rightarrow X) + L_{cycle}(G,F)\\\\where\\ G/F\\ are\\ generators\\)[math. CycleGAN loss = GAN loss (in bot direction) + Cycle-consistency loss][img. Cycle-consistency loss 텀이 존재 하지 않을 시의 CycleGAN Loss 설명]Cycle-consistency loss 텀이 존재 하지 않을 시의 구조이다.  G, F: generator  $D_x, D_y$: discriminator  GAN loss : $L(D_x)+L(D_Y)+L(G)+L(F)$  일종의 2개의 GAN이다.하지만 이런 GAN loss만 사용시 Mode Collapse 문제가 발생한다.  Input의 상관없이 하나의 Output만 계속 출력하는 문제  즉 Input과 Output이 서로 영향을 미치지 않음(양방향 모델이기 때문에)[img. Mode Collapse 문제]이를 해결하기 위해 Cycle-consistency loss가 등장하였다.  Style 결과 뿐만아니라 content도 유지시켜 줌X에서 Y, 그리고 다시 X로 돌아왔을 때 원본 X와 같아야 한다(contents 유지).supervision이 없는 self-supervision 방법(레이블링 필요 없음)[img. X-&gt;Y-&gt;X와 Y-&gt;X-&gt;Y 처럼 돌아왔을 때의 Cycle-consistency loss]Perceptual lossGAN은 Discriminator, Generative 모델이 번갈아가며 학습되어야 하므로 학습하기 쉽지 않다.더 쉬운 방법을 알아보기 위해 Perceptual loss가 나타났다.Peceptual loss는 높은 질의 결과를 얻기위해 제안된 방법이다.GAN loss(Adversarial loss)의 경우,  트레이닝과 코딩이 힘듬(두 모델을 반복, 왕복 학습해야하므로)  대신, pre-training network 필요 없어, 데이터만 있으면 다양한 상황에 활용 가능Peceptual loss의 경우,  학습과 코딩이 편함(평범한 foward &amp; backward computation), 따라서 더 빠름  대신 learned loss를 위해 pre-trained network가 필요pretrained-network의 filter를 visualization 해보면, 사람의 visual perception 과 비슷하다.이미지에서 filter들이 방향성, edge, 색깔 등을 찾아 peceptual space로 변환한다.[img. pretraine된 network의 low level layer에서의 filter][img. perceptual loss의 결과물 예시]Perceptual loss를 활용해 Input 이미지를 원하는 Style로 바꾸는 Image Transform Net의 예시를 보면,여기서는 VGG-16을 Loss Network로 사용하며, 이를 이용해 feature를 activation map 형태로 뽑아낸다.이때 Loss Network는 Pretrained-Network이므로 고정되어 업데이트 되지 않으며, 그 앞단에 있는 Image Transform Net을 업데이트하기 위해 Backpropagation을 진행하여 업데이트한다.[img. Perceptual loss를 활용하는 Image Transform Net 구조]이 때, Style Target과 Content Target 2개에 관한 Loss를 구하게 되는데, 각각 Feature Reconstruction loss, Style Reconstruction loss라고 한다.  Feature Reconstruction loss[img. Feature Reconstruction loss의 원리]중간 레이어에 feature 1개를 뽑는다.Transformed Image net의 결과물인 $\\hat{y}$가 Content Target과 얼마나 일치하는지 측정하는 Loss로,   일반적으로 원본 이미지 x를 Input으로 loss network에 넣어얻어낸 feature와 loss network에서 얻어낸 $\\hat{y}$의 feature를 비교하여 L2 Loss로 계산한다.이후 , 이 값으로 Backpropgation을 하여 Transformed Image Net을 학습시킨다.  Style Reconstruction loss[img. Style Reconstruction loss의 원리]Feature Reconstruction loss와 비슷하게, Style Target과  $\\hat{y}$의 Feature를 뽑아낸다.다른 점은 이때, Feature를 직접 비교하는게 아니라, 전체적인 Style을 비교하기 위해, Gram matrices라는 feature map의 통계적 특징을 담아낸 feature의 channel size X channel size의 tensor를 비교하여 L2 Loss를 계산한다.Gram matrices란?  Gram matrices는 Feature의 공간적 정보를 없애기 위해 pooling을 이용하며, Feature 채널들을  channel X (Height*Width) 형태로 바꾼 뒤,  내적하여 곱해서 얻는다.  diagonal component(행렬의 행좌표와 열좌표가 같은 부분)은 자기자신의 통계적 특성을 의미하며, 그 이외에는 해당 채널과 다른 채널의 연관성을 의미한다.          공분산 행렬 구하기        즉 Gram Matrices는 채널간의 관계와 통계적 특성이 포함된 정보임  각 feature의 채널은 일종의 detection 역할을 하기 때문에, Gram Matrices는 이 스타일은 어떤 detection들이 많이 나타나는 가?를 분석한 것이다.Task에 따라 스타일과 관계없는 일이라면, Style reconstruction loss 대신 Feature reconstuction loss를 사용하지 않은 경우를 사용하지 않을 수도 있다.Various GAN applicationsGAN의 예시를 알아보자.  Deepfake[img. 사람 얼굴 생성기, 가짜 연설 생성기]이를 오남용할 수 있으므로, 이러한 Deepfake를 방어하기 위해 여러 시도 또한 이루어지고 있다.Face de-identification[img. Face de-identification]프라이버시 침해 방지를 위해 인간은 차이를 알지 못하지만 컴퓨터는 혼돈을 가질 수 있게끔, 조금의 변경을 하는 연구도 진행중[img. password를 이용해 침해 방지 예시]기타 비디오를 통해 포즈를 따라하게 만드거나,  CG 생성, 게임 등에도 사용 가능Multi-modal learning: Captioning and Speaking[img. Unimodal vs Multi-modal]Multi-modal learning : 다른 특성을 가진 데이터들을 함께 활용하는 학습(ex) Text + Sound)Overview of multi-modal learningmulti-modal learning의 어려움  데이터의 표현 방법이 모두 다름  이미지 : H X W X 3 배열, Text : Word Embedding + Positional Encoding 등[img. 데이터 표현 차이]  정보량의 불균형, feature space의 불균형.[img. 아보카도 모양 가구에 대한 글 하나는 여러 이미지를 포함할 수 있다]  특정 modality에 편향된 모델이 생성될 수 있음[img. 주어진 데이터가 동일해도 참조하는 modality의 비율은 달라질 수 있음]  예를 들어, 동물 Classification Task에서 사진과 울음소리, 동물에 대한 설명이 적혀있는 글을 줘도, 사진만 보고 동물 Class를 결정할 수 있음  딥러닝은 쉬운 길만 선택하려하기 때문Multi modal learning의 여러 패턴[img. Multi modal learning의 여러 패턴]  Matching  서로 다른 Modality를 같은 Space로 보내어 서로 Matching  Translating  서로 다른 Modality를 다른 Modality로 변환  Referencing  어떤 Modality 정보를 Input으로 같은 Modality의 결과물로 변환할 때, 다른 Modality를 참조하여 성능을 향상Multi-modal tasks (1) - Visual data &amp; TextText embeddingAscii 코드를 사용하는 Character 관점에서는 사용하기 힘들고, Word 레벨의 embedding을 Input으로 이용함.[img. Word embedding 예시]각 Word embedding은 단어의 대략적인 의미와 연관성을 가진 feature를 표현한는 Vector의 형태이다.이를 차원공간에 표현하면 비슷한 의미를 가진 단어는 비슷한 곳에 위치하며, 비슷한 관계를 가진 단어쌍 벡터 둘의 방향(차이 벡터) 또한 비슷한 방향을 가지게 된다. (일반화가 되어 있음)Word embedding 생성 방법 : word2vec대표적으로 Skip-gram model이라는 방법이 있다.[img. Skip-gram model의 예시]Input으로 단어의 one hot vector(V차원)를 의미하며, 이를 W와 곱하여 N-차원의 embedding vector가 나오게 된다.  이때 one-hot vector에 의해 W의 한 Row만 slicing 되게 된다. 즉 W는 embedding vecotr의 Row들의 집합이다.이후 그 Embedding vector를 이용해 해당 단어의 주변에 나타난 n개의 단어들을 예측하는 Task로 학습한다.  나타나는 주변 단어를 통하여 관계성을 학습할 수 있다.Joint embedding서로 다른 Modality의 Matching을 하기 위한 공통된 Embedding 벡터를 학습하기 위한 방법[img. Joint embedding은 Multi-Modality learning에서 Matcing 패턴을 위해 사용 ]Image taggingImage tagging은 사진에 tag를 지정하거나, 반대로 tag를 통해 사진을 가져오는 Task이다.[img. Image Taggin의 예시][img. Text와 Image의 matching 예시]위의 예시의 경우 각각 Text와 Image를 feature vector와 한 후,  그 이후 서로 다른 모델을 통하여 같은 차원의 (d-dimension) vector로 바꾼 뒤, 그 둘을 통하여 Joint embedding  vector를 학습한다.이때 Joint embedding Vector는 두 다른 Modality 데이터의 연관성, 거리를 의미한다.[img.  joint visual-semantic embedding space 내부]이렇게 구한 joint embedding vector 둘의 두 차원 상의 거리를 관련이 있는 Label은 가깝게, 관련 없으면 멀게 되도록 학습한다.  이런 Distance 기반으로 학습하는 것을 Metric learning이라고 한다.[img. Multi-modal analogy]또한, 이렇게 학습된 embedding을 이용하여 Multi-modal analogy라는 property 생겨난다.  서로 다른 Modality embedding를 포함하여, embedding 더하거나 빼서 가장 가까운 embedding을 데이터 형태로 가져올 수 있다.  예를 들어 위의 이미지처럼 개 사진에 개 단어를 빼고 고양이 단어를 추가하면, 고양이 사진들이 나타난다.          심지어, 각 첫번째 사진들의 입력한 사진의 배경과 비슷하다.      [img. 레시피를 통해 사진을 예상하는 application 예시][img. 위 어플리케이션의 구조]      Text는 encoder를 통하여 instruction과 Ingredient를 하나의 output으로 concat 한 뒤, FCL을 통하여 d 차원의 vector로 만든다.    Image는 conv layer을 통하여 feature를 뽑아낸 결과를 FCL을 통하여 d0 차원의 vector로 만든다.  두 embedding vector를 cosine similarty loss로 loss를 구하여 학습하며, 또는 추가 정보를 제공하여 더욱 좋은 성능의 semantic regularization loss를 이용하여 학습할 수 있다.Cross modal translation[img. modal 간의 변환을 하는 Translating][img. Image captioning ]Image CaptioningImage Captioning은 이미지의 설명 Text를 생성하는 대표적인 cross modal translation Task이다.[img. Image 분석을 위한 CNN과 Text 생성을 위한 RNN으로 이루어져있다.]Image Captioning 에서는 CNN과 RNN 구조가 필요하며 대표적인 구조로 Show and tell이 있다.  Encoder 구조로 ImageNet에 의해 pre-train된 CNN model을 사용하여 이미지를 vector로 바꾼 뒤,  이를 Decoder인 LSTM RNN의 Condition으로 제공하고, 시작 토큰(보통 0이나  토큰)을 준 뒤,  해당 LSTM의 Output을 다음 LSTM의 Input으로 주는 과정을 반복한다.       토큰이 나올때 까지 반복하여 결과물은 만든다.  [img. Show and Tell 구조, 좌측의 CNN과 우측 RNN을 활용]Show and Tell은 단 한번의 Image 분석 뒤에 태깅을 하나, 실제로는 단어 마다 Image에서 중요시 해야할 feature가 다를 수 있다.Show, attend, and tell 이라는 구조는 attend 구조를 통하여 단어별로 attention을 달리하여 순차적으로 단어를 생성 시, 이미지에서 feature의 가중치를 바꿔가며 할 수 있다.[img. show, attend, and tell의 attention]  CNN을 이용하여 Input Image의 14x14 feature map을 생성한다.          기존의 Vector 형태가 아니라는 점이 특징        해당 feature map을 RNN에 입력하여 단어를 생성할 때마다 다른 attention으로 단어를 생성[img. Show, attend, and tell 구조]Show, attend, and tell 구조의 경우, 사람이 사진을 인식할 때 전체적인 부분을 보는 것이 아닌, 일부에 관심(attention)을 가중하여 본다는 것에 착안되었다.1) Input image를 CNN을 통해 얻은 feature map과,2) 위 featuremap을 RNN에 넣어 얻은 spatial attention1)과 2)을 결과물을 inner product(weighted sum)하여 얻은 soft attention embedding(z) 벡터를 얻어낸다.  사실, Translating 보다는 Reasoning의 Cross modal reasoning에 더 가깝다.[img. 사람이 사진을 인식할 때 보는 부분 (좌), attention 결합 방법]RNN에서 결과를 내는 과정을 좀 더 자세히 살펴보면,  Feature map을 hidden stat로 첫번째 RNN 모듈 h0에 넣어주고 spatial attention s1을 얻는다.  이렇게 얻은 s1과 feature map을 inner product하여 얻은 z1 vector를 start token y1과 함께 두번째 RNN 모듈 h1에 넣어준다.  그 결과 첫번째 단어 d1(‘A’)와 두번째 spatial attention s2가 나온며, 이를 다시 feature map과 합쳐 soft attention embedding z2를 만든다.  이 이후 세번째 RNN 모듈 h2에는 z2와 이전에 출력한 단어 y2(또는 d1, ‘A’)를 함께 넣어주고, End Token 나올때 까지 반복한다.[img. RNN의 단어 build 과정]또한, 반대로 Text를 통하여 Image를 생성하는 것이 가능하며, 이때, 여러 Output Image가 나오는 것이 가능하므로 Conditional GAN을 이용한다.[img. Text-to-image by generative model]Text to image 모델의 Generator의 경우Text의 Vector가 주어지면, 이를 Gaussian Random factor와 결합하여 다양한 output이 나오도록 해준 뒤, Decoder를 거쳐서 image를 생성해준다.Discriminator의 경우Input된 Image를 encoder로 뽑은 feature map과 위에 사용했던 Text Vector를 합친 벡터를  label된 데이터와 비교하여 판단함.                                    [img. . Text-to-image Generator와 Discriminator 구조]Cross modal reasoning[img. Modality간의 Referencing을 이용하는 Cross Modal reasoning]Visual question answering영상과 질문을 받으면 이를 통해 답을 도출하는 Task각각 Text와 Image에서 추출한 같은 차원의 vector를 point-wise multiplication을 통하여 Joint embedding 한 뒤, 이 vector를 FCL을 통하여 답을 도출한다.모든 구조에서 학습이 가능한 End-to-End 구조이다.[img. Visual question answering 구조]Multi-modla tasks(2) - Visual data &amp; AudioSound representationSound data는 자연상태에서 1차원 Waveform 형태로 존재하지만, 우리가 사용하기 위해서는 Spectrogram이나, MFCC 등의 형태로 바꿔줘야 한다.[img. Sound data의 다양한 형태]  Fourier transform대중적으로 많이 사용되는 소리의 형태인 Spectrogram으로 변환을 위한 방법wave 형태의 data를 분석하여 각 frequency 별 세기를 기록한 것이 Power spectrum 형태이다.  Power spectrum : 주파수와 세기에 대한 그래프[img. Fourier transform]Waveform에 Fourier transform이용하면 하나의 파장으로 표현가능 하나, 시간에 대한 정보가 사라지게 된다.구체적으로 이를 방지하기 위해 아주 작은 시간 구간 t에 대해서만 FT하여 Spectrogram으로 바꾸는  Short-Time Fourier transform(STFT)라는 방법이 사용된다.  Hamming window의 형태처럼 Boundary 부분은 조금, 가운데 부분은 강조하는 식으로 element wise 곱을 해준다.  이때 window가 달라질 때마다(= 정의한 시간 t가 지날때마다) 값이 확달라지게 되는데, 이를 막기 위해 window가 조금씩 겹치게 하면서 Spectrum을 구하게 된다.  하단의 예시는 시간 t인 A를 20~25ms로 잡고, B를 10ms로 잡았으니 각 window들은 10~15ms(A-B)만큼 이전과 이후 window들과 겹치면서 변환이 진행된다.  이렇게 구한 Spectrum들을 stack하여 Spectrogram을 구하게 된다.[img. STFT의 예시]Spectrogram은 시간축과 Frequecy 축으로 표현한 그래프에 강도(세기, Magnitude)를 색으로 표현한 3차원 그래프이다.  Dimension을 조금 낮추면 Melspectrogram, MFCC 등의 다른 표현 방법도 있다.[img. Spectrogram. 시간별로 색이 대비되는 부분은 windowing의 흔적이다.]Application- Scene recognition by soundSound-Image Task 중 Matching에 해당하는 Scene recognition by sound task를 알아보자[img. Scene recognition by sound, 영상에 대한 sound 태깅 Task]SoundNet오디오 표현에 대한 학습을 처음 제시함, Teacher-student 학습 모델  label되지 않은 영상을 프레임별로 pretrained된 Object detection과 Scene detection 모델들에게 각각 Input으로 넣고 output을 얻는다.  Raw waveform을 CNN layer에 넣어준 뒤, 위의 output dimension과 같은 차원의 two head output을 얻는다.          이때, Spectrogram이 아니라 Raw Waveform을 쓴 이유는 단순히 연구 초기라 모르고 안썼다고 한다.        1의 output과 2의 output을 KL loss를 통해 loss를 얻은 뒤, 1번 모델들은 fixed한 채로 2번 모델을 학습시킨다. (Teacher-student 학습)  이렇게 학습된 2번의 모델을 다른 Task에 적용할 때에는 중앙의 pool5의 output인 feature vector를 task에 맞게 layer를 추가로 쌓아 사용한다.          주로 이렇게 Pre-trained 모델로 사용하기 위해 학습시킨다.      [img. SoundNet의 구조]Cross modal translation1. Speech2Face음성을 통하여 사람의 얼굴을 상상하는 Network각각 담당 Task에 대하여 Pretrained된 모델을 활용하는 Module 구조를 활용사람이 말하는 영상을 그대로 쓰면 되므로 annotation이 필요없는 self-supervised 모델이때 사용된 Pretrained 모델로  Face Recognition : VGG-Face Model          얼굴 사진을 4096-D의 Face Feature vector로 바꿔 줌        Face Decoder : facenet          얼굴 사진을 Landmark location을 이용하여 무표정으로 바꿔줌      [img.Speech2Face 구조]이후,  Spectrogram 형태로 바꾼 사운드 데이터를  Voice Encoder에 넣어 앞서 구했던 Face Recogntion의 Feature dimension과 같은 vector를 생성하고  Face feature와 비교하여 Loss를 구하여 학습한다.          이때, 학습되는 것은 Speech2Face Model인 Voice Encoder 부분이며, 기타 Pretrained 된 부분은 업데이트하지 않는다.      [img. Speech2Face에서 학습되는 부분]  Image-to-speech synthesis사진에 대한 묘사를 음성으로 출력해주는 Task, Module network 구조를 활용[img. Image-to-speech Task]  Input Image를 14x14 feature map으로 형성 후 Attention을 활용한 RNN 구조에 hidden state로 사용한다.  기본적으로 Show, Attend, and Tell 구조와 같지만, subword unit이라는 토큰 비슷한  것이 output 이다.(Learned Units)  해당 Unit을 Unit-to-Speech Model인 Tacotron 2를 이용해 subword를 음성으로 변환한다.          원본 Tacotron 2는 TTS(Text To Speech) 모델, 즉, text를 input으로 받지만 여기서는 subword를 받는다는 점이 다르다.      [img. Image to speech Task]이때, 위의 두 모델을 학습시키기 위해, Pre-trained model(ResDAVEnet-VQ)을 이용해 speech를 Unit으로 바꾸고 이를 Learned Units의 Ground-Truth로 사용한다.1) 즉 Image-to-Unit Model은 Ground-Truth Unit과 비교하여 Loss로 나와야 하고,2) Unit-to-Speech Model은 Ground-Truth Unit을 Input으로 받으면 Pre-trained model(ResDAVEnet-VQ)의 Input Speech가 나와야 한다.[img. 학습을 위한 Speech-to-Unit Model]Cross modal reasoningSound source localization사운드가 주어지면 해당 사운드가 사진의 어떤 Object가 내는지 예측하는 Task[img. Sound source localization은 Cross modal Referencing에 속한다.]label된 데이터의 여부에 따라 3가지 버전이 있으며 기초적인 과정은1) image Input을 통해서 Visual net에서 WxHxF image feature map을 내보낸다.2) audio Input을 통해서 Audio net에서 1x1xF audio feature map을 내보낸다.3) image feature map의 각 pixel 마다 audio feature map을 내적하여 관계성(attention)을 파악하고, 이 결과물 map이 Localization Score map이다.여기서 부터는 각 버전에 따라 다르다.  Fully supervised version : label이 된 데이터셋이 있는 경우4) 결과물로 나온 Localization Score를 Ground-truth Loclization score와 비교하여 loss를 구한 뒤, Backpropagation 한다.  unsupervised verison: label된 데이터셋이 없음  비디오에는 보통 Sound가 포함되어 있다는 점을 annotation으로 활용4) 1)에서 구했던 WxHxF image feature map을 결과물이었던 Localization Score map과 Weighted sum pooling하여 1x1xF의 Attended visual feature를 만든다.5) Audio net에서 만든 1x1xF audio feature map과 비교해서 metric learn Loss를 구한다.  같은 비디오에서 나온 소리면 positive pair  다른 비디오에서 나온 소리면 negative pair로 이용한다.  여러 영상에서 특정 사운드가 나올 때마다 비슷한 image feature map이 나온다면, 그 image feature map의 가중치가 높은 지점이 sound source이기 때문  semisupervised version: label된 데이터셋이 있지만 Audio net output과도 비교함4) 1. 2. 방법을 전부 사용하여 loss를 2개를 구하고 맞춰본다.[img. Sound source localization의 여러가지 버전]Speech separation동시에 말하는 사람들의 말을 각각 1사람씩 말하는 Audio를 가져오는 Task[img. Speech separation]Dataset 필요한 Supervised Learning이며, 이때 데이터셋은 단순히 목소리 2개를 겹쳐서 만들 수 있다.            과정      도식      설명                  Visualstream            N개의 나눌 사람 만큼, Face Embedding을 통해 각자 feature를 구한다.              Audiostream            Audio Spectrogram으로 speech feature를 구한다.              Audio-visualfusion            위에서 구한 feature들을 concat한 뒤, N개의 complex mask를 뽑아낸다.              output            오리지널 spectrogram과 mask를 곱해 필터링 결과 spectrogram을 ISTFT로waveform으로 바꾼 뒤,원본과 비교해서 L2 Loss를 구하여 학습      [table. Speech separation 과정]이외에도 Cross modal task로, Lip movements generation, Tesla self-driving 등이 있다.3D undersandingSeeing the world in 3D perspective우리는 3D 세상에 살고 있기 때문에, 3D 공간에 대한 이해가 중요하다.            예시      영역                        VR                    AR                    3D Print                    Medical                    Bio      [img. 3D를 활용하는 영역의 예시]빛은 직진성을 띄기 때문에 3D의 형태는 2차원에 표현이 가능하며, 우리가 실제로 3D 물체를 인식하는 방법은 3D world를 2D space에 projection하는 image이다.[img. 원근에 대한 연구와 과거와 현재의 결실]3차원 공간을 2차원 공간에 표현하는 것을 Projection을 통하여 가능했다면, 반대로 2차원 공간의 정보를 이용해 3차원 공간으로 표현하는 것은 Triangulation으로 가능하다.  2장 이상의 이미지에서 3D pint의 한 점의 pixel 이동값과 이미지를 촬영했던 위치를 알고 있으면 Triangulation을 통해 이론상 3D 모델을 예측할 수 있다.[img. Triangulation의 예시]2차원 이미지는 각 픽셀을 의미하는 2차원 array에 RGB 값을 저장함으로써 표현 가능하다.3차원 데이터의 표현은 여러가지 방법이 있다.                   3D 데이터 표현 예시                                                   Multi-view images      Volumetric(voxel)      Part assembly              2차원 이미지 여러장      3차원 array      기본 3D 도형의 집합                                        Point cloud      Mesh (Graph CNN)      Implicit shape(function)              3차원 Point 들의 집합      Vertex의 삼각 edge      고차원의 함수      [img. 3D의 다양한 표현 방법들]3D datasetsShapeNet대용량 3D model 데이터셋(51,300개, 55 카테고리)[img. ShapeNet의 object들]PartNet(ShapeNetPart2019)ShapeNet의 개량, 추가로 Detail이 annotation 되어있음(ex)자동차 모델 -&gt; 자동차 바퀴, 창문, 천장 등이 따로 나눠짐)(26,671개의 3D model 총 573,585개의 part로 나눠짐)[img. PartNet 예시들]SceneNet500만개의 랜덤하게 Generation된 RGB-Depth Indoor image들[img. SceneNet, 랜덤하게 만들어짐]ScanNetRGB-Depth 250만개의 실제 Indoor scan data[img. ScanNet]Outdoor 3D scene datasets주로 자율주행을 위한 야외 데이터셋들, Lidar로 스캔한 것이 많음[img. Outdoor 3D scene 모음]3D tasks2차원 classification이나 object detection, semantic segmentation 등, 3D에도 같은 Task가 있다.자율주행, 의료, 제조업 등에 활발히 사용 중[img. 3D task의 예시]Mesh R-CNN2D image를 입력으로 감지된 object의 3D 메쉬를 출력Mask R-CNN 구조를 변경해서 구현[img. Mesh R-CNN 예시]Mesh R-CNN은 기존의 Mask R-CNN에 3D mesh를 출력하는 3D branch가 추가된 형태[imgs. Mask R-CNN VS Mesh R-CNN]3D mesh를 생성하는 문제를 조금더 작은 여러개의 부 문제로 나누어 해결하여 더 좋은 성능을 낼 수 있다.  normal map, depth map, silhouette 검출 -&gt; 3D 오브젝트 생성  Depth 탐지-&gt; Spherical map(어느 한점을 중심으로 물체를 보았을 때의 이미지) 생성 -&gt; voxel화 -&gt; 3D mesh화[img. 더 복잡하고 정확한  3D mesh 생성법]3D application example- Photo refocusing사진의 depth map을 이용하여 사진의 focus를 바꾸는 applicationPhoto refocusing 또는 post-refocusing이라고도 함.[img. 앞의 조각상에 focusing 된 사진]depth map은 depth sensor나 neural network를 이용해 검출 가능하다.[img. 사진의 depth map]구현 과정  depth thrshold range 최소치 ~ 최대치($D_{min}\\sim D_{max}$)를 정하기  즉 $D_{min}\\sim D_{max}$까지만 focus하고 나머지는 blur 처리하겠다는 의미우리의 경우 0~255로 표현함[img.  depthmap 예시]  depth map thresholding으로 masking          focusing할 focusing area와 blur 처리할 defocsing area로 마스킹      [img. Threshold가 170일 때의 masking]focus_mask = depth_map[..., :] &amp;#38;#62; threshold_valuedefocus_mask = depth_map[..., :] &amp;#38;#60;= threshold_value[img. masking code 예시]  blurr버전의 image를 생성  Depth에 따라 adaptive하게 적용하는 방법도 있음[img. Blur kernal을 이용한 image]blurred_image= cv2.blur(original_image, (20, 20))[code. cv2를 이용한 blur 처리]  Masked focused image와 Masked defocused image를 생성하고 이미지 blending을 통해 refocused된 이미지 생성  간단한 image array의 연산으로 생성 가능[img. Masked images]focused_with_mask = focus_mask * original_imagedefocused_with_mask = defocus_mask * blurred_image[code. Masked Image 생성 코드][img. 결과물 blend][img. Threshold에 따른 blur 차이]"
  }
  , 
  
  "/articles/AI/DEEP_LEARNING/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EB%B3%B8.html": {
    title: "딥러닝 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/DEEP_LEARNING/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","DL","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true딥러닝 기본(Deep learning Basic)  본 자료는 Naver BoostAI camp의 강의를 정리한 내용입니다Historical Review소개  구현(코딩) 실력, 수학 스킬, 최신 논문 기술 등의 능력이 중요하다.[img 0. 인공지능의 대분류]  인공지능 : 인간의 지능을 흉내  머신러닝 : 데이터를 통해 인공지능을 학습      딥 러닝 : 심층 신경망을 활용한 모델 이용하는 머신러닝, network를 깊게 쌓음    딥러닝에 필요한 4가지 요소          모델이 학습할 데이터 : 풀고자할 문제에 따라 필요한 데이터가 다르다.                  Detection, Classification, Visual QnA 등                    데이터로 학습, 판단할 모델 : 데이터를 필요한 데이터로 바꿔주는 것                  AlexNet, GoogLeNet, GAN 등                    모델 학습 방법인 loss 함수 : 모델을 학습하는 방법                  단순히 줄이는 것이 아니라 학습하지않은 데이터등에도 동작해야함.          MSE, CE, MLE 등                    loss 함수를 최소화할 알고리즘 : loss 를 어떻게 줄일 것인가?                  SGD, Adagrad 등이 있음          추가로 Ensemble, MixUp, Dropout 등 테크닉이 있음                    딥러닝의 역사  Denny Britz의 Deep Learning’s Most Importat Ideas - A Bref Historical Review를 참조함  2012 - AlexNet: 최초로 인공지능 대회에서 1등을 한 DeepLearning 방법론. 시초  2013 - DQN : 강화학습에 쓰인 방법론, Q Learning 접목, Deepmind의 작품  2014 - Encoder/Decorder : 인공지능 번역에 쓰이는 방법론, 다른 언어의 연속으로 번역  2014 - Adam Optimizer :  효과 좋은 optimizer, 왠만하면 잘된다라는 뜻이라고 함.  2015 - Generative Adversarial Network(GAN) : 새로운 것을 생성하는 데 많이 사용하는 AI  2015 - Residual Networks(ResNet) :  너무 깊어진 Network layer의 성능 저하를 막아줌          input을 추가로 넣어주는 것        2017 - Transformer : attention 구조를 이용한 google의 방법론  2018 - BERT(fine-tuned NLP models) : Transformer + bidirection 구조를 활용한 모델          Bidirectional Encoder Representations from Transformers의 약자        2019 - Big Language Models(GPT-X) : OpenAI에서 만든 BERT의 Language 모델, 굉장히 많은 parameter로 이루어짐  2020 - Self-Supervised Learning: SimCLR( a simple framework for contrastive learning of visual representations)의 줄인말, 학습 데이터 외의 라벨을 모르는 데이터를 활용, 지도 학습 + 비지도 학습          시뮬레이터, 도메인 지식을 활용해 학습 데이터를 추가로 만드는 연구도 활발히 이뤄지는 중      뉴럴 네트워크(Neural Networks) - MLPNeural Networks[img 1. 두뇌 속의 신경망]  동물의 생물학적 신경망에서 영감을 받은 컴퓨팅 시스템 - wikipedia      생물학적 구조만 비슷할 뿐, 실제 작동원리와는 관계없음.        행렬의 곱과 비선형 연산의 반복을 통하여 함수(논리)를 근사추정하는 것.          neural networks are function approximators that stack affine transformations followed by nonlinear transformations.      Linear Neural Networks[img 2. 선형 모델 그래프]  Data: $\\mathcal{D} = {(x_i,y_i)}^N_{i=1}$ : input 값과 output 값이 각각 하나      Model: $\\hat{y} = wx+b,\\ \\hat{y} : 모델의\\ 예상치$ : 선형 그래프로 이루어짐    Loss: $loss =\\frac{1}{N}\\sum^N_{i=1}(y_i-\\hat{y_i})^2$ : 실제 값과 얼마나 다른가에 대한 척도, 보통 MSE loss 함수로 loss 측정\\[\\frac{\\partial loss}{\\partial w} = \\frac{\\partial}{\\partial w} \\frac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y_i})^2 = \\frac{\\partial}{\\partial w} \\frac{1}{N}\\sum^N_{i=1}(y_i - wx_i-b)^2 =-\\frac{1}{N}\\sum^N_{i=1}-2(y_i-wx_i-b)x_i \\\\\\frac{\\partial loss}{\\partial b} = \\frac{\\partial}{\\partial b} \\frac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y_i})^2 = \\frac{\\partial}{\\partial b} \\frac{1}{N}\\sum^N_{i=1}(y_i - wx_i-b)^2 =-\\frac{1}{N}\\sum^N_{i=1}-2(y_i-wx_i-b)\\][math 2. backprogation을 이용한 w와 b의 편미분값 구하기]\\(w = w - \\eta\\frac{\\partial loss}{\\partial w},\\ b = b-\\eta \\frac{\\partial loss}{\\partial b}\\)[math 2-1. loss 값을 줄이기 위한 새로운 w와 b 업데이트]      이러한 방식으로 최적값을 구하는 것을 gradient descent라고 한다.        matrix 연산을 통하여 여러 차원의 input과 output 또한 해결 가능          matrix 연산은 두 벡터 공간 상의 변환을 의미함      Activation function and Multi-layer Perceptron[math 3. Activation fucntion의 종류와 그래프 모양]  각 문제, 데이터마다 사용해야할 Activation function이 다르다.[img 3. Multi-Layer Perceptron]      이러한 여러 matrix 연산과 matrix 연산 사이의 activation function에 의해 nonlenar transform을 거쳐서 여러 층의 neural network가 된다.        각 문제마다 loss function을 다르게 하게 된다.              Regression Task : 선형 문제 (집 크기 vs 집 가격) 같은 문제에서는 MSE 등을 사용      Classification Task : 분류 문제(손글씨 숫자 구분) 같은 문제는 CE 등을 사용(가장 높은 확률의 class를 선택)      Probabilistic Task : 확률 문제(나이 맞추기 ) 같은 문제에는 MLE를 사용.      [img 3. Multi-Layer Perceptron]  실습은 https://colab.research.google.com/drive/14lEFtnt3kEn-LiwTKTwpUB-3VQ0Xx84W#scrollTo=3AS5BdrMw1E9 또는 mlp.ipynb 파일 참조Optimization관련 실습 : https://colab.research.google.com/drive/1p4H1mZpa41n3C8fQCtknQ0NfJGtEUIl6#scrollTo=B-uu6x8DFwZ9 혹은 optm.ipynb 참조용어의 정의      Gradient Descent(경사 하강): 반복 1차 미분을 통하여 loss의 국소 최소점을 찾는 알고리즘        First-order iterative optimization algorithm for finding a local minimum of a differntiable function.        Generalization(일반화): training error와 test error의 차이가 적음을 의미.        [img 4. generalization의 그래프]        [img 4-1. fitting의 도식화]          underfitting은 너무 training을 안해서 그래프가 적절하지 않음      overfitting은 너무 training을 많이해서 유연성이 없고, 해당 데이터 이외의 데이터에 부적합            Cross-validation(교차 검증, 또는 k-fold validation)          데이터를 k개로 나눈 뒤 학습 데이터와 검증(validation) 데이터를 바꿔가며 hyper parametr를 정하는 모델 검증 기술              training, validation, test 데이터로 나누게 된다.            parameter : 최적해에서 찾는 값(weight, bias 등)      hyper-parameter: 내가 시작할 때 주는 값(loss function, learning rate 등)            Bias(편향) and Variance(분산도):  분산이 적은 것이 좋다.          우리가 줄이는 cost는 사실 여러 부분으로 나뉘며 무엇을 줄일 지 생각해봐야한다.      noise가 많은 데이터면 bias와 variance를 둘다 줄이는 것이 힘드므로 골라야함      [img 4-2. bias, variance 그림]\\(Given\\ \\mathcal{D} = \\{(x_i,t_i)\\}^N_{i=1},\\ where t = f(x)+ \\epsilon\\ and\\ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\\\\\stackrel {\\mathbb{E}\\left[(t-\\hat{f})^2\\right]}{cost} = \\mathbb{E}\\left[(t-f +f-\\hat{f})^2\\right]=\\dots=\\stackrel {\\mathbb{E}\\left[(f-\\mathbb{E}[\\hat{f}]^2)^2\\right]}{bias^2}+\\stackrel {\\mathbb{E}[(\\mathbb{E}[\\hat{f}]-\\hat{f})^2]}{variance}+\\stackrel {\\mathbb{E}[\\epsilon]}{noise}\\)[math 4. cost(loss)의 구성]  bootstrapping : 학습 데이터를 일부만(예를 들어 80%만) 쓴 데이터를 각기 달리하여 여러개 만들어 랜덤 샘플링하여 학습시켜 보는것          학습결과가 일정하면 데이터가 일정한 것이고, 결과가 각양각색이면 편차가 큰 것이다.      이렇게 만든 여러 학습 데이터의 여러 모델의 평균이나 voting을 취하기도 함.(앙상블)        bagging(Bootstrapping aggregating) vs boosting          bagging : bootstraping으로 만들어진 여러개의 모델 (앙상블 기법)      boosting : 전체 데이터로 학습 해본 뒤, 해당 모델로 결과를 측정해 잘 예측못하는 데이터만 모아서 가중치를 더 크게 준 뒤, (랜덤 뽑기에 더 많이 할당?) 새로운 모델로 만든 뒤 이전 모델과 합치는 형식으로 진행 (앙상블 기법의 한 종류)      [img 4-3. bagging boosting 그림]Practical Gradient Descent MethodsGradient Descent Methods  Stochastic gradient descent          update with the gradient computed from a single sample      하나의 샘플마다  경사를 계산        Mini-batch gradient descent          update with the gradient computed from a subset of data      batch 크기의 샘플마다 경사를 계산      가장 자주 사용함        Batch gradient descent          update with the gradient computed from the whole data      한번에 모든 샘플을 활용하여 경사를 계산      Batch-size Matters  일반적으로 batch size가 너무 작으면 너무 오래걸리고, 크면 계산량이 너무 많다.  연구 결과 batch-size가 작을 수록 유리하다는 것이 실험적으로 증명됨[img 5. batch size가 작을 수록 좋은 이유]  batch size가 작으면 Flat Minimum, 크면 Sharp Minimum으로 도착하는 경향이 크다.  Flat Minimum은 test data 에서도 generalization이 잘되있지만 sharp minimu에서는 실제 testing 데이터와 갭이 크다.optimizer  특성을 확인하고 상황에 따라 골라서 사용해야함Gradient Descent$W_{t+1} \\leftarrow W_t - \\eta g_t,\\ \\eta:learning\\ rate,\\ g_t:Gradient$[math 6. 경사하강법 ]  가장 기본적인 방법  적절한 learning rate를 잡는 것이 힘듦Momentum\\[a_{t+1} \\leftarrow \\beta a_t + g_t\\\\ a_{t+1}:accumulation,\\ \\beta: momentum \\\\W_{t+1} \\leftarrow W_t - \\eta a_{t+1}\\\\ \\eta:learning\\ rate\\][math 6-1. 모멘텀 개념]  이전 gradient의 값이 영향을 조금 받은 gradient로 업데이트  기본버전보다 조금 낫다.Nestrerov Accelerated Gradient\\[a_{t+1} \\leftarrow \\beta a_t + \\nabla \\mathcal{L}(W_t-\\eta \\beta a_t)  \\\\ \\nabla \\mathcal{L}(W_t-\\eta \\beta a_t):Lookahead\\ gradient,\\ \\beta: momentum \\\\W_{t+1} \\leftarrow W_t - \\eta a_{t+1}\\\\ \\eta:learning\\ rate,\\ g_t:Gradient\\][math 6-2. NAG][img 6. NAG와 Momentum 차이점]  momentum을 계량함  최소 지점에 도달하는 것이 증명됨  이전 gradient와 현재 그레디언트로 구하는 방법과 달리 이전 momentum gradient 벡터에서 현재 벡터로 이동한다는 다른점이 있음Adagrad\\[W_{t+1} = W_t - \\frac{\\eta}{\\sqrt {Gt+\\epsilon}}g_t\\\\G_t : Sum\\ of\\ gradient\\ squares,\\ \\epsilon:for\\ numerical\\ stability\\][math 6-3. Adagrad 개념]  파라미터의 변화량이 너무 적게 변하면 크게, 많이 변화해온 파라미터는 적게 learning rate를 잡아주어 조정해줌  뒤로 가면 갈수록 G~t~가 커져서 무한대로 가까이 변해 거의 learning rate가 0으로 수렴되는 단점  $\\epsilon$은 분모가 0이 되는 것을 막기 위해 주는 아주 작은 값.Adadelta\\[G_t = \\gamma G_{t-1} + (1-\\gamma)g_t^2\\\\W_{t+1} = W_t - \\frac{\\sqrt{H_{t-1}+\\epsilon}}{\\sqrt {G_t+\\epsilon}}g_t\\\\H_t=\\gamma H_{t-1}+ (1-\\gamma)(\\Delta W_t)^2\\\\G_t:EMA\\ of\\ gradient\\ squares,\\ H_t: EMA\\ of\\ difference\\ squares\\][math 6-4. Adadelta 개념]  learning rate를 사용하지 않음.  window size를 정하고 해당 size step 만큼만 learning rate에 영향을 주게하여 무한대로 수렴하는 것을 막음          예를 들어 윈도우 사이즈 10이번 11번 바뀌면 첫번째 파라미터 변화는 영향을 안주게 하고 11번째를 대신 추가.        최근 100개의 값들을 모두 저장하면 메모리가 터지므로, exponential을 이용해서 구함RMSprop\\[G_t = \\gamma G_{t-1} + (1-\\gamma)g_t^2\\\\W_{t+1} = W_t - \\frac{\\eta}{\\sqrt {G_t+\\epsilon}}g_t\\\\G_t:EMA\\ of\\ gradient\\ squares,\\ \\eta: stepsize\\][math 6-5. RMSprop 개념]  adadelta에 stepsize만 추가, 그냥 경험적, 실험적으로 깨달은 식Adam\\[m_t = \\beta_1 m_{t=1} + (1-\\beta_1)g_t\\\\v_t = \\beta_2v_{t-1} - (1-\\beta_2)g_t^2\\\\W_{t+1} = W_t - \\frac{\\eta}{\\sqrt {v_t+\\epsilon}}\\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}m_t\\\\M_t:Momentum,\\ v_t: EMA\\ of\\ gradient\\ squares,\\ \\eta: Step\\ size\\][math 6-6. Adam 개념]  RMSdrop에 momenturm을 합친 개념  무난하고 좋은 성능을 보인다.Regularization  generalization을 위해 학습에 제한을 거는 방법Early Stopping[img 7. early stopping]  validation error와 training error를 비교하며 generalization gap이 가장 적을 때 stop하는 방법Parameter Norm Penalty\\[total\\ cost = loss(\\mathcal{D;W}) + \\frac \\alpha 2 \\left \\| W \\right \\|^2_2\\\\\\frac \\alpha 2 \\left \\| W \\right \\|^2_2:Parameter\\ Norm\\ Penalty\\][math 7. parameter Norm Penalty에 의한 cost 계산]  parameter들의 합이 너무 커지는 것을 방지  부드러운 parameter일 수록 generalization이 좋은 경향이 있음Data Augmentation  데이터가 적을 때는 오히려 전통적인 머신러닝이 성능이 좋지만, 데이터가 크면 클수록 최신 딥러닝이 좋다.  문제는 데이터가 적으므로, 기존의 데이터를 여러가지 방법으로 바꾸어서 늘리는것  이미지 데이터로 예시를 들면, 흑백, 일부 가림, 이미지 방향 반전 등이 있다.Noise Robustness  data Augmentation과 비슷하지만, 데이터 뿐만 아니라 weights에도 노이즈를 주어서 성능 향상Label Smoothing  데이터 2개를 뽑아서 섞어 decision boundary를 부드럽게 해줌  mix-up 방법, cumMix 방법 등이 있음[img 7-1. Label Smoothing의 그림예시]  성능이 되게 좋다.Dropout[img 7-1. Label Smoothing의 그림예시]  랜덤하게 neuron을 버린다.  성능은 좋아지지만 수학적으로 증명이 되진 않음Batch Normalization\\[\\mu_B = \\frac 1 m \\sum_{i=1}^m x_i\\\\\\sigma^2_B = \\frac 1 m \\sum^m_{i=1}(x_i-\\mu_B)^2\\\\\\hat{x}_i =\\frac {x_i - \\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}}\\][math 7-1. Batch Normalization 계산]  논란이 크지만 성능이 좋아짐.  layer들의 parameter들의 값을 평균과 분산을 이용하여 같은 값으로 바꿈.[img 7-2. 다른 normalization의 종류]Convolutional Neural NetworksConvolution 연산  2개의 함수가 있을때 2개의 함수를 섞는 operator  연속 공간, 이상 공간에 따라 수식 다름  I는 전체 공간, K는 필터\\(\\cdot Coninuous\\ convolution:\\ (f*g)(t) = \\int f(\\tau)g(t-\\tau)d\\tau=\\int f(t-\\tau)g(t)d\\tau\\\\\\cdot Discrete\\ convolution:\\ (f*g)(t) = \\sum^\\infty_{i=-\\infty} f(i)g(t-i)=\\sum^\\infty_{i=-\\infty} f(t-i)g(i)\\\\\\cdot 2D\\ image\\ convolution:\\ (I*K)(i,j) = \\sum_m\\sum_n I(m,n)K(i-m,j-n)=\\sum_m\\sum_n I(i-m,i-n)K(m,n)\\\\\\)[math 8. Convolution operator]  2차원 콘볼루션 연산의 예시.[img 8. 2D image Convolution 그림]\\(O_{11}=I_{11}K_{11}+I_{12}K_{12}+I_{13}K_{13}+I_{21}K_{21}+I_{22}K_{22}+I_{23}K_{23}+I_{31}K_{31}+I_{32}K_{32}+I_{33}K_{33}+bias\\\\O_{12}=I_{12}K_{11}+I_{13}K_{12}+I_{14}K_{13}+I_{22}K_{21}+I_{23}K_{22}+I_{24}K_{23}+I_{32}K_{31}+I_{33}K_{32}+I_{34}K_{33}+bias\\\\O_{13}=I_{13}K_{11}+I_{14}K_{12}+I_{15}K_{13}+I_{23}K_{21}+I_{24}K_{22}+I_{25}K_{23}+I_{33}K_{31}+I_{34}K_{32}+I_{35}K_{33}+bias\\\\O_{14}=I_{14}K_{11}+I_{15}K_{12}+I_{16}K_{13}+I_{24}K_{21}+I_{25}K_{22}+I_{26}K_{23}+I_{34}K_{31}+I_{35}K_{32}+I_{36}K_{33}+bias\\)[math 8-1. 2D image Convolution operation][img 8-2. 2D image Convolution filter operation]  2차원 이미지의 경우 tensor로 표현되며, 보통 rgb로 계산 시 뒤의 X3(R,G,B)은 생략이 된다.  즉 5X5 convolution 연산은 기본적으로 5x5x3에서 x3이 생략된 것이다.  계산 결과는 x1이 된다.[img 8-3. 2D image Convolution featuremap]  feature map을 연산할 때 여러 층의 feature가 나오는 방법은 여러겹의 필터를 곱하여 만드는 것이다.maxpool2d층 원리[img. Maxpool 2d층 예시]  각 구역을 kenelsize와 stride 만큼 나누어 가장 큰값을 취함  Max값을 취하는 Maxpool이외에도 평균값을 취하는 averagepool등도 있다.  featureamp의 크기가 줄어들어 성능을 줄이고 특징을 두드러지게 할 수 있다.  다만, 공간 정보(위치, 방향, 비율)등이 모호해 지기도 한다.CNN 구조와 용어[img 8-4. CNN 구조]  Convolution과 pooling layer는 feature extraction을 하는 역할  Fully connected Layer는 decision making(ex) classification)을 위한 층          고전적인 CNN와 달리 최근에는 파라미터 수를 줄이기 +  generalization 성능 향상을 위해 FCL을 줄이는 추세        Stride[img. Stride 1과 2의 차이 그림]  pixel을 뛰어넘는 수,  filter의 밀도,  filter가 stride 수 만큼 pixel을 넘어가며 생성한다.  2차원의 경우 x,y 2개로 설정 가능  Padding[img. padding의 유무 차이 그림]  Stride 등으로 인해 외부로 나가는 픽셀을 padding으로 추가함  zero padding은 0을 넣는다는 의미, 이로써 input과 output의 space dimension이 같아짐[img. stride, padding의 예시]Convolution Arithmetic[img. $3 \\times 3$  kernel, Padding 1, Stride 1 의 연산  parameter 계산 예시]  parameter의 수는 가중치의 수      convolution layer의 학습 파라미터 수는 (필터 폭  X 필터 높이  X 입력 채널 수 X 출력 채널 수)로 계산        위 예시는 $3 \\times 3 \\times 128 \\times 64 = 73,728 $ 개의 학습 파라미터 수    Max pooling layer는 parameter output이 없다.          메모리 성능 제한 때문에 2개로 나누어 trainig 하는 layer?      그러므로 나뉜 수만큼 곱해주면 된다.      [img. convolution 연산 추가 예시]  $5 \\times 5 \\times 48 \\times 128*2 \\approx 307k$  굳이 정확히 숫자를 세는 것이 아니라 대략적인 양(마치 알고리즘의 big O 표기처럼) 성능을 측정할 수 있어야 한다.[img. dense layer (fully connected layer) 연산 예시]      input의 neuron과 output의 neuron의 수를 곱한 만큼이다.        $13 * 13 * 128 * 2 \\times 2048 *2 \\approx 177M$    $2048 * 2 \\times 2048 *2 \\approx 16M$  $2048*2 \\times 1000 \\approx 4M$  Convolution operator는 같은 kernel을 연산에 쓰면서 parameter가 공유되므로 비교적 적다.  1000배 이상의 parameter가  fully connected layer에 쓰이므로 이 부분을 줄이는 추세이다.1x1 convolution를 활용한 최적화[img. 1x1 convolution 적용에 의한 차원(filter)의 감소 효과]  1x1 convolution layer 연산을 통하여 차원(filter)를 감소시켜 parameter 수를 줄이며, 층 수는 늘릴 수 있다.  bottleneck 구조의 원리Modern CNN  ~2018년 까지의 CNN 기술  ImageNet Large-Scale Visual Recognition Challenge 위주          Classification, Detection, Localization, Segmentation 등의 부문이 있음        딥러닝의 최근 Error rate는 3.5% 이하로 인간의 5.1% 보다 에러가 적다.  AlexNet[img. AlexNet 구조]      컴퓨터 성능의 한계를 극복하기 위해 네트워크를 2개의 길로 나눈 8단의 layer.    11x11x3 filter 사용          fitler의 크기가 클수록 convolution 연산 시, 고려되는 input의 크기(receptive field)가 커짐      receptive field : feature map 추출시 고려 가능한 입력의 spacial dimension.      단, parameter의 수가 커짐        ReLU 활성 함수 사용.  [img. Relu 함수, 0 이하는 0으로 바꾼다.]          선형 모델의 장점, 학습이 용이, generalization 효과가 좋고, Vanishing gradient problem 극복        2개 GPU 사용, Data augmentation, Dropout 활용          그 외에도 Local Response normalization, Overlapping pooling 활용        VGGNet[img.VGGNet 구조]      3x3 convolution filter를 활용하여 파라미터 수 줄임            [img. 3x3 filter 두번 사용 vs 5x5 filter 한번 사용 파라미터 수 비교]          필터를 통해 보는 input field의 크기는 같으나 2번 걸침으로 써 파라미터의 수는 줄일 수 있다.      이 방법을 통해 보통 최대 7x7 필터를 넘지 않는다.            Dropout과 1x1 convolution을 dense layer에 활용        16층 버전(VGG16), 19층 버전(VGG19)이 있음    GoogleNet[img. googlenet 구조]      NIN 구조(Network in Network) : 네트워크 내부에 모듈 형식의 작은 네트워크들의 반복이 존재        Inception blocks: 여러개로 퍼졌다고 다시 합쳐지는 블록                  [img. inception 모듈]                  여러 개의 responsed를 추출 가능                    1x1 Conv layer에 의해 파라미터의 수 감소.                    채널 방향의 차원을 줄이는 효과가 있음                  [img. 1x1 convolution의 채널 감소 효과에 의한 파라미터 수 감소]        VGGNet, AlexNet에 비해 layer는 깊지만 파라미터 수는 오히려 적음    ResNet      깊은 층을 가진 DNN의 training error와 test error의 갭을 줄이고 학습을 용이하게 함.        이를 통해 깊은 층의 DNN을 활용할 수 있게 해줌.        parameter  수는 줄고, 성능을 늘어나기 시작함        Residual connection (or Identity map)                          [img. identity map(residual map) 비교]                    출력 값을 일부 layer 너머의 출력에 더해 줌(skip connection)            위 처럼 더해주는 simp shortcut 방식과 1x1 conv layer를 거쳐서 더해주는 Projected shortcut 방식(차원을 맞춰줘야 더 해지므로)이 있다.              일반적으로 convolution layer 다음에 batch Norm, activation 함수 순으로 배치되며, Residual 합산은 batch Norm 뒤에, activation 앞에서 이루어진다.                  논란이 있으며, 순서가 바뀌어야 성능이 좋아질 때도 잇다.                          Bottleneck architecture          1x1 conv layer을 통해 input channel을 줄여서 parameter 수를 줄이고,  다시 채널을 늘려서 값을 더할 수 있게 함.      [img. bottleneck architecture 그림]  DenseNet[img. Resnet과 DenseNet 차이]  Resnet과 달리 결과값을 더하는 것이 아닌 concatenation 하는 방식  채널이 점점 기하 급수적으로 커지므로, 중간에 한번씩 채널을 줄여줌          Dense Block : layer결과를 concatenate하여 채널을 늘림      Transition Block : batchnorm과 1x1 conv, 2x2 avgPooling을 통하여 채널 수 줄임      위 두 block의 반복        간단하고 성능이 좋다.Computer Vision ApplicationsSemantic Segmentation  이미지 내부의 일부(픽셀)를 물체로써 식별하는 문제  자율 주행에서 사람, 인도, 자동차 등을 식별하는 등에 사용Fully Convolutional Network(FCN)[img. 기존의 CNN vs Fully Convolutional Network]      dense layer을 거치지 않고, convolution layer로 바꾸어 결과의 크기를 10이 아닌 차원의 수를 10으로 만드는 것을 convolutionalization이라고 한다.        양쪽 다 parameter 수는 똑같이 4x4x16x10 = 2560으로 같다.        하지만 이를 통하여 원본보다 size가 줄어든 heat map을 구할 수 있다.  [img. convolutionalize 를 통한 heat map 생성, 고양이의 추정위치 확인이 가능해짐. ]Deconvolution(conv transpose)[img. Deconvolution 개념]  위의 줄어든 size를 원래대로 돌리기 위해 Deconvolution을 진행할 수 있다.  원래 픽셀을 그대로 돌려주진 않으나 원본 크기로 돌아가게 된다.[img. Deconvolution의 도식화]Detection  이미지 내 물체의 바운딩 박스를 찾는 문제R-CNN[img. R-CNN의 절차와 예시]  이미지에서 Selective search를 통해 물체로 추정되는 부분의 bounding box를 bounding box regression을 통하여 전부 뽑는다.  해당 bounding box를 같은 크기로 바꾼 뒤, CNN(여기서는 AlexNet)을 통하여 feature를 뽑는다.  features를 SVM(support vector machine)을 통하여 classification한다.  1번의 물체를 추정되는 부분의 bounding box를 전부 뽑는 부분이 엄청나게 느리다.SPPNet[img. SPPNet의 구조]  CNN을 한번만 돌린 뒤, 해당 바운딩 박스 하나에서 feature를 뽑고 나서 그것을 spatial pyramid pooling을 통하여 classification함.Fast R-CNN[img. fasc R-CNN]  SPPNet과 비슷하다.  인풋 이미지의 바운딩 박스를 여러개 뽑는다.  CNN feature map을 만든다.  ROI(region of interest) pooling을 통하여 feature map을 뽑고, classification과 bounding-box regressor를 뽑는다.Faster R-CNN      Fast R-CNN + Region Proposal Network        Region Proposal Network(RPN)            [img. RPN 예시]          바운딩 박스를 찾는 알고리즘 또한 교육함, classification은 하지 않음.      Anchor Boxes: 미리 정의한 물체 크기로 이루어진 kernel                [img. RPN 차원]          RPN의 Fully Conv에 의해 해당 공간이 원하는 물체를 가지고 있는지 판단      3개의 region 크기(128, 256,512)와 3개의 비율(1:1, 1:2, 2:1)을 가진 총 9개의 anchor boxes를 가짐      각 bouding box가 조정되어야할 크기 (width 크기, height 크기, x offset, y offest) 4개      해당 bounding box가 classification에 쓸모 있는가?(use it or not) 2개      총 9*(4+2) = 54개의 채널을 가진 Fully Conv를 가진다.            좀 더 좋은 성능의 detection 가능  YOLO(You only look once)[img. yolo 예시]      v5 까지 나왔음, 아주 빠름, 리얼 타임을 유지할 수 있다.        추출한 bounding box들의 feature를 통해 각각 classification 하는 방식이 아니라, 한꺼번에 모든 bounding box를 classification 함        여러 bounding box를 동시에 한번만 하므로 YOLO라고 한다.        [img. YOLO 절차]          먼저 주어진 이미지를 SxS 그리드로 나눈다.                  찾고 싶은 물체의 중앙점이 속해있는 그리드에서 bouding box와 classification을 진행한다.                    무언가 물체의 중앙점을 갖는 여러개의 bounding box의 x,y 위치와 w,h 크기 그리고 쓸모 여부를 예측한다 ( 이 정보 5개를 B 라고 하자.).      위 2번과 동시에 각 그리드가 속한 물체의 classification(C개의 class가 있다고 가정하자)을 진행한다.      해당 정보를 취합한 뒤, SxSx(B*5+C) 사이즈를 가진 tensor가 된다.            v2의 경우 ROI 처럼 미리 정의된 크기의 bounding box를 이용하기도 하고, 다른 모델들 또한 yolo의 방법을 사용하는 등의 상호의 장점을 이용한 발전을 한다.  Sequential Models - RNNSequential Model  sequential data란 순서 관계가 중요한 연속형 데이터로, 입력의 차원의 크기를 정확히 알 수 없다는 문제가 있다.(언제 부터 언제까지의 데이터를 사용해야하는가? 언제 데이터는 끝이 나는가?)  이러한 문제 때문에 CNN이나 Fully connected layer는 사용 못한다.  Naive sequence Model[img. Naive sequence Model]  과거의 정보들을 모두 고려하는 모델  Autoregressive model(AR model)[img. Autoregressive model]  fixed timespan $\\tau$만큼 만을 고려하는 모델  Markov model(first-order autoregressive model)[img. Markov model]  바로 전 정보만을 이용하는 모델, joint distribution 표현이 쉬움  Latent autoregressive model[img. Latent autoregressive model]  중간의 과거 정보들을 요약하는 Hidden state를 생성하여 해당 정보를 이용하는 모델Recurrent Neural Network(RNN)[img. RNN 그림]  RNN은 AR model들을 구현한 신경망,  Short-term dependecies : RNN의 단점, 과거 시점의 정보가 미래에 영향을 끼치기 힘듦, 이를 해결하기 위해 밑의 LSTM이 나타남.[img.  RNN hidden state의 gradient 문제의 원인]  또한 Activation function의 종류에 따라 Vanishing/exploding gradient 문제가 생길 수 있다.          RNN에서 ReLU를 잘 안쓰는 이유      Long Short Term Memory(LSTM)[img. Vanilla RNN Unit]  tanh(hyperparabolic) 함수를 activation 함수로 활용하는 기본 유닛이다.[img. LSTM Unit]  Long Term dependency 문제를 해결하는데 좋은 LSTM 유닛  이전 LSTM Unit에서 이후 LSTM Unit으로 cell state와 hidden state 를 넘겨주게 된다.  cell state는 hidden state와 달리 output으로 나오지 않으며, 일종의 이전 정보들을 summary를 해주는 정보, LSTM의 Core ideaGate  LSTM을 이루는 3개의 게이트가 존재, LSTM의 데이터를 조작  Forget Gate[img. Forget gate 구조]  어떤 정보를 잊어버릴지 결정.  f~t~는 sigmoid를 사용하여 0 에서 1 사이 값으로 나오며,  이전 cell state 정보의 일부를 버리거나 살린다.  Input Gate[img. Input Gate 구조]  어떤 정보를 cell state에 올릴지 결정  i~t~는 이전 Hidden state와 X~t~를 통하여 어떤 정보를 올릴지 말지 결정한 결과인 i~t~를 만든다.  또 한 마찬가지로 이전 Hidden state와 X~t~를 통하여 올릴 정보인 $C_t^{\\sim}$(C 틸다)를 만든다.[img. 새로 통과시킬 Cell State 형성]  $C_t^{\\sim}$와 i~t~,f~t~ 를 이용해 업데이트할 Cell을 만들게 된다.  Ouput Gate[img. Oupt Gate 구조]  위에서 만든 Update cell state와 input 을 이용해 output값을 만든다.Gated Recurrent Unit(GRU)[img. GRU unit 구조]  reset gate와 update gate만 존재하며 cell state가 존재 하지 않다.          forget gate와 비슷한 reset gate와 비슷한 update gate가 존재한다.        LSTM에 비해 구조가 단순하여 parameter 수가 적어 generalization performance가 좋으며, 성능이 좋은 편이다  하지만 최근에는 위 세가지 구조 전부를 transfromer 구조로 대체되는 추세이다.Transformer 모델      Jay Alammar의 블로그에서 가져온 그림들임(http://jalammar.github.io/illustrated-transformer/)        불규칙적이고 예상하기 힘든 sequential 데이터의 문제점을 해결한 모델  [img. sequential data의 대표 오류]Transformer[img. Transformer 모델 예시]      재귀적 구조가 없는 대신, attention이란 구조를 활용한 sequence model        기계어 번역 문제를 해결하기 위해 시작했지만 여러 문제를 해결 할 수 있다.        Encoder와 Decoder 구조로 이루어져 잇다.  [img. NMT 문제에서 encoder, decoder 구조]  동일한 구조, 다른 파라미터를 받는 encoder, decoder가 쌓여있는 구조  하나의 모델에 입력과 출력 값이 각각 도메인, 입력의 숫자 등을 다르게 줄 수 있다.  즉 encoder-decoder 모델의 경우, encoder가 하나씩이 아닌 한번에 입력을 처리한다.어떻게 encoder는 한번에 n개의 입력을 동시에 처리하는가?[img. encoder 구조]      Feed Forward Neural Network: MLP때와 동일        Self-Attention: encoder와 decorder 구조의 핵심, Attention이란 해당 단어를 처리할 때 다른 단어에 얼마나 관계성을 할당하는 가?이다.  [img. 단계1, 2 ]  먼저 각 단어들을 embedding vector로 바꾼 뒤, self attention 층에서 입력된 n개의 단어들을 모두 고려하여 새로운 z벡터를 생성한다.  그 후 Feed Forward에서는 동일한 조건의 Feed-forward 층을 각 단어 독립적으로 통과 시킨다.좀 더 자세한 벡터 처리 예시[img. 단어가 2개 주어졌을 시 예시][img. 세 벡터 생성]  Self attention 구조는 embedding된 벡터 형태로 단어가 주어지면, 각 단어 마다 Neutral network를 이용해 Queries, Keys, Values 라는 세개의 벡터(Q,K,V 벡터)를 생성한다.          이 세 벡터를 통해 embedding vector를 새로운 벡터로 바꿔준다.(=encoding)      [img. Thinking 단어의 Score 생성]  Thinking의 Queries 벡터와 모든 단어들의 Keys 벡터를 내적(inner product)하여 Score를 생성          Score를 통해 다른 단어와의 관계성, 유사성 등(=attention)을 구할 수 있다.      [img. Score의 normalize 및 z1 벡터 생성]  Score 값을 8(키 벡터의 차원(여기서는 64)의 루트,$\\sqrt d_k$)로 나눠 주어 Normalize(일정 범위에만 머무르게 하기 위해서)한다.  이 후, softmax 함수로 0~1 사이로 만들어 Attention weights를 만든다.  Attention Weights를 Value vector로 Weighted Sum을 하여 한 단어의 z(인코딩 벡터) 벡터를 생성한다.[img. Key, Query, Value vector 생성]  W^Q^,W^K^,W^V^는 모든 단어가 공유한다.[img. 인코딩 벡터 생성]  Value vector의 차원은 엄밀히 말해 weighted sum만 하므로 Query vector, Key vector와 달라도 된다.Transformer 구조의 장단점  이런식으로 모든 단어들이 서로 영향을 주므로, 같은 단어라도 다른 단어가 들어가면 결과값이 달라지므로, 변화에 용이한 모델이 나온다..  대신 모든 단어를 고려해야하므로 많은 컴퓨팅 자원이 필요하다.Multi-headed attention(MHA)[img. attention이 2번 실행된 단어]  attention 과정을 여러번 실행함, 단어마다 Query, Key, Value 벡터가 여러개 생성된다.[img. 여럿 생성된 인코딩 벡터]  이를 통해 여러개(예시에선 8개)의 인코딩 벡터(z0~z7)를 생성하게 된다  이 8개를 합쳐서 다음 layer에 input 되어야 한다.[img. learnable linear map을 통해 통합된 차원의 벡터(Z)로 생성]  input된 단어, embedding vector와 output 인코딩 벡터(z)들의 차원이 같아야한다.  그러므로 Learnable Linear amp을 이용해 교육시킨 W^o^값을 곱해서 차원을 맞춰준다.[img. 전체적인 MHA의 동작]  실제로는 위의 방법보다는 input의 embdding vector를 n개로 나눈 뒤, 나눠진 일부들로 attention을 만든 뒤, 다시 concatenate 한다.          ex) 100차원 input -&gt; 10개로 나누어 10차원 z0~z10 10개 생성 -&gt; 100차원 output(Z)으로 합침            (위의 n개의 동시 처리 예제에서 output을 구한 뒤 부터 이어짐) attention을 하기 이전에 embedding vector에 POSITIONAL ENCODDING 이라는 벡터를 더해준다.        [img. positional encdoding의 합]          일종의 bias와 비슷하며, 위 attention 과정을 보면 data의 sequence와 independent 하기 때문에(즉, 단어의 순서가 뒤바껴도 같은 값이 나오게 되어있다.) 이를 방지하기 위해 더해준다.          [img. 512-dimensional 일시, positioinal encoding 벡터 구하는 법1][img. 최신 방법의 Positional encoding 구하는 법 2]  포지션별로 특정 그래프의 값을 가져와 더해주면 된다.(predefined)[전체적인 encoder의 과정]decoder와 encoder 사이에는 어떤 정보가 교환되는가?[img. encoder와 decoder 사이의 정보교환 그림]  decoder에서는 주어진 vector로 유의미한 결과를 만드는 역할을 한다.[gif. encoder decoder 통신 애니메이션1][gif. encoder decoder 통신 애니메이션2]  가장 상위 layer의 encoder의 결과값(z) 벡터의  key와 value, 두 벡터를  decoder layer들로 보낸다.decoder는 어떻게 결과값을 만들어 내는가?  이후 decoder에 들어가는 Query vector와 k, v 벡터로 auto regressive 하게 결과물을 출력한다.[img. decoder 학습 과정]  이후,  decoder의 slef-attention layer에서 masking을 통하여 생성하려는 단어와 그 뒤 생성해야할 단어들을 가린 뒤, 앞에서 이미 생성한 단어에 의존해서 학습하게 만든다.  또, Encoder-Decoder attention layer에서 encoder에서 준 벡터 둘을 받아서 학습시킨다.[img. decoder 학습의 최종 과정]  마지막 층에서는 단어들의 배열에서 단어를 샘플링해서 결과 값을 낸다.Transform 모델의 근황[img. encoder만 활용하여 이미지 class 구분하는 모델]  단순히 단어나 다른 sequential data 뿐만아니라 vision 영역에도 활용되고 있다.  openAI의 DALL-E에서 문장을 통해 이미지를 생성하는 연구 또한 Transformer의 decoder를 활용하여 진행했다.Generative Models(생성 모델)Introduction  Generative Model이란, 이미지 등을 생성하거나, 확률 밀도를 탐색하거나 비지도 특색 학습에 사용되는 모델을 의미한다.          Generation: 이미지 생성 등(sampling)      Density estimation: 이미지가 강아지 같은가? 고양이 같은가? (anomaly detection), classify 모델을 포함하고 있음. (explicit 모델, &lt;=&gt; inplicit model: 생성 위주가 가능한 모델)      Unsupervised representation learning:  이미지 내부의 특색 탐색 (feature learning)      Basic Discrete Distributions  Bernoulli distribution : 동전 던지기 처럼 0 또는 1이 나오는 형태          $D={Heads, Tails}$      Specify P(X = Heads)=p. Then P(X=Tails) = 1-p.                  예를 들어 앞면이 p 면 뒷면이 나올 확률은 1-p다.                    Write: X ~ Ber(p).        Categorical distribution: 주사위 던지기 같이 구분되는(discrete) 여러 결과값이 나오는 형태(1~6)  $D={1,\\dots,m}$  Specify P(Y = i) = pi, such that $\\sum^m_{i=1}p_i=1$.          모든 확률을 합해서 1        Write: Y ~ Cat(p~1~, …, p~m~)  예시  RGB pixel이 가지는 경우의 수는 256 * 256 *256이며, 필요한 파라미터의 수는 255*255*255.Structure Through Independence &amp; Conditional Independence      binary pixel의 수가 100개 라고하면 가능한 파라미터는 2^100^-1개가 되고, 이는 너무 많다.        만약 모든 Pixel이 서로 independent 한다고 가정하면 경우의 수는 같지만, 파라미터의 수는 n(=100)개로 줄일 수 있다.    하지만 실제로 independent 하지 않으므로 너무 말이 안되는 가정이다.  이 둘 사이의 타협점을 찾기위한 것이 Conditional independence 이다.\\[Chain\\ Rule:\\ p(x_1,\\dots,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\dots p(x_n|x_1,\\dots,x_{n-1})\\\\Bayes'\\ rule:\\ p(x|y)=\\frac {p(x,y)}{p(y)}=\\frac {p(y|x)p(x)}{p(y)}\\\\Conditional\\ independence:\\ if\\ x\\perp y | z,\\ then\\ p(x|y,z)=p(x|z)\\][math. Conditional independence의 세가지 룰]  conditional independence: z가 주워 졌을때, x,y가 independence라고 가정하면 성립, 이를 chain rule과 섞으면 좋은 타협점을 가진 모델을 생성할 수 있다.\\[Chain\\ Rule:\\ p(x_1,\\dots,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\dots p(x_n|x_1,\\dots,x_{n-1})\\\\\\\\if\\ assume\\ \\ X_{i+1}\\perp X_1,\\dots,X_{i-1}|X_i (Markov\\ assumption)\\\\become\\ \\ \\ p(x_1,\\dots,x_n) =p(x_1)p(x_2|x_1)p(x_3|x_2)\\dots p(x_n|x_{n-1})\\][math. chain rule과 conditional indepence의 조합]  Markov assumption을 이용하면 parameter 수가 기존의 2^n^-1 에서 2n -1로 변한다.  이러한 conditional indepency 방법으로 생성한 모델을 Auto-regressive model이라고 한다.Auto-regressive Model  28 X 28 binary pixel 이미지의 경우 우리는 p(x)를 구하기 위해 autoregressive model로 만들 수 있다.  pixel의 order 순서에 따라 모델과 방법론이 달라지기도 한다.(아래 Pixel RNN 참조)  NADE(Neural Autoregressive Density Estimator) 모델[img. NADE 모델]  i 번째 픽셀을 첫번째 부터 i-1번째 픽셀에 dependent하게 생성(dense layer)                                                      즉, $p(x_i              x_{1:i-1}) = \\sigma(\\alpha_ih_i+b_i)\\ where\\ h_i=\\sigma(W_{&lt;i}x_{1:i-1}+c)$ 이다.                                            입력 차원이 점점 더 커지게 된다.  explicit 모델이며 확률분포를 구할 수 있다.\\[p(x_1,\\dots,x_{784})=p(x_1)p(x_2|x_1)\\dots p(x_{784}|x_{1:783}) \\\\where\\ each\\ conditional\\ probability\\ p(x_i|x_{1:i-1})\\ is\\ computed\\ independently\\][math. chaine rule을 통한 joint probability ]  연속적인 분포(continuous random variables)일 경우 a mixture of gaussian을 사용해 표현 가능  Pixel RNN  이미지 내의 pixel 생성하는 auto-regressive 모델\\[p(x)=\\prod^{n^2}_{i=1}p(x_{i,R}|x_{&lt;i})p(x_{i,G}|x_{&lt;i},x_{i,R})p(x_{i,B}|x_{&lt;i},x_{i,R},X_{i,G})\\][math.nXn RGB image 생성]  ordering에 따라 Row LSTM, Diagonal BiLSTM으로 나눠짐[img. 빨간 색이 생성할 pixel, 파란 색이 참조할 pixel이다.]Latent Variable ModelsVariational Auto-encoder      Variational inference(VI, 변분 추론)          VI의 목적은 복잡한 posterior distribution(사후확률 분포)을 variational distribution(변분 분포)으로 최적화하는 것이다.                                                                              Posterior distribution($p_\\theta(z                  x)$):  관심있는 random variable의 확률 분포, 이것의 반대, $p_\\theta(x                  z)$는 likelihood라고 한다. z는 latent vector를 의미한다.                                                                                                                          Variational distribution($q_\\theta(z                  x)$): Posterior distribution을 알기 쉽게 근사하는 분포.                                                                        KL divergence를 loss처럼 이용하여 Variational distribution과 Posterior distribution의 차이를 줄인다.            [img. VI의 그림화]        하지만 우리는 posterior distribution에 근접한 variational distribution을 구하기 이전에, posterior distribution 자체를 모른다.        이를 구하기 위해 ELBO(Evidence lower bound)를 최대로 키우면 반대로 objective 구간은 줄어들게 된다.          objective 구간은 KL divegence를 포함하므로 작아질수록 loss가 작아지는 효과와 비슷하다.      [img.이 방법을 Sandwitch method라고도 부른다.]  Posterio distribution은 알 수 없지만, ELBO는 계산할 수 있다.[img. ELBO가 가지고 있는 두개의 텀]  ELBO는 두개의 텀을 가지고 있는데, 각각 Reconstruction Term과 Prior Fitting Term로 이루어져 있다.          Reconstruction Term : encoder와 latent space를 거쳐 decoder로 돌아오는 reconstruction loss를 줄이는 부분      Prior Fitting Term : latent space의 점들의 분포가 Prior distribution(사전 분포)와 비슷하게 만들어 줌        위의 두 텀 때문에 Variational Auto-encoder는 generative model이 된다.          입력 -&gt; latent space -&gt; 분포 찾아서 샘플링-&gt; decoder -&gt; output image 생성      그냥 Auto-encoder에는 존재하지 않으므로 generative model이 아니다.        Variational Auto-encoder는 다음과 같은 단점을 가지고 있다.          likelihood를 측정하기 힘듬(intractable model)      prior fitting term의 KL divergence을 loss 처럼 사용하려면 SGD, Adam 등으로 최적화가 되어야하므로 미분 가능해야 함.      따라서 보통 isotropic Gaussian을 loss funtion에 넣어서 이용함                                                                              isotropic Gaussian: $D_{KL}(q_\\phi(z                  x)                                     \\mathcal N(0,I))=\\frac{1}{2}\\sum^D_{i=1}(\\sigma^2{z_i}+\\mu^2{z_i}-ln(\\sigma^2_{z_i})-1)$                                                              모든 output dimension이 independent한 gaussian distribution을 의미함                    Adversarial Auto-encoder(AAE)[img. AAE 구조]  KL divergence라는 약점이있는 prior fitting term 대신에 GAN을 활용하여 latent distribution 사이의 분포를 맞춰줌  샘플링 가능한 distribution 이라면 latent prior distribution으로 활용 가능하다.  성능 또한 비교적 좋은 경우가 많다Generative Adversarial Network(GAN)GAN 소개  GAN은 대략 2가지 단계로 이루어져 있는데, 샘플을 생성하는 모델(Generator)과, 샘플을 구별하는 모델(discriminator)로 되어있다.  새로운 샘플을 생성해서 구별 모델에 전달 :arrow_right: 실제 정보와 비교하여 샘플을 구별하여 생성 모델에 전달하고 학습 :arrow_right: 구별한 결과를 학습하여 더 나은 샘플을 생성해서 전달 :arrow_right: 무한 반복  마치 두 모델이 서로 싸우는 형식의 모델이다.[img. VA vs GAN 비교]  VA의 경우, X의 이미지가 들어오면 인코더, latent vector(z), 디코더를 통과하는 학습을 거친 뒤, generation 단계에서는 p(z)(latent distribution)에서 샘플링한 z를 decoder에 통과시킨 뒤, 그 결과값이 생성된 샘플이다.      GAN의 경우, z(latent distribution)을 통해서 Generator에서 Fake 이미지를 만들고, Real 이미지와 Fake 이미지를 Discriminator가 구별,학습해서 그 결과를 Generator에게 보내 학습 시킨다.    이를 수학적으로 표현하면 이와 같다.   (implicity 모델이다.)          Discriminator 입장\\(\\stackrel {max}{D}\\ V(D,G)=\\mathbb E_{x\\sim p_{data}(x)}[logD(x)] + \t\\mathbb E_{z\\sim p_z(z)}[log(1-D(G(z)))]\\\\where\\ optimal\\ discriminator\\ is\\ D^*_G(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_G(x)}\\)      Generator 입장    \\[\\stackrel {min}{G}\\ V(D,G)=\\mathbb E_{x\\sim p_{data}(x)}[logD(x)] + \\mathbb E_{z\\sim p_z(z)}[log(1-D(G(z)))]\\]          optimal discriminator(=최적의 discriminator  일시 값) 적용시    \\[V(G,D^*_G(x)) = E_{x\\sim p_{data}}\\left[log\\frac{p_{data}(x)}{p_{data}(x)+p_G(x)}\\right]+E_{x\\sim p_G}\\left[log\\frac{p_G(x)}{p_{data}(x)+p_G(x)}\\right] \\\\= E_{x\\sim p_{data}}\\left[log\\frac{p_{data}(x)}{\\frac{p_{data}(x)+p_G(x)}{2}}\\right]+E_{x\\sim p_G}\\left[log\\frac{p_G(x)}{\\frac{p_{data}(x)+p_G(x)}{2}}\\right] - log4 \\\\ = D_{KL}\\left[p_{data},\\frac{p_{data}+p_G}{2}\\right]+D_{KL}\\left[P_G,\\frac {p_{data}+p_G}{2}\\right]-log4\\]          여기서,    \\[D_{KL}\\left[p_{data},\\frac{p_{data}+p_G}{2}\\right]+D_{KL}\\left[P_G,\\frac {p_{data}+p_G}{2}\\right] = \\\\ 2 \\times Jenson-Shannon\\ Divergence\\ (JSD) = 2D_{JSD}[p_{data},p_{G}]\\]          이며, 최종적으로    \\[V(G,D^*_G(x)) = 2D_{JSD}[p_{data},p_{G}] - log4\\]          이는 이론상 최적의 discrimniator 일시, 최소화 해야할 generator 값이다.      여러 GAN 모델들[img. DCGAN 모델]  이미지 생성하는 GAN 모델[img. Info-GAN]  class를 추가로 인풋으로 넣어줌[img. 주어진 문장에 맞는 이미지를 만들어주는 Text2Image]  DALL-E와 비슷함[img. Puzzle-GAN]  원래 이미지를 복원하는 모델[img. CycleGAN]  이미지 내부의 도메인을 바꿔주는 모델[img. Cycle-consistency loss]  GAN 구조가 2개 들어있는 형식[img. Star-GAN]  이미지를 컨트롤할 수 있게 해줌[img. Progressive-GAN]  고해상도의 이미지 생성하는 모델"
  }
  , 
  
  "/articles/AI/Data_Vis/seaborn%EA%B3%BC%20matplotlib.html": {
    title: "DataVis-seaborn과 matplotlib 배우기",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/Data_Vis/seaborn%EA%B3%BC%20matplotlib.html",
    tags: ["AI","TOOL","PYTHON","DATA_VIS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueDataVis-seaborn과 matplotlib 배우기  naver AI boost camp의 강의를 정리한 내용입니다.conda install seabornconda install matplotlibpip install jupyterthemes # matplotlib의 테마를 바꿔서 그래프가 잘보이게 하자jt -l # jupyter notebook 테마 리스트jt -t [원하는 테마][code -1. 모듈 설치]matplotlib[img 0. matplotlib 로고]  파이썬에서 널리 사용되는 데이터 시각화 모듈  pyplot 객체를 사용하여 데이터를 표시, pyplot 객체에 그래프들을 쌓은 다음 flush(plt.show()를 해줘야 출력)  단점으로 고정된 argument가 없어 문서화가 애매함.  기본 figure 객체에 그래프가 그려짐import matplotlib.pyplot as pltimport numpy as npX_1 = range(100)Y_1 = [np.cos(value) for value in X]X_2 = range(100)Y_2 = [np.sin(value) for value in X]plt.plot(X_1, Y_1)plt.plot(X_2, Y_2)plt.plot(range(100), range(100))plt.show()[code 0-1. 기본적인 사용 방법][img 0.  이러한 창이 팝업된다.]Figure &amp; Subplot  Matplotlib는 Figure 안에 여러개의 Axes를 생성하는 방식입니다.[img 1. 생성 방식에 대한 그림]fig = plt.figure()fig.set_size_inches(10, 10)  # 싸이즈 설정ax = []colors = [&amp;#34;b&amp;#34;, &amp;#34;g&amp;#34;, &amp;#34;r&amp;#34;, &amp;#34;c&amp;#34;, &amp;#34;m&amp;#34;, &amp;#34;y&amp;#34;, &amp;#34;k&amp;#34;]for i in range(1, 7):    ax.append(fig.add_subplot(2, 3, i))  # 두개의 plot 생성    X_1 = np.arange(50) # 이걸 랜덤으로 하면 자기멋대로 그려진다.(즉 작은거부터 큰거 순으로 그리자)    Y_1 = np.random.rand(50)    c = colors[np.random.randint(1, len(colors))]    ax[i - 1].plot(X_1, Y_1, c=c)[code 1. figure 위에 쌓아서 그리는 법][img 1. 결과]style# plt.style.use(&amp;#34;ggplot&amp;#34;)    # 스타일적용 사이트 참조plt.plot(X_1, Y_1, color=&amp;#34;b&amp;#34;, linestyle=&amp;#34;dashed&amp;#34;, label=&amp;#34;line_1&amp;#34;) # 선의 색깔, 선의 종류, 선의 라벨 설정, 색깔은 rgb color, predefined color 등 사용 가능plt.plot(X_2, Y_2, color=&amp;#34;r&amp;#34;, linestyle=&amp;#34;dotted&amp;#34;, label=&amp;#34;line_2&amp;#34;)plt.text(50, 70, &amp;#34;Line_1&amp;#34;)# 해당 x,y 좌표에 글자 생성plt.annotate( # 해당 위치에 arrow 생성    &amp;#34;line_2&amp;#34;,    xy=(50, 150),    xytext=(20, 175),    arrowprops=dict(facecolor=&amp;#34;black&amp;#34;, shrink=0.05),)plt.grid(True, lw=0.4, ls=&amp;#34;--&amp;#34;, c=&amp;#34;.90&amp;#34;) # grid 생성plt.legend(shadow=True, fancybox=False, loc=&amp;#34;upper right&amp;#34;) # 우측 위의 범례표 생성# shadow, fancybox= 그림자, 예쁜 표 설정plt.xlim(-10, 200) # 보여줄 좌표 범위 설정plt.ylim(-20, 200)plt.title(&amp;#34;$y = ax+b$&amp;#34;) # 상단의 그래프 이름 설정, $$ 사이에 수학 latex 수식 사용 가능plt.xlabel(&amp;#34;$x_line$&amp;#34;) # x좌표측 라벨 이름 설정plt.ylabel(&amp;#34;y_line&amp;#34;)plt.savefig(&amp;#34;test.png&amp;#34;, c=&amp;#34;a&amp;#34;)# 표 저장plt.show()[code 2. styling을 위한 변수들][img 2. 결과]scatter(선점도)data_1 = np.random.rand(512, 2)data_2 = np.random.rand(512, 2)plt.scatter(data_1[:, 0], data_1[:, 1], c=&amp;#34;b&amp;#34;, marker=&amp;#34;x&amp;#34;) # c로 색깔, marker로 모양 지정# matplotlib의 공식 marker 또한 사용 가능plt.scatter(data_2[:, 0], data_2[:, 1], c=&amp;#34;r&amp;#34;, marker=&amp;#34;o&amp;#34;)plt.show()[code 3. 기본값 scatter 함수 사용][img 3. 함수 결과]x = np.random.rand(N)y = np.random.rand(N)colors = np.random.rand(N)area = np.pi * (15 * np.random.rand(N)) ** 2plt.scatter(x, y, s=area, c=colors, alpha=0.5) # area로 크기를 지정하고 alpha로 투명도 조정 plt.show()[code 3-1.  area를 적용한 scatter 함수 사용][img 3-1. 함수 결과]기타 grpahdata = [[5.0, 25.0, 50.0, 20.0], [4.0, 23.0, 51.0, 17], [6.0, 22.0, 52.0, 19]]X = np.arange(0, 8, 2)plt.bar(X + 0.00, data[0], color=&amp;#34;b&amp;#34;, width=0.50)plt.bar(X + 0.50, data[1], color=&amp;#34;g&amp;#34;, width=0.50)plt.bar(X + 1.0, data[2], color=&amp;#34;r&amp;#34;, width=0.50)# plt.barh 함수를 이용하면 가로 세로가 바뀐 그래프가 나온다.plt.xticks(X + 0.50, (&amp;#34;A&amp;#34;, &amp;#34;B&amp;#34;, &amp;#34;C&amp;#34;, &amp;#34;D&amp;#34;)) # 자리를 벌려줌, yticks도 존재하지만 y는 bottom arg를 이용하기도함plt.show()[code 4. bar 함수 사용][img 4. bar 함수 결과]X = np.random.randn(1000)plt.hist(X, bins=100)plt.show()[code 4-1. histogram 함수 사용][img 4-1. histogram 함수 사용 결과 ]data = np.random.randn(100, 5)plt.boxplot(data)plt.show()[code 4-2. boxplot 함수 사용][img 4-2. histogram 함수 사용 결과 ]  박스 범위는 IQR(Interquartile Range), 박스 사이는 25~75% 사이이며, 그 위에 박힌 점은 outlier, 즉 이상치다.  중간의 노란선은 50% 지점이다.seabornintroduction  통계적 데이터의 visualization, matplotlib의 wrapper이며, 더욱 쉽게 사용하게 해준다.  튜토리얼이 잘되있으니 추천conda install seaborn[code 5. seaborn 인스톨]import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snssns.set(style=&amp;#34;darkgrid&amp;#34;)# 테마 제공1, 홈페이지 참조tips = sns.load_dataset(&amp;#34;tips&amp;#34;)# 기본 제공 datasetfmri = sns.load_dataset(&amp;#34;fmri&amp;#34;)# 기본 제공 dataset2[code 5-1. seabron 및 기본 datset import]기본 사용법sns.lineplot(x=&amp;#34;timepoint&amp;#34;, y=&amp;#34;signal&amp;#34;, hue=&amp;#34;event&amp;#34;, data=fmri) # 선으로 되어있는 그래프 생성# 중앙선은 평균, 넓이는 분포, 기본적으로 정렬이 되서 연속적으로 표현됨.# hue를 통해 해당 category 별로변하는 데이터를 보여줌fmri.sample(n=10, random_state=1) # seaborn의 샘플 추출법, random으로 추출 가능# 물론, 기본적인 fmri.head() 또한 가능하다.[code 6. line plot 사용과 샘플 추출][img 6. lineplot example][img 6-1. sample example]sns.scatterplot(x=&amp;#34;total_bill&amp;#34;, y=&amp;#34;tip&amp;#34;, hue=&amp;#34;time&amp;#34;, data=tips)sns.regplot(x=&amp;#34;total_bill&amp;#34;, y=&amp;#34;tip&amp;#34;, data=tips) # 중간에 선형함수를 줄쳐주는 scatter plot[code 6-1. scatter plot 코드][img 6-2. scatter plot 와 regplot 결과]  이외에도 barplot, distplot 등의 plot이 있다.기타 추가 적인 plots  vilolinplot : boxplot에 distribution을 함께 표현  stripplot : scatter와 category 정보를 함께 표현  swarmplot : 분포와 함께 scatter를 함께 표현          catplot의 kind에 ‘swarm’값을 넣은것과 같음        pointplot : category 별로 numeric의 평균, 신뢰구간 표시  regplot : scatter + 선형함수를 함께 표시  FacetGrid : 4개 정도의 grid를 만든 뒤, map함수를 통해각 그리드에 들어가야할 plot을 설정해줌multiple plots  한개 이상의 도표를 하나의 plot에 작성  Axes를 사용해서 grid를 나누는 방법[img 8. multiple plot 사용 예제]  replot : Numeric 데이터 중심의 부포/ 선형 표시  catplot : category 데이터 중심의 표시  FacetGrid : 특정조건에 따른 다양한 plot을 grid로 표시  pairplot : 데이터 간의 상관관계 표시  implot : regression 모델과 category 데이터를 함께 표시"
  }
  , 
  
  "/articles/AI/Data_Vis/%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%8B%9C%EA%B0%81%ED%99%94%20%EA%B8%B0%EB%B3%B8.html": {
    title: "데이터 시각화 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/Data_Vis/%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%8B%9C%EA%B0%81%ED%99%94%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","DATA_VIS","DATA","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true데이터 시각화 기본데이터를 그래픽 요소로 매핑하여 시각적으로 표현하는 것시각화의 다양한 고려 요소  목적 : 왜 시각화 하는가?  독자 : 시각화 결과는 누구를 위한 것인가?  데이터 : 어떤 데이터를 시각화할 것인가?  스토리 : 어떤 흐름으로 인사이트를 전달할 것인가?  방법 : 전달하고자 하는 내용에 맞게 효과적인 방법을 사용하고 있는가?  디자인 : UI에서 만족스러운 디자인을 가지고 있는가?시각화의 요소시각화를 위해서 데이터의 관점을 먼저 생각해봐야한다.크게 1. 데이터셋 관점 (global)과  개별 데이터의 관점 (local) 관점이 존재한다.데이터셋, 데이터의 종류정형 데이터, 시계열 데이터, 지리 데이터, 관계형(네트워크) 데이터, 계층적 데이터, 다양한 비정형 데이터 등이 존재한다.이때, 정형 데이터는 테이블 형태로 제공되는 데이터로 csv, tsv 파일 등으로 제공됨.Row가 데이터 1개 item, Column은 attribute(feature)를 의미하며, 시각화가 비교적 쉽다.시간 흐름에 따른 데이터를 Time-Series 데이터기온, 주가 등의 정형 데이터와 음성, 비디오 같은 비정형 데이터가 존재추세(Trend), 계절성(Seasonality), 주기성(Cycle) 등을 살핌지리/ 지도 데이터의 경우, 지도 정보와 보고자 하는 정보간의 조화 중요 + 지도 정보를 단순화 시킨 경우가 중요하며, 거리 경로, 분포 등을 Visualization관계 데이터(Graph, Network Visualization)의 경우 객체와 객체 간의 관계를 시각화,객체를 Node, 관계를 Link라고 하며, 크기, 색, 수 등으로 객체와 관계의 가중치 표현계층적 데이터의 경우, 관계 중에서도 포함관계가 분명한 데이터, Tree, Treemap, Sunburst 등이 대표적데이터의 종류는 4가지로 분류 가능수치형(numerical): 수로 표현 가능한 데이터  연속형(continuous) : 실수값으로 표현 가능한 연속적인 데이터          길이, 무게, 온도 등        이산형(discrete) : 정수값으로 표현 가능한 데이터          주사위 눈금, 사람 수 등      범주형(categorical): class로 표현 가능한 데이터  명목형(norminal) : 순서나 대소 관계, 우열관계가 존재하지 않음          혈액형, 종교 등        순서형(ordinal) : 순서, 대소관계, 우열 관계가 존재          학년, 별점, 등급 등      시각화의 이해Mark는 기본적인 시각적 요소로, 점, 선, 면으로 이루어진 시각화 방법이다.질의 데이터, 시계열 데이터 등이 표현 가능위의 기본적인 점선면 Mark에 다양성을 줄 수 있는 요소이다, 크기, shape, color 등을 바꾸어 다차원적인 요소를 표현 가능하다.전주의적 속성(Pre-attentive Attribute)는 시작적인 주의를 주지 않아도 인지하게 되는 요소를 의미한다.동시에 사용하면 인지하기 어려우므로 적절하게 사용할 때, 시각적 분리(visual pop-out)이 필요하다.기본적인 차트의 사용기본 Bar PlotBar plot이란 직사각형 막대를 사용하여 데이터의 값을 표현하는 차트/그래프를 의미하며, 범주(category)에 따른 수치 값을 개별 또는 그룹 별로 비교하기에 적합하다.막대의 방향에 따라 .bar()  / .barh()으로 수직과 수평형 그래프를 만들 수 있다.  수평은 범주가 많을 때 적합하다.여러 Bar plot범주형 A, B, C, D, E column을 가진 두 그룹 파랑 = [1,2,3,4,3], 분홍 = [4,3,2,5,1]이 있을 때,  플롯을 여러 개 그리는 방법  한 개의 플롯에 동시에 나타내는 방법1)  Stacked Bar Plot : 위에 쌓여있는 bar의 분포는 파악하기 쉬움, 2개의 그룹이면 y axis = 0를 중심으로 +, - 로 축조절로 극복 가능  .bar()에서는 bottom 파라미터로 사용, .barh()에서는 left 파라미터로 사용1-1) Percentage Stacked Bar Chart : Stacked Bar Plot의 응용 형태2) Overlapped Bar Plot : 3개 미만의 급룸 비교는 겹쳐서 표현 가능, 투명도(alpha)를 조정해 겹치는 부분 파악, Area plot에 효과적3) Grouped Bar Plot: 구현이 까다로움, 그룹이 5~7개 이하일때 효과적 그 이상은 etc로 처리할 것  (.set_xticks(), .set_xticklabels())로 구현정확한 Bar plotPrinciple of Proportion Ink : x축의 시작은 zero(0)부터이며, 실제 값과 그래픽으로 표현 되는 잉크양은 비례해야 한다.데이터의 정렬을 통해 데이터의 패턴을 발견할 수 있다.시게열(시간순), 수치형(크기순), 순서형(범주의 순서), 명목형(범주의 값)에 따라 정렬이 가능하며, 대시보드에서는 Interactive 하게 제공하는 것이 유용하다.pandas에서는 sort_values(), sort_index()를 사용하여 정렬 가능여백과 공간을 조정해서 가독성을 높일 수 있다      X/Y axis Limit (.xset_xlim(), .set_ylim())    Spines(.spines[spine].set_visible())  Gap(width)  Legend(.legend())  Margins.(.margins())등으로 조절 가능하다.무의미한 3D, 직사각형 의외의 bar 형태 지양      Grid(.grid())    Ticklabels(.set_ticklabels())          Major &amp; Minor        Text 추가 장소 (.text(), .annotate())          Bar의 middle/ upper      제목 (.set_titile())      라벨 (.set_xlabel(), .set_ylabel())      오차 막대 (error bar)로 Uncertainty 정보 추가 가능Bar 사이의 Gap이 없으면 히스토그램(Histogram)이 되며, 연속된 느낌이며, .hist()를 통해 사용 가능Line Plot연속적으로 변화하는 값을 순서대로 점으로 나타내고, 이를 선으로 연결한 그래프Line Plot의 기본Line plot은 꺾은선 그래프, 선 그래프, line chart, line graph 등의 이름으로도 불리며, 시간/순서에 대한 변화에 적합하여 추세(시계열 분석)를 살피기 위해 사용함.(.plot()으로 사용)가독성을 위해 5개 이하의 선을 여러 요소로 구별하며 사용하는 것이 좋다.  색상(color)  마커(marker, markersize)  선의 종류(linestyle, linewidhth)또한, Noise로 인하여 패턴 및 추세 파악이 힘들 경우 smoothing을 통해 가독성을 늘릴 수 있다.정확한 Line plot  추세에 집중하기      추세가 목적이므로 굳이 0을 시작점으로 두지 않아도 된다.        Grid, Annotate 등을 제거한 뒤, 디테일한 정보는 표로 제공하자.        생략되지 않는 성에서 범위를 조정해 변화율 관찰 (.set_ylim())    간격 조정  규칙적인 간격이 아닐 경우 점을 추가하여 데이터가 있는 부분만 표시하자.  보간  Line은 점을 이어 만드는 요소이므로 데이터가 없어도 이를 이어서 만드는 보간을 하게 된다.  데이터의 이해를 도울 수 있지만, 없는 데이터를 있다고 생각하거나 적은 차이를 못보게 할 수 있으므로 일반적인 분석에서는 지향 하자.          특히 곡선 보간은 조심하자        데이터의 error나 noise가 포함되어 있는 경우, Moving Average 방법, Smooth Curve with Scipy(scipy의 interpolate 내부의 make_interp_spline(), interp1d() 또는 scipy의 ndimage.gaussian_filter1d() 등을 사용 가능)  이중 축 사용  한 plot에 대해 2개의 축을 사용하는 것을 이중 축(dual axis)라고 함, 왠만하면 이중 축보단 다중 plot으로 해결하는 게 좋다.          상관관계가 있어보이는 착시를 일으킨다.        같은 시간 축에 대해 서로 다른 종류의 데이터를 표현하기 위해 축이 2개 필요 (.twinx() 사용)  한 데이터에 대해 단위가 다른 경우, .secondary_xaxis(), .secondary_yaxis()를 사용해 보자.  기타  범례 대신 라인 주위에 레이블을 추가하면 가독성이 좋다.  Min/Max 정보(또는 특정 포인트)를 추가(annotation)로 가독성 증가  보다 연한 색을 사용하여 uncertainty 표현 가능 (신뢰구간, 분산 등)Scatter Plot기본 Scatter plot점을 사용하여 두 feature 간의 관계를 알기 위해 사용하는 그래프, 산점도라고도 말함직교 좌표계에서 x축/y축에 feature r값을 매핑해서 사용, .scatter()로 사용색, 모양, 크기 등을 바꾸어 다양한 차원의 데이터 표현 가능Scatter plot을 통해 상관관계(양, 음의 상관 관계, 상관 없음), 군집, 값 사이의 차이, 이상치 등을 알 수 있다.정확한 Scatter plot  Overplotting 방지  점이 너무 많으면 분포 파악이 힘드므로, 투명도 조정, 지터링(jittering, 점의 위치 약간 변경, 효율성이 조금 떨어짐), 2차원 히스토그램(히트맵을 사용하여 깔끔한 시각화), Contour plot(분포를 등고선을 사용해 표현) 등으로 표현하면 좋다.          이외에도 joint plot 등이 있음        점의 요소와 인지  색 : 연속은 gradient, 이산은 개별 색상으로 표시  마커 : 거의 구별하기 힘들고, 크기가 달라 보이므로 사용 유의  크기 : 흔히 버블 차트(bubble chart)라고 부름, 오용이 잦으며, 관계 보다는 각 점간의 비율에 초점을 두자, SWOT 분석등에 자주 사용  인과 관계와 상관 관계  인과 관계(causal relation, x축 때문에 y축이 변한다)과 상관 관계(correlation, x축과 y축이 동시에 어떠한 요인에 의해 변하는 관계이다.)을 잘 구별하자  인과 관계는 사전 정보와 함께 가정으로 제시  추세선  추세선을 이용하면 scatter의 패턴을 유추할 수 있지만, 2개 이상 사용하면 가독성이 떨어진다.  기타  Grid는 왠만하면 사용하지 않으며 무채색을 활용하자  범주형이 포함되어 있으면 heatmap 또는 bubble chart 추천차트의 요소Text 사용하기Matplotlib에서 TextText는 Data Visualization에서 명확하고 오해를 방지할 수 있는 수단이지만 과용하면 가독성을 떨어뜨리고 오히려 이해를 방해할 수 도 있다.Title : 가장 큰 주제를 설명Label : 축에 해당하는 데이터 정보를 제공Tick Label : 축에 눈금을 사용하여 스케일 정보를 추가Legend : 한 그래프에서 2개 이상의 서로 다른 데이터를 분류하기 위해서 사용하는 보조 정보Annotation(Text) : 그 외의 시각화에 대한 설명을 추가Text 원하는 대로 사용하기[Data_Visualization_3]3_1_Text.ipynbColor 사용하기Color에 대한 이해Visualization에 있어서 위치와 색은 가장 효과적인 채널 구분 방법특히, 위치와 달리 사람이 직접 개입하여 골라야 하며, 심미성을 가진다.  하지만 심미성보다는 정보 전달에 좀 더 신경쓰자.일부 색의 사용 장소는 사회적인 약속, 색이 주는 공통적인  느낌 그리고 자연에 대한 모방 등에 의해 이미 결정되어 있다.          무지개 빛을 응용한 수치 표현  파랑과 빨강을 이용한 온도 표현 등Color Palette의 종류범주형(Categorical, Discrete, Qualitative) Color Palette범주형 변수에는 독립되고 구별하기 쉬운 색상을 사용한다.  즉 채도, 명도로 구분하지 않고, 색상의 차이로 구분한다.최대 10개의 색상까지 사용하며, 그외에는 기타로 묶는다.연속형 (Sequential) Color Palette정렬된 값을 가지는 순서형, 연속형 변수에 적합하며, 연속적인 색상 (주로 명도를 조절)을 통해 값을 표현한다.단일 색조로 표현하는 것이 좋고, 균일한 색상 변화가 중요하다.발산형(Diverge) Color Palette연속형과 유사하지만 중앙을 기준으로 발산하며, 상반된 값, binary 값을 표현하는데 적합양 끝으로 갈수록 색이 진해지며, 중앙의 색은 양쪽의 점에서 편향되지 않아야 함그 외 색 Tips색상 대비(Color Constrast)  가까운 색은 차이가 더 크게 보임(파랑보라, 빨강보라)명도 대비  밝은 색과 어두운 색을 배치하면 밝은 색은 더 밝게, 어두운 색은 더 어둡게 보임 (회색검정)채도 대비  채도의 차이, 채도가 더 높아보임 (회색주황)  더 선명해보여서 주로 사용함보색 대비  정반대 색상을 사용하면 더 선명해 보인다.(빨강초록)를 통하여 강조(Highlighting)할 수 있다.삼원색 (빨파노) 중 특정 색을 감지 못하면 색맹이라 하며, 부분적 인지 이상이 있다면 색약이라 한다.색인지가 중요한 분야에서는 필수적으로 고려하자.FacetFacet은 분할이라는 의미로, 화면 상에 View를 분할 및 추가하여 다양한 관점을 전달함.  같은 데이터셋에 서로 다른 인코딩을 통해 다른 인사이트  같은 방법으로 동시에 여러 feature 보거나 큰 틀에서 볼 수 없는 부분 집합을 세세하게 보여줌.Matplotlib에서 구현Figure는 큰 틀, Ax는 각 플롯이 들어가는 공간, Figure는 언제나 1개, 플롯은 N개주로  plt.subplot()  plt.figure() + fig.add_subplot()  plt.subplots()으로 subplot을 더하며,figuresize, dpi, sharex, sharey, squeeze, aspect 등의 argument를 조정할 수 있다.가장 가본적인 형태로 subplot을 추가하면 위와 같다.위 subplot을 표현하기 위해 Slicing 또는 x, y, dx, dy를 사용하며,Slicing 방법의 경우 fig.add_spec() 함수를 통해 자세하게 구현할 수 있다.위의 예시의 경우, 처럼 slicing한 모양이라 할 수 있다.같은 모양일 경우, (x,y), dx, dy로 작성하려면, fig.subplot2grid() 함수를 통해 구현할 수 있으며, 의 비율을 통해 구현할 수 있다.ax.inset_axes()(좌)를 통하여 Ax 내부에 서브플롯을 추가하여 외부 정보나 미니맵 같은 형태를 추가할 수 있으며, make_axes_locatable(ax)(우) 함수를 통하여 그리드 외부에 사이드 subplot을 추가하여 제목, 통계 정보등을 제공할 수 있다.More TipsGrid 이해하기기본적인 Grid는 축과 평행한 선을 사용하여 거리 및 값 정보를 보조적으로 제공색은 다른 표현들을 방해하지 않도록 무채색(color)이며,항상 Layer 순서 상 맨 밑에 오도록 조정(zorder),큰 격자/ 세부 격자(which=’major’, ‘minor’, ‘both’)X축, Y축 또는 동시에(axis=’x’, ‘y’, ‘both’)또한 numpy + matplotlib를 통하여 x+y=c, y=cx, xy=c, 2차 함수등 여러가지 형태로 구현이 가능하다.심플한 처리Setting 바꾸기여러가지 미리 정의된 테마로 바꿀 수 있다.ggplot, fivethirtyeight 이 대표적으로 많이 사용됨실습"
  }
  , 
  
  "/articles/AI/Graph/%EA%B7%B8%EB%9E%98%ED%94%84%20%EA%B8%B0%EB%B3%B8.html": {
    title: "그래프 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/Graph/%EA%B7%B8%EB%9E%98%ED%94%84%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","GRAPH","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true그래프(Graph) 기본  네이버 boostcamp AI Tech의 강의를 정리한 내용입니다정점 집합과 간선 집합으로 이루어진 수학적 구조로, 다양한 복잡계(Complex Network)를 분석할 수 있는 언어Graph IntroductionGraph란 무엇이고 왜 중요할까?[img. 정점과 간선, 그래프의 그림]  정점(Vertex) : 노드(Node)라고도 불림. 관계를 가지거나 가지지 않는 하나의 객체  간선(Edge): 링크(Link)라고도 불림. 두 개의 정점을 연결하는 선  그래프(Graph): 네트워크(Network)라고도 불림. 정점 집합과 간선 집합으로 이루어진 수학적 구조우리의 주변에는 친구관계, 통신, 인터넷과 같은 많은 구성 요소 간의 복잡한 상호작용인 복잡계(Complex System)이 있으며, 그래프는 이 복잡계를 표현하고 분석하기 위한 언어이다.그래프 관련 인공지능 문제  정점 분류(Node Classification) 문제 : 정점의 특성, 연결에 따라 분류할 수 있는가?          트위터의 공유 관계를 분석하여 사용자의 정치적 성향 판단        연결 예측(Link Prediction) 문제: 앞으로 그래프의 성장이 어떻게 될 것인가?          페이스북 소셜 네트워크는 어떻게 진화할까?        추천(Recommendation) 문제: 한 정점이 어떠한 경향으로 간선을 만드는가?          상품, 영화 추천        군집 분석(Community Detection) 문제: 연결 관계로부터 사회적 무리(Social Circle)을 찾아낼 수 있을까?          연결 관계를 통한 가족 관계, 친구 관계, 대학 동기 들의 무리를 찾아내기        랭킹(Ranking) 및 정보 검색(Information Retrieval) 문제: 그래프 내부에서 찾고 싶은 정점 찾기          웹이라는 거대한 그래프로부터 찾고자 하는 웹페이지를 찾아내는 법        정보 전파(Information Cascading) 및 바이럴 마케팅(Viral Marketing) 문제: 어떻게 최대한 간선을 통하여 전파 시킬 수 있는가?          마케팅, 홍보 문제      그래프 관련 필수 기초 개념그래프의 유형 및 분류            명칭      무방향 그래프(Undirected Graph)      방향 그래프(Directed Graph)                  설명      간선엔 방향이 없는 그래프      간선엔 방향이 있는 그래프              예시      협업 관계 그래프, 페이스분 친구 그래프 등      인용 그래프, 트위터 팔로우 그래프              그림                  [fig. 방향성에 따른 그래프 분류]            명칭      가중치가 없는 그래프(Unweighted Graph)      가중치 그래프(Weighted Graph)                  설명      간선에 가중치가 없는 그래프      간선에 가중치가 있는 그래프              예시      웹 그래프, 페이스북 친구 그래프      전화 그래프, 유사도 그래프              그림                  [fig. 가중치 여부에 따른 그래프 분류]            명칭      동종 그래프(Unpartite Grpah)      이종 그래프(Bipartite Graph)                  설명      동종 그래프는 단일 종류의 정점만 포함      두 종류의 정점을 가지는 그래프, 다른 종류의 정점 사이에만 간선이 연결됨.              예시      웹 그래프, 페이스북 친구 그래프      전자 상거래 구매 내역(사용자, 상품), 영화 출연 그래프(배우, 영화)              그림                  [fig. 정점의 종류에 따른 그래프 분류][fig. 방향성이 없고 가중치가 있는 이종 그래프의 예시]그래프 관련 필수 기초 개념[img. 그래프]보통 정점들의 집합을 V, 간선들의 집합을 E, 그래프를 G = (V,E)로 표현[img. 정점 1의 이웃 집합]정점의 이웃(Nieghbor)은 그 정점과 연결된 다른 정점을 의미하며 v의 이웃 집합을 보통 N(v) 혹은 $N_v$로 표기, 위 그림의 경우 N(1) = {2,5}[img. 방향 그래프에서의 이웃]방향성이 있는 그래프에서는 나가는 이웃과 들어오는 이웃을 구분하며,  정점 v에서 간선이 나가는 이웃(Out-Neighbor)의 집합을 보통 $N_{out}(v)$로 표기  정점 v에서 간선이 들어오는 이웃(In-Neighbor)의 집합을 보통 $N_{in}(v)$로 표기  그림의 경우, $N_{out}(5)={1,2},\\ N_{in}(4)={3}$그래프의 표현 및 저장  NetworkX 그래프 Vis 코드 + 간선 저장 코드의 경우 LAB의 [Graph-1]Grpah_Basic.ipynb 참조  간선 리스트(Edge List) : 그래프를 간선들의 리스트로 저장          정점 간의 간선 유무를 알아보기 위해 모든 리스트를 다 확인해봐야함, 성능상 비효율적임.                  무방향성      방향성                                      간선이 연결하는 두 정점들의 순서쌍(Pair)로 저장      방향성이 있는 경우에는 (출발점, 도착점) 순서로 저장      [fig. 간선리스트의 예시]  인접 리스트(Adjacent list) : 정점들의 이웃들을 리스트로 저장          간선 리스트에 비해 성능상 효율적임                  그래프      도식      설명                  무 방향성            각 정점의 이웃들을 리스트로 저장              방향성            각 정점의 나가는 이웃들과 들어오는 이웃들을 각각 리스트로 저장      [fig. 인접리스트의 예시]  인접 행렬(Adjacency Matrix)          연산 성능 상 문제 없고 구현이 쉽지만 메모리 낭비가 심하다.                  무방향성      방향성                                      정점 수 $\\times$ 정점 수 크기의 행렬      정점 수 $\\times$ 정점 수 크기의 행렬              정점 i 와 j 사이에 간선이 있는 경우, 행렬 (i, j) 그리고 (j, i) 원소가 1 없는 경우 0      정점 i 에서  j 로 가는 간선이 있는 경우, 행렬 (i, j)가 1 없는 경우 0, 반대로, 정점 j에서 i로 가는 간선이 있는 경우, 행렬 (j,i)가 1 없는 경우 0      [fig. 인접 행렬의 예시]그래프를 이용한 기계학습실제 그래프 vs 랜덤 그래프실제 그래프(Real Graph)란 다양한 복잡계로 부터 얻어진 그래프를 의미[img. 실제 그래프의 예]랜덤 그래프(Random Graph)는 확률적 과정을 통해 생성한 그래프를 의미  에되스-레니 랜덤 그래프(Erdos-Renyi Random Graph) : $G(n,p)$          n 개의 정점을 가짐      임의의 두 개의 정점 사이에 간선이 존재할 확률은 p, 모든 정점 사이가 동일함      정점 간의 연결은 서로 독립적(Independent)      [img. $G(3,0.3)$일때 생성될 수 있는 그래프와 확률]작은 세상 효과            개념      설명                  경로(Path)      첫번째로 u에서 시작해서 정점 v에서 끝나야 하며, 두번째로, 순열에서 연속된 정점은 간선으로 연결되어 있어야 함.              경로의 길이      경로 상에 놓이는 간선의 수로 정의됨              정점 사이의 거리(Distance)      u와 v 사이의 최단 경로의 길이              최단 경로(Shortest Path)      u와 v사이의 거리가 최소가 되는 경로              그래프 지름(Diameter)      정점 간 거리의 최댓값      [fig. 그래프의 개념 설명]            그래프                        경로 예시      정점 1과 8사이의 경로는 [1,4,6,8], [1,3,4,6,8],[1,4,3,4,6,8]              경로 길이 예시      경로 [1,3,4,6,8]의 길이는 4              거리 예시      정점 1과 8 사이의 최단 경로는 [1,4,6,8]로 거리는 3              지름 예시      정점 2에서 8까지 4      [fig. 그래프의 개념 예시]작은 세상 효과(Small-world Effect) : 실제 그래프의 임의의 두 정점 사이의 거리는 생각보다 작다.  여섯 단계 분리(Six Degrees of Separation) 실험에서 6단계 만에 오마하에서 보스턴까지 편지를 전달  MSN 메신저 사용자의 평균 거리는 7  랜덤 그래프에도 존재하지만 체인(Chain), 사이클(Cycle), 격자(Grid) 그래프에서는 작은 세상 효과가 존재 하지 않음.[img. 작지 않은 세상들]연결성의 두터운-꼬리 분포정점의 연결성(Degree)은 그 정점과 연결된 간선의 수, 해당 정점의 이웃들의 수를 의미                              $d(v),\\ d_v,\\          N(v)          $로 표기함.                                연결성      그래프                  $d(1)=2\\d(2)=3\\d(3)=2\\d(4)=3\\d(5)=3\\d(6)=1$            [fig. 연결성(이웃)의 예시]  정점의 나가는 연결성(Out Degree)은 그 정점에서 나가는 간선의 수를 의미                                                      $d_{out}(v),              N_{out}(v)              $로 표시                                            정점의 들어오는 연결성(In Degree)은 그 정점에서 들어오는 간선의 수를 의미.                                                      $d_{in}(v),              N_{in}(v)              $로 표시                                                      연결성      그래프                  $d_{in}(1)=1,\\ d_{out}(1)=1\\d_{in}(2)=3,\\ d_{out}(1)=1\\d_{in}(3)=0,\\ d_{out}(3)=2\\d_{in}(4)=1,\\ d_{out}(4)=2\\d_{in}(5)=2,\\ d_{out}(5)=2\\d_{in}(6)=1,\\ d_{out}(6)=0\\$            [fig. 방향 그래프의 연결성(이웃)의 예시]실제 그래프의 연결성 분포는 두터운 꼬리(Heavy Tail)를 가짐 (ex)BTS와 강사님의 트위터 팔로워 수)  이는 연결성이 매우 높은 허브(Hub) 정점이 존재함을 의미.반면에 랜덤 그래프의 연결성 분포는 정규분포와 유사함(ex) 인간의 키 분포).  허브 정점이 존재할 확률이 아주 적음.[img. 랜덤 그래프와 실제 그래프의 차이]거대 연결 요소연결 요소(Connected Component)는 다음 조건들을 만족하는 정점들의 집합을 의미합니다.  연결 요소에 속하는 정점들은 경로로 연결될 수 있습니다.  1.의 조건을 만족하면서 정점을 추가할 수 없습니다.[img. {1,2,3,4,5}, {6,7,8}, {9} 총 3개의 연결 요소가 있다.]실제 그래프에는 거대 연결요소(Giant Connected Component)가 존재하며, 대다수의 정점을 포함함.랜덤 그래프에도 높은 확률로 거대 연결요소가 존재함.  단, Random Graph Theory에 의해 정점들의 평균 연결성이 1보다 충분히 커야함.            실제 그래프      랜덤 그래프                              [fig. 실제 그래프 vs 랜덤 그래프에서의 거대 연결 요소]군집 구조군집(Community)이란 다음과 같은 조건을 만족하는 정점들의 집합.  집합에 속하는 정점 사이에는 많은 간선이 존재함  집합에 속하는 정점과 그렇지 않은 정점 사이에는 적은 수의 간선이 존재[img. 군집 그래프]지역적 군집 계수(Local Clustering Coefficient)는 한 정점에서 군집의 형성 정도를 측정 가능  정점 $i$의 지역적 군집 계수($C_i$)는 정점 $i$의 이웃들의 쌍 중 간선으로 직접 연결된 것의 비율을 의미  연결성(이웃)이 0인 정점에서는 지역적 군집 계수 정의가 안됨.[img. 지역적 군집 계수의 예시]전역 군집 계수(Global Clustering Coefficient)는 전체 그래프에서 군집의 형성 정도를 측정  그래프 G의 전역 군집 계수는 각 정점에서의 지역적 군집 계수의 평균          단, 지역적 군집 계수가 정의되지 않는 정점은 제외      실제 그래프에서 군집 계수가 높은(=군집이 많은) 이유  동질성(Homophily) : 서로 유사한 정점끼리 간선으로 연결될 가능성이 높음          같은 인종간 관계 그래프        전이성(Transitivity) : 공통 이웃은 간선 연결이 생길 확률이 높음          친구에게 친구를 소개      반면 랜덤 그래프는 간선 연결 확률이 독립적(Independent)이므로 지역적, 전역 군집 계수가 높지 않음.군집 계수 및 지름 분석Lab의 [Graph-2]Graph_Property.ipynb 참조                   균일 그래프(Regular Graph)      작은 세상 그래프(Small-world Graph)      랜덤 그래프(Random Graph)                  군집 계수      큼      큼      작음              지름      큼      작음      큼      페이지 랭크 알고리즘페이지 랭크의 배경웹은 웹페이지와 하이퍼링크로 구성된 거대한 방향성 있는 그래프.  웹페이지 : 정점 + 키워드 정보  하이퍼링크: 간선[img. 웹은 그래프이다.]검색엔진의 발전  웹을 거대한 디렉토리로 정리          웹페이지의 수가 증가하며 카테고리의 수와 깊이가 무한정 커짐      카테고리 구분이 모호하고 저장과 검색이 어려움        웹페이지에 포함된 케워드에 의존한 검색엔진          사용자가 입력한 키워드에 대해 키워드를 여럿 포함한 웹페이지를 반환      악의적인 웹페이지에 취약하다는 단점(축구라는 단어가 여러번 포함된 성인 사이트)      사용자 키워드와 관련성이 높고 신뢰할 수 있는 웹페이지를 찾기 위해 PageRank 알고리즘을 활용함.  래리 페이지와 세르게이 브린이 발표페이지랭크의 정의하이퍼링크를 통한 웹페이지의 투표를 통해 사용자 키워드와 관련성이 높고 신뢰할 수 있는 웹페이지를 찾을 수 있음  웹페이지 u가 v를 하이퍼링크로 연결(투표)한다면, v는 관련성이 높고 신뢰하고 있다는 것을 의미.(논문과 비슷)  즉, 들어오는 간선이 많을 수록 신뢰할 수 있다.          단, 악의적인 인용 등이 있으며 이를 막기위해 가중 투표를 도입      가중 투표: 관련성이 높고 신뢰할 수 있는 웹사이트의 투표에 더 높은 점수를 주는것  그러한 웹사이트가 높은 관련성과 신뢰성을 얻으려면, 투표를 많이 받아야함. =&gt; 재귀 구조이러한 웹페이지의 관련성 및 신뢰도를 페이지랭크 점수라고 함.  웹페이지는 각각의 나가는 이웃에게 $\\frac{자신의\\ 페이지랭크\\ 점수}{나가는\\ 이웃의\\ 수}$만큼의 가중치로 투표를 함  웹페이지의 페이지랭크 점수는 받은 투표의 가중치 합으로 정의됨.\\[r_j=\\sum_{i\\in N_{in}(j)}\\frac{r_i}{d_{out}(i)}\\][math. 페이지랭크 점수의 정의][img. $r_j$가 웹페이지 j의 페이지랭크의 점수일 때, 예시]      위 그림에서 웹페이지 j의 투표시 가중치는 $r_j$/3        웹페이지 j의 페이지랭크 점수 $r_j=r_i/3+r_k/4$              정점 별 페이지랭크      웹페이지 그래프                  $r_y=r_y/2+r_a/2\\r_a=r_y/2+r_m\\r_m=r_a/2$            [fig. 웹페이지 별 페이지랭크 점수 연립 방정식]페이지 랭크는 임의 보행(Random Walk)의 관점에서도 정의 가능.      웹서퍼가 t번째 방문한 웹페이지가 웹페이지 i일 확률을 $p_i(t)$        웹서퍼가 무한히 방문해 t가 무한히 커지면, 다음이 성립  \\[확률분포\\ p(t) = p(t+1) = 정상분포\\ p\\ (Stationary\\ Distribution)\\\\p_j(t+1)=\\sum_{i\\in N_{in}(j)}\\frac{p_i(t)}{d_{out}(i)}\\rightarrow p_j=\\sum_{i\\in N_{in}(j)}\\frac{p_i}{d_{out}(i)}\\][math. 임의 보행 관점에서의 페이지랭크]\\[r_j=\\sum_{i\\in N_{in}(j)}\\frac{r_i}{d_{out}(i)}: 투표\\ 관점에서\\ 정의한\\ 페이지랭크\\ 점수\\ r\\\\p_j=\\sum_{i\\in N_{in}(j)}\\frac{p_i}{d_{out}(i)}:임의\\ 보행\\ 관점에서\\ 정의한\\ 정상\\ 분포\\ p\\][math. 투표 관점의 페이지 랭크 점수 == 임의 보행 관점에서의 정상 분포]페이지랭크의 계산반복곱(Power Iteration)을 이용해 페이지 랭크 점수 가능  각 웹페이지 i의 페이지랭크 점수 $r_i^{(0)}$를 동일하게 $\\frac{1}{웹페이지의\\ 수}$로 초기화  아래 식을 이용하여 각 웹페이지의 페이지랭크 점수를 갱신\\[r_j^{(t+1)}=\\sum_{i\\in N_{in}(j)}\\frac{r_i^{(t)}}{d_{out}(i)}\\][math. 페이지랭크 점수 갱신 식]  페이지랭크 점수가 수렴($r^t와\\ r^{t+1}이\\ 유사해질때\\ 까지$)하였으면 종료, 아닌 경우 2로 회귀graph LR\ty((y))--&amp;#38;#62;a\ty--&amp;#38;#62;y\ta((a))--&amp;#38;#62;y\ta--&amp;#38;#62;m\tm((m))--&amp;#38;#62;a| 반복  | 0    | 1    | 2    | 3     |      | 수렴 || :—: | —- | —- | —- | —– | —- | —- || $r_y$ | 1/3  | 1/3  | 5/12 | 9/24  |      | 6/15 || $r_a$ | 1/3  | 3/6  | 1/3  | 11/24 | …  | 6/15 || $r_m$ | 1/3  | 1/6  | 3/12 | 1/6   |      | 3/15 |[fig. 페이지랭크 점수 갱신의 예시]단, 언제나 수렴하지 않을 수도 있으며, 합리적인 점수로 수렴하는 것도 보장하지 않는다.  수렴하지 않는 예시graph LR\ta((a))--&amp;#38;#62;b\tsubgraph 스파이더트랩\tb((b))---&amp;#38;#62;c((c))---&amp;#38;#62;b    end스파이더 트랩(Spider Trap) : 들어오는 간선은 있지만 나가는 간선은 없는 정점 집합            반복      0      1      2      3             수렴                  $r_a$      1/3      0      0      0             X              $r_b$      1/3      2/3      1/3      2/3      …      X              $r_c$      1/3      1/3      2/3      1/3             X      [fig. 수렴하지 않는 스파이더 트랩의 예시]  반복 1, 2, 1, 2,가 무한히 반복  합리적인 점수로 수렴하지 않는 예시(0으로 수렴함)graph LR\ta((a))--&amp;#38;#62;b\tsubgraph 막다른 정점\tb((b))\tend들어오는 간선은 있지만 나가는 간선은 없는 막다른 정점(Dead End) 문제| 반복  | 0    | 1    | 2    | 3    |      | 수렴 || :—: | —- | —- | —- | —- | —- | —- || $r_a$ | 1/2  | 0    | 0    | 0    |      |  0  || $r_b$ | 1/2  | 1/2  | 0    | 0    | …  |  0  |[fig. 막다른 정점의 예시]  전부 0으로 수렴위와 같은 문제를 해결하기 위해 순간이동(Teleport) 도입수정된 웹서퍼의 순간이동 행동  현재 웹페이지에 하이퍼링크가 없으면, 전체 웹페이지 중, 균일 확률로 임의의 웹페이지로 순간이동  현재 웹페이지에 하이퍼링크가 있다면, 앞면이 나올 확률이 $\\alpha$인 동전을 던집니다.          $\\alpha : 감폭 비율 (Dampling\\ Factor)$, 보통 0.8 정도임.        앞면이라면, 하이퍼링크 중 하나를 균일한 확률로 선택해 클릭 (이전 웹서퍼와 동일)  뒷면이라면, 전체 웹페이지 중, 균일 확률로 임의의 웹페이지로  순간이동순간이동의 도입으로 인해 페이지 랭크 점수 계산은 다음과 같이 변한다.  각 막다른 정점에서 (자신을 포함) 모든 다른 정점으로 가는 간선을 추가  아래 수식을 사용하여 반복곱을 수행\\(r_j=\\left(\\sum_{i\\in N_{in}(j)}\\left( \\alpha\\frac{r_i}{d_{out}(i)}\\right)\\right)_{가}+\\left((1-\\alpha)\\frac{1}{|V|}\\right)_나\\\\|V|: 전체\\ 웹페이지의\\ 수\\\\식\\ 가:하이퍼링크를\\ 따라\\ 정점\\ j에\\ 도착할\\ 확률\\\\식\\ 나: 순간이동을\\ 통해\\ 정점\\ j에\\ 도착할\\ 확률\\)[math. 순간이동에 의한 새로운 페이지랭크 점수 계산식][img. 수정된 페이지랭크 점수 예시]  C는 1표만 받지만 B의 귀중한 한표라서 점수가 큼  1.6 보라색 들은 들어오는 간선이 없지만 Teleport에 의해서 점수 획득페이지 랭크 실습Lab의 [Graph-3]PageRank.ipynb 참조그래프를 이용한 바이럴 마케팅그래프를 통한 전파의 예시  SNS를 이용한 마케팅, 정보, 단체 행동 등이 전파됨.  컴퓨터 네트워크 장비의 고장, 파워 그리드의 정전 전파  사회에서의 질병 전파이러한 전파 과정을 체계적으로 이해하고 대처하기 위해서 수학적 모형화가 필요함.  의사 결정 기반 전파 모형  확률적 전파 모형의사 결정 기반 전파 모형주변 정점(사람)들의 의사결정을 고려하여 각자 의사결정을 내리는 경우에 사용하는 모형  각 국가별 주요 사용 SNS, 비디오 테이프 표준화 등이 있음간단한 형태의 의사결정 기반의 전파 모형으로, 선형 임계치 모형(Linear Threshold Model)이 있음            사용 기술      A      B      A,B                  도식                                증가하는 행복      +2a      +2b      0      [fig. u, v 두 사람이 A, B라는 호환되지 않는 기술을 사용시 얻는 행복]  위 두 사람은 행복이 증가하지 않는 0보다, +2a, +2b를 비교하여 더 이득이 되는 방향으로 바꿀 것이다.[fig. 임계치 q를 구하기 위한 모형]p 비율의 u의 이웃이 A를 , 1-p 비율의 u의 이웃이 B를 선택했을 때,  A를 골랐을 때의 u의 행복이 ap, B를 골랐을 때의 u의 행복이 b(1-p)라고 할 때  u는 ap &gt; b(1-p)일 때, A를, ap &lt; b(1-p)일 때, B를 고를 것이다.  이 식을 바꾸어 $p &gt; \\frac{b}{a+b}$일 때 A를 고르며 이를 임계치 q$(=\\frac{b}{a+b})$라고 한다.[img. u와 v가 처음으로 A 기술를 사용한 얼리어 답터(시드집합)일 때의 상황]만약 A기술과 B 기술의 행복 a, b가 같고, u와 v가 해당 기술을 처음 사용한 얼리어답터(시드집합, Seed Set)이라고 가정하면, 해당 모형의 경과는 다음과 같다.            1      2      3      4                                          [fig. A기술의 선형 임계치 모형 전파, 4번 이후로 더이상 전파는 없다.]  4번 이후로 2개의 파란 정점은 A기술을 사용하는 간선이 u 하나 뿐이며, 이는 33%으로 50%를 넘기지 못하므로 전파되지 않는다.확률적 전파 모형질병, 정전 등의 모형에서는 위의 의사결정 기반 모형이 어울리지 않다.  누구도 질병에 걸리도록 의사결정을 내리는 사람은 없기 때문이다.질병에 걸리는 것은 확률적 과정이므로 확률적 전파 모형을 고려해야 한다.그 예시로 독립 전파 모형(Independent Cascade Model)이 있다.[img. 독립 전파 모형은 보통 방향성이 있고 가중치가 있는 그래프를 가정한다.]간선 (u, v)의 가중치 $p_{uv}$는 u가 감염자, 이웃 v가 감염자가 아닐 때, u가 이웃 v를 감염시킬 확률을 의미한다.이때 서로 다른 이웃이 전염될 확률은 독립적이다.  예를 들어, u가 감염되었을 때 u가 v를 감염시킬 확률은 u가 w를 가염시킬 확률과 독립적이다.이전 모형과 마찬가지로 첫 감염자들을 시드 집합(Seed Set)이라고 부르며, 더이상 새로운 감염자가 없을 때까지 감염시킨다.감염자의 회복을 가정하는 SIS, SIR 등의 다른 전파 모형도 있음바이럴 마케팅과 전파 최대화 문제바이럴 마케팅은 소비자들로 하여금 상품에 대한 긍정적인 입소문을 내게 하는 기법이다.소문의 시작점(시드 집합, Seed Set)이 중요하며, 누굴 고르냐에 따라 전파되는 범위가 크게 영향을 받는다.  소셜 인플루언서(Social Influencer)들이 높은 광고비를 받는 이유  영국 윌리엄 왕자 부인 케이트 미들턴의 이름을 딴 미들턴 효과 라는 용어도 있음            시드 집합의 선택에 따른 전파 크기 비교                                2 정점을 제외하고 모두 전파                            첫 2명을 제외하고 전파되지 않았음.      [img. 이전 선형 임계치 모형의 시드집합 선택에 따른 결과 비교]이러한 이유로 그래프, 전파 모형 그리고 시드 집합의 크기가 주어졌을 대 전파를 최대화하는 시드 집합을 찾는 문제를 전파 최대화(Influence Maximization) 문제라고 부름.  하지만 최고 효율의 시드 집합을 찾는 이 문제는 아주 어려운 문제이다.                              V          개의 정점의 그래프에서 시드 집합의 크기를 k개로 제한하여도 경우의 수가 $(\\frac{          V          }{k})$개이다.                            사람이 10000명, 찾을 인플루언서가 10명이어도 약 $2.75\\times 10^{33}$개 정도의 수        이론적으로 NP-hard 문제임이 증명됨          NP-난해(NP-hard) : 적어도 NP(푸는 시간이 $O(n)$ 이상 드는 문제) 문제 이상은 어려운 문제를 의미      너무나도 어려운 문제이므로 최고의 시드 집합을 찾는 것은 불가능하며, 휴리스틱 위주로 정답에 가까운 시드 집합을 찾아야한다.  휴리스틱으로 찾은 답은 최적의 답임이 보장되지 않는다.      휴리스틱 방법 - 정점의 중심성(Node Centrality)          시드 집합의 크기가 k개로 고정되어 있을시, 정점의 중심성이 높은 순으로 k개 정점을 선택      페이지랭크 점수, 연결 중심성, 근접 중심성, 매개 중심성 등이 있음            탐욕 알고리즘(Greedy Algorithm)                  시드 집합의 원소, 즉 최초 전파자를 한번에 한 명씩 선택하는 방법                    최초 전파자 간의 조합의 효과를 고려하지 않고, 근시안적으로 최초 전파자를 선택하는 과정을 반복함.                                                        즉, 정점의 집합을 {1,2, …,              V              }라고 할 때                                                                          1) 집합 {1},{2},…,{              V              }를 비교하여, 전파를 최대화하는 시드 집합을 찾는다.                                                이때, 전파의 크기 비교 방법은 시뮬레이션을 반복하여 생긴 평균 값을 사용함.                                                    2) 집합 {x}가 뽑히면, 집합 {x, 1}, {x, 2}, …, {x,              V              }를 비교하여 전파를 최대화 하는 시드 집합을 찾음.                                                                          3) 집합 {x, y}가 뽑히면 또 집합 {x, y, 1}, {x, y, 2}, …, {x, y,              V              }를 비교하여 전파 최대화 집합을 찾음                                      4) 위 과정을 목표하는 크기의 시드 집합에 도달할 때까지 반복            독립 전파 모형일 시, 이론적으로 최저 성능 정확도가 보장됨(보장 됬으므로 휴리스틱은 아님)\\(r_{greedy}\\geq (1-\\frac{1}{e}) * r_{optimal} \\\\where\\ (1-\\frac{1}{e})\\approx0.632\\\\r_{greedy}:\\ 탐욕\\ 알고리즘으로\\ 찾은\\ 시드\\ 집합의\\ 의한\\ 전파의\\ (평균)크기\\\\r_{optimal}:\\ 이론상\\ 최고의\\ 시드\\ 집합에\\ 의한\\ 전파의\\ (평균)크기\\)[math. 탐욕 알고리즘의 최저 성능]      전파 모형 시뮬레이터 구현Lab의 [Graph-4]Influence_Model.ipynb 참조그래프 군집 구조군집 구조와 군집 탐색 문제군집(Community)의 정의 : “기계학습을 이용한 그래프 학습에서 군집 구조” 참조온라인 소셜 네트워크의 군집들은 사회적 무리(Social Circle)을 의미하는 경우가 많음[img. SNS에서의 사회적 군집]군집을 분석하여 조직내 부정 행위 군집, 분란을 탐색하거나, 키워드 군집 등을 확인할 수 있다.그래프 내부에 군집들을 적절하게 나누는 문제를 군집 탐색(Community Detection) 문제라고 하며, 각 정점이 한 개의 군집에 속하도록 나누며, 비지도 기계학습 문제인 클러스터링(Clustering)과 상당히 유사함.  클러스터링 : feature들의 벡터 인스턴스를 그룹으로 묶음  군집 탐색: 군집의 정점들을 그룹으로 묶음군집 구조의 통계적 유의성과 군집성배치 모형(Configuration Model)은 각 정점의 연결성(Degree, 이웃의 수)을 보존한 상태에서 간선들을 무작위로 재배치하여 얻은 그래프이다.  임의의 두 정점 사이에 간선이 존재할 확률은 두 정점의 연결성에 비례  아래 예시 이웃들은 각 1, 3, 2 ,2 이며, 이웃의 수가 1, 3, 2, 2 로만 같게 연결하면 됨(누구와 연결되는 지는 관계 없음)[img. 배치모형의 형성]군집성(Modularity): 군집 구조의 통계적 유의성  이를 이용해 군집 탐색의 성공 여부를 판단할 수 있으며 다음과 같이 계산됨.\\(\\frac{1}{2|E|}\\sum_{s\\in S}(그래프에서\\ 군집\\ s\\ 내부\\ 간선\\ 수 - 배치\\ 모형에서\\ 군집\\ s\\ 내부\\ 간선\\ 수의\\ 기대값)\\)[math. 군집성의 계산]  -1 ~ +1 사이의 값을 가지며, 0.3~0.7 정도의 값을 가질 때 통계적으로 유의미한 군집임.  그래프와 군집들의 집합 S가 주어졌을 때, 각 군집 $s \\in S$가 군집의 성질을 잘 만족하는 지 알아보기 위해, 군집 내부의 간선의 수를 그래프와 배치 모형에서 비교.          다만 배치 모형은 무작위로 형성되므로 기대값을 비교        즉, 무작위로 연결된 배치 모형과 비교하여 통계적 유의성을 판단함.  배치모형 대비 그래프에서 군집 내부 간선의 수가 월등히 많을 수록 성공한 군집 탐색.군집 탐색 알고리즘  Girvan-Newman(걸반-뉴먼) 알고리즘전체 그래프에서 시작해서 점점 작은 단위를 검색하는 하향식(Top-Down) 군집 탐색 알고리즘[img. Girva-Newman 알고리즘]1) 전체 그래프에서 시작2) 매개 중심성이 높은 순서로 간선을 제거하면서, 군집성을 변화를 기록  매개 중심성(Betweenness Centrality) 간선 : 다른 군집을 연결하는 다리(Bridge) 역할의 간선  매개 중심성 : 정점 간의 최단 경로에 놓이는 횟수\\[간선 (x, y) 매개\\ 중심성=\\sum_{i&lt;j}\\frac{\\sigma_{i,j}(x,y)}{\\sigma_{i,j}}\\\\\\sigma_{i,j}: 정점\\ i로\\ 부터\\ j로의\\ 최단\\ 경로\\ 수\\\\\\sigma_{i,j}(x,y): 정점\\ i \\sim j까지의\\ 최단\\ 경로\\ 중\\ 간선\\ (x, y)를\\ 포함한\\ 것\\][math. 매개 중심성의 계산][img. 매개 중심성의 예시, 16이 다리 역할을 한다.]3) 간선 제거 때마다 재계산하여 매개 중심성 갱신, 군집성을 기록하고, 간선이 모두 제거될 때 까지 반복.            1      2      3      4                                          [math. 간선의 제거, 제거 정도 따라 다른 입도(Granularity)의 군집 구조가 나타남 ]4) 군집성이 가장 커지는 상황을 복원  기록한 군집성을 비교하여 군집성이 최대가 되는 지점으로 복원[img. 군집성이 최대가 되는 시점]5) 이때, 서로 연결된 정점들, 즉 연결 요소를 하나의 군집으로 간주  Louvain 알고리즘개별 정점에서 시작해 큰 군집을 형성하는 대표적인 상향식(Bottom-Up) 군집 탐색 알고리즘[img. Luvain 알고리즘]1) Louvain 알고리즘은 개별 정점으로 구성된 크기 1의 군집들로부터 시작2) 각 정점 u를 기존 혹은 새로운 군집으로 이동하며, 이 때, 군집성이 최대화되도록 군집을 결정3) 더 이상 군집성이 증가하지 않을 때까지 2)를 반복[img. 첫 시작시 각 정점을 군집으로 친다.]4) 각 군집을 하나의 정점으로 하는 군집 레벨의 그래프를 얻은 뒤 3) 을 수행[img. 각 군집을 하나의 정점으로 바꿈]5) 한 개의 정점이 남을 때까지 4)를 반복[img. 군집의 수가 줄다가 최종적으로는 군집이 하나만 남게 된다.]중첩이 있는 군집 탐색실제 그래프의 군집들은 중첩되어 있는경우가 많다.중첩 군집 탐색은 주어진 그래프의 확률을 최대화하는 중청 군집 모형을 찾는 과정이다.  최우도 추정치(Maximum Likelihood Estimate)를 찾는 과정이라고도 한다.[img. 실제 사회에서의 중첩 군집]중첩 군집 모형의 가정      각 정점은 여러 개의 군집에 속할 수 있음        각 군집 A에 대하여, 같은 군집에 속하는 두 정점은 $P_A$확률로 간선으로 직접 연결됨        두 정점이 여러 군집에 동시에 속할 경우 간선 연결 확률은 독립적임                  두 정점이 군집 A와 B에 동시에 속할 경우, 두 정점이 간선으로 직접 연결될 확률은        1-(1-$P_A$)(1-$P_B$).                  어느 군집에도 함께 속하지 않는 두 정점은 낮은 확률 $\\epsilon$으로 직접 연결됨.  위의 중첩 군집 모형으로 주어진 그래프의 확률을 다음과 같이 계산할 수 있다.그래프의 확률 \t=그래프의 각 간선의 두 정점이 (모형에 의해) 직접 연결될 확률 \tX그래프에서 직접 연결되지 않은 각 정점 쌍이 (모형에 의해) 직접 연결되지 않을 확률[code. 그래프 확률 계산식][img. 그래프 확률 계산]중첩 군집 탐색을 용이하게 하기위해 완화된 중첩 군집 모형을 사용할 수 있다.[img. 완화된 중첩 군집 모형의 느슨한 소속감]군집에 속하거나 속하지 않거나, 둘 뿐만아니라 그 중간 상태를 표시하기 위해 정점이 각 군집에 속해 있는 정도를 실숫값으로 표현 가능실숫값으로 표현되므로 경사하강법 등의 최적화 도구로 모형을 탐색할 수 있다.Girvan-Newman 알고리즘 구현 및 적용[Graph-5]Girvan-Newman_Algorithm_Real_World_Graph_Community_Detection.ipynb추천 시스템우리 주변의 추천 시스템아마존, 넷플릭스, 유튜브, 페이스북 등에서 추천 시스템을 이용한다.추천 시스템은 사용자 각각이 구매할 만한 혹은 선호할 만한 상품을 예측하거나 선호를 추정함.  즉 미래의 간선을 예측하는 문제 또는,  누락된 간선의 가중치를 추정하는 문제로 해석 가능하다.사용자 구매 기록을 그래프로 표현 가능하며,  구매 기록 같은 암시적(Implicit) 선호와  평점같은 명시적(Explicit) 선호가 존재한다.[img. 추천 시스템의 문제에 대한 해석]내용 기반 추천 시스템각 사용자가 구매/만족했던 상품과 유사한 것을 추천하는 방법[img. 내용 기반 추천의 4단계]      상품 프로필(Item Profile) 수집 단계          상품 프로필 : 해당 상품의 특성을 나열한 벡터      ex) 영화 : [로맨스, 코미디, 액션, 공포] = [0,1,0,0] =&gt; 원-핫 인코딩 형태            사용자 프로필(User Profile) 구성 단계          사용자 프로필 : 선호한 상품의 상품 프로필을 선호도를 사용하여 가중 평균하여 계산한 벡터      ex) 여러 영화 프로필의 가중 평균 : [로맨스, 코미디, 액션, 공포] = [0.1,0.9,0.2,0.3] =&gt; 원-핫 인코딩 형태            사용자 프로필과 다른 상품들의 상품 프로필을 매칭하는 단계                  사용자 프로필 벡터 $\\vec u$와 상품 프로필 벡터 $\\vec v$ 일 때, 코사인 유사도를 계산\\(코사인\\ 유사도 = \\frac{\\vec u \\cdot \\vec v}{\\left\\|\\vec u\\right\\|\\left\\|\\vec v\\right\\|}\\)        [img. 코사인 유사도 계산]                    즉, 두 벡터 사이각의 코사인 값을 계산                    코사인 유사도가 높을 수록, 해당 사용자가 과거 선호했던 상품들과 해당 상품이 유사함                  사용자에게 상품을 추천하는 단계          코사인 유사도가 높은 상품들을 추천                  장점      단점                  1. 다른 사용자의 구매 기록이 필요하지 않음2. 독특한 취향의 사용자에게도 추천이 가능3. 새 상품에 대해서도 추천이 가능4. 추천의 이유를 제공할 수 있음- 예시 : 로멘스영화를 많이 보니 로맨스 영화 추천했음      1. 상품에 대한 부가 정보가 없는 경우에는 사용 불가능2. 구매 기록이 없는 사용자에게 사용 불가능3. 과적합(Overfitting)으로 지나치게 협소한 추천 가능성      [fig. 내용 기반 추천 시스템의 장단점]협업 필터링유사한 취향의 사용자들을 이용해 추천하는 방법  사용자와 유사한 취향의 사용자들을 찾기  유사한 취향의 사용자들이 선호한 상품 찾기  선호한 상품을 사용자에게 추천[img. 협업 필터링의 3단계]취향의 유사성은 상관계수(Correlation Coefficient)를 통해 측정\\(sim(x,y)=\\frac{\\sum_{s\\in S_{xy}}(r_{xs}-\\bar{r}_x)(r_{ys}-\\bar{r}_y)}{\\sqrt{\\sum_{s\\in S_{xy}}(r_{xs}-\\bar{r}_x)^2}\\sqrt{\\sum_{s\\in S_{xy}}(r_{ys}-\\bar{r}_y)^2}}\\\\r_{xs}:사용자\\ x의\\ 상품\\ s에\\ 대한\\ 평점\\\\\\bar{r_x}:사용자\\ x가\\ 매긴\\ 평균\\ 평점\\\\S_{xy}:사용자\\ x와\\ y가\\ 공동\\ 구매한\\ 상품들\\)[math. 상관계수 식]  통계에서 나온 개념임                   반지의제왕      겨울왕국      라푼젤      해리포터      미녀와 야수                  지수      4      1      2      5      ?              제니      5      1      ?      4      2              로제      2      5      5      ?      4      [fig. 취향 유사도 계산]지수와 로제의 취향 유사도는 다음과 같다.\\(0.88=\\frac{(4-3)(5-3)+(1-3)(1-3)+(5-3)(4-3)}{\\sqrt{(4-3)^2+(1-3)^2+(5-3)^2}\\sqrt{(5-3)^2+(1-3)^2+(4-3)^2}}\\\\\\)둘의 취향은 상당히 비슷함이런 식으로 비슷한 사람들을 모아 위에서 구한 취향의 유사도를 가중치로 사용한 평점의 가중 평균을 통해 평점을 추정\\(평점\\ \\hat{r}_{XS}=\\frac{\\sum_{y\\in N(x;s)}sim(x,y)\\cdot r_{ys}}{\\sum_{y\\in N(x;s)}sim(x,y)}\\\\N(x;s): 상품\\ s를\\ 구매한\\ 사용자\\ 중에\\ x와\\ 취향이\\ 가장\\ 유사한\\ k명의\\ 사용자들\\)[math. 평점의 추정]마지막으로 사용자가 구매하지 않은 상품들의 추정한 평점 중에서 가장 평점이 높은 상품을 추천.            장점      단점                  상품에 대한 부가 정보가 없는 경우에도 사용 가능      1. 충분한 수의 평점 데이터가 누적되어야 효과적2. 새 상품, 새로운 사용자에 대한 추천 불가능3. 독특한 취향의 사용자(비슷한 사람 적음)에게 추천 힘듬      [fig. 협업 필터링의 단점]추천 시스템의 평가            실제 데이터      가린 데이터      추정한 데이터                                    [fig. 추천 시스템의 평가 방법]  실제 데이터를 훈련(Training) 데이터와 평가(Test) 데이터로 분리한 뒤  평가 데이터를 가리고 추천 시스템으로 추정한 뒤  이를 실제 데이터와 지표를 비교하여 평가한다.[img. 추정한 평가데이터 vs 실제 데이터]오차를 측정하는 지표로 평균 제곱 오차(Mean Squared Error, MSE)와 평균 제곱근 오차(Root Mean Squared Error, RMSE)가 많이 사용됨.\\(MSE =\\frac{1}{|T|}\\sum_{r_{xi}\\in T}(r_{xi}-\\hat{r}_{xi})^2\\\\RMSE = \\sqrt{\\frac{1}{|T|}\\sum_{r_{xi}\\in T}(r_{xi}-\\hat{r}_{xi})^2}\\\\where\\ T: 평가\\ 데이터\\ 내의\\ 평점들을\\ 집합\\)[math. 추천 시스템의 오차 측정 지표 식]이외에도 다음과 같은 지표 등이 사용됨.  추정한 평점으로 순위를 매긴 후, 실제 평점으로 매긴 순위와의 상관 계수  추천한 상품 중 실제 구매로 이루어진 것의 비율  추천의 순서 혹은 다양성까지 고려하는 지표협업 필터링 구현Lab의 [Graph-6]Collaborative_Filtering.ipynb 참조그래프의 벡터 표현정점 표현 학습그래프의 정점들을 벡터의 형태로 표현하는 것, 정점 임베딩(Node Embedding)이라고도 부름.정점이 표현되는 벡터 공간을 임베딩 공간이라고 부름.[img. 임베딩 공간의 예시]정점 표현 학습의 입력은 그래프이며, 주어진 그래프의 각 정점 u에 대한 임베딩 벡터 표현 $z_u$가 정점 임베딩의 출력[img. 벡터 표현의 예시]이러한 정점 임베딩의 결과로 벡터 형태의 데이터를 위한 도구를 그래프에도 적용할 수 있음.  분류기(로지스틱 회귀분석, 다층 퍼셉트론)  군집 분석 알고리즘(K-Means, DBSCAN)  정점 분류(Node Classification), 군집 분석(Community Detection)정점간 유사도를 임베딩 공간에서도 보존하는 것을 목표로 하며, 유사도로는 내적(Inner Product)을 사용\\(similarity(u,v) \\approx z_v^{\\top}z_u=\\left\\|z_u\\right\\|\\cdot\\left\\|z_v\\right\\|\\cdot\\cos(\\theta)\\)[math. 두 정점 벡터의 내적==유사도]  두 벡터가 클수록, 같은 방향을 향할수록 큰 값을 갖게 됨.  이 이외에도 여러가지 유사도 측정 방법이 있음즉 정점 임베딩은  그래프의 정점 유사도를 정의하는 단계  정의한 유사도를 보존하도록 정점 임베딩을 학습하는 단계로 이루어짐인접성 기반 접근법두 정점이 인접할 때(둘을 직접 연결하는 간선이 있을 때) 유사하다고 간주하는  방법인접 행렬(Adjacency Matrix) A의 u행 v열 원소 $A_{u,v}$는 원소 u와 v가 인접한 경우 1, 아니면 0이며, 이를 유사도로 가정[img. 그래프의 인접행렬 표현]손실함수(Loss Function)가 최소가 되는 정점 인베딩을 찾는 것이 목표이며, 이를 위해 (확률적) 경사하강법을 사용.\\(손실함수\\ \\mathcal{L} = \\sum_{(u,v)\\in V\\times V}\\left\\|z_u^\\top z_v - \\sum_u A^k_{u,v}\\right\\|^2\\\\\\sum_{(u,v)\\in V\\times V} : 모든\\ 정점\\ 쌍에\\ 대하여\\ 합산,\\\\ z_u^\\top z_v:임베딩\\ 공간에서의\\ 유사도,\\ A_{u,v}: 그래프에서의\\ 유사도\\)[math. 인접성 기반 접근법의 손실 함수(Loss Function)]다만 이 방법은 정점 간의 거리가 고려되지 않는다.[img. 인접성 기반 유사도 판단의 한계 예시]빨간색 정점과 파란색 정점의 거리는 3, 다른 군집 소속초록색 정점과 파란색 정점의 거리는 2, 같은 군집 소속이지만 인접성만 고려할 시, 두 경우 유사도가 0 이다.-&gt; 인접성 기반은 군집과 거리 등의 속성을 무시한다.거리/경로/중첩 기반 접근법  거리 기반 접근법두 정점 사이의 거리가 충분히 가까운 경우 유사하다고 간주'’충분히’‘의 기준 거리를 가정한 그보다 거리가 작거나 같으면 1, 크면 0으로 유사도를 간주  경로 기반 접근법두 정점 사이의 경로가 많을 수록 유사하다고 간주u와 v 사이의 경로는 u에서 시작해서 v에서 끝나야하며, 순열에서 연속된 정점은 간선으로 연결되어야 함.두 정점 u와 v의 사이의 경로 중 거리가 k 인 것의 수를 $A^k_{u,v}$라고 할 때  이는 인접 행렬 A의 K 제곱의 u행 v열 원소와 같음.\\[손실함수\\ \\mathcal{L}=\\sum_{(u,v)\\in V\\times V}\\left\\|z_u^{\\top}z_v-A^K_{u,v} \\right\\|^2\\][math. 경로 기반 접근법의 손실함수]  중첩 기반 접근법두 정점이 많은 이웃을 공유할 수록 유사하다고 간주[img. 빨간색 정점은 파란색 정점과 2명의 이웃을 공유한다.(==유사도 2)]\\(공통\\ 이웃\\ 수\\ S_{u,v}=|N(u)\\cap N(v)|=\\sum_{w\\in N(u)\\cap N(v)}1\\\\손실함수\\ \\mathcal{L}=\\sum_{(u,v)\\in V\\times V} \\left\\| z_u^{\\top}z_v-S_{u,v} \\right\\|^2\\\\N(x): 정점\\ x의\\ 이웃\\ 집합\\)[math. 중첩 기반 접근법에서의 손실함수]손실함수를 이용하며, 이때 공통 이웃수가 아닌 자카드 유사도 혹은 Adamic Adar 점수를 기반으로 사용할 수도 있다.\\(자카드\\ 유사도(Jaccard\\ Similarity): \\frac{N_u\\cap N_v}{N_u\\cup N_v}\\\\Adamic\\ Adar\\ 점수: \\sum_{w\\in N_u\\cap N_v}\\frac{1}{d_w}, d_w: 연결성\\)[math. 공통 이웃 수 대신 사용가능한 지표]  자카드 유사도(Jaccard Similarity) : 공통 이웃의 수 대신 비율을 계산  Adamic Adar 점수 : 공통 이웃 각각에 가중치를 부여하여 가중합을 계산          그 이웃의 연결성이 클수록(=이웃이 많을 수록) 가중치가 낮은데, 유명한 사람을 연결한 경우는 큰 의미가 없는 경우가 많기 때문이다      트와이스를 팔로우한 두 사람은 관계없을지도 모르지만, 김철수씨를 팔로우한 두 사람은 친구일 확률이 높다.      임의보행 기반 접근법한 정점에서 시작하여 임의보행을 할 시, 다른 정점에 도달할 확률을 유사도로 간주  임의 보행이란 현재 정점의 이웃 중 하나를 균일한 확률로 선택하며 이동하는 과정을 반복하는 것시작 정점 주변의  지역적 정보 ( 무작위 보행시 시작 주변을 마구 돌아다닐 확률이 높으므로)  그래프 전역 정보 (거리나 제한을 두지 않으므로 마음대로 돌아다닐 수 있으므로)둘을 모두 고려한다는 장점이 있음임의 보행 기반 접근의 순서  각 정점에서 시작하여 임의보행을 반복 수행  각 정점에서 시작한 임의보행 중 도달한 정점들의 리스트를 구성          정점 u에서 시작한 임의보행 중 도달한 정점들의 리스트를 $N_R(u)$라고 함                  정점의 중복 허용                      손실함수를 최소화하는 임베딩을 학습함\\[\\mathcal{L}=\\sum_{u\\in V}\\sum_{v\\in N_R(u)}-\\log(P(v|z_u))\\\\P(v|z_u)=\\frac{\\exp(z_u^{\\top}z_v)}{\\sum_{n\\in V}\\exp(z_u^{\\top}z_n)}\\\\P(v|z_u): u에서\\ 시작한\\ 임의보행이\\ v에\\ 도달할\\ 확률을\\ 임베딩으로부터\\ 추정한\\ 결과를\\ 의미\\\\\\sum_{u\\in V}: 모든\\ 시작점에\\ 대하여\\ 합산\\\\\\sum_{v\\in N_R(u)}: 임의보행\\ 중\\ 마주친\\ 모든\\ 정점에\\ 대하여\\ 합산\\][math. 임의보행에서의 손실함수와 도달 확률]                              임의 보행 확률 $P(v          z_u)$는 (u에 도달할 확률(내적))/(전체 정점들 도달할 확률의 합(내적))으로 구한다.                      유사도 $z_u^{\\top}z_n$가 높을 수록 도달 확률이 높음.또한, 임의보행의 방법에 따라 다음과 같이 구분된다.  DeepWalk  기본적인 임의보행, 현재 정점의 이웃중 하나를 균일한 확률로 선택하여 이동  Node2Vec  2차 치우친 임의보행(Second-order Biased Random Walk)을 사용  현재 정점(v)과 직전에 머물렀던 정점(u)을 모두 고려하여 다음 정점을 선택  즉, 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여[img. 2차 치우친 임의 보행의 선택]            Node2Vec      K-means 군집 분석 결과                                      멀어지는 방향에 높은 확률 부여      가까워지는 방향에 높은 확률 부여              정점의 역할(다리 역할, 변두리 정점 등)이 같은 경우 인베딩이 유사      같은 군집(Community)에 속하는 경우 임베딩이 유사      [fig. 부여하는 확률에 따라 다른 종류의 임베딩 획득]손실 함수의 근사법\\(\\mathcal{L}=\\sum_{u\\in V}\\sum_{v\\in N_R(u)}-\\log(P(v|z_u))\\\\P(v|z_u)=\\frac{\\exp(z_u^{\\top}z_v)}{\\sum_{n\\in V}\\exp(z_u^{\\top}z_n)}\\\\\\sum_{u\\in V}, \\sum_{n\\in V}: 중첩된\\ 합 \\rightarrow 정점\\ 수의\\ 제곱에\\ 비례하는\\ 시간이\\ 소요\\)[math. 기존의 손실함수와 느린 이유]기존 손실함수는 정점이 많은 경우 천문학적 수 이상의 연산이 필요하다.(1억의 정점은 10000조번 연산)따라서 많은 경우 근사식을 사용함\\(P(v|z_u)=\\frac{\\exp(z_u^{\\top}z_v)}{\\sum_{n\\in V}\\exp(z_u^{\\top}z_n)}\\\\\\approx \\log(\\sigma(z_u^{\\top}z_v))-\\sum^k_{i=1}\\log(\\sigma(z_u^{\\top}z_v)),n_i\\sim P_V\\\\\\sigma(x) : sigmoid\\ 함수,\\ P_V: 확률\\ 분포\\)[math. 임의보행 확률의 근사식]  모든 정점에 대해서 정규화하지 않고 몇 개의 정점을 뽑아서 비교하며, 이를 네거티브 샘플이라고 함.  연결성에 비례하는 확률로 샘플을 뽑으며 많이 뽑을 수록 안정적인 학습이 이루어짐.변환식 정점 표현 학습의 한계앞에서 배웠던 방법들은 모두 변환식(Transductive) 방법으로, 학습의 결과로 정점의 임베딩 자체를 얻는다는 특성이 있다.  정점을 임베딩으로 변화시키는 함수, 즉 인코더를 얻는 귀납식(Inductive) 방법(9강에서 배움)과 대조[img. 귀납식 방법 도식화]변환식 임베딩의 한계  학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음  모든 정점에 대한 임베딩을 미리 계산하여 저장해야 함.  정점이 속성(Attribute) 정보를 가진 경우에 이를 활용 불가능          엄밀히 말하면 3번째는 변환식 임베딩의 한계가 아닌, 위에 소개된 방법들의 한계이다.      이를 극복하기 위해 귀납식 임베딩 방법이 존재하며 대표적인 방법으로 그래프 신경망(Graph Neural Network)Node2Vec을 사용한 군집 순석과 정점 분류Lab의 [Graph-7]Node2Vec.ipynb 참조추천 시스템 심화넷플릭스 챌린지 소개넷플릭스에서 수많은 사용자별 영화 평점 데이터 사용하여 성능을 10% 이상 향상시키는 것이 목표[img. 당시 평균 제곱근 오차]기본 잠재 인수 모형잠재 인수 모형(Latent Factor Model)의 핵심은 사용자와 상품을 벡터로 표현하는 것[img. 사용자와 영화를 임베딩한 예시]잠재 인수 모형에서는 고정된 인수 대신 효과적인 인수(잠재 인수, Latent Factor)를 학습하는 것이 목표임베딩의 목표는사용자 x의 임베딩 $p_x$, 상품 i의 임베딩을 $q_i$라고 하며, 사용자 x의 상품 i에 대한 평점을 $r_{xi}$라 할때,내적  $p_x^\\top q_i$와 평점 $r_{xi}$를 유사하도록 하는 것이다.[img. 행렬 차원에서 본 임베딩의 목표]  위 행렬 차원에서는 R과 비슷해지는 Q과 $P^\\top$을 찾는 것이 목표이다.\\[\\sum_{(i,x)\\in R}\\overset{오차}{\\overbrace{(r_{xi}-p_x^\\top q_i)^2}}+\\overset{모형\\ 복잡도}{\\overbrace{[\\lambda_1\\sum_x\\left\\|p_x\\right\\|^2+\\lambda_2\\sum_i\\left\\|q_i\\right\\|^2]}}\\\\\\sum_{(i,x)\\in R} : 훈련\\ 데이터에\\ 있는\\ 평점에\\ 대해서만\\ 계산합니다.\\\\\\lambda_1, \\lambda_2 : 정규화의\\ 세기(하이퍼파라미터)\\][math. 잠재 인수 모형에서의 손실 함수]  오른쪽의 모형 복잡도 항을 추가하여 정규화하여 과적합(Overfitting)을 방지한다.          과적합 이란 기계학습 모형이 훈련 데이터의 잡음(Noise)까지 학습하여, 평가 성능이 오히려 감소하는 현상      정규화를 통하여 절대값이 너무 큰 임베딩을 방지(너무 튀는 값 방지)      위 손실함수를 최소화 하기 위해 (확률적) 경사하강법을 사용함.[img. 경사하강법 vs 확률적 경사하강법]  경사하강법: 손실함수를 안정적으로 하지만 느리게 감소(step은 더 확실히 들어가지만 연산이 느림)  확률적 경사하강법: 손실함수를 불안정하지만 빠르게 감소, 더 많이 사용됨고급 잠재 인수 모형단순히 사용자와 상품의 임베딩만으로 평점을 고려하는 것보다 더 현실적이고 오차가 적은 모형을 만들기 위해 두 가지를 추가로 고려하였다.  편향(bias)과 평균  사용자의 편향은 해당 사용자의 평점 평균과 전체 평점 평균의 차          나연의 평점이 4.0, 평균 평점이 3.7 이라면 나연의 사용자 편향은 0.3      다현의 평점이 3.5, 평균 평점이 3.7 이라면 다현의 사용자 편향은 -0.2        상품의 편향은 해당 상품에 대한 평점 평균과 전체 평점 평균의 차          영화 클레멘타인의 평점 평균이 3.0이고 평균 평점이 3.7이라면 상품 편향은 -0.7        시간적 평향[img. 넷플릭스의 시스템 업데이트에 의한 평점 상승과, 영화 출시 이후에 평점 상승 경향 ]  평점이 시간에 따라 달라지는 경향을 추가로 고려\\[r_{xi}=\\mu+b_x+b_i+p_x^\\top q_i \\rightarrow 평점에\\ 편향과\\ 평균을\\ 고려\\\\\\\\ \\\\ r_{xi}=\\mu+b_x(t)+b_i(t)+p_x^\\top q_i \\rightarrow 시간적\\ 편향을\\ 고려한\\ 평점\\\\ b(t): 시간에\\ 따른\\ 편향\\ 변화\\ 함수\\\\\\mu: 전체 평균,\\ b_x: 사용자\\ 편향\\ b_i: 상품\\ 편향,\\ p_x^\\top q_i:상호작용\\][img. 위 두가지를 고려하여 개선한 예상 평점]\\(\\sum_{(i,x)\\in R}{(r_{xi}-(\\mu+b_x(t)+b_i(t)+p_x^\\top q_i))^2}+{[\\lambda_1\\sum_x\\left\\|p_x\\right\\|^2+\\lambda_2\\sum_i\\left\\|q_i\\right\\|^2+\\lambda_3\\sum_x b_x^2+\\lambda_4\\sum_ib_i^2]}\\)[math. 최종적으로 개선된 손실 함수]넷플릭스 챌린지의 결과[img. 위의 방법들을 고려한 RMSE]  후에 추가로 앙상블 기법을 활용해 목표 오차에 도달하게 되고 같은 성능의 다른 팀보다 20분 제출이 더 빨랐던 BellKor 팀이 우승하게 된다.[img. 앙상블 기법]Surprise 라이브러리와 잠재 인수 모형의 활용Lab의 [Graph-8]Latent_Factor_based_Recommendation_System.ipynb 참조그래프 신경망(Graph Neural Network) 기본귀납식 임베딩은 다음과 같은 장점을 가진다.  학습이 진행된 이후에 추가된 정점에 대해서도 임베딩 획득 가능  모든 정점에 대한 임베딩을 미리 계산하여 저장해둘 필요 없음  정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 있음$ENC(v) = 그래프\\ 구조와\\ 정점의\\ 부가\\ 정보를\\ 활용하는\\ 복잡한\\ 함수$[math. 귀납식 임베딩은 인코더 함수를 활용한다.]그래프 신경망(GNN, Graph Neural Network)이란?그래프 신경망(GNN, Graph Neural Network) : 출력으로 인코더를 얻는 귀납식 임베딩의 대표적인 방법그래프 신경망의 구조            그래프 신경망은 *그래프(인접 행렬 형식A,      V      x      V      )와 정점의 속성(Attribute) 벡터($X_u$)*를  입력을 받음.        속성은 속성의 갯수 만큼의 차원을 가지며, 성별, 연령, 정점 중신성, 군집 계수 등이 있음            입력 그래프      계산 그래프                              [fig. 대상 정점에 따른 그래프 신경망의 계산 그래프 ]특정 정점의 임베딩을 얻기 위해 이웃 정점들의 임베딩을 집계하는 과정을 반복하는 방식  각 집계 단계를 층(Layer)라고 부르며, 각 층마다 임베딩을 계산          층의 갯수는 Hyperparameter인 듯?      https://arxiv.org/pdf/1706.02216.pdf        최초의 0번 층의 입력은 각 정점의 속성 벡터($X_u$)를 이용  대상 정점 별 집계되는 구조를 계산 그래프(Computation Graph)라고 부름[img. 층 별 집계함수는 모두 공유함]이웃의 정점의 임베딩을 이용해 특정 정점의 임베딩을 구할 때의 집계 함수를 이용하며, 각 층마다 모두 공유하며,1) 이웃들 정보의 평균을 계산하고2) 신경망에 적용하는 단계를 거침.[img. k층에서의 임베딩($h^k_v$)을 구하는 집계함수 식]마지막 층의 임베딩 결과 $h_v^k$가 해당 정점의 출력 임베딩이 되며, 학습 변수(Trainable Parameter)는 층별 신경망의 가중치인  $W_k$  $B_k$이다.그래프 신경망의 학습그래프 신경망의 학습을 위해 가장 먼저 손실 함수를 결정해야 하며, 정점간 거리를 보존하는 것이 목표이다.  즉 임베딩 공간에서의 거리와 실제 그래프에서의 거리가 비슷한 것이 목표  주로 비지도 학습에서의 목표임[img. 인접성 기반 유사도를 이용한 손실 함수 정의]만약,  지도 학습인 후속 과제(Downstream Task)가 있다면,해당 후속과제의 손실함수를 이용하는 것  또한 가능(종단종(End-to-End) 학습)[img. 정점 분류기의 경우 손실함수를 교차 엔트로피(Cross Entropy)로 사용 가능]  정점 분류 문제는 정점이 어떠한 class를 가지는 가를 분류하는 문제(사람인가? 봇인가?)[img. 후속 과제의 학습 모델 예시][img. GCN(아래에서 배울 Graph Convolutional Network)이 성능이 제일 높다.]GNN End-to-End 학습이 변환적 정점 임베딩 + 분류기 학습 보다 정확도가 대체로 높음[img. 학습할 정점(A,B,C)와 그를 위한 학습 데이터 구성]학습할 정점에 따라 다른 학습 데이터를 구성한 후, 오차역전파(Backpropagation)을 통해 손실함수를 최소화하며 학습 변수를 학습시키는 형식으로 학습이 진행된다.그래프 신경망의 활용그래프 신경망을 통해 이전에 언급했던 장점들이 발휘되게 된다.  이미 학습한 집계함수를 사용하여 학습에 사용되지 않은 정점의 임베딩을 얻을 수 있음          예를 들어[img. 학습할 정점(A,B,C)와 그를 위한 학습 데이터 구성]에서 이미 정점 A,B,C를 학습하기 위해 학습한 집계함수(구체적으로는 집계함수 학습변수의 가중치)를 다른 정점 임베딩에 사용할 수 있다.        학습이 끝난 이후 추가된 정점의 임베딩 또한 위와 마찬가지로 쉽게 구할 수 있음[img. 그래프의 정점 추가 시, 새로운 정점 임베딩의 학습]  학습 된 그래프 신경망의 집계함수를 새로운 그래프에 적용 가능[img. 서로 다른 단백질의 상호작용 그래프를 예시로 들 수 있다.]그래프 신경망 변형  그래프 합성곱 신경망(Grpah Convolutional Network, GCN)그래프 합성곱 신경망(Graph Convolutional Network, GCN)은 집계함수로 기존의 GNN과 2가지가 다르다.1) 학습변수가 GNN의 2개($W_k, B_k$)와 달리 $W_k$ 하나로 통합            2) 정규화 방법 : 현재 정점 v만의 연결성만 사용하던 GNN과 달리 현재 정점 v와 이전 정점 u의 기하 평균($\\sqrt{      N(u)             N(v)      }$)을 사용한다.      ​\t- 기하 평균은 비선형 그래프에서 많이 사용됨, 그냥 산술 평균은 비선형 그래프에서 나올 수 없는 값이 나올 확률이 큼\\(h_v^k = \\sigma\\left(W_k\\sum_{u\\in N(v)\\cup v}\\frac{h_u^{k-1}}{\\sqrt{|N(u)||N(v)|}}\\right),\\ \\forall k \\in \\{1,\\dots,K\\}\\)[math. GCN의 집계함수]  GraphSAGEGraphSAGE의 집계함수 기존의 GNN과 2가지가 다르다.[img.GraphSAGE의 집계 함수]1) 이전 층에서의 자신의 임베딩과 이전 층에서 이웃들의 임베딩 연산 결과를 더하는 것이 아니라 연결(Concatenation)                              GNN에서는 둘의 합을 구하였다($h_v^k=\\sigma\\left( W_k\\sum_{u\\in N(v)}\\frac{h_u^{k-1}}{          N(v)          }+B_kh_v^{k-1}\\right)$).                    2) 이웃들의 임베딩의 연산 결과를 평균을 통하여 구하는 것이 아닌, AGG 함수로 구함.  AGG 함수는 평균, 풀링, LSTM 등이 사용됨\\(Mean:\\ AGG = \\sum_{u\\in N(v)} \\frac{h_u^{k-1}}{|N(v)|}\\\\Pool:\\ AGG = \\gamma(\\{Qh_u^{k-1},\\forall u \\in N(v)\\})\\\\LSTM:\\ AGG = LSTM([h_u^{k-1},\\forall u \\in \\pi(N(v))])\\)[math. GraphSAGE가 이웃들의 임베딩을 합치는 방법]합성곱 신경망과의 비교            그래프 신경망      그래프 합성곱 신경망                                      집계하는 이웃의 수가 정점의 연결성에 따라 다름      집계하는 이웃의 수가 모두 동일예시에는 주변 이웃 8개+ 자기자신 1개의 픽셀을 집계      [fig. 그래프 신경망 vs 그래프 합성곱 신경망]그래프의 인접 행렬(A)에는 그래프 합성곱 신경망이 아닌 그래프 신경망만을 적용해야함.  인접행렬의 인접원소는 이미지와 달리 무작위로 구성되어있음,  따라서 주변 원소가 관계없는 임의의 원소인 경우가 많음  따라서 의미없는 정보가 이웃으로 집계가 되면 잘못된 결과가 나온다.DGL 라이브러리와 GraphSAGE를 이용한 정점 분류Lab의 [Graph-9]Using_GDL_Library-1.ipynb 참조그래프 신경망 심화그래프 신경망에서의 어텐션이웃들의 정보를 동일한 가중치로 평균을 내는 기본 그래프 신경망과 연결성을 고려한 가중치로 평균을 내는 그래프 합성곱 신경망과 달리,그래프 어텐션 신경망(Graph Attention Network, GAT)에서는 셀프-어텐션(Self-Attention)을 활용해 가중치 자체도 학습함.  이를 통해 이웃 별로 미치는 영향을 다르게 줄 수 있음.[img. 그래프에 셀프 어텐션 구조를 적용]각 층에서 정점 i로부터 이웃 j로의 가중치 $a_{ij}$는 다음과 같은 과정을 통해 계산 됨            단계      설명      수식                  1      해당 층의 정점 i의 임베딩 $h_i$에 신경망 W를 곱해 새로운 임베딩을 얻습니다.      $\\tilde h_i=h_iW$              2      정점 i와 정점 j의 새로운 임베딩을 연결한 후, 어텐션 계수 a를 내적.- 어텐션 계수 a는 모든 정점이 공유하는 학습 변수      $e_{ij}=a^\\top[CONCAT(\\tilde h_i,\\tilde h_j)]$              3      2)의 결과에 소프트맥스(Softmax)를 적용      $a_{ij}=softmax_j(e_{ij})=\\frac{\\exp(e_{ij})}{\\sum_{k\\in \\mathcal{N}i \\exp(e{ik})}}$      [fig. 그래프 어텐션 신경망 가중치 학습 단계]또한, 여러 개의 어텐션을 동시에 학습하는 멀티 헤드 어텐션 또한 가능.\\({h}'_i=\\underset{1\\leq k\\leq K}{CONCAT}\\sigma\\left(\\sum_{j\\in \\mathcal{N}_i}a_{ij}^kh_jW_k\\right)\\)[math. 멀티 헤드 어텐션에서의 정점 임베딩][img. 세 개의 어텐션을 사용한 GAT][img. 어텐션의 결과, 성능이 좋아짐]그래프 표현 학습과 그래프 풀링그래프 표현학습 또는 그래프 임베딩은 그래프 전체를 벡터의 형태로 표현하는 것  그래프 임베딩은 벡터 형태로 표현된 그래프 자체를 의미하기도 함  개별 정점이 아닌 그래프 자체를 벡터 형태로 표현한다는 점이 정점 임베딩과 다름  그래프 분류, 화합물 분자 구조 특성 예측 등에 활용그래프 풀링(Graph Pooling) : 정점 임베딩들로부터 그래프 임베딩을 얻는 과정  정점 간의 평균 같은 단순한 방법보다 좋은 성능을 보임  그래프 풀링 방법 중 하나로 미분가능한 풀링(Differentiable Pooling, DiffPool)이 있다          군집을 임베딩 벡터로 바꾸어 가며 하나의 벡터로 바꾸는 방식      [img. 미분가능한 풀링의 도식]미분 가능한 그래프 풀링에서는 다음과 같은 3가지 목적을 위해 기존의 GNN의 사용함.  개별 정점 임베딩 얻기  정점들을 군집으로 묶기  군집들을 합산해서 더 큰 군집을 만들기지나친 획일화(Over-smoothing) 문제지나친 획일화(Over-Smoothing) 문제는 그래프 신경망의 층의 수가 증가하며 정점 임베딩이 서로 유사해지는 현상이다.  정점 간의 거리가 생각보다 작기 때문에, 적은 수의 층만으로도 다수의 정점의 임베딩을 포함해 버리기 때문(작은 세상 효과)[img. 지나친 획일화의 예시]이로 인해 그래프 신경망의 층을 늘렸을 때, 오히려 성능이 감소하는 현상이 나타남      실제로는 다른 특색을 가진 정점을 비슷한 정점이라고 오판단하기 때문        보통 2~3개 층이 정확도가 가장 높음  잔차항(Residual)을 넣으면 조금 효과가 있지만 제한적이다.[img. 층 수에 따른 정확도 그래프]\\(h_u^{(l+1)}=h_u^{(1+1)}+h_u^{(l)}\\)[math. 잔차항(Residual)을 사용하는 방법]지나친 획일화를 해결하기 위해 2가지 해결 방법에 대해 알아보자  JK 네트워크(Jumping Knowledge Network)마지막 층에 모든 층의 임베딩을 더해주는 방식[img. JK 네트워크 도식]지나친 획일화를 완전히 해결하는 방법은 아니며, 조금 완화해준다.  APPNP(Approximate personalized propagation of neural predictions)  신경망 예측의 대략적인 개인화 전파?  PPNP의 빠른 버전0번째 층에서만 신경망을 적용, 그 이외에 층은 단순화한 집계함수를 사용[img. APPNP 도식]\\(Z^{(0)} = H = f_\\theta(X)\\\\Z^{(k+1)} = (1 − \\alpha)\\tilde{\\hat{A}}Z^{(k)} + \\alpha H\\\\Z^{(K)} = softmax\\left((1 − \\alpha)\\tilde{\\hat{A}}Z^{(K-1)} + \\alpha H\\right)\\\\\\alpha : teleport\\ probability, H: prediction matrix, the teleport set,\\\\X : feature\\ matrix, f_\\theta = a\\ neural\\ network\\ with\\ parameter\\ set\\ \\theta\\\\\\tilde{\\hat{A}}: the\\ symmetrically\\ normalized\\ adjacency\\ matrix\\ with\\ self-loops\\)[math. APPNP에서의 학습 과정]  정확히 말하면 PageRank 알고리즘 개선에 사용된 식이다.  https://arxiv.org/pdf/1810.05997.pdf층의 수가 늘어날 수록 성능 향상은 줄어들지만 층의 증가에 따른 정확도 감소 효과는 극복함.[img. APPNP의 성능]그래프 데이터 증강(Data Augmentation)그래프의 누락되거나 부정확한 간선을 보완하고, 임의 보행을 통해 정점간 유사도를 계산하여 유사도가 높은 정점 간의 간선을 추가하는 방법[img. 데이터 증강의 예시]AI비전의 이미지 증강 처럼, 그래프 데이터 증강을 통하여 정확도를 개선할 수 있다.[img. 그래프 데이터 증강 효과]GraphSAGE의 집계 함수 구현Lab의 [Graph-10]Using_GDL_Library-2.ipynb 참조"
  }
  , 
  
  "/articles/AI/Lightweight/%EB%AA%A8%EB%8D%B8%20%EA%B2%BD%EB%9F%89%ED%99%94%20%EA%B8%B0%EB%B3%B8.html": {
    title: "모델 경량화 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/Lightweight/%EB%AA%A8%EB%8D%B8%20%EA%B2%BD%EB%9F%89%ED%99%94%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","모델_경량화","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true모델 경량화(Lightweight) 기본  Naver AI boostcamp 내용을 정리하였습니다.Lightweight model 개요1. 결정 (Decision making)연역적 (deductive) 결정이미 참으로 증명(Axiom)되거나 정의를 통하여 논리를 입증하는 것삼단 논법과 조합 $\\begin{pmatrix} n\\r\\end{pmatrix}=\\frac{n!}{r!(n-r)!}$ 증명 등이 있다.  삼단 논법 예시 : 소크라테스는 사람이다. 사람은 모두 죽는다. 소크라테스는 죽는다.\\[Definition:\\begin{pmatrix} n\\\\r\\end{pmatrix}=\\frac{n!}{r!(n-r)!}\\\\n!=n\\cdot(n-1)\\cdot(n-2)\\dots\\cdot2\\cdot1라면,\\\\Theorem:1+2+\\dots+(n-1)=\\begin{pmatrix} n\\\\r\\end{pmatrix}이다.\\][math. 조합의 가정][img. 조합의 증명]귀납적 (inductive) 결정반복된 관찰과 사례를 통하여 논리를 입증많은 통계학적 기법의 증명이 속한다.예시: 몇만년동안 해는 동쪽에서 떠서 서쪽에서 진다 -&gt; 고로 내일도 그럴 것이다.단점으로, 100% 보장하지 않는다. (ex) 만약, 내일 태양이 폭발한다면?)2. 결정기 (Decision making machine)이전의 기계들은 사람이 결정을 하면 해당 결정의 목적 달성을 위해 도움을 주는 형태였다.최근의 인공지능이 포함된 기기는 이때의 결정 또한 해주거나, 결정에 도움을 제공.(ex) 공기질이 나빠지면 자동으로 켜지는 공기청정기)[img. 청소기를 예시로 든 결정기][img. 가장 간단한 통계학적 결정기인 평균][img. 세상에는 중요한 결정과 쉬운 결정, 그리고 결정할 수 없는것이 있다]  근본적 가치에 관계된 일, 예측할 수 없는 일, 책임질 수 없거나 책임이 너무 큰일 등은 결정하기 힘들거나 결정할 수 없다.[img. 과거에는 성능 부족으로 추천 정도만 했지만 최근에는 의사결정에 관여 가능한 수준][img. 현재 기술로는 잘해봐야 Rules &amp; conditions 까지만 결정 가능, 가치와 중요 결정은 불가]3. 가벼운 결정기 (Lightweight decision making machine)경량화(Lightweight)는 성능이나 가치가 차이가 없거나 적게 희생하고 규모와 비용 등을 줄이는 것이 것소형화(Miniaturization)은 말 그대로 물건의 크기를 줄이는 것[img. 소형 기기에 ML 모델을 넣기 위한 Cycle]최근에는 TinyML이라 하여 칩 수준의 소형기기를 위한 메모리 사용량과 연산량이 적은 Project 또한 진행되고 있다.[img. Edge Computing의 장점 시나리오]Edge Computing은 네트워크에서 끝 부분에 해당하는 보통 소형기기에서의 연산하는 서비스를 의미하며, 많은 장점을 가지고 있다예를 들어, 자율주행의 연산을 Cloud Computing으로 처리할 시, 보안 문제, 연결안전성 문제, 대규모 연산에 의한 전력과 기기 소모 등이 필요하지만Edge Computing은 이러한 문제를 해결할 수 있다.4. Backbone &amp; dataset for model compression성능좋고 안정성 있는 Pre-trained 모델이 많이 나와있다.이러한 Pre-trained 모델을 경량화하여 사용하면, 검증된 성능과 문제 해결, 빠른 개발 등의 장점이 있다.[img. 최근에는 다양한 pretrained model이 많다.]MiB(메비바이트) : 2진법 기준 용량 계산법, 2^20 바이트[img. Classification을 위한 Dataset들, 점점 커진다]5. Edge devices[img. 각종 computing 방법]Cloud는 높은 가격과 보안 문제, 네트워크 연결이 필수이며, 연결이 불안전하거나 부하가 걸리면 사용하기 힘들다.On-premise는 직접 서버를 두는 방식으로, 법적으로 정보가 네트워크에 누출 되지 않은 경우나, 대규모로 구현할 수 있는 회사등에서만 사용 가능하며, 유지보수 비용이 크다.Edge device는 저비용, 높은 보안성, stand-alone 동작과 네트워크 연결 여부를 선택할 수 있지만, 연산량이나 전력 소모, 메모리가 제한된다는 단점이 있다.(Dumb and fast)최근에는 Rasberry Pi, Jetson nano 처럼 합리적인 가격으로 사용해 볼 수 있다.6. Edge intelligence[img. Cloud intelligence vs edge intelligence][img. edge intelligence의 내부 분류]1) Edge trainingEdge device에서 모델을 학습하는 것,아직 Edge device 들의 성능이 떨어져 산업에서 사용하진 않는다.[img. Inference vs Training에 드는 비용 비교]2) Edge cachingEdge가 처리하기 힘들지만 외부에서 연결을 통해 가져오기 힘든 정보를 연결없이 저장공간에 저장, 또는 불러오기이번 강의에서 다루진 않음컴퓨터 구조에서 메모리에서 hit과 miss에 대한 개념을 비슷하게 사용3) Edge offloadingEdge와 가까운 곳에 있는 Edge 서버(클라우드와 Edge의 중간 형태)로부터 연결하여 정보를 가져옴, 하드웨어와 비슷한 개념4) Edge Inference수업에서 주로 다룰 분야, Edge에서 Output을 출력[img. model 설계의 컴파일 수준]동전의 뒷면(The flip-side of the coin: on-device AI)topnvidia-smi # Gpu 상황watch -n 0.5 nvida-smivmstat -h# sudo apt-get install lm-sensorssensors # 발열 체크# sudo apt-get install atopatop# sudo apt install sysstatmpstatsar 3sar 3 4[code. system resource monitoring 방법 (Ubuntu 환경)]AI 모델은 입력에 대해 정확한 출력을 보장하지 않음Underspecification : 모델의 학습 결과가 매번 다름, 잘 결과를 내는 데이터가 다름모델링 뿐만 아니라 여러가지 모두 중요하다.[img. 실제 ML project cycle vs 우리가 상상하던 것][img. 모두 모델링을 하고 싶어하고 데이터 작업은 피해요 - ML project에서의 문제는 모두 하기싫어하고 천시하는 부분에서 터진다.]가장 적당하게 (Optimization)제약조건 하에서의 의사결정(Decision-making under constraints)사용가능한 자원 내에서 목적 달성 해야함from scipy.spatial import distancedistance.euclidean([1,0,0], [0,1,0]) # euclidean 거리distance.hamming([1, 0, 0], [0, 1, 0]) # hamming 거리distance.cosine([1, 0, 0], [0, 1, 0]) # cosine 거리# loss 구할 때 필요하곤 함[code. scipy를 이용한 거리 제기]문제란, 바라는 것과 인식하는 것 간의 차이[img. 문제에 대한 그림][img. 문제의 해결 과정 예시]계산(Compute)이란, 유한한 자원과 시간 안에 decision을 통하여 Initial state부터 Terminal state까지 진행하는것 .[img. 수학적 증명과 IT project의 해결과정은 비슷하다]Decision problem : 목적만 존재 (어떻게 MST를 만드는가?)Opitmization problem : 자원과 step에 제한이 걸린 상태에서 목적을 달성하는 법(최소 가중치의 MST를 어떻게 만드는가?)모델의 시공간(Timespace of ML model)search space: 답이 될 수 있는 decision, state들의 집단, state들의 합, solution space라고도 함ex) 바둑의 기보, 알고리즘 stepproblem space: 처음 문제가 정의되어 있는 공간, state, initial state라고도 함ex) 비어있는 바둑판, 알고리즘 Input또한, 시간을 희생해서 공간 사용량을 줄이거나 공간 사용량을 늘려 시간을 줄일 수 있는 경우가 많다.[img. Time과 space의 trade off]Time Complexity : Input size에 따라 얼마나 Problem solving Time이 늘어나는가?기존의 알고리즘은 P 문제까지 손쉽게 해결 가능하다.NP 문제부터는 머신러닝으로 해결할 수 있다.[img. 시간 복잡도에 따른 문제 분류]entropy : 무질서도 그리고 정보, 놀라움, 불확실성의 레벨문제 해결은 높은 entropy의 상태를 에너지를 투자하여 낮은 entropy로 바꾸는 것,[img. ML에서 Loss값을 줄이는 행위 또한 entropy를 낮추는 것이다.]parameter search과거의 도구는 인간이 직접 설계하고, 제작하여, 사용하여야 했다.공학과 에너지의 발전으로 인간이 직접 설계하고 제작하면, 자동으로 사용되는 기계가 생겨났다이후 프로그래밍의 발전으로 인간이 프로그램을 설계하면 프로그램의 제작과 사용이 자동화되는 시대가 되었다머신러닝을 통하여 Parameter Search를 통하여 프로그램의 로직을 자동으로 설계할 수 있게 되었고,앞으로 딥러닝과 먼 미래에는 Hyperparameter와 Architecture 또한 자동으로 설계가 되어, 완벽한 자동화가 이루어질 것이다.[img. 도구의 역사]딥러닝에서 데이터는 벡터나 다차원 array 등으로 다양하게 표현할 수 있다.[img. Data의 Vector 표현과 map 표현]Classification task의 경우 위에서 표현한 데이터들의 차원상의 Decision boundary를 결정하기 위해, layer를 통하여 차원에서의 표현을 바꾸는 과정을 거친다.[img. Decision boundary를 결정하기 위한 geomerty 변경]Hyperparameter searchHyperparameter search의 경우 parameter search와 다르게 하나의 weight가 아닌 전체적인 구조에 영향을 주는 경우가 많아 parameter search 보다 더욱 리스크와 cost가 크다. (ex) 레이어의 수, learning rate 등)이를 해결하기 위해 다양한 연구가 진행되고 있다.[img. Hyperparameter seach에 대한 연구]Hyperparameter 탐색 시마다 간격을 일정하게 주는 Grid search와 random한 값을 주는 Random search가 있으며, 기존의 Manual search와 비교해서 다음과 같은 장단이 있다.[img. hyper parameter search 방법][img. search 방법에 따른 성능 차이]또한 단순한 Random search 말고도, Hyperparameter에 따른 결과를 예측하는 모델을 만드는 방법인 Surrogate model이라는 방법이 있다.  Gaussain Process가 대표적인 방법  조금더 적은 비용으로 많은 탐색이 가능하다.[img. Surrogate model 도식][img. Gaussian model 예시]Neural Architecture search(NAS)NAS는 보통 사람이 직접 만든는 것도 포함하는 개념이지만 좁은 의미에서는 마치 parameter를 찾는 것 처럼, 효율적인 ML구조를 Neural Archtecture 또한 탐색을 통하여 찾는 방법이다.Multi-objective NAS의 경우, 성능 뿐만 아니라 최적화도 신경쓰므로, Optimization에도 뛰어나다.결과물은 사람이 이해할 수 없지만 괴상한 구조인 경우가 많다.이 때, 단순히 완전 탐색을 통하여 Search 하는 것이 아니라, NAS 전략을 위한 머신러닝 모델을 만들 수도 있다.[img. Automatic Neural Architecture Search]NAS for edge device  MnasNetedge device를 위한 NASmodel들을 샘플로, 실제 Mobile phone환경에서 실험한 결과를 reward로 하는 강화학습을 통하여 기존의 모델 보다 성능과 latency 를 개선함.Hyperparameter search를 하듯 Block 별로 layer의 종류와 parameter를 지정하고 search함.[img. MnasNet 논문 내부의 figure]  PROXYLESSNAS과거에는 Proxy를 이용해 NAS를 하였지만 이 방법의 결과가 실제 결과와 괴리가 있는 경우가 많으므로 ,PROXY를 사용하지 않은 NAS 방법  Proxy는 직접적으로 실제 모델을 학습하면서 NAS를 돌리면 너무나도 비용이 크므로 일종의 간략화된 가짜 모델을 이용해 학습하는 방법[img. PROXYLESSNAS의 figure]  ONCE-FOR-ALLTraining한 모델을 단순히 한 Device 뿐만 아니라 조금의 구조 변경을 통해 여러 Platform에 적용[img. ONCE-FOR-ALL의 비결, Pruning 방법과 비슷함]  MobileNet 시리즈CV 시간에 배운 Depth-wise Separable Convolution에 대한 내용나중에 또 나온다고함[img. Depth-wise Separable Convolution]압축압축 (Compression)\\[Compression\\ rate =\\frac{size\\ before\\ compression}{size\\ after\\ compression}\\][math. 압축율(Compression rate)의 정의]  손실 압축데이터를 눈치채지 못하는 수준의 손실시켜 압축압축률이 상당한 경우가 많으며, 손상되도 문제 없는 경우에 사용됨  예를 들어, 화질, 음질의 저하는 눈치 못채는 경우가 많다.  jpg, mp3, mp4, avi, gif 등이 있다.  MP3는 일반 WAV보다 약 12% 이하의 크기          가청주파수 이외의 음은 잘라내버리는 방식      [img. audio data의 손실 압축]  비손실 압축복원 시, 데이터가 손실되지 않은채로 완벽하게 복원됨압축률은 데이터마다 다르겠지만, 보통 손실 압축보다 덜압축됨  비손실 압축의 예시로 Run-length encoding (RLE)가 있다\\[AAAAAAAAAABBBBBBTTTTPPPPPP(27byte) = 10A6B5T6P(9byte),\\\\ compression\\ rate : \\frac{27}{9} ==3\\\\ABC(3byte)=1A1B1C(6byte)\\\\compression\\ rate :\\frac{3}{6}==0.5\\][img. RLE의 압축 방법]  zip, 7-zip, wav, flac, png 등이 있다.Huffman coding압축 알고리즘 중 하나문자에 대한 코드 map을 생성하여 문자대신 해당 코드로 압축, 이때 많이 등장한 단어에는 짧은 코드를, 적게 등장한 단어에는 긴 코드를 할당하여 압축률을 극대화[img. Huffman coding 중 하나]부호 (Coding)압축은 특정 정보를 인코딩(enCODING)하고 또, 그것을 디코딩(deCODING)하여 원본 크기로 바꿀 수 있을 때, 압축 크기가 원본크기가 작으면 압축이라고 정의한다.즉 압축은 Coding의 일부Finite State Machine(FSM)상태를 의미하는 정점과 상호작용을 의미하는 간선들로 객체의 상태와 변화, 동작을 표현하는 방법각 정점 또한 초기 상태, 중간 상태, 종료 상태로 나뉜다.컴퓨터 그 자체와, 그와 관련된 개념들이 이를 이용한다.ex) Client-Sever 관계(Client의 Request와 Server의 Response)DB의 데이터에 대한 CRUD 로직알고리즘 문제의 Input state, Step, Output state등이 존재함부호화(EnCoding)한다는 공통점이 존재[img. FSM의 그림]머신러닝 또한 Input data를 encoding 해서 output을 만드는 Coding 이다.[img. 머신러닝 모델 생성의 FSM 표현]부호화 (Encoding)KL divergence는 Loss를 많이 구하는데 사용하지만, 엄밀히 말하면 entropy의 차이를 측정할 수 있는 방법이며, 압축 등의 효율을 구하는 데도 사용한다.P분포와 Q분포의 차이를 찾음[img. Mnist Model의 예측 결과]\\(D_{KL}(P\\|Q)=\\mathbb{KL}(P\\|Q) = -\\mathbb{E}_{x\\sim P(x)}[logQ(x)] + \\mathbb{E}_{x\\sim P(x)}[logP(x)]\\\\=-\\sum^9_{i=0}P(i)\\ln\\frac{Q(i)}{P(i)}=\\sum^9_{i=0}[(-P(i)\\ln Q(i))-(-P(i)\\ln P(i))]\\\\ 크로스\\ 엔트로피: -\\mathbb{E}_{x\\sim P(x)}[logQ(x)], 엔트로피:\\mathbb{E}_{x\\sim P(x)}[logP(x)]\\\\\\\\파란\\ 예측 : D_{KL}(P\\|Q)=0.5108\\\\빨간\\ 예측: D_{KL}(P\\|Q)=0\\)[img. KL divergence를 통한 Loss값 측정 및 결과]머신러닝이 Loss값을 줄이는 행위 또한 Encoding이라고 함압축에서의 Cross-entropy의 의미는 q의 분포 코드 북으로 p 코드북을 사용하는 문장을 해석 했을 때의 평균 길이[img. 압축에서의 Cross-Entropy의 의미]압축률(Compression rate)머신러닝의 압축에서 다음과 같이 정의할 때                   Original Model      Compressed model                  Models      $M$      $M^*$              The number of the parameters      $a$      $a^*$              The running time      $\\mathcal{s}$      $\\mathcal{s}^*$      [img. 압축된 모델의 정의]각 성능을 의미하는 지표를 구하는 방법은 다음과 같다.\\(Compression\\ rate: \\alpha(M,M^*)=\\frac{a}{a^*}\\\\Space\\ saving\\ rate: \\beta(M,M^*)=\\frac{a-a^*}{a^*}\\\\Speedup\\ rate: \\delta(M,M^*)=\\frac{\\mathcal{s}}{s^*}\\)[img. 성능 비교용 지표 ][img. 각 모델 성능의 비교 ]DEEP Compression 논문에서 Pruning, Quantization, Huffman coding을 이용한 모델 압축 후의 성능과 압축율에 대한 정보가 나와있다.[img. 기존 모델의 압축과 압축에 따른 성능 비교]압축이 크게 된 Layer는 쓸모없는 Parameter가 많았다는 의미이다.[img. AlexNet의 Layer 별 압축 결과][img. Compression 비율에 따른 정확도 그래프]TFlite를 사용하면 크게 압축 된다.Acceleration우리가 배운 pytorch는 framework 수준이며 실제 구동에서는 더욱 더 낮고 Low level로 compile 되게 된다.[img. DL Software~Hardware 까지]AccelerationNumpy는 C 기반이라 병렬적으로 처리가 가능, 메모리에 효율적으로 배치되어서 Python List보다 훨씬 빠름Python은 Intepreter 언어C는 Compile 언어[img. Bandwidth와 Throughput에 대한 개념][img. Latency에 대한 개념]Bandwidth는 시간당 얼마나 CPU가 데이터를 주고 받을 수 있는가에 대한 능력Latency는 정보가 전송될 때 지연되는 시간(메모리간의 속도 차이, 데이터의 병목, 너무 많은 명령 처리 등으로 인해)Throughput은 실제 데이터가 전송되는 양Parallel processing은 Throughput이 좋아지며, latency와 badnwidth는 변화없다.[img. ML에서의 가속 방법]이때, 모델의 속도를 개선하기 위해, Multithreding 디자인 또는 Low-Precision arth metic, data flow 개선, in-memory computing capabilty 증가 등의 방법이 있다.Hardwares (chip)하드웨어 가속(Acceleration)을 통하여 latency를 줄이고, throughput을 늘리는 효과를 가질 수 있다.  이때, 하드웨어 가속(Acceleration)은 잘 수행할 수 있는 특정 목적에 맞게 하드웨어를 이용하거나 설계하는 것이다.[img. about hardware acceleration]Processing Unit은 많은 종류가 있다, ARM이 대세            명칭      의미      설명                  CPU      Central Processing Unit      범용              GPU      Graphic ProcessingUnit      이미지, 컴퓨터 그래픽, 게임              DPU      Data ProcessingUnit      데이터              TPU      Tenor ProcessingUnit      2차원 이상의 array, NPU에 속함              NPU      Neural ProcessingUnit      딥러닝              IPU      Inelligence ProcessingUnit      그래프코어에서 생산              VPU      Vision ProcessingUnit                     FPGA      Field Programmable Gate Array      조립 가능, 아두이노 같은 것              SoC      System on Chip      CPU,GPU,Memory,Camera등 모든 기능을 탑재              ASIC      Applicatioin Specific Integrated Circuits      특정 목정만을 위한 칩(냉장고, 밥솥 등)              Neuromorphic IC      Neuromorphic Integrated Circuit             [img. Processing uint 차이]IPU는 메모리와 코어와의 거리가 짧아 병목현상과 Latency가 적다.[img. 가장 많이 사용되는 4개의 Processing uint]CPU는 범용 목적이며, 순차적인 업무에 전반적으로 성능이 좋음GPU는 병렬 처리에 유리하며 그래픽 업무, 병렬 처리가 필요한 ML에 좋음[img. CPU vs GPU]Compression &amp; AccelerationCompression : 보통 소프트웨어 단에서 용량을 줄이는 등에 사용 (Space complexity로 계산)Acceleration : 보통 로우레벨 또는 하드웨어 단에서 연산속도, Latency 등을 개선하는데 사용 (Time complexity로 계산)칼같이 구분하진 않음[img. Compression과 Acceleration의 차이]하드웨어를 고려한 Compression이 더욱 효과적임하드웨어 + 소프트웨어 co-designDeep learning compilerDL compiler는 다른 framework에서 만들어진 DL model을 Input으로 받아서 여러 hardware에 최적화된 DL code를 출력하는 compiler다.  XLA, TVM, GLOW, ONNC 등이 존재함multi-level IRs : 여러 단계를 걸쳐 시행frontend/backend optimization : High level &lt;-&gt; lowlevel 구조  frontend/high level은 하드웨어에 Independent 하다  backend/low level은 하드웨어에 따라 다르다.[img. High level과 low level의 예시]LLVMLLVM IR을 사용하여 언어 &lt;-&gt; 하드웨어 간의 번역을 통합함컴파일러 수가 크게 줄어듦[img. LLVM의 장점]MLIR(Multi-Level Intermediate Representation)머신러닝 컴파일러 생성을 도와주는 library[img. 자세한 ML 모델의 컴파일 과정]compiler backend optimization 예시polytotype method : loop 문에 관한 optimization 방법Locality of reference : 사용했던 데이터는 또 다시 쓸 확률이 높으므로, 메모리 위치를 조정해줌  Temporal locality  spatial locality  Branch locality  Equidistant locality[img. Hardware단의 compile의 여러 예시]Structured pruning + Mixed precision quantization = Harware-ware compression단순히 소프트웨어 만으로 최적화하지 않고 hardware에 맞추어 최적화하는 것,[img. Hardware-aware compression ][img. compression의 최신 트랜드]가지치기(Pruning)Weighted sum model\\[S = \\sum^N_iw_ix_i=(w_1,w_2,\\dots,w_N)\\cdot(x_1,x_2,\\dots,x_N)=W\\cdot X, (W,X) \\in \\mathbb{R}^{N\\times N}\\][math. weighted sum 공식]Pruning이란?중요한 node만 남기는 방식으로 optimization장점으로 추론 속도증가, 모델의 complexity 감소, Regularization으로 인한 generalization 성능 향상단점으로 정보의 손실압축 형식, 하드웨어 기반 optimization이 까다로워짐, acc loss그다지 중요하지 않은 정보는 weight가 0에 가깝게 생성되므로, 큰영향을 끼치지 않는다.weight가 0에 가까운 neuron을 pruning하여 opimization할 수 있다.[img. Pruning의 결과]Dropout 또한 Regularization을 위해 사용하며, 랜덤으로 Drop하게 되면 다른   모델처럼 되므로 앙상블과 비슷한 효과를 가질 수 있다.Dropout과 Pruning의 차이는 Dropout은 매번 training마다 랜덤하게 끄고, Inference 시 다시 켜지지만, Pruning은 아니다.[img. Dropout][img. Pruning(좌), Dropout(우)][img. Prunnig과 Dropout의 수식적 차이][img. Pruning시, Iterative하게하고, retrain을 하면 성능을 좀더 보존할 수 있다.][img. Regulariazation에 대한 설명]여러가지 Pruning들Pruning 방법은 분류에 따라 아주 많다  Global Magnitude Pruning, Layerwise Magnitude Pruning, Random Pruning 등등…[img. Prunning의 분류 방법]Unstructured VS Structured PruningUnstructured pruning은 weight 연산을 구분없이 사라지게 하며​\t- Neuron이 성긴 모양으로 생기게 되는 장점Structured pruning은 커널이나, Layer 등의 구분을 두고, 해당 구분에서 통째로 사라지게 만든다.- 하드웨어 등에서 좀더 잘 Pruning 해준다.[img. Unstructure Pruning vs structured pruning]Scratch-trained는 시작부터 다시 트레이닝 하는 거며,Fine tuning은 Pretraing 된 것을 pruning 한것,Unstructed Pruning은 Finetuning을 하여도 0 주변 weight가 되살아나지 않는다.[img. 두 방법의 결과 그래프]한꺼번에 Pruning 하면 Retraining 해도 Accuracy가 다시 오르지 않는다고 한다.[img. 한꺼번이 아니라 조금씩 pruning한 결과][img. 자세한 Pruning 과정]Lottery Ticket HypothesisLottery Ticket Hypothesis란, 같은 accuracy를 가진 Pruning한 network가 원래 기존의 원본 network 안에 subnetwork로 존재했을 것이라는 가설.즉, 굳이 Pruning을 하지 않아도 처음부터 Pruning한 network 구조를 알 수 있으면 처음부터 Pruning한 Network(Lottery ticket)를 가지고 올 수 있다.가설이므로 아직 증명은 안됨[img. Lottery Ticket Hypothesis에 대한 개념]retraining시 처음 Initial state의 W로 시작하지 않고, 어느정도 train된 $W_k$부터 시작하는 방법,랜덤화를 위해 조금의 noise를 섞어준다.어느 정도 training 구간을 skip 할 수 있으므로 training cost를 줄이면서 성능도 좋아진다.[img. Lottery Ticket을 구하는 방법 중 하나인 Rewinding][img. Rewinding algorithm]양자화 (Quantization for model compression)Fixed point, floating point32-bit fixed-point는 앞에 지수 10자리를 정수부분, 뒤의 22 자리를 소수점을 표현하는데 사용  덧셈과 뺄셈시 erorr가 없음, 정수 분수 표현 가능      연산 속도가 빠르고 메모리 사용량, 전력 사용량이 적음    곱셈이 힘들고(수가 자주 표현범위를 넘어가므로), 수의 표현 범위가 작음32bit floating-point는 가수부에서 일의 자리 + 소수점 부분을 표현하며, 앞의 지수부에서 10의 지수를 표현함  비교적 많이 사용되며, 아주 넓은 범위의 수를 손실없이 표현 가능  hardware에 구현이 힘듦, 특히 ARM에서 구현 힘듦  과거에는 FPU(Floting point precessing unig)라는 CPU와 별개의 연산 장치가 연산했지만 요즘에는 CPU에 내장[img. fixed-point vs floating point]소수점 연산은 수가 단계에 맞아떨어지는 정수와 달리 연속적인 수이므로 연산이 힘들고 복잡하다.Quantization의 아이디어는 float으로 이루어진 AI weight를 연산이 빠른 정수형으로 바꾸는데에 있다.[img. 소수점 연산의 힘듦 : CPU 내의 FPU의 크기]정확도 vs 정밀도정확도(Precision) : 정답과 값들의 거리정밀도(Variance) : 값 간의 모임 거리[img. Variance vs bias]양자화(Quantization)모델크기 감소, Inference 시간 감소에 효과적이며, 메모리 bandwidth 요구에 맞추는데 사용기존에 float32형태의 weight 값들에서 int8형으로 mapping하여 속도의 향상을 얻을 수 있다대신 lossy conversion이므로 정보가 조금 손실된다.[img. 8비트 정수값에 맞게 mapping한 Quantization(아래)과 그에 따른 에러 그래프(위)][img. Deep Learning은 아주 많은 compuation을 필요로 한다.]Affine quantizationAffine transformation : 공간 dimension이나 평행 선의 길이의 비율(parallel line segemnt)은 유지한채로 affine space로 변환하는 변환모양은 유지한 채로, 비율만 변하는 변환? “닮음 변환”Affine map은 결합 법칙이 성립함 $m_f(x-y)=f(x)-f(y)$ex) $y=\\sigma\\left(\\sum_{i=1}^nx_iw_i + b\\right): ML의\\ 선형변환의\\ Activation\\ 적용\\ 전까지$[img. Affine quantization의 예시][img. Quantization mapping의 과정]Quantizing activation and weightsWeight뿐만 아니라 Activation또한 Quantization이 가능하다.[img. 정수값마다 계단형으로 바뀐 activation]하지만 Activation을 Quantization할 경우, Backpropagation 시 문제가 생긴다.이를 해결하기 위해,1) 계단식의 그래프와 가장 비슷한 기울기 1의 그래프로 쳐서 Backpropagation 한다.2) 또는, Quantization을 할 때 smothing을 주어 미분가능한 형태로 만든다.[img. Quantization에 의한 Backpropagation 문제 해결 1][img. Quantization에 의한 Backpropagation 문제 해결 2]DoReFa-Net style 3-bit activation quantizer function 이나Binarized Neural Networks 처럼8단계(3bit)나 극단적으로 1bit(0,1)로만 activation을 바꾸는 방법도 있다.Quantization의 종류  Quantization 대상에 따라 : Weight, Activation  Quantize 방법에 따라 : Dynamic(DQ, Acitvation들을 Inference 할 때 그 순간만 양자화, Weight는 처음부터 양자화 ), Static(양자화된 채로 아예 변형)  Quantize 강도에 따라 : 1bit~16bit, Mixed-precision  Quantize 시기에 따라: Post-training(PTQ, Training된 모델에 Quantize, 파라미터 size가 클수록 성능 하락 적음), Quantization aware training(QAT, Training 하면서 Quantization을 시뮬레이션, 성능하락 적음)[img. Quantization의 종류 구분]DQ, PTQ, QAT 모두 원본 Model과는 성능이 똑같지 않다.[img. DQ, PTQ, QAT에 대한 도식]GPU, TPU 등의 큰 분류 뿐만 아니라, 제조사, 제조 모델, 운영체제나 라이브러리에 따라 지원하지 않을 수도 있다.[img. Quantization 들의 장점과 하드웨어 제한][img. 여러가지 방법을 섞어서 Quantization을 할 수 있다.]Hardware-aware-quantization하드웨어에 따라 가능한 Quantization 방법이 다르고, 적절한 방법도 다르다.강화학습과, 성능 측정을 통해 하드웨어 적합한 Policy를 찾아 적용 가능하다.[img. bit 크기에 따른 quantization의 성능 비교][img. 강화학습을 이용한 Hardware-aware quantization]Quantization 결과 테이블 읽기주로 여러가지 방법의 Quantization 간의 Latency와 용량, accuracy를 비교해본다.Resnet의 경우 Batch Normalization layer 때문에 안되는 것으로 추정[img. Quantization 결과][img. Flexible bit를 이용한 Quantization 결과 비교 예시]지식 증류 (Knowledge distillation for network compression)Knowledge[img. Knowledge의 정의]$C_K$는 Reference class(모든 확률을 더한 값?)[img. logit, softmax, sigmoid의 정의]Knowledge distillationtraining 시와 deploment 간 필요한 setting과 parameter가 다르다.그러므로 deploy를 위한 compressed된 model이 따로 필요하다.[img. train model과 deploy 모델 차이]Transfer learning은 다른 도메인의 모델을 Task를 위해 기존의 모델을 통해 learning 하는 것이며,Teacher-Student model은 같은 도메인 내에서 model size를 줄이기 위해 Learning 한다.[img. knowledge distillation 방법 중 하나인 Teacher-student model]Teacher-Student networks &amp; Hinton lossTeacher-student 모델로 생성된 최적화된 모델은 데이터에서 직접 트레이닝한 모델보다 비슷한 크기, 비슷한 파라미터 하에, 성능이 더 좋은 경향이 있다.Teacher의 결과와의 차이인 distllation loss와 Ground-Truth와의 차이인 student loss를 합쳐서 종합 Loss(Hinton loss)를 만든다.이때 $p_i$를 통하여 결과값인 class별 확률을 soft하게 만들어준다.  $p_i$의 T는 Temperature로 knowledge distillation의 핵심이다.[img. Teacher-Student 모델의 개념도]기존의 one-hot vector의 경우나, 확률 편차가 극도로 큰 ouput은 정보가 많이 없는 반면,soften한 output의 경우, 개와 고양이의 정보값이 큰것으로 보아, 개와 고양이를 조금 헷갈려 한다는 것을 알 수 있다.(Dark knoweldge)이 정보를 이용하기 위해 결과값을 soften 한다.  다 합쳐서 1이 되는 것은 모두 같아야 한다, 아래 그림은 나머진 숨긴 것[img. 결과값을 soft하게 하는 이유][img. T의 크기에 따른 softmax output]Zero-mean assumptionknowledge distillation == model compression이 되려면 zero-mean Assumption이 True여야 한다.zero-mean Assumption이란, logit들이 모두 Zero-mean일 경우를 의미한다.\\(\\frac{\\partial C}{\\partial z_i}=\\frac{1}{T}(q_i-p_i)=\\frac{1}{T}\\left(\\frac{e^{z_i/T}}{\\sum_je^{z_j/T}}-\\frac{e^{v_i/T}}{\\sum_je^{v_j/T}}\\right)\\\\\\approx\\frac{1}{T}\\left(\\frac{1+z_i/T}{N+\\sum_jz_j/T}-\\frac{1+v_i/T}{N+\\sum_jv_j/T}\\right)\\\\\\approx \\frac{1}{NT^2}(z_i-v_i) : compression\\\\z_i:distilled\\ model\\ logits, v_i:teacher\\ model\\ logits\\)[math. zero-mean assumption이 참이 되어야 성립하는 식]여러 distillation들이외에도 여러 distillation 방법들이 존재한다.[img. 여러 distilation 기법, 우리가 배운 것은 밑줄 친 BLKL]예를 들자면,Overhaul Feature Distillation  중간에 Distillation 상황을 점검하고 결과값을 비교하는 Layer을 추가,  feature를 뽑는 distillation output의 위치 ReLU이전으로 조정,  partial L2 distance 제안Data-Free Knowledge Distillation  데이터가 적거나 없을 때 사용할 수 있는 방법  meta data로 data를 reconstruction 하여 사용함          Pruning, Quantization, Matrix factorization 등의 방법도 data-free임.      행렬 분해(Low-rank approximation for model compression)Three maps  Matrix 또는 Tensor는 data modeling tool 이다.Matrix와 달리 Tensor는 2차원 이상의 Matrix를 의미함.[img. Matrix 또는 Tensor의 해석 방법 ]  Matrix(Tensor)는 linear transformation(선형 변환) (map)[img. 차원공간에서의 위치 변경이 가능함]  terminology[img. 행렬에 대한 전문용어]행렬을 통하여 연립방정식 또한 표현 가능하다.\\(\\left\\{\\begin{matrix}x+2y+z=5\\\\-2x-3y+z=8 \\\\3x+5y+0z=2 \\\\\\end{matrix}\\right. \\Leftrightarrow \\begin{bmatrix}1 &amp; 2 &amp; 1\\\\ -2 &amp; -3 &amp; 1\\\\ 3 &amp; 5 &amp; 0\\end{bmatrix}\\begin{bmatrix}x\\\\ y\\\\ z\\end{bmatrix}=\\begin{bmatrix}5\\\\ 8\\\\ 2\\end{bmatrix}\\)[img. 연립방정식의 행렬에 의한 표현]Gaussian eliminationbasis : n차원 공간을 결정하기 위한 n개의 벡터의 집합을 basis라고 한다고 함.rank + nullity로 matrix의 dimension을 구할 수 있음[img. Rank를 구하는데 사용][img. filter, matrix, tensor의 처리 방법]Low-rank matrix approximations : large-scale learning problem kernel method에 필요한 툴Kernel methodkernel: a central or essential part[img. CNN에서의 Kernel]kernel method를 통하여 고차원으로 변환할 때 연산량을 줄일 수 있다.  저차원에서 내적값을 미리 계산한 뒤, 변환[img. kernel method의 고차원 변환에서의 사용][img. kernel method의 적용 연산 비교]연산량이 줄어들지만 연산결과가 완전 같지는 않다.optimization에서 많이 사용함[imgs. ML에서 low-rank approximation을 이용한 파라미터 감소 예시]Matrix decomposition행렬을 n개의 행렬의 곱으로 나타내는 것,approx 값일 때도 있고, equal 값일 때도 있다.ML이전에 PCA, SVD 등을 이용한 추천 시스템 등에서 많이 사용함[img. Matrix decomposition의 예시]Eigen Vector: 변환 시켰을 때, 길이만 변하는 벡터Eigen Value: 변환 시켰을 때, 길이만 변하는 벡터가 변한 길이의 량위 두 개념을 이용해 행렬을 변환시킬 수 있다.[img. Eigenvalue Decomposition]Singular value decomposition (SVD): a generalization (nm case) of EVD가로 세로 길이가 다른 행렬을 가로 세로 길이를 뒤바꾼 형태로 transpose하여, 정행렬과 정행렬의 transpose, 그리고 singular value의 제곱($\\sum^2_{nn}$= Eigen value)으로 나타내는 방법다항식의 인수분해로 비유할 수 있음[img. SVD 예시][img. SVD를 이용한 PCA 차원축소][img. 다양한 방식의 decomposition]Tensor decomposition[img. Tensor의 예시]각각 다차원 matrix나 tensor는 outer product를 통해 rank 1 짜리 matrix나 tensor로 표현 가능[img. Matrix decomposition]CP (Canonical Polyadic) decompositiontensor 버전의 SVD (Rank1 짜리 tensor의 linear combination 표현)Tucker의 경위 CP의 일반화된 버전 (Rank 1은 아니지만 조금더 저차원의 tensor로 표현)두뇌의 신경 구조 분석 등을 위해 사용[img. CP의 예시]Tensor decomposition on network compression모바일넷의 선형대수적 증명과 모바일넷 v2의 차이에 관한 논문에 대한 설명network compression은 위에서 설명한 decomposition을 이용해 설명할 수 있는 경우가 많다."
  }
  , 
  
  "/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html": {
    title: "AI 수학 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","MATH"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueAIMathVector벡터란?      숫자를 원소로 가지는 리스트 또는 배열을 의미          벡터의 코딩상 의미                $X$ = 열 벡터, $X^T$ = 행 벡터        수학적으로는 공간에서의 한 점, 원점으로 부터의 상대적 위치를 의미          벡터의 수학적 의미        각 벡터가 가지는 행과 열의 수를 벡터의 차원이라고 한다.벡터의 성질1. 벡터에 양수를 곱해주면 방향은 그대로, 길이만 변한다.      이 때 곱해주는 숫자를 스칼라곱($\\alpha$)이라고 표현한다.        스칼라곱이 음수이면 방향이 정반대 방향이 된다.        벡터의 길이는 1보다 크면 길이가 증가, 1보다 작으면 길이가 감소한다.2. 같은 모양(같은 행과 열)을 가지면 덧셈, 뺄셈, 곱셈, 나눗셈이 가능하다.    - 이때의 곱셈을 성분곱(Hadamard product)라고 한다.    - numpy array에도 적용된다.3. 벡터와 벡터의 덧셈과 뺄셈은 다른 벡터로부터 상대적 위치 이동을 표현함.  벡터 노름(norm)과 기하학적 성질벡터의 노름      벡터의 노름(norm)은 원점에서부터의 거리를 의미          차원의 수와 관계없이 모든 벡터는 노름을 구할 수 있다.      L~1~-노름은 각 성분의 변화량의 절대값의 합을 의미                  $(x,y)$는 $|x|+|y|$ 만큼 거리                    L~2~-노름은 피타고라스 정리를 이용해 유클리드 거리를 계산                  $(x,y)$는 $\\sqrt{|x|^2 +|y|^2}$를 의미          np.linalg.norm으로 구현 가능                    $\\parallel \\parallel $ 기호는 노름이라고 부름    \\[\\parallel x\\parallel_1 = \\sum_{i=1}^d|x_i|\\\\\\parallel x\\parallel_2 = \\sqrt{\\sum_{i=1}^d|x_i|^2}\\\\\\]    백터 노름의 코드 구현def l1_norm(x):    x_norm = np.abs(x)    x_norm = np.sum(x_norm)    return x_normdef l2_norm(x):    x_norm = x*x    x_norm = np.sum(x_norm)    x_norm = np.sqrt(x_norm)    return x_norm노름의 활용과 성질      노름의 종류를 무엇으로 적용하느냐에 따라 기하학적 성질이 달라진다.          L1 노름은 마름모 모양의 원을 그리며, L2 노름은 기존의 원 모양의 원을 가진다.      노름의 종류에 따라 머신러닝에서의 활용도 달라진다.            노름을 이용해 두 벡터 사이의 거리를 뺄셈을 통하여 계산할 수 있다.              $\\parallel y - x\\parallel  = \\parallel x - y\\parallel$ 인점을 이용하게 된다.      즉 x-y 또는 y-x를 한 원점에서의 좌표를 이용해 L2 norm을 구하면 두벡터 사이의 거리가 나온다.            L2 노름 한정으로 내적을 이용해 이렇게 구한 벡터사이의 거리를 이용해 각도 또한 계산 가능하다.              &lt;x, y&gt; 는 내적(inner product)을 의미하며 성분 곱들의 합을 의미한다.                  예를 들어 x = (0, 1), y = (0, 2)의 내적은  0 * 0 + 1 * 2 = 2 이다.                      내적을 이용한 각도 계산def angle(x,y):    # np.inner(x, y)가 내적을 구하는 numpy 함수    v = np.inner(x, y) / (l2_norm(x) * l2_norm(y))    theta = np.arccos(v)    return theta내적의 해석      내적은 정사영(orthogonal projection)된 벡터의 길이와 관련 있다.          Proj(x)는 벡터 y로 정사영된 벡터 x의 그림자를 의미한다.      Proj(x)의 길이는 코사인법칙에 의해      \\[\\parallel x \\parallel cos\\theta\\]      가 된다.            이때 내적은 정사영의 길이를 벡터 y의 길이 $\\parallel y \\parallel$만큼 조정한(곱한) 값이다.        내적을 이용해 유사도(similarity)를 구할 수 있다.  Matrix행렬이란?  벡터를 원소로 가지는 2차원 배열  행렬의 수식 표현\\[X = \\begin{bmatrix} 1 &amp; -2 &amp; 3 \\\\ 7 &amp; 5 &amp; 0 \\\\ -2 &amp; -1 &amp; 2\\end{bmatrix}\\]  행렬의 코드 표현x = np.array([[1, -2, 3], [7, 5, 0], [-2, -1, 2]]) # numpy에선 행(row)이 기본단위\\(\\boldsymbol{X} = \\begin{bmatrix}symbol{x_{1}} \\\\symbol{x_{2}} \\\\symbol{\\vdots}\\\\symbol{x_{n}} \\end{bmatrix} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1m}\\\\ x_{21} &amp; x_{22} &amp; \\dots &amp; y_{2m}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{nm}\\end{bmatrix}\\begin{aligned}\\boldsymbol{x_{1}}\\\\symbol{x_{2}}\\\\symbol{x_{7}}\\end{aligned}\\)      n x m 행렬의 표현        행렬은 행(row)과 열(column)이라는 인덱스(index)를 가집니다.        행렬의 특정 행이나 열을 고정하면 행 벡터 또는 열 벡터라 부른다.        전치 행렬(transpose matrix) $X^T$는 행과 열의 인덱스가 바뀐 행렬을 의미함.        벡터 또한 동일하게 행과 열이 바뀐 전치 벡터가 존재한다.  행렬의 이해1. 첫번째 의미  벡터가 공간의 한점을 의미한다면 행렬은 공간에서 여러 점들의 집합을 의미함.  행렬의 행벡터 $x_i$는 i번째 데이터를 의미함.  행렬 $x_{ij}$는 i번째 데이터의 j 번째 변수값을 의미함.  벡터와 마찬가지로 같은 모양을 가지면 같은 인덱스 위치끼리 덧셈, 뺄셈. 성분곱을 계산할 수 있다.  벡터와 마찬가지로 스칼라곱($\\alpha$) 또한 가능하다.2. 두번째 의미  행렬은 벡터 공간에서 사용되는 연산자(operator)를 의미.  행렬 곱을 통해 벡터를 다른 차원의 공간을 보낼 수 있음          행렬을 X벡터와 곱하면 m차원에서 n차원 벡터로 변환되어 n차원의 z벡터가 됨.      이를 통해 맵핑, 디코딩 등이 가능함.      이를 선형 변환(linear transform)이라고도 함.      딥러닝은 선형 변환과 비선형 변환의 합성으로 이루어짐        패턴 추출, 데이터 압축 등에도 사용함.행렬의 곱셈(matrix multiplication)과 내적1. 행렬의 곱셈(matrix multiplication)  행렬 곱셈은 i번째 행벡터와 j 번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 만듭니다.  고로 행과 열의 갯수가 같아야 가능하다.  행렬의 곱셈 code 구현 시X = np.array([[1, -2, 3],            \t[7, 5, 0],            \t[-2, -1, 3]])Y = np.array([[0, 1],            \t[1, -1],            \t[-2, 1]])print(x @ Y) # numpy에선 @ 연산으로 행렬 곱셈 계산# array([[-8, 6],#        \t[5, 2],#         \t[-5, 1]])2. 행렬의 내적  np.inner는 i번째 행벡터와 j번째 행벡터 사이의 내적을 성분으로 가지는 행렬을 계산합니다.  수학의 행렬 내적 $tr(XY^T)$과 다름  행렬의 내적 code 구현 시X = np.array([[1, -2, 3],            \t[7, 5, 0],            \t[-2, -1, 3]])Y = np.array([[0, 1, -1],            \t[1, -1, 0]])print(np.inner(x, Y)) # numpy에선 np.inner() 함수로 행렬 내적 계산# array([[-5, 3],#        \t[5, 2],#         \t[-3, -1]])역행렬의 이해  행렬 A의 연산을 같은 연산으로 거꾸로 돌리는 행렬을 역행렬(Inverse matrix)라고 부르며, $A^{-1}$라 표기한다.  행과 열 숫자가 같고 행렬식(determinant)가 0이 아닌 경우에만 계산 가능.  역행렬과의 행렬곱의 결과\\[AA^{-1} = A^{-1}A = I(항등행렬)\\]  항등행렬(Identity Matrix)은 곱하게 될 시 자기 자신이 나오는 행렬이다.  역행렬의 코드 구현Y = np.array([[1, -2, 3], [7, 5, 0], [-2,-1,2]])print(Y @ np.linalg.inv(Y)) # np.linalg.inv(Y) Y 행렬의 역행렬이 리턴# array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) # 정확히는 float으로 비슷한 값이 나온다.  역행렬을 계산할 수 없는 조건이라면 유사역행렬(pseudo-inverse) 또는 무어-펜로즈(Moore-Penrose) 역행렬 $A^ +$을 이용한다.  유사역행렬의 성질\\[n \\geq m 인\\ 경우, \\  A^+ = (A^TA)^{-1}A^T,\\ A^+A = I\\\\  n \\leq m 인\\ 경우, \\  A^+ = A^T(A^TA)^{-1},\\ AA^+ = I\\\\\\]  순서를 바꾸면 결과가 달라지므로 유사역행렬의 순서에 주의!  유사 역행렬의 코드 구현Y = np.aray([[0, 1], [1,-1], [-2,1]])print(Y @ np.linalg.pinv(Y)) # np.linalg.pinv(Y) Y 행렬의 유사역행렬이 리턴# array([[1, 0], [0, 1]]) # 정확히는 float으로 비슷한 값이 나온다.행렬의 응용1. 연립방정식 풀기\\[a_{11}x_1 + a_{12}x_2 + \\dots + a_{1m}x_{m} = b_{1}\\\\a_{12}x_1 + a_{22}x_2 + \\dots + a_{2m}x_{m} = b_{2}\\\\\\vdots\\\\a_{n1}x_1 + a_{n2}x_2 + \\dots + a_{nm}x_{m} = b_{n}\\\\n \\leq m \\ 인\\ 경우:\\ 식이\\ 변수\\ 개수보다\\ 작거나\\ 같아야\\ 함\\]sol)  n이 m보다 작거나 같으면 무어-펜로즈 역행렬을 이용해 해를 하나 구할 수 있다.\\(Ax = B \\\\\\Rightarrow x = A^+b\\\\=A^T(AA^T)^{-1}b\\)2. 선형회귀분석      np.linalg.pinv를 이용하면 데이터를 선형모델(linear model)로 해석하는 선형회귀식을 찾을 수 있다.        sklearn의 LinearRegression과 같은 결과를 가져옴  # Scikit Learn을 활용한 회귀분석from sklearn.linear_model import LinearRegressionmodel = LinearRegression()model.fit(x,y)y_test = model.predict(x_test)# Moore-Penrose 역행렬, y절편(intercept)항을 직접 추가해야한다.X_ = np.array([np.append(x,[1]) for x in X]) # intercept 항 추가beta = np.linalog.pinv(X_) @ yy_test = np.append(x, [1]) @ betaGradient Algorithm(경사하강법)미분 (differentiation)이란?  변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구.  변화율의 극한, 최적화에서 제일 많이 사용하는 기법.\\[f'(x) = \\lim_{h\\rightarrow 0}\\frac{f(x+h) - f(x)}h\\]  미분 코드 구현import sympy as sym # 기호를 통해 함수를 이해하게 해줌from sympy.abc import xsym.diff(sym.poly(x**2 + 2*x + 3), x)#Poly(2*x + 2, x, domain=&amp;#39;zz&amp;#39;)미분 활용  함수 그래프의 접선의 기울기와도 같으며, 이를 통해 함수값의 증감을 알 수 있다.      이렇게 구한 기울기를 통하여 미분값을 더하거나 빼서, 고차원 공간에서도 최적화 가능                미분값을 더하면 경사상승법이라 하며, 함수의 극대값을 찾는데 사용      미분값을 빼면 경사하강법이라 하며, 함수의 극소값을 찾는데 사용            극소값이나 극대값에 도달하면 미분값이 0이므로 최적화가 종료됨.      경사하강법 (Gradient Algorithm) 구현  코드 구현 슈도 코드Input : gradeint, init, lr, eps, Output: var# gradient: 미분을 계산하는 함수# init: 시작점, lr: 학습률, eps: 알고리즘 종료 조건var = initgrad = gradient(var)while(abs(grad) &amp;#38;#62; eps): # 미분값이 정확이 0이 되는 경우는 거의 없음, 즉 아주 작은값(eps)이하가 되면 종료    var = var - lr * grad # lr: 학습률이 높을수록 넓게 업데이트함, 너무 크거나 작으면 안됨    grad = gradient(var) # 미분값 업데이트다변수 함수일 경우?  벡터가 입력인 다변수 함수의 경우 편미분(partial differentiation)을 사용.\\[\\partial _{x_{i}}f\\left( x\\right) =\\lim _{h\\rightarrow 0}\\dfrac{f\\left( x+he_{i}\\right) -f\\left( x\\right) }{h}\\\\e_{i}\\ :\\ i번째\\ 값만 \\ 1이고\\ 나머지는\\ 0인\\ 단위벡터\\]  코딩을 이용한 편미분import sympy as symfrom sympy.abc import x, ysym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)# 2*x + 2*y - sin(x+2*y)  각 변수 별로 편미분을 계산한 그레이언트 벡터를 이용하여 경사하강/경사상승법에 사용 가능.\\[\\partial _{x_{i}}f\\left( x\\right) =\\lim _{h\\rightarrow 0}\\dfrac{f\\left( x+he_{i}\\right) -f\\left( x\\right) }{h}\\\\e_{i}\\ :\\ i번째\\ 값만 \\ 1이고\\ 나머지는\\ 0인\\ 단위벡터 \\\\ \\nabla f = (\\partial_{x1}f,\\partial_{x2}f,\\dots,\\partial_{xd}f)\\]그레디언트 벡터 (Gradient vector) 이용한 경사하강법  ∇f (x,y)는 임의의 점(x,y)에서 가장 빨리 함수값이 증가하는 방향이다  그러므로  -∇f (x,y)방향으로 가면 가장 빨리 함수값이 감소하는 방향이 되며, 이를 적용해 경사하강을 한다.  그레디언트 벡터가 적용된 경사하강법 슈도코드Input : gradeint, init, lr, eps, Output: var# gradient: 그레디언트 벡터를 계산하는 함수# init: 시작점, lr: 학습률, eps: 알고리즘 종료 조건var = initgrad = gradient(var)while(norm(grad) &amp;#38;#62; eps): # 벡터의 절대값 대신 노름(norm)을 계산해 종료조건 설정    var = var - lr * grad # lr: 학습률이 높을수록 넓게 업데이트함, 너무 크거나 작으면 안됨    grad = gradient(var) # 미분값 업데이트경사하강법의 선형 회귀 적용 (apply to linear regression)      무어-펜로즈 행렬을 이용해서 선형회귀가 가능했지만, 경사하강법을 이용하는게 일반적이다.        **선형회귀의 목적식은 ∥y-Xβ∥~2~ 또는 ∥y-Xβ∥~2~^2^ ** 이며, 이를 최소하하는 β를 찾는게 목적이므로 다음과 같은 그레디언트 벡터를 구해야한다.(loss : RMSE 기준)\\(\\nabla_\\beta\\left\\| y - X\\beta\\right\\|_2  = (\\partial_{\\beta_1}\\left\\| y - X\\beta\\right\\|_2,\\dots,\\partial_{\\beta_d}\\left\\| y - X\\beta\\right\\|_2)\\\\\\partial_{\\beta_k}\\left\\| y - X\\beta\\right\\|_2 = \\partial_{\\beta_k}\\left\\{\\frac{1}n\\sum_{i=1}^n{\\left(y_i - \\sum_{j=1}^{d}X_{ij}\\beta_j\\right)^2}\\right\\}^\\frac{1}2 = -\\frac{X^T_{\\cdot k}(y - X\\beta)}{n\\left\\| y - X\\beta\\right\\|_2}\\\\X^T_{\\cdot k} = 행렬\\ X의\\ k번째\\ 열(column)\\ 벡터를\\ 전치시킨\\ 것\\\\즉,\\\\\\nabla_\\beta\\left\\| y - X\\beta\\right\\|_2  = (\\partial_{\\beta_1}\\left\\| y - X\\beta\\right\\|_2,\\dots,\\partial_{\\beta_d}\\left\\| y - X\\beta\\right\\|_2) = \\left( -\\frac{X^T_{\\cdot 1}(y - X\\beta)}{n\\left\\| y - X\\beta\\right\\|_2},\\dots, -\\frac{X^T_{\\cdot d}(y - X\\beta)}{n\\left\\| y - X\\beta\\right\\|_2}\\right)\\)        이에 목적식을 최소화하는 β를 구하는 경사하강법 알고리즘은 다음과 같다.  \\[\\beta^{(t+1)}\\leftarrow\\beta^{(t)} - \\lambda\\nabla_\\beta\\left\\| y - X\\beta\\right\\|_2 =\\beta^{(t)} + \\frac{\\lambda}n\\frac{X^T(y - X\\beta^{(t)})}{\\left\\| y - X\\beta\\right\\|_2}\\\\\\]  목적식으로 ∥y-Xβ∥~2~ 대신 ∥y-Xβ∥~2~^2^을 최소화하면 식이 좀더 간단해진다.\\[\\nabla_\\beta\\left\\| y - X\\beta\\right\\|_{2}^2  = (\\partial_{\\beta_1}\\left\\| y - X\\beta\\right\\|_{2}^2,\\dots,\\partial_{\\beta_d}\\left\\| y - X\\beta\\right\\|_{2}^2) = -\\frac{2}nX^T(y - X\\beta) \\\\\\beta^{(t+1)}\\leftarrow\\beta^{(t)} + \\frac{2\\lambda}{n}X^T(y - X\\beta^{(t)})\\]  경사하강법 기반 선형회귀 알고리즘 슈도 코드Input: X, y, lr, T, Output: beta# norm: L2-노름을 계산하는 함수# lr: 학습률, T: 학습횟수 = hyperparameterfor t in range(T):  # 학습횟수 제한, 또는 이전 처럼 일정한 수준 이하로 떨어질 때까지 해도 된다.    error = y - X @ beta    grad = - transpose(X) @ error    beta = beta - lr * grad # 베타 업그레이드확률적 경사하강법 (stochastic gradient descent)      이론적으로 적절한 학습률과 학습횟수를 선택시, 수렴이 보장되어있다.        하지만 비선형회귀의 경우 목적식이 볼록하지 않으므로(non-convex) 수렴이 항상 보장되지 않음          딥러닝의 목적식은 대부분 볼록함수가 아니다, 즉 대부분 보장하지 않음      아래와 같은 경우 특정 부분에 수렴했지만 함수의 최소지점이 아니다.          ​      확률적 경사하강법(SGD)은 모든 데이터를 사용해서 업데이트 하는 대신 데이터 한개 또는 일부활용하여 업데이트합니다.          가정          \\[\\theta^{(t+1)}\\leftarrow\\theta^{(t)}-\\widehat{\\nabla _{a}L}(\\theta^{(t)})\\]  만능은 아니지만 딥러닝에서 mini-batch 방식을 추가하여 일반적으로 사용함.  SGD는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용          연산량이 b/n으로 감소      \\[\\beta^{(t+1)}\\leftarrow\\beta^{(t)} + \\frac{2\\lambda}{b}X^T_{(B)}(y_{(b)} - X_{(b)}\\beta^{(t)})\\]      경사하강법은 전체 데이터를 가지고 목적식 그레디언트 벡터를 계산하는 반면, SGD는 미치배치(데이터의 일부)를 가지고 그레디언트 벡터를 계산        그러므로 매번 다른 데이터셋을 사용하는 것과 비슷하기 때문에 함수 곡선의 모양이 미니배치마다 바뀌게 된다.        하지만 최종적인 방향성은 유사하게 이동하게 된다.            즉 볼록모양이 아니어도 효율적으로 활용가능 하다.    경사하강법(좌) vs 확률적 경사하강법(우)      다만 mini-batch 사이즈를 너무 작게 잡으면 경사하강법에 비해 너무 느려진다.        최근에는 경사하강법으로 전체 데이터를 이용해 학습시키면, 방대한 데이터셋에 의하여 메모리가 초과되므로 미니배치로 나누어서 학습하는 SGD를 사용한다.  인공지능 학습의 수학적 이해신경망의 해석  비선형, 복잡한 모델이 대부분인 신경망은 사실 선형 모델과 비선형 함수들의 결합으로 이루어져 있다.\\[\\begin{aligned}\\begin{aligned}\\begin{bmatrix} O_1 \\\\ O_2 \\\\ \\vdots \\\\ O_n \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\begin{bmatrix} w_{11} &amp; w_{12} &amp; \\dots &amp; w_{1p}\\\\  w_{21} &amp; w_{22} &amp; \\dots &amp; w_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{d1} &amp; w_{d2} &amp; \\dots &amp; w_{dp} \\end{bmatrix} + \\begin{bmatrix} \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\b_1&amp;b_2&amp;\\dots&amp;b_p \\\\ b_1&amp;b_2&amp;\\dots&amp;b_p \\\\ b_1&amp;b_2&amp;\\dots&amp;b_p\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\end{bmatrix}\\\\O\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ X\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ W \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ b\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\end{aligned}\\\\(n \\times p)\\ \\ \\ \\ (n \\times d)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (d\\times p)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (n\\times p) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\end{aligned}\\][math 0. 신경망 수식 분해]  b는 각 한 행의 모든 값이 같다.으에에... 이게 머야 언젠간 고치기graph BT\tx1((x&amp;#38;#60;sub&amp;#38;#62;1&amp;#38;#60;/sub&amp;#38;#62;)) &amp;#38; x2((x&amp;#38;#60;sub&amp;#38;#62;2&amp;#38;#60;/sub&amp;#38;#62;)) &amp;#38; x((x...)) &amp;#38; xd((x&amp;#38;#60;sub&amp;#38;#62;d&amp;#38;#60;/sub&amp;#38;#62;))--&amp;#38;#62;o1((O&amp;#38;#60;sub&amp;#38;#62;1&amp;#38;#60;/sub&amp;#38;#62;)) &amp;#38; os((o...)) &amp;#38; op((O&amp;#38;#60;sub&amp;#38;#62;p&amp;#38;#60;/sub&amp;#38;#62;))[chart 0. 신경망 모델의 차트화]      d개의 변수로 p개의 선형모델을 만들어 p개의 잠재변수를 설명하는 모델의 도식화이다.        화살표는 w~ij~들의 곱을 의미한다.  소프트맥스 연산\\[softmax(o) = softmax(Wx+b) = \\left( \\dfrac{\\exp(0)}{\\sum_{k=1}^p\\exp(o_k) }, \\dots, \\dfrac{\\exp(0_p)}{\\sum_{k=1}^p\\exp(o_k) },\\right)\\][math1.softmax]  소프트맥스(softmax) 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산  분류 문제를 풀 때 선형모델과 소프트 맥스 함수를 결합하여 예측def softmax(vec):    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True)) # 각 출력의 값    # 너무 큰 벡터가 들어오는 것을 막기위해 np.max(vec)을 vec에서 빼서 방지    numerator = np.sum(denumerator, axis=-1, keepdims=True)#모든 출력의 값들의 합    val = denumerator / numerator    return val[code 1.softmax의 코드 구현]  이러한 소프트맥스 함수로 벡터를 확률 벡터(각 성분의 합이 1인 벡터)로 변환할 수 있다.def one_hot(val, dim):    return [np.eye(dim)[_] for _ in val]def one_hot_encoding(vec):    vec_dim = vec.shape[1]    vec_argmax= np.argmax(vec, axis=-1)    return one_hot(vec_argmax, vec_dim)[code 1-1.one_hot 함수 구현]  학습이 아닌 추론시에는 one_hot 벡터를 이용해 가장 큰 벡터를 정답으로 삼으면 되므로 softmax()를 씌울 필요 없다.(one_hot(softmax(o)=&gt; X, one_hot(o) =&gt; O)활성화 함수(activation function)와 다층 신경망(MLP)  신경망은 선형모델과 활성함수(activation function)을 합성한 함수\\[H = (\\sigma (z_1), \\dots, \\sigma(z_n)), \\sigma(z) = \\sigma(Wx + b)\\\\\\sigma = 활성함수(비선형),\\ z = (z_1,\\dots,z_q) = 잠재벡터,\\ H = 새로운\\ 잠재벡터 = 뉴런\\][math 2. 신경망 뉴런][img 2. 신경망 뉴런 도식화]  잠재벡터들을 이용해 만든 새로운 잠재벡터들,(그리고, 이 새로 만든 잠재벡터로 만들 새로운 잠재벡터들)을 뉴런(neuron) 또는 이라고 하며, 이러한 구조의 인공신경망을 퍼셉트론(perceptron)이라고 한다.          각 뉴런(노드) 가 가지고 있는 값은 텐서(tensor)라고 한다.        활성화 함수는 실수값을 받아 실수값을 돌려주는 비선형(nonlinear) 함수          비선현 근사를 하기 위해 존재        이로 인해 딥러닝이 선형모형과 차이를 보였으며, 시그모이드(sigmoid), $tanh$, 그리고 주로 쓰이고 있는 ReLU 함수 등이 있다.[img 3. sigmoid, tanh, ReLu 함수 그래프]  만약, 이렇게 구한 잠재 벡터 H에서 가중치행렬 $W^{(2)}$와 $b^{(2)}$를 통해 다시 한번 선형 변환해서 출력하게 되면 ($W^{(2)}, W^{(1)}$)를 패러미터로 가진 2층(2-layers) 신경망이 된다.\\[O =  H W^{(2)} + b^{(2)},\\  H = (\\sigma (z_1), \\dots, \\sigma(z_n)) = \\sigma( Z^{(1)}),\\ \\sigma(z) = \\sigma(W^{(1)}x + b^{(1)}) \\\\ Z^{(1)} =  X W^{(1)} + b^{(1)}\\][math 2-1. 2중 신경망][img 2-1.2중 신경망의 구조]  이렇게 신경망이 여러층 합성된 함수를 다층(multi-layer) 퍼셉트론(MLP)라고 한다.\\[\\\\ O = Z^{(L)}\\\\ \\vdots\\\\ H^{(l)} = \\sigma(Z^{(l)})\\\\ Z^{(l)} = H^{(l-1)} W^{(l)} + b^{(l)}\\\\ \\vdots\\\\{H^{(1)}} = \\sigma(\\bold Z^{(1)})\\\\ Z^{(1)} = X\\bold W^{(1)} + b^{(1)}\\][math 2-2. n층으로 이루어진  다중신경망의 합성함수]  $l = 1,\\dots,L$까지 순차적인 신경망 계산을 순전파(forward propagation)이라 부른다.[img 2-3. 다층 신경망 구조]  이론적으로 2층 정도의 신경망으로도 임의의 연속함수를 근사(universal approximation theorem)할 수 있다.  층이 깊을 수록 필요한 뉴런(텐서를 가지고 있는 노드), 파라미터의 숫자가 기하급수적으로 줄어들어 좀 더 효율적이다.          즉, 층을 깊이 하면 넓이를 얇게 해도 된다.      물론 최적화는 여전히 어렵다.(CNN에서 깊게 설명)      역전파(backpropagation) 알고리즘  각 층에 사용된 패러미터 ${W^{l},b^{l}}^L_{l=1}$을 역순으로 학습하는데 사용된다.  합성미분의 연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)을 이용하여 역순으로 구한다.\\[z = (x+y)^2의\\ 그레디언트\\ 벡터,\\ \\dfrac{\\partial z}{\\partial x} =\\ ?\\\\z = w^2 \\rightarrow \\dfrac{\\partial z}{\\partial w} = 2w w = x + y \\rightarrow \\dfrac{\\partial w}{\\partial x} = 1,\\  \\dfrac{\\partial w}{\\partial y} = 1\\\\\\dfrac{\\partial z}{\\partial x} = \\dfrac{\\partial z}{\\partial w}\\dfrac{\\partial w}{\\partial x} = 2w \\cdot 1 = 2(x+y)\\][math 3. 편미분을 이용한 역전파 알고리즘 예시]  먼저 윗층의 그레디언트 벡터를 구한 뒤, 그 벡터를 이용해 그 아래 그레디언트 벡터를 구한다.          포워드 프로파 게이션과 달리, 각 뉴런 또는 노드의 텐서 값을 메모리에 넣어야 하므로, 메모리를 많이 먹는다.      [img 3. 2층 신경망 어려운 예제]  파란색 : forward propagation  빨간색 : back propagation확률론  딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있으며, 통계적 해석은 손실함수 들의 기본 작동원리이다.          회귀 분석의 L2 노름은 예측오차 분산을 최소화하는 방향으로 학습하며      분류 문제의 교차 엔트로피는 모델 예측의 불확실성을 최소화하는 방향으로 학습한다.      확률분포      확률분포란, 확률 변수가 특정한 값을 가질 확률을 나타내는 함수를 의미한다.        확률분포는 데이터공간 $(x, y)$ 에서 데이터를 추출하는 분포이다.        확률 변수는 이산형(discrete)과 연속형(continuous)으로 구분된다.          이산형 확률 변수 : 확률 변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해 모델링\\(\\mathbb{P}(X \\in A) = \\sum_{x\\in A}{P(X= x)}\\\\\\mathbb{P} = 확률변수\\)        [math 4. 이산형 확률변수]          연속형 확률 변수 : 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다.    \\[\\mathbb{P}(X \\in A) = \\int_{A}{P(x)dx}\\\\P(x)=\\lim_{h\\rightarrow0}\\frac{\\mathbb{P}(x - h \\leq X \\leq x + h)}{2h}= 확률변수의\\ 밀도\\]    [math 4-1. 연속형 확률변수]        $P(x)$는 입력 x에 대한 주변확률 분포로 y에 대한 정보를 주진 않음  기대값      조건부 확률 $P(x|y)$는 데이터 공간에서 입력 x와 출력 y 사이의 관계를 모델링하며, 입력 변수 x에 대해 정답이 y일 확률을 의미함.        $softmax(W\\phi + b )$은 데이터 x으로부터 추출된 특징패턴 $\\phi (x)$과 가중치행렬 W을 통해 조건부확률 $P(y|x)$을 계산        기대값(expectation)은 데이터를 대표하는 통계량, 확률 분포를 통해 다른 통계적 범함수를 계산하는데 사용          회귀 문제의 경우 조건부 기대값을 추정하며 이는 각 확률분포에 따라 다음과 같이 구한다.    \\[\\mathbb{E}_{x\\sim P(x)}[f(x)] = \\int_\\chi f(x)P(x)dx \\rightarrow연속확률분포\\ \\ \\\\\\mathbb{E}_{x\\sim P(x)}[f(x)] = \\sum_{x\\in\\chi} f(x)P(x) \\rightarrow 이산확률분포\\]    [math 5. 기대값 구하기]          분산, 첨도 공분산 등의 통계량을 계산하는데 사용함.      \\[분산\t : \\mathbb{V}(x) = \\mathbb{E}_{x\\sim P(x)}[(x-\\mathbb{E}[x])^2]\\\\비대칭도 : Skewness(x) = \\mathbb{E}\\left[\\left(\\frac{x-\\mathbb{E}[x]}{\\sqrt{\\mathbb{V}(x)}}\\right)^3\\right]\\\\공분산 : Cov(x_1,x_2) = \\mathbb{E}_{x_1,x_2\\sim P(x_1,x_2)}(x_1 -\\mathbb{E}[x_1])(x_2 - \\mathbb{E}[x_2])\\]​\t[math 5-1. 기대값의 사용]몬테카를로 샘플링(Monte Carlo sampling)  확률 분포를 모를 때 데이터를 이용하여 기대값을 계산하는 방법.  이산형, 연속형이든 관계없이 다음과 같이 표현함.\\[\\mathbb{E}_{x\\sim P(x)}[f(x)] \\approx \\frac{1}{N}\\sum^N_{i=1}f(x^{(i)}),\\ x^{(i)}\\stackrel{\\text{i.i.d.}}{\\sim} P(x)\\][math 6.몬테카를로 샘플링]  독립 추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장import numpy as np#f(x) = e^(-x^2), [-1, 1]def mc_int(fun, low, high, sample_size=100, repeat=10):    int_len = np.abs(high - low)    stat = []    for _ in range(repeat):        x = np.random.uniform(low=low, high=high, size=sample_size)        fun_x = fun(x)        int_val = int_len * np.mean(fun_x)        stat.append(int_val)    return np.mean(stat), np.std(stat)def f_x(x):    return np.exp(-x**2)print(mc_int(f_x, low=-1, hight=1, sample_size=10000, repeat=100))[code 6. f(x), [-1, 1] 몬테카를로 코드구현]통계학  통계적 모델링은 적절한 가정 위에서 확률 분포를 추정(inference)하는 것이 목표이며, 이는 기계학습이 결과를 예측하는 것과 같은 목표이다.  유한한 개수의 데이터로는 모집단의 분포를 정확하게 알아낼 수 없으므로 근사적으로 확률분포를 추정하여 불확실성을 최소화모수  데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적(parametirc) 방법론이라고 함.          이와 반대로 데이터에 따라 모델의 주조 및 개수가 유연하게 바뀌면 비모수(nonparametric) 방법론이라 부르며 기계학습의 많은 방범론이 이를 따르기도 함.      데이터 모수 추정확률 분포 가정  히스토그램의 모양을 관찰하여 확률분포를 가정할 수 도 있다.[img 7. 여러가지 모양의 확률 분포]            확률 분포명      데이터 모양                  베르누이 분포      데이터가 2개의 값(0 또는 1)만 가짐              카테고리 분포      데이터가 n개의 이산적인 값만을 가짐              베타 분포      데이터가 [0, 1] 사이에서 값을 가짐              감마 분포, 로그 정규 분포      데이터가 0 이상의 값을 가짐              정규 분포, 라플라스 분포      데이터가 $\\mathbb{R}$(실수) 전체에서 값을 가짐      [fig 7.  확률 분포의 예시]  하지만 이런식으로 가정하는 것보다 데이터를 생성하는 원리를 고려한 뒤, 각 분포마다 모수를 추정 후  각 확률분포의 검정방벙으로 검정하는 방식이 원칙이다.데이터 모수 추정      데이터 확률분포를 가정한 수 데이터 모수를 추정한다.        평균 $\\mu$와 분산 $\\sigma^2$으로 이를 추정하는 통계량(statistic)은 다음과 같다.\\(\\stackrel{표본 평균}{\\bar{X} = \\frac{1}{N}\\sum^N_{i=1}X_i},\\ \\ \\ \\stackrel{표본 분산}{S^2 = \\frac{1}{N-1}\\sum^N_{i=1}(X_i-\\bar{X})^2}\\\\\\mathbb{E}[\\bar{X}] = \\mu,\\ \\ \\ \\mathbb{E}[S^2] = \\sigma^2,\\ 표본 표준 편차 = \\sqrt{표본분산} = \\sqrt{S^2} = S\\)[math 7. 통계량 계산]          표본분산을 구할 때 N이 아닌 N-1로 나누는 이유는 불편(unbiased) 추정량을 구하기 위해서라고 하며, 고급 통계학 내용이므로 일단 넘어가겠다.            통계량의 확률 분포를 표집분포(Sampling distribution)이라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포 $\\mathcal{N}(\\mu,\\sigma^2/N )$를 따릅니다.          이를 중심 극한 정리(Central Limit Theorem)이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립합니다.      최대 가능도(maximum likelihood estimation, MLE) 추정      통계량을 측정하는 적절한 방법은 확률분포마다 다르다.        이론적으로 가장 가능성이 높은 모수 측정 방법은 최대 가능도 추정법(maximum likelihood estimation, MLE)입니다.\\(\\hat{\\theta}_{MLE} = argmax\\ L(\\theta;x) = argmax\\ P(x|\\theta)\\)[math 8. 최대가능도 추정법]        데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도를 최적화합니다.          이때 모수 $\\theta$는 가능도를 최적화하는 MLE가 됩니다.      \\[L(\\theta;X) = \\prod^n_{i=1}P(x_i|\\theta) \\Rightarrow log\\ L(\\theta; X) = \\sum^n_{i=1}logP(x_i|\\theta)\\]​\t[math 8. 독립 추출 시의 추정법 최대가능도 추정법]  데이터의 숫자가 수억단위가 되면 컴퓨터의 연산으로 계산 불가능하므로, 데이터가 독립일 시, 로그 가능도의 덧셈으로 바꾸면 컴퓨터로 연산이 가능해짐.          경사하강법으로 가능도 최적화시, 미분연산을 사용하며, 음의 로그가능도(negative log-likelihood)를 사용하면 연산량이 O(n^2^)에서 O(n)으로 줄여준다.      불편 추정량을 보장하진 않음.      최대 가능도 추정법 예제정규분포\\[\\hat{\\theta}_{MLE}= argmax\\ L(\\theta; x) = argmax\\ P(x|\\theta)\\\\log\\ L(\\theta;X) = \\sum^n_{i=1}logP(x_i|\\theta) = \\sum^n_{i=1}log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{{|x_i-\\mu|^2}}{2\\mu^2}} = -\\frac{n}{2}log2\\pi\\sigma^2 - \\sum^n_{i=1}\\frac{{|x_i-\\mu|}^2}{2\\sigma^2}\\][math 8-1. 모수 추정을 위한 로그 가능도 계산]\\(0 = \\frac{\\partial logL}{\\partial\\mu}= -\\sum^n_{i=1}\\frac{x_i - \\mu}{\\sigma^2} \\Rightarrow \\hat{\\mu}_{MLE}=\\frac{1}{n}\\sum^N_{i=1}x_i\\\\0 = \\frac{\\partial logL}{\\partial\\sigma}= -\\frac{n}{\\sigma}+\\frac{1}{\\sigma^3}\\sum^n_{i=1}|{x_i - \\mu}|^2 \\Rightarrow \\hat{\\sigma}_{MLE}^2=\\frac{1}{n}\\sum^N_{i=1}(x_i -\\mu)^2\\\\\\)[math 8-2. 미분을 통한 모수 추정]카테고리 분포\\[\\hat{\\theta}_{MLE}= argmax\\ P(x_i|\\theta) = argmax\\ log(\\prod^n_{i=1}\\prod^d_{k=1}p_k^{x_i,k})\\\\log(\\prod^n_{i=1}\\prod^d_{k=1}p_k^{x_i,k})=\\sum^d_{k=1}(\\sum^n_{i=1}x_{i,k})logp_k = \\sum^d_{k=1}n_klogp_k\\ with\\ \\sum^d_{k=1}p_k=1\\\\\\Rightarrow \\mathcal{L}(p_1,\\dots,p_k,\\lambda) = \\sum^d_{k=1}n_k logp_k + \\lambda(1-\\sum_kp_k) (라그랑주\\ 승수법)\\\\0 = \\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{n_k}{p_k} - \\lambda,\\ \\ \\ 0=\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = 1 - \\sum^d_{k=1}p_k \\rightarrow\\ p_k =\\frac{n_k}{\\sum^d_{k=1}n_k}\\]**[math 8-3. 모수 추정] **딥 러닝에서 최대가능도 추정법      딥러닝 모델의 가중치를 $\\theta=(W^{(1)},\\dots,W^{(L)})$라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리 분포의 모수 $(p_1,\\dots,p_k)$를 모델링합니다.        원핫벡터로 표현한 정답레이블 $y= (y_1, \\dots,y_k)$을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있습니다.\\(\\hat{\\theta}_{MLE} = argmax \\frac{1}{n}\\sum^n_{i=1}\\sum^K_{k=1}y_{i,k}log(MLP_\\theta(x_i)_k)\\) [math 9. 분류 문제 최대가능도 추정]  확률분포의 거리 구하기 - 쿨백-라이블러 발산  기계학습에서 사용되는 함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도됩니다.  두 개의 확률분포 P(x), Q(x)가 있을 경우 두 확률분포 사이의 거리(distance)를 계산할 때 여러 함수를 이용          총변동 거리 (Total Variation Distance, TV)      쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)      바슈타인 거리 (Wasserstein Distance)        이 중 쿨백-라이블러 발산(KL Divergence)은 다음과 같이 정의.\\(\\mathbb{KL}(P\\|Q) = -\\mathbb{E}_{x\\sim P(x)}[logQ(x)] + \\mathbb{E}_{x\\sim P(x)}[logP(x)]\\\\ -\\mathbb{E}_{x\\sim P(x)}[logQ(x)] = 크로스\\ 엔트로피,\\ \\mathbb{E}_{x\\sim P(x)}[logP(x)] =  엔트로피\\)[math 10. 쿨백-라이블러 발산 구하기]  분류 문제에서 정답레이블을 P, 모델 예측을 Q라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화 하는 것과 같음.베이즈 통계학  베이즈 통계학이란, 모수 추정에 사용되는 베이즈 정리에 대한 내용, 데이터 추가시 데이터 업데이트 방법에 대한 이론  베이즈 정리란, 조건부확률을 이용해 정보를 갱신하는 방법을 알려줌, 예측 모형의 방법론조건부 확률  조건부 확률 $P(A|B)$는 사건 B가 일어난 상황에서 사건 A가 발생할 확률  이를 통해 $P(A|B)$ 또한 구할 수 있다.\\[P(A\\cap B) = P(B)P(A\\|B)\\\\P(B\\|A) = \\frac {P(A\\cap B)}{P(A)} = P(B)\\frac {P(A\\|B)}{P(A)}\\][math 11. 조건부확률에 대한 식]\\(P(\\theta\\|\\mathcal{D}) = P(\\theta)\\frac{P(\\mathcal{D}\\|\\theta)}{P(\\mathcal{D})}\\\\P(\\theta\\|\\mathcal{D}) : 사후확률(posterior),\\ P(\\theta):사전확률(prior), \\ P(\\mathcal{D}|\\theta): 가능도(likelihood),\\ P(\\mathcal{D}):Evidence\\)[math 11-1. 베이즈 정리 용어 정리]  예시를 들어보자면 코로나 발병률이 10%(사전확률 $P(\\theta)$:0.1), 실제로 걸려서 확진될 확률 99%, 안걸렸는데 오진될 확률 1% (가능도, $P(\\mathcal{D}|\\theta)$: 0.99, 0.01 )라 할때 질병에 걸린 사람의 검진결과가 나왔을 때 정말로 코로나에 감염되었을 확률?\\[P(\\theta) = 0.1,\\ P(\\neg\\theta) = 0.9,\\ P(\\mathcal{D}\\|\\theta)=0.99,\\ p(\\mathcal{D}\\|\\neg\\theta)=0.01\\\\P(\\mathcal{D}) = \\sum_\\theta P(\\mathcal{D}\\|\\theta)P(\\theta) = 0.99 \\times 0.1 + 0.01 \\times0.9 = 0.108\\\\P(\\theta\\|\\mathcal{D}) = 0.1 \\times\\frac{0.99}{0.108} \\approx 0.916 \\rightarrow 정답\\][math 11-2. 사후확률 계산]  $\\theta : 코로나\\ 발병사건으로\\ 정의(관찰 불가),\\ \\mathcal{D}: 테스트\\ 결과로\\ 정의(관찰 가능), \\neg\\theta : ~가\\ 아닐\\ 확률$  오탐율(False alarm)이 오르면 테스트의 정밀도(Precision)가 떨어진다. (0.1로 10배 오를시 0.524까지 떨어짐)[img 11. 조건부 확률의 시각화]베이즈 정리를 통한 정보의 갱신  베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있음.\\[new\\ P(\\theta\\|\\mathcal{D}) = P(\\theta\\|\\mathcal{D})\\frac{P(\\mathcal{D}\\|\\theta)}{P(\\mathcal{D})}\\][math 12. 갱신된 사후확률 구하기]\\(new\\ P(\\theta\\|\\mathcal{D}) = 0.1 \\times \\frac {0.99}{0.189} \\approx 0.524,\\ P(\\theta\\|\\mathcal{D}) = 0.99,\\ P(\\theta\\|\\neg\\mathcal{D}) = 0.1 \\\\ P(\\mathcal{D}^*)=0.99\\times0.524+0.1\\times0.476 \\approx0.566\\\\갱신된\\ 사후확률\\ P(\\theta\\|\\mathcal{D}^*) = 0.524 \\times\\frac{0.99}{0.566}\\approx0.917\\)[math 12-1. 갱신된 사후확률 계산]  코로나 확정을 받은 사람이 오진율이 10%일시 두번째 검진이 양성일 시에도 확진일 확률?조건부확률과 인과관계의 차이      데이터가 많아도 조건부 확률은 인과관계(causality)와 다르다    인과관계는 데이터 분포의 변화에 강건한 예측 모형을 만들 때 고려함.          인과 관계를 고려하지 않으면 시나리오나 바뀐 데이터에 따라 정확도가 크게 떨어짐        인과관계는 중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산 해야함.          키가 클수록 지능지수가 높다? =&gt; 연령이 클수록 나이와 지능이 높아서 생기는 인과관계      여기서 중첩요인은 연령      simpson’s paradox에 의한 인과관계 오류[img 13. 신장 결석 치료법에 따른 치료율]  실제로는 작은 결석 또, 큰 결석 관계없이 외과수술이 치료율이 높지만, 약물 치료법이 성공율이 높은 작은 결석 치료에 많이 사용되므로 전체 치료율은 높아보인다.  do(T=a)라는 조정 효과를 통해 Z의 개입을 제거해야한다.\\[P_a(R=1) = \\sum_{z\\in \\{0,1\\}}P(R=1\\|T=b,Z=z)P(Z=z) = \\frac{81}{87}\\times\\frac{(87+270)}{700} + \\frac{192}{263}\\times\\frac{263+80}{700}\\approx 0.8325 \\\\P_b(R=1) = \\sum_{z\\in \\{0,1\\}}P(R=1\\|T=b,Z=z)P(Z=z) = \\frac{234}{270}\\times\\frac{(87+270)}{700} + \\frac{55}{80}\\times\\frac{263+80}{700}\\approx 0.7789\\][math 13. 조정 효과를 통한 Z(중첩요인) 개입 제거]CNNCNN(Convolution Neural Network)의 이해  기존의 모델들은 뉴런이 모두 연결된 (fully connected) 구조였지만 CNN은 동일한 고정된 가중치 값을 가진 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조임.  선형의 변환의 한 종류임은 같음,  커널 사이즈는 고정되므로 parameter 사이즈가 작다.\\[h_i =\\sigma\\left(\\sum^k_{j=1}V_jx_{i+j-1}\\right)\\][math 14.  Convolution 연산][img 14. Convolution 연산 그림]  CNN의 수학적 의미는 신호(signal)를 커널을 이용해 국소적으로 증폭 또는 감소시켜 정보를 추출 또는 필터링하는 것이며, 이는 크게 2가지 정의 할 수 있다.          정의역이 연속적(continuous)인 공간 : 적분으로 표현      정의역이 이상(discrete) 공간 : 급수로 표현      \\[continuous\\ \\  \\[f*g\\]\\(x\\) = \\int_{\\mathbb{R}^d}f(z)g(x-z)dz=\\int_{\\mathbb{R}^d}f(x-z)g(z)dz=\\[g*f\\]\\(x\\)\\\\discrete\\ \\  \\[f*g\\]\\(i\\) = \\sum_{a \\in \\mathbb{Z}^d}f(a)g(i-a)=\\sum_{a \\in \\mathbb{Z}^d}f(i-a)g(a)=\\[g*f\\]\\(i\\) \\\\g(x-z), g(i-a) : signal\\ term,\\ f(z), f(a): kernal\\ term\\][math 14-1.  Convolution 연산 수식]  z 또는 a만 움직이는 형태로 연산  사실 x-z, i-a가아니라 x+z, i+a 이며 cross-correlation 이다.          전체 공간에서는 +,-가 차이가 크지않으므로 convolution이라 불러왔음                  Convolution 연산 그래픽적 이해                                      [fig 14-1. Convolution 연산 그래픽적 이해]  커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)로 적용.\\[1D-conv\\ \\  \\[f*g\\]\\(i\\) = \\sum^d_{p=1}f(p)g(i+p)\\\\2D-conv\\ \\  \\[f*g\\]\\(i,j\\) = \\sum_{p,q}f(p,q)g(i+p, j+q)\\\\3D-conv\\ \\  \\[f*g\\]\\(i,j,k\\) = \\sum_{p,q,r}f(p,q,r)g(i+p, j+q, k+r)\\\\\\][math 14-2.  Convolution 여러 차원 연산]  1차원 뿐만 아니라 다양한 차원에서 계산 가능  1차원(text), 2차원(흑백), 3차원(컬러)별로 적용 가능  앞의 f항은 바뀌지 않는다.다차원 CNN(Convolution Neural Network)의 이해[img 14-2. 2차원 Convolution 연산 그래픽적 이해-1][img 14-3. 2차원 Convolution 연산 그래픽적 이해-2]\\(O_H=H-K_H+1\\\\O_W=W-K_w+1\\)[math 14-3.  Convolution 출력 크기 계산]  예를 들어 28x28 입력을 3x3 커널로 연산시 26x26이 된다.[img 14-4. 3차원 Convolution 연산 그래픽적 이해]  3차원의 경우 2차원 Convolution을 3번 적용하는 것이s다.          커널의 갯수도 늘어남        3차원 부터는 입력을 텐서라고 말한다.CNN의 역전파  커널이 모든 입력데이터에 공통으로 적용되므로 역전파 계산시 convolution 연산을 함\\[\\frac \\partial {\\partial x}\\[f*g\\]\\(x\\) = \\frac \\partial {\\partial x}\\int_{\\mathbb{R}^d}f(y)g(x-y)dy =\\int_{\\mathbb{R}^d}f(y)\\frac {\\partial g}{\\partial x}(x-y)dy =\\[f*g'\\]\\(x\\)\\][math 14-4.  Convolution 연산 연속시 역전파]  Discrete 구조에도 마찬가지로 성립한다.\\[\\frac {\\partial \\mathcal{L}}{\\partial w_i} = \\sum_j \\delta_jx_i+j-1, \\\\ex) \\frac {\\partial \\mathcal{L}}{\\partial w_1}= \\delta_ix_i + \\delta_2x_2+\\delta_3x_3\\][math 14-5.  Convolution 연산]RNN시퀀스(sequence) 데이터  소리, 문자열, 주가 추이 등, 순차적으로 들어오는 데이터          시계열(time-series) 데이터는 시간 순서에 따라 나열된 데이터로, 시퀀스 데이터에 속함.        독립 동등 분포 ($i.i.d$)가정을 위배하기 때문에 순서를 바꾸거나 과거 정보이 변형되면 데이터의 확률 분포도 바뀜.          ex) 개가 사람을 물었다. $\\leftrightarrow$ 사람이 개를 물었다. $\\rightarrow$ 위치를 바꾼것 만으로, 데이터의 확률, 의미가 달라짐.      sequence data handling  이전 시퀀스의 정보로 앞으로의 데이터의 확률 분포를 다루기 위해 조건부확률 결합법칙 이용\\[P(X_1,\\dots,X_t) = P(X_t\\|X_1,\\dots,X_{t-1})P(X_1,\\dots,X_{t-1})\\\\= P(X_t\\|X_1,\\dots,X_{t-1})P(X_{t-1}|X_1,\\dots,X_{t-2})\\times P(X_1,\\dots,X_{t-2})\\\\=\\prod^t_{s=1}P(X_s\\|X_{s-1},\\dots,X_1)\\\\\\prod_{s=1}^t = s =1,\\dots,t 까지\\ 전부\\ 곱하라\\\\즉,\\ X_t \\sim P(X_t\\|X_{t-1},\\dots,X_1), \\\\X_{t+1} \\sim P(x_{t+1}\\|X_t,X_{t-1},\\dots,X_1)\\][math. 베이즈 법칙에 의한 P(X~s~) 추론 ]  과거 정보를 사용하지만 0부터 t-1까지 모든 데이터가 필요한건 아님, 오히려 지나친 과거 정보는 제외          시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있어야 한다.      예를 들어, 시퀀스 데이터 $X_t$ 예측 시 전부가 아닌, $X_{t-1}\\sim X_{t-\\tau}$개 만큼만 사용하는 모델을 AR($\\tau$), 즉 자기 회귀 모델(Autoregressive Model)이라고 부름.            위의 자기 회귀 모델의 경우 $\\tau$를 파라메터로 하는데 이 값을 짐작하기 힘들거나 $\\tau$ 이상의 과거 정보가 필요할지도 모른다.          이를 보완하기 위한 모델이 잠재 자기 회귀 모델(Latent autoregressive Model, 잠재 AR 모델)이라고 부르며 RNN의 기본 모델이다.      \\[X_t \\sim P(X_t\\|X_{t-1},H_t), \\\\X_{t+1} \\sim P(x_{t+1}\\|X_t,X_{t-1},H_{t+1})\\\\잠재변수\\ H_t=X_{t-2},\\dots,X_{1}=Net_\\theta(H_{t-1},X_{t-1})\\][math. 잠재변수 H~t~를 신경망을 통해 반복 사용해 시퀀스 데이터의 패턴을 학습하는 것이 RNN]RNN의 이해와 BPTT[img. 기본 RNN 모형, 순전파와 역전파 화살표 포함]  기본적인 RNN 모형은 Multi Layer Perceptron과 유사하다.  이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링\\[O_t = HW^{(2)}+b^{(2)}\\\\H_t = \\sigma(X_tW_X^{(1)}+H_{t-1}W^{(1)}_H+b^{(1)})\\\\O_t = 출력,\\ H_t=잠재변수,\\ \\sigma=활성화함수,\\ X_tW^{(1)}=가중치행렬,\\ b^{(1)}=bias\\\\O,H와\\ 달리\\ 가중치\\ 행렬\\ W\\ 들은\\ t(시간)에\\ 따라\\ 변하지\\ 않음.\\][math. 잠재변수 H~t~의 생성에서 이전 잠재변수인 H~t-1~ 활용]  RNN의 역전파는 잠재변수의 연결 그래프에 따라 순차적으로 계산되며 이를 Backpropagtion Through Time(BPTT)라고 한다.\\(L(x,y,w_h,w_o)=\\sum^T_{t=1}l(y_t,o_t)\\\\\\partial_{w_h}L(x,y,w_h,w_o)=\\sum^T_{t=1}\\partial_{w_h}l(y_t,o_t)=\\sum^T_{t=1}\\partial_{o_t}l(y_t,o_t)\\part_{h_t}g(h_t,w_h)[\\part_{w_h}h_t],\\\\\\part_{w_h}h_t=\\part_{w_h}f(x_t,h_{t-1},w_h)+\\sum^{t-1}_{i=1}\\left(\\prod^t_{j=t+1}\\part_{h_{j-1}}f(x_j,h_{j-1},w_h)\\right)\\part_{w_h}f(x_i,h_{i-1},w_h)\\\\while\\ h_t=f(x_t,h_{t-1},w_h)\\ and\\ o_t =g(h_t,w_o).\\)[math. BPTT 역전파 계산 과정]  RNN의 가중치행렬의 미분을 계산해보면 미분의 곱으로 이루어진 항이 계산됨.          이 미분의 곱은 시퀀스의 길이가 길어질 수록 값이 불안정해진다.(무한대로 수렴 또는 0으로, 값이 크게 바뀜 등)      이를 막기 위해 적절한 길이 시점에서 끊어 준다.(truncated BPTT 기술)      [img. LSTM과 GPU 그림]  최근에는 길이가 긴 시퀀스를 처리하기 위해 다른 RNN unit을 사용함."
  }
  , 
  
  "/articles/AI/NLP/%EA%B8%B0%EA%B3%84%20%EB%8F%85%ED%95%B4%20%EA%B8%B0%EB%B3%B8.html": {
    title: "기계 독해 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/NLP/%EA%B8%B0%EA%B3%84%20%EB%8F%85%ED%95%B4%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","NLP","MRC","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true기계 독해(MRC) 기본  Naver AI boostcamp 기계 독해 강의를 정리한 내용입니다.1. MRC Intro &amp; Python BasicsIntroduction to MRCMachine Reading Comprehension(MRC. 기계독해)란?주어진 지문(Context)를 이해하고, 주어진 질의 (Query/Question)의 답변을 추론하는 문제  1) Extractive Answer Datasets      질의 (qeustion)에 대한 답이 항상 주어진 지문 (context)의 segment (or span)으로 존재  MRC 종류로 Cloze Tests(CBT), Span Extraction(SQuAD, KorQuAD, NewsQA) 등이 존재  2) Descriptive/Narrative Answer Datasets      답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성된 sentence (or free-form)의 형태  MRC 종류로 MS MARCO, Narrative QA 등이 존재  3) Multiple-choice Datasets      질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태  MRC 종류로 MCTest, RACE, ARC, 등이 존재MRC의 해결해야할 점      단어들의 구성이 유사하지는 않지만 동일한 의미의 문장을 이해 (paraphrasing)    대명사가 지칭하고 있는 것이 무엇인가?(Coreference Resolution)      답변이 존재하지 않는 경우는?    여러 문서에서 supporting fact를 찾아야 답을 알 수 있는 경우(Multi-hop reasoning)MRC의 평가 방법1)  Exact Match / F1 Score : For extractive answer and multiple-choice answer datasets  Exact Match (EM) or Accuracy : 예측한 답과 Ground-truth가 정확히 일치하는 샘플의 비율, (Number of correct samples) / (Number of whole samples)  F1 Score : 예측한 답과 ground-truth 사이의 token overlap을 F1으로 계산          F1 = $\\frac{2\\times Precision \\times Recall}{Precision+Recall}$, $Precision=\\frac{num(same\\ token)}{num(pred\\ tokens)}$, $Recall=\\frac{num(same\\ token)}{num(ground_tokens)}$      예를 들어, 답이 5 days, 예측이 for 5days 이면, EM은 0이지만 F1: 0.8이다.2) ROUGE-L / BLEU : For descriptive answer datasets, Ground-truth와 예측한 답 사이의 overlap을 계산  ROUGE-L Score : 예측한 값과 ground-truth 사이의 overlap recall (ROUGE-L =&gt; LCS (Longest common subsequence) 기반)  BLEU (Bilingual Evaluation Understudy) : 예측한 답과 ground-truth 사이의 Precision (BLEU-n =&gt; uniform n-gram weight)Unicode &amp; TokenizationUnicode란, 전 세계의 모든 문자를 일관되게 표현하고 다룰 수 있도록 만들어진 문자셋으로, 각 문자마다 숫자 하나에 매핑됨.인코딩이란, 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것UTF-8(Unicode Transformation Format)란, 현재 가장 많이 쓰는 인코딩 방식, 문자 타입에 따라 다른 길이의 바이트를 할당한다.1 byte: Standard ASCII2 bytes: Arabic, Hebrew, most European scripts3 bytes: BMP(Basic Multilngual Plane) - 대부분의 현대 글자 (한글 포함)4 bytes: ALL Unicode characters - 이모지 등한국어의 경우, 모든 한글 경우의 수를 따진 완성과, 조합하여 나타낼 수 있는 조합형으로 나뉘어 분포되어 있다.토크 나이징은 텍스트를 토큰 단위로 나누는 것으로, 단어, 형태소, subword 등 여러 기준이 존재한다.Subword 토크나이징은 자주 쓰이는 글자 조합은 한 단위로 취급하고, 자주 쓰이지 않는 조합은 subword로 쪼갠다.  ##은 디코딩 (토크나이징의 반대 과정)을 할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 의미BPE(Byte-Pair Encoding)은 데이터 압축용으로 제안된 알고리즘으로, NLP에서 토크나이징용으로 활발하게 사용되고 있다.  사람이 직접 짠 토크나이징보다 성능이 좋은 경우가 많음  가장 자주 나오는 글자 단위 Bigram (or Byte pair)를 다른 글자로 치환한다.  치환된 글자를 저장해둔다, 1로 다시 반복ex) aaabdaaabac -&gt; Z=aa, ZabdZabac -&gt; Y=ab, Z=aa, ZYdZYac -&gt; X=ZY, XdXacLooking into the DatasetKorQuAD란, LG CNS가 AI 언어지능 연구를 위해 공개한 질의응답/기계독해 한국어 데이터셋  인공지능이 한국어 질문에 대한 답변을 하도록 필요한 학습 데이터셋  위키피디아 문서 + 크라우드 소싱을 통해 제작한 질의응답 쌍으로 구성되어 있음  누구나 데이터를 내려받고, 학습한 모델을 제출하고 공개된 리더보드에 평가 받을 수 있음  2.0은 보다 긴 분량의 문서, 복잡한 표와 리스트 등을 포함하는 HTML 형태로 표현Title과 Context, Question-Answer Pairs로 이루어져 있다.HuggingFace datasets 라이브러리에서 ‘squad_kor_v1’, ‘squad_kor_v2’로 불러올 수 있음  여러 라이브러리에 호환됨, memory-mapped, cached 등의 메모리 공간 부족, 전처리 과정 등을 피할 수 있음  기본적인 데이터셋 함수 구현되어 있음.Extraction-based MRCExtraction-based MRC질문(question)의 답변(answer)이 항상 주어진 지문(context)내에 span으로 존재하는 문제MRC 데이터 셋으로 SQuAD, KorQuAD, NewsQA, Natural Questions 등이 존재한다.  Hugging Face Dataset에서 구하면 쉽다.Text를 생성하는 것이 아니라, 답의 시작점과 끝점을 찾는 문제가 된다.분류(Classification) 문제와 비슷하며, 주로 이전에 배웠던 F1 score나 EM을 지표로 삼는다.  답이 여러개가 될 수 있을 때는 보통 가장 높은 것을 인정해준다.Pre-processing다음과 같은 예시의 data를 전처리한다고 생각하자.Tokenization 단계텍스트를 작은 단위(Token)으로 나누는 단계띄어쓰기, 형태소, subword 등 여러 단위 토큰 기준이 사용되지만 최근 Out of Vcabulary(OOV) 문제를 해결하고 정보학적으로 이점을 가진 Byte Pair Encoding(BPE)을 주로 사용함BPE 방법론에는 WordPiece Tokenizer가 존재.“미국 군대 내 두번째로 높은 직위는 무엇인가?” =&gt; [‘미국’, ‘군대’, ‘내’, ‘두번째’, ‘##로’, ‘높은’, ‘직’, ‘##위는’, ‘무엇인가’, ‘?’]Tokenizing과 Speicial Token을 이용해 Tokenizing 하면 결과가 다음과 같다.Attention Mask입력 시퀀스 중에서 attention을 연산할 때 무시할 토큰을 표시0은 무시, 1은 연산에 포함되며, 보통 [PAD]와 같은 의미가 없는 특수토큰을 무시하기 위해 사용input_ids 또는 input_token_ids입력된 질문의 형태를 인덱스의 형태로 바꾸어 학습을 용이하게 만든다.Token Type IDs입력이 2개 이상의 시퀀스(예: 질문 &amp; 지문),일때, 각각에게 ID를 부여하여 모델이 구분해서 해석하도록 유도모델 출력값정답은 문서내 존재하는 연속된 단어토큰(span)이므로, span의 시작과 끝 위치를 알면 정답을 맞출 수 있음Extraction-based에서 답안을 생성하기 보다, 시작위치와 끝위치를 예측하도록 학습한. 즉 Token Classification 문제로 치환.Fine-tuningBERT의 output vector의 각 element는 해당 token이 답의 시작 또는 끝일 확률을 나오게 만든다.Post-processing불가능한 답 제거  End position이 start position보다 앞에 있는 경우 제거 (예상 답변 부분의 앞부분이 뒷부분 보다 뒤에 있을 경우)      예측한 위치가 context를 벗어난 경우 제거(ex) question이 있는 곳에 답이 태깅)    미리 설정한 max_answer_length 보다 길이가 더 긴 경우최적의 답 찾기      Start/end position prediction에서 score(logits)가 가장 높은 N개를 각각 찾는다.        불가능한 start/end 조합 제거        가능한 조합들 중 score의 합이 큰 순서대로 정렬        Score가 가장 큰 조합을 최종 예측으로 선정        Top-k 가 필요한 경우 차례대로 내보낸다.  Generation-based MRCGeneration-based MRC주어진 지문과 질의(question)을 보고, 답변을 생성하는 Generation 문제지표는 동일하게 EM과 F1을 쓸 수 있다, 추가적으로 ulgi? blue? 같은 다른 지표를 사용할 수도 있다.Extraction base와 달리 seq 2seq 구조를 이용하며, output의 형태 또한 답의 위치가 아닌 생성된 답을 사용하게 된다.Pre-processing오히려 Extraction base 보다 더욱 쉬워졌다.입력의 경우, 동일하게 WordPiece Tokenizer를 활용하며, Special Token 사용이 조금 다르다.Extraction-based MRC에선 CLS, SEP, PAD 토큰을 사용 했지만Generation-based MRC에서는 토큰을 자연어를 이용한 텍스트 포맷으로 사용한다.Attention mask는 이전 BERT와 같지만 우리가 사용할 Generation-based MRC를 위한 BART 모델의 경우 입력에 token_type_ids가 존재하지 않는다, 즉, 여러 입력 sequence 간의 구분이 없음.  직접 제공하지 않아도 모델이 충분히 구분 가능하고, 성능 차이가 없어서 지워짐.ModelBART기계 독해, 기계 번역, 요약, 대화 등 sequence to sequence 문제의 pre-training을 위한 denoising autoencoder (noise(mask)를 없애는 방식으로 학습하는 것)BART의 인코더는 BERT처럼 bi-directional이며, BART의 디코더는 GPT처럼 uni-directional (autoregressive)  아래 1일 경우 정보가 주워진다는 의미이다.Pre-training BARTBART는 텍스트에 노이즈를 주고 원래 텍스트를 복구하는 문제를 푸는 것으로 Pre-training함Post-processingDecoding디코더에서 이전 스텝에서 나온 출력이 다음 스텝의 입력으로 들어감, 이를 autoregressive라고 함맨 첫 입력은 문장 시작을 뜻하는 스페셜 토큰output을 생성할 때는 대부분 Beam Search 방법을 사용한다.Passage Retrieval - Sparse EmbeddingIntroduction to Passage RetrievalPassage RetrievalDatabase 등에서 질문(query)에 맞는 문서(Passage)를 찾는 것이를 MRC와 결합하여 Open-domain Question Answering 같은 2-stage 질문 답 찾기 등이 가능하다.Query(실시간)와 Passage(또는 문서, 미리 해놓음)를 임베딩한 뒤 유사도로 랭킹을 매기고, 유사도(inner product 또는 공간상의 거리)가 가장 높은 Passage를 선택Passage Embedding and Sparse EmbeddingPassage Embedding SpacePassage Embedding의 벡터 공간이며, 벡터화된 Passage를 이용하여 Passage 간 유사도 등을 알고리즘 계산 가능Sparse EmbeddingDense하지 않고 0이 아닌 숫자가 드문 embedding 방법, Bag-of-Words(BoW)가 예시이다.위의 경우가 BoW의 방법으로, 단어의 종류만큼의 차원을 가진 벡터에서 단어가 존재할 경우 해당 단어의 차원을 1로 놓아 문서를 표현하는 방법이다.단어나 문서의 길이가 너무 많으면 차원의 수가 엄청 커진다.  BoW를 구성하는 방법 -&gt;  n-gram 방법  unigram(1-gram): It was the best of times =&gt; It, was, the, best, of, time  bigram(2-gram): It was the best of times =&gt; It was, was the, the best, best of, of times(2개씩 짝지어 단어로, 기하급수적으로 vocab 사이즈(차원 수)가 크지만, 더욱 정확한 Embedding 값을 가져올 수 있다. )  Term value를 결정하는 방법(ex)TF-IDF)  Term이 document에 등장하는지 (binary)  Term이 몇번 등장하는지 (term frequency), 등특징으로,  Dimension of embedding Vector = number of terms  등장하는 단어가 많아질수록 차원 수 증가  N-Gram의 n이 커질수록 증가  Term overlap을 정확하게 잡아 내야 할 때 유용(즉, 정확히 해당 단어가 필요하다면 잡아낼 수 있음).  반면, 의미(semantic)가 비슷하지만 다른 단어인 경우 비교가 불가(즉, 비슷한 의미를 가진 단어는 찾을 수 없음)TF-IDF (Term Frequency - Inverse Document Frequency)TF와 IDF를 고려하여 Embedding 하는 방법TF(Term Frequency)해당 문서 내 단어의 등장 빈도, 많이 등장할 수록 높다.보통 다음과 같은 방법으로 측정한다.  Raw count, 그냥 등장 숫자 세기, 잘 안사용한다.  Adjusted for doc lengh: raw count/ num words (TF), 비율로 측정  OTher variants: binary, log normalization, etc.IDF (Inverse Document Frequency)단어가 제공하는 정보의 양, 주로 명사나 형용사가 포함되며, 정보를 많이 가지는 단어일 수록 크다.로그(모든 다큐먼트 갯수/등장한 다큐멘터 수)로 구한다한 문서의 등장 빈도인 TF와 무관하다.\\(IDF(t) = log\\frac{N}{DF(t)}\\\\Document\\ Frequency (DF) : Term\\ t가\\ 등장한\\ document의\\ 개수\\\\N: 총\\ document의\\ 개수\\)Combine TF &amp; IDFTF와 IDF를 곱한 값, 높을 수록 정보를 많이 담고 있는 단어이다.TF-IDF(t, d): TF-IDF for term t in document d,\\(TF(t,d)\\times IDF(t)\\)a, the 같은 관사는 TF가 높아도 IDF가 0에 가까우므로 낮게 된다.고유명사 등은 IDF가 아주 커지면서 TF-IDF 값이 크게 된다.TF-IDF를 이용해 유사도 구하기Lab.[MRC-2]TF-IDF 참조목표: 계산한 TF-IDF를 가지고 사용자가 물어본 질의에 대해 가장 관련있는 문서를 찾자.  사용자가 입력한 질의를 토큰화  기존에 단어 사전에 없는 토큰들은 제외  질의를 하나의 문서로 생각하고, 이에 대한 TF-IDF 계산  질의 TF-IDF 값과 각 문서별 TF-IDF 값을 곱하여 유사도 점수 계산\\[Score(D,Q)=\\sum_{term\\in Q}TFIDF(term,Q)*TFIDF(term,D)\\]  가장 높은 점수를 가지는 문서 선택BM25TF-IDF의 개념을 바탕으로, 문서의 길이까지 고려하여 점수를 매김  TF 값에 한계를 지정해두어 일정한 범위를 유지하도록 함  평균적인 문석의 길이보다 더 작은 문서에서 단어가 매칭된 경우 그 문서에 대해 가중치를 부여  실제 검색 엔진, 추천 시스템 등에서 아직까지도 많이 사용되는 알고리즘\\[Score(D,Q)=\\sum_{term\\in Q}IDF\\cdot\\frac{TFIDF(term,D)\\cdot(k_1+1)}{TFIDF(term,D)+k_1\\cdot(1-b+b\\cdot\\frac{|D|}{avgdl})}\\]Passage Retrieval -Dense EmbeddingIntroduction to Dense EmbeddingPassage Embedding은 구절(Passage)을 벡터로 변환하는 것을 의미하며,Sparse Embedding은 TF-IDF가 대표 예시로, 단점으로 차원의 수가 크고 비슷한 단어의 유사성을 고려하지 못한다.(Compressed format으로 차원의 수 문제는 해결가능 하다)Dense EmbeddingSparse Embedding의 단점을 보완하기 위해 나타남  더 작은 차원의 고밀도 벡터 (length = 50 - 1000)  각 차원이 특정 term에 대응되지 않음  대부분의 요소가 non-zeroDense는 Sparse에 비해 vector들의 유사성을 파악하기 쉽고, 알고리즘 또한 더욱 많은 것을 적용할 수 있다.각자의 장점을 고려해서 둘다 동시에 사용하거나 서로 보완한다.주로 두가지 모델을 이용해 Context의 벡터를 구하는 모델과 question의 벡터를 구하는 모델을 이용해 dot product, space similarity 등을 계산하여 유사도를 비교해 결정한다.Training Dense Encodercontext를 Dense Embedding으로 Encoding 하는 Dense Encoder의 모델로, BERT, ELMo와 같은 Pre-trained language model(PLM)이 자주 사용하며, [CLS] token이 encoding된 output이 해당 context의 최종 Embedding output으로 사용된다.연관된 question과 passage dense embedding(또는 inner product) 간의 거리의 좁음, 즉, similarity를 높이는 것이 목표이며, 이를 통해 question/passage의 연관성을 알 수 있다.1) 연관된 question과 passage 간의 dense embedding 거리를 좁히는 것 (higher similarity) -&gt; positive2) 연관 되지 않은 question과 passage 간의 embedding 거리는 멀어야 함 -&gt; Negative  이를 학습하기 위해 Negative sampling을 통해 학습한다          Corpus 내에서 랜덤하게 뽑거나, 높은 TF-IDF 스코어를 가지지만 답을 포함하지 않는 샘플 같은 헷갈리는 샘플을 뽑는다.      이때 loss로 Positive passage에 대해서 negative log likelihood (NLL) loss를 사용한다.분모에는 모든 passage의 similarity score, 분자에는 positive sample의 score를 놓은 뒤, negative log를 취하여 계산된다.또한 Top-k retrieval accuracy(retrieve된 passage 중에 답을 포함하는 passage의 비율)을 계산해서 성능을 측정한다.Passage Retrieval with Dense Encoder  From dense encoding to retrievalInference: Passage(미리 embedding되있음)와 query를 각각 embedding한 후, query로부터 가까운 순서대로 passage의 순위 매김  From retrieval to open-domain question answeringRetriever를 통해 찾아낸 Passage을 활용, MRC 모델로 답을 찾음.이러한 과정을 학습 방법 개선(DPR 등)이나 인코더 모델 개선 (더 좋은 모델), 데이터 개선 등으로 성능을 향상 시킬 수 있다.Passage Retrieval - Scaling UpPassage Retrieval and Similarity SearchMIPS(Maximum Inner Product Search)주어진 질문(query) 벡터 q에 대해 Passage 벡터 v들 중 가장 질문과 관련된 벡터를 찾아야함이때 현업에서는 주로 inner product로 구하는 경우가 많음.(좀더 효율적임)\\(argmax_{vi\\in V}q^T_{v_i}\\\\argmax:검색(search),\\ 인덱싱된\\ 벡터들\\ 중\\ 질문\\ 벡터와\\\\ 가장\\ 내적값이\\ 큰\\ 상위\\ k개의\\ 벡터를\\ 찾는\\ 과정 \\\\q^T_{v_i}:인덱싱(indexing),\\ 방대한\\ 양의\\ passage\\ 벡터들을\\ 저장하는\\ 방법\\)이전에 배운 방법은 brutre-force(exhaustive) search 방법으로, 저장해둔 모든 Sparse/Dense 임베딩에 대해 일일이 내적값을 계산하여 가장 값이 큰 Passage를 추출문제는 검색해야할 데이터가 방대(위키피디아만 500만개, 그이상 수십억, 조단위 까지 커질 수 있음)즉, 더이상 모든 문서 임베딩을 일일히 보면서 검색할 수 없음Tradeoffs of similarity search즉, 모두 완벽하게 할 순 없고 다음 3가지 중에 Trade-off 해야한다.1) Search Speed : 쿼리 당 유사한 벡터를 k개 찾는데 걸리는 시간, Pruning으로 개선할 수 있다.  일반적으로 속도를 빠르게 하면 정확도가 떨어진다.2) Memory Usage : 벡터를 저장할 공간, Compression으로 개선할 수 있다.3) Accuracy : 검색결과의 질, Exhaustive search로 개선할 수 있다.또한, 코퍼스(corpus)의 크기가 커질수록 탐색 공간이 커지고 검색이 어려워지며, Memory space 또한 많이 요구 됨  그래도 Dense Embedding의 경우 Sparse Embedding 보단 낫다.Approximating Similarity SearchCompression - Scalr Quantization(SQ)Vector를 압축하여, 하나의 Vector가 적은 용량을 차지하도록 함,압축량이 클수록 메모리 사용량은 줄고 정보 손실을 늘어난다.상단 그림의 4byte의 float point를 1-byte의 unsigned integer로 압축하는 Scalar quantization이 예시Pruning - Inverted File (IVF)Search space를 줄여 search 속도 개선(dataset의 subset만 방문)=&gt; Clustering + Inverted file을 활용한 search1) Clustering: 전체 vector space를 k 개의 cluster로 나눔 (ex. k-means clustering)2) Inverted file (IVF) : Vector의 index = inverted list structure=&gt; (각 cluster의 centroid id)와 (해당 cluster의 vector들)이 연결되어있는 형태즉,  주어진 query vector에 대해 근접한 centroid 벡터를 찾음  찾은 cluster의 inverted list 내 vector들에 대해 서치 수행Introduction to FAISSFAISS란, similarity search와 dense vector의 clustering에 사용되는 라이브러리다.Passage Retrieval with FAISS1) Train index and map vectors  Train phase과 add phase로 나뉜다.2) Search based on FAISS indexScaling up with FAISS실습Linking MRC and RetrievalIntroduction to Open-domain Question Answert (ODQA)ODQA는 지문이 따로 주어지지 않으며 방대한 World Knowledge에 기반해서 질의 응답Question processing + Passage retrieval + Answer processing 이 합쳐진 형태1) Question processingQuery formulation: 질문으로부터 키워드를 선택 / Answer type selection (ex. LOCATION : country)2) Passage retrieval기존의 IR 방법을 활용해서 연관된 document를 뽑고, passage 단위로 자른 후 선별 (Named entity/Passage 내 question 단어의 개수 등과 같은 hand-crafted feature 활용)3) Answer processingHand-crafted features와 heuristic을 활용한 classifier, 주어진 question과 선별된 passage들 내에서 답을 선택Retriever-Reader ApproachRetriever-Reader 접근 방식은 데이터베이스에서 관련있는 문서를 검색한 뒤, 검색된 문서에서 질문에 해당하는 답을 찾아내는 방식즉 Retriever의 입력은 문서셋(Document corpus)와 질문(qeury)이며 출력은 관련성 높은 문서(document)이다.  이때 TF-DF, BM25를 활용하면 학습하지 않고, Dense embedding일 경우 학습이 필요하다.Reader의 입력은 Retrieved된 문서(document)과 질문(query)이며, 출력은 답변(answer)이다.  sQuAD와 같은 MRC 데이터셋으로 학습하며, 학습데이터 추가를 위해 Distant supervision을 활용한다.Distant supervision질문-답변만 있는 데이터셋 (CuratedTREC, WebQuestions, WikiMovies)에서 MRC 학습 데이터 만들기, Supporting document가 필요함.  위키피디아에서 Retriever를 이용해 관련성 높은 문서를 검색  너무 짧거나 긴 문서, 질문의 고유명사를 포함하지 않는 등 부적합한 문서 제거  answer가 exact match로 들어있지 않은 문서 제거  남은 문서 중에 질문과 (사용 단어 기준) 연관성이 가장 높은 단락을 supporting evidence로 사용함Inference  Retriever가 질문과 가장 관련성 높은 5개 문서 출력  Reader는 5개 문서를 읽고 답변 예측  Reader는 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용함Issues and Recent ApproachesDifferent granularites of text at indexing time위키피디아에서 각 Passage의 단위를 문서, 단락, 또는 문장으로 정의할지 정해야 함. 이렇게 정한 단위 기준을 Granularity라고 함.또한, Retriever 단계에서 몇 개의 문서를 정할지 고려해야하며(top-k), Granularty에 따라 k가 달라짐.(e.g article -&gt; k=5, paragraph -&gt; k=29, sentence -&gt; k=78)  Single-passage training VS Multi-passage trainingSingle-passage : 현재 우리는 k개의 passages들을 reader이 각각 확인하고 특정 answer span에 대한 예측 점수를 나타냄, 그리고 이 중 가장 높은 점수를 가진 answer span을 고르도록 함이 경우 각 retrieved passages들에 대한 직접적인 비교라 볼 수 없으며, 이를 방지하기위해 Multi-passage가 등장Multi-passage는 retrieved passages 전체를 하나의 passage로 취급하고, reader 모델이 그 안에서 answer span 하나를 찾도록 하는 것,단, 문서가 너무 길어지므로 GPU에 많은 메모리가 할당되야 하며, 처리해야하는 연산량이 많아짐Importance of each passageRetriever 모델에서 추출된 top-k passage들의 retrieval score를 reader 모델에 전달, 단순히 reader가 span만 보고 판단하지 않고 context의 적절성도 고려함Reducing Training BiasDefinition of BiasBias의 종류  학습에서의 Bias :          학습할 때 과적합을 막거나 사전 지식을 주입하기 위해 특정 형태의 함수를 선호하는 것 (inductive bias)      경향과 의도를 위해 일부러 집어넣는 경우가 많음        A Biased World :          현실 세계가 편향되어 있기 때문에 모델에 원치 않는 속성이 학습되는 것 (historical bias)      성별과 직업 간 관계 등 표면적인 상관관계 때문에 원치 않는 속성이 학습되는 것 (co-occurrence bias)                  Gender Bias : 특정 성별과 행동을 연관시켜서 예측 오류가 발생(의사의 사진은 높은 확률로 여성을 남성으로 잘못 판단함.)                      Bias in Data Generation :          입력과 출력을 정의한 방식 때문에 생기는 편향 (specification bias)      데이터를 샘플링한 방식 때문에 생기는 편향 (sampling bias)                  리터러시 다이제스트 잡지의 정반대의 여론 조사 결과 : 잡지 정기 구독자, 자동차 등록자, 사교클럽 명단 등에서 샘플 채취-&gt; 부자들만 샘플을 채취한 것이 원인으로, 예측 실패                    어노테이터의 특성 때문에 생기는 편향 (annotator bias)      Bias in Open-domain Question AnsweringRetriever-Reader Pipeline에서 Reading Comprehension 부분의 bias에 집중함.만약 reader 모델이 한정된 데이터셋에서만 학습된다면, Reader는 항상 정답이 문서 내에 포함된 데이터쌍만(Positive)을 보게 됨특히 SQuAD와 같은 (Context, Query, Answer)이 모두 포함된 데이터는 positive가 완전히 고정되어 있음예를 들어 Inference 시 만약 데이터 내에서 찾아볼 수 없었던 새로운 문서를 주면, Reader 모델을 문서에 대한 독해능력이 떨어져 정답을 내지 못함(ex) 학습시에 문학과 관련 주제만 주어졌는데, 실제 Inference 때에는 공학관련 지문들이 나온다면?)이를 막기 위해      Train negative examples    훈련 시, 잘못된 예시를 보여줘야 retriever이 negative한 내용들은 먼 곳에 배치할 수 있음, 또한, negative sample 또한 다양성을 고려해야 함.    Corpus 내에서 랜덤하게 뽑거나 좀더 헷갈리는 negative 샘플들 뽑기 위해 높은 BM25/ TF-IDF 매칭 스코어를 가지지만, 답을 포함하지 않는 샘플을 뽑거나, 같은 문서에 나온 다른 Passage/Question 선택        Add no answer bias    입력 시퀀스의 길이가 N일 시, 시퀀스의 길이 외 1개의 토큰이 더 있다고 생각하기,    훈련 모델의 마지막 레이어 weight에 훈련 가능한 bias를 하나 더 추가    Softmax로 answer prediction을 최종적으로 수행할 때, start end 확률이 해당 bias 위치에 있는 경우가 가장 확률이 높으면 이는 “대답 할 수 없다”라고 취급  Annotation Bias from DatasetsAnnotaion Bias란, ODQA 학습 시 기존의 MRC 데이터셋 활용시, ODQA 세팅에는 적합하지 않음 bias가 데이터 제작 (annotation) 단게에서 발생 가능예를 들어 SQuAD나 TriviaQA는 질문을 하는 사람이 답을 아는 상태로 tagging 했으므로, 너무 쉬운 학습이 된다.또한, SQuAD는 고작 500개의 article에서 데이터를 추출했다.데이터셋 별 성능 차이가 annotation bias로 인해 발생 가능(BM25 : Sparse embedding / DPR : dense embedding)이를 막기 위해, Annotation bias를 고려하고 데이터를 모아야 한다.Natural Questions dataset : Supporting evidence가 주어지지 않은, 실제 유저의 question들을 모아서 dataset을 구성SQuAD : Passage가 주어지고, 주어진 passage 내에서 질문과 답을 생성하므로, ODQA에 applicable하지 않은 질문들이 존재한다(미국의 대통령은 누구인가? =&gt; 어느 시기냐에 따라 다름).Closed-book QA with T5Closed-book Question Answering사전학습을 통해 대량의 지식을 학습 한 뒤, 굳이 Retriever 단계를 거치지 않고 모델 내부의 Knowledge storage를 통해 Answering 하는 것GPT-2를 통해 Zero-shot QA를 해보면 어느 정도 대답이 가능함                   Open-book QA      Closed-book QA                  지식을 찾는 방식      대량의 지식 소스를 특정 문서 단위로 나누어 Dense/Sparse 형태로 표현한 후, query가 들어오면 가장 그와 관련된 문서를 search      대량의 지식 소스(위키피디아 등)를 기반으로 사전학습된 언어 모델이 그 지식을 기억하고 있을 것이라 가정함. Search 과정 없이 바로 정답을 생성함              문제점      지식 소스를 저장하기 어려움, 검색하는 시간 소요      사전학습된 언어 모델이 얼마나 지식을 잘 기억하고 있는지가 매우 중요함, 학습 시간이 길고 parameter수가 큼, 해석하기 어려움      Text-to-Text FormatText-to-Text format : 모든 종류의 문제를 Text 대 Text로 매핑되는 문제로 바꿈Closed-book QA as Text-to-Text FormatGeneration-based MRC와 유사하지만 Context가 없이 질문만 들어가며, Retriever 단계가 없다.사전학습된 언어 모델은 BART와 같은 seq-to-seq 형태의 Transformer 모델을 사용함Text-to-Text format에서는 각 입력값(질문)과 출력값(답변)에 대한 설명을 맨 앞에 추가함.Text-to-Text FormatText-to-text problem은 input으로 text를 받아서, output으로 새로운 text를 생헝하는 문제이며, 다양한 text processing problem이 text-to-text 문제로 변형될 수 있다.Text-to-Text format 문제의 예시  Task-specific prefix를 추가하여 특정 task에 알맞은 output text를 생성하도록 함  Machine translation의 경우, prefix로 translate A to B (A: source language/ B: target language)를 통해 가능          “translate English to German: That is good” =&gt; “Das ist gut.”        Text classification(MNLI)  두개의 sentence가 주어지고 이 둘의 관계를 예측하는 task (neutral, contradiction, entailment)          Input: “mnli hypothesis:  premise: \"      Output: “neutral” or “contradiction” or “entailment”      T5Text-to-Text format이라는 형태로 데이터의 입출력을 만들어 거의 모든 자연어처리 문제를 해결하도록 학습된 seq-to-seq 형태의 Transformer 모델T5의 Pre-training 모델의 경우 다양한 모델 구조, 사전학습 목표, 사전학습용 데이터, Fine-tuning 방법 등을 체계적으로 실험함, 가장 성능이 좋은 방식들을 선택하여 방대한 규모의 모델을 학습 시킴T5-xlarge의 경우 parameter수가 11B라는 방대한 크기를 자랑함pre-trained 된 T5를 활용 시, Fine-tuning된 MRC 데이터셋(TriviaQA, WebQuestions, Natural Questions)의 QA pair만(context 제외)를 활용하고, Task-specific prefix(어느 데이터셋, 즉 어느 task인가?)을 추가 한뒤, 답이 여러개일 경우도 고려해서 학습 시킴ex) Input: trivia question :how many legs does a ladybird have? Trget: sixExperiment Results &amp; AnalysisDataset : Open-domain QA 데이터셋 또는 MRC 데이터셋에서 지문을 제거하고 질문과 답변만 남긴 데이터셋을 활용Salient Span Masking : 고유 명사, 날짜 등 의미를 갖는 단위에 속하는 토큰 범위를 마스킹한 뒤 학습Fine-tuning : Pre-trained T5 체크포인트를 Open-domain QA 학습 데이터셋으로 추가 학습대부분 Open-book 모델보다 성능이 뛰어나며, 모델 크기가 커질수록 성능이 증가했으며, 특히 Salient Span Masking이 성능을 크게 끌어올림또한, 오답의 62% 가량이 실제 오답이며, 나머지 38%는 중복 답안(Incomplete Annotation), 각기 다른  시기, 관점, 질문의 해석에 따라 정답이 될 수 있거나(Unanswerable), 정답의 다른 표현을 낸 경우(Phrasing Mismatch)이므로 실제 성능은 더욱 증가한다.Closed-book QA의 한계로,  모델의 크기가 너무 커서 계산량이 많고 속도가 느림 -&gt; 더 효율적인 모델 필요  모델이 어떤 데이터로 답을 내는지 알 수 없음 -&gt; 결과의 해석 가능성(interpretability)를 높이는 연구 필요  모델이 참조하는 지식을 추가하거나 제거하기 어려움QA with Phrase RetrievalPhrase Retrieval in Open-Domain Question Answering기존의 Retriever-Reader 방식은 다음과 같은 한계를 갖는다.  Error Propagation: 5-10개의 문서만 reader에게 전달  Query-dependent encoding: query에 따라 정답이 되는 answer span에 대한 encoding이 달라짐2단계로 이루지 말고 바로 context에서 정답을 search하는 방법인 Phrase Indexing 고안미리 계산된 key vector와 Query vector를 비교하여 답을 구하게 된다.기존의 a,q,d 조합 중 가장 점수가 높은 것을 찾는 방법(F 함수)에서 아래의  Query만(G 함수) 다시 계산하는 방법올 바꾸어 더욱 효율적이다.다만 실제로 F 함수를 G와 H 함수로 정확히 대체할 수 없어 Approximation 하는 방법을 사용하며, 이때 실제 F 함수값과 G, H  함수 값의 차이인 Decomposition Gap이 성능 하락의 주요 원인이 된다.이때, 각 phrase를 vector space 상에 잘 mapping 하기 위해 Dense와 Sparse 임베딩을 둘다 이용하게 된다.Dense-sparse Representation for PhrasesDense vectors는 통사적, 의미적 정보를 담는 데 효과적이며,(유연함)Sparse vectors는 어휘적 정보를 담는 데 효과적이므로, (명확함)이 둘을 전부 이용하여 phrase (and question) embedding을 할 수 있다.Dense vector를 만드는 방법  Pre-trained LM (e.g. BERT)를 이용  Start vector와 end vector를 재사용해서 메모리 사용량 줄임Coherency vector 생성법  phrase가 한 단위의 문장 구성 요소에 해당하는지를 나타냄  구를 형성하지 않는 phrase를 걸러내기 위해 사용함  Start vector와 end vector를 이용하여 계산Question embedding 생성법  Question을 임베딩할 때는 [CLS] 토큰 (BERT)을 활용Sparse vector를 만드는 방법  문맥화된 임베딩(contextualized embedding)을 활용하여 가장 관련성이 높은 n-gram으로 sparse vector 구성Scalability ChallengeWikipedia 같은 대량의 phrases를 처리하기 위해, Storage, indexing, search의 scalabilty가 고려되어야 하며,Storage의 경우 Pointer, filter, scalar quantization을 활용하여 1/130 수준 까지 줄일 수 있음Search 속도의 경우 FAISS를 활용해 dense vector에 대한 search를 먼저 수행 후, sparse vector로 rerankingExperiment Results &amp; AnalysisPhrase retrieval 방식은 발표 당시에는 약간의 성능 상승과 대단히 큰 inference speed를 자랑했었지만, Decomposability gap이 불러오는 효과로 최근 발표된 Retrieval-Reader 방식의 연구들 보다 성능이 뒤쳐지며, Storage 용량을 크게 필요로 한다는 단점을 가지게 된다."
  }
  , 
  
  "/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html": {
    title: "자연어처리 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","NLP","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true자연어처리(Natural Language Processing, NLP) 기본  NAVER AI boost camp 수업을 정리한 내용입니다.Intro to Natural Language Processing(NLP)  컴퓨터가 주어진 단어나 문장, 문단, 글을 이해하는 NLU(Natural Language Understading)      이런 한 글을 생성할 수 있는 NLG(Natural Language Generation)로 이루어짐    NLP의 영역(ACL, EMNLP, NAACL 등에서 연구)          우리가 주로 다룰 분야      Low-level parsing                  Tokenization: 문장을 이루는 각 단어를 정보 단위(Token)로 쪼개나가는 것          stemming: 단어의 다양한 표현 변형을 없애고 의미만 남기는 것                          맑고, 맑지만, 맑았는데, 맑은 = 맑다.                                          word and phrase level                  Named entity recognition(NER) : 고유 명사 인식                          (Newyork times는 newyork + times가 아니다)                                part-of-speech(POS) tagging : 단어의 품사, 성분이 무엇인가?(명사, 목적어, 형용사 등)                    Senetence level                  문장의 감정분석, 기계번역                    Multi-sentence and paragraph level                  Entailment prediction : 두 문장의 모순관계, 논리적 내포를 확인                          문장이 앞서 했던 말과 다른가?                                질의 응답, 문서 요약          dialog systems: 챗봇                      Text mining의 영역(KDD, WSDM, WWW 등에서 연구중)          트랜드 분석, 상품 반응 분석, SNS 사회과학 분석 등        Information retrieval(검색 기술)          많이 성숙해진 상태, 상대적으로 느림,      추천 시스템, 개인화 광고 등      NLP의 트랜드  문장의 단어들을 vector로 표현하여 그래프 내의 점으로 바꾸어 처리하게 됨(Word Embedding).  자연어 처리에서 RNN 모델을 위한 LSTM, GRU 유닛이 사용되다 최근 구글의 attention module과 Transformer model의 발표로 성능이 크게 늘어남.  더이상 rule base 기계 번역은 딥러닝 base 기계번역에 성능으로 뒤쳐지고 있음.  최근에는 자가 지도 학습 또는 pretrained model을 통하여 더 이상 labeling이 필요하지 않은 BERT, GPT-3 같은 기술이 나타남.  다만 이러한 기술들은 대규모의 컴퓨팅 능력과 데이터가 필요함.Bag-of-Words and NaiveBayes Classifier  Bag-of-Words는 딥러닝 기반 이전 시대 때, 단어와 문서를 숫자형태로 나타내는 표현 기법  NaiveBayes Classifier : Bag-of-Words 방식을 이용한 전통적인 문서 분류 기법Bag-of-Words Representation      Bag-of-Words의 과정          주어진 문장의 단어가 각각 포함되어있는 사전(vocabulary) 생성                  “Jon really really loves this movie”, “Jane really likes this song”에서          Vocabulary: {“John”, “really”, “loves”,  “this”,”movie”,”Jane”,”likes”,”song”} 생성                    사전에 포함된 유일한 단어들을 one-hot vector로 인코딩                  Vocabulary: {“John”, “really”, “loves”,  “this”,”movie”,”Jane”,”likes”,”song”}          차례대로 one-hot vector 형식으로 인코딩                          예를 들어  John:[1 0 0 0 0 0 0 0], really: [0 1 0 0 0 0 0 0] … song: [0 0 0 0 0 0 0 1]              word- embedding 방법과의 차이점                                모든 vector쌍의 거리는 $\\sqrt 2$, cosine similarity 는 0으로 계산된다.                          이렇게 구성된 문장이나 단어는 one-hot vector들의 합으로 표현할 수 있다.          예시 1. “John really really loves this movie”                  John + really + really + loves + this + movie: [1 2 1 1 1 0 0 0]                    예시 2. “Jane really likes this song”                  Jane + really + likes + this + song: [0 1 0 1 0 1 1 1]                    NaiveBayes Classifier for Document Classification  NaiveBayes Classifier란, Bag of words 방법으로 표현된 문서를 구분할 수 있는 전통적인 방법  d개의 문서와 C의 문서분류 집합이 있다고 가정하고, c 분류는 C 문서분류 집합의 원소일 때, 각각의 클래스에 d문서가 속할 확률 분포는 아래와 같이 표현된다.\\[C_{MAP}=argmax\\ P(c|d) :\"maximum\\ a\\ posteriori\" = most\\ likely\\ class \\\\ = argmax\\ \\frac{P(d|c)P(c)}{P(d)}: Bayes\\ Rule \\\\ = argmax\\ P(d|c)P(c):Dropping\\ the\\ denominator\\\\where\\ c \\in C\\][math. Bayers’ Rule Applied to Documents and Classes ]      P(d)는 d document가 뽑힐 확률 이며, 고정된 값으로 볼 수 있어서 무시할 수 있음        단어들 w로 이루어진 c 분류의 d 문서에서 특정 분류 c가 고정일 때, 문서 d가 나타날 확률은                                                      $P(d              c)P(c) = P(w1,w2,\\dots,w_n              c)P(c)\\rightarrow P(c)\\prod_{w_i\\in W}P(w_i              c)$                                          각 단어(w~1~,w~2~,…,w~n~)가 나타날 확률을 독립적이라고 가정      NaiveBayes 분류 예시                   Doc(d)      Document(words, w)      Class(c)                  Training      1      Image recognition uses convolutional neural networks      CV                     2      Transformer can be use for image classification task      CV                     3      Language modeling uses transformer      NLP                     4      Document classification task is language task      NLP              Test      5      Classification task uses transformer      ?      [fig. NaiveBayes 예시]  위 표의 상황에서 class의 속할 확률은          $P(c_{cv})=\\frac{2}{4}=\\frac{1}{2}$      $P(c_{NLP}) = \\frac {2}{4}=\\frac {1}{2}$        이며, 이때 각 단어가 나타날 확률은                              $P(w_k          c_i)=\\frac {n_k}{n}$, where n~k~ is occurreneces of w~k~ in documents of topic c~i~                                Word      Prob      Word      Prob                                $P(w_{“classification”}      c_{CV})$      $\\frac {1}{14}$      $P(w_{“classification”}      c_{NLP})$      $\\frac {1}{10}$              $P(w_{“task”}      c_{CV})$      $\\frac {1}{14}$      $P(w_{“task”}      c_{NLP})$      $\\frac {2}{10}$              $P(w_{“uses”}      c_{CV})$      $\\frac {1}{14}$      $P(w_{“uses”}      c_{NLP})$      $\\frac {1}{10}$              $P(w_{“transformer”}      c_{CV})$      $\\frac {1}{14}$      $P(w_{“transformer”}      c_{NLP})$      $\\frac {1}{10}$      [fig. 각 단어가 class 내에 나타날 확률들]  document d~5~=”Classification task uses transformer”를 통하여 class에 속할 확률을 구해보면                                                      $P(c_{CV}              d_5)=P(c_{CV})\\prod_{w\\in W}P(w              c_{CV})=\\frac{1}{2}\\times \\frac{1}{14}\\times \\frac{1}{14}\\times \\frac{1}{14}\\times \\frac{1}{14}\\times = 0.000013$                                                                                      $P(c_{NLP}              d_5)=P(c_{NLP})\\prod_{w\\in W}P(w              c_{NLP})=\\frac{1}{2}\\times \\frac{1}{10}\\times \\frac{2}{10}\\times \\frac{1}{10}\\times \\frac{1}{10} = 0.0001$                                            여기서 가장 높은 확률인 분류를 뽑아 확정하게 된다(argmax).  다만 학습 데이터에 없는 단어가 포함될 경우 다른 단어들이 있더라도 확률이 0이 되버리므로 regularization 같은 다른 방법을 강구해야함.Word Embedding: Word2Vec, GloVe  Word2Vec과 GloVe는 하나의 차원에 단어의 모든 의미를 표현하는 one-hot-encoding과 달리 단어의 distributed representation을 학습하고자 고안된 모델.  문장을 단어라는 단위 정보로 이루어진 sequence data라고 가정했을 때, 각 단어들을 공간상의 한 점이나 벡터로 표현하는 기술을 Word Embedding이라고 한다.          예를 들어 cat과 kitty는 비슷한 의미를 가지므로 비슷한 좌표값이나 벡터를 가지고 있겠지만, hamburger는 전혀 다른 좌표나 벡터에 있을 것이다.      Word2Vec[img. Word2Vec 의 예시]  같은 문장에 자주 포함되는 단어들은 관련성이 많다는 가정 하에  Word Embedding을 진행하는 알고리즘          예를 들어 “The furry cat hunts mice.” 에서 cat은 furry하고 mice를 hunts 하므로 연관이 있다.        문장 내의 단어 w가 나타날 때, 그 주위에 각각의 단어가 나타날 확률분포를 구하여 이용한다.  Skipgram 방법과 CBOW 방법 두가지로 나뉨 (실습 2_word2vec.ipynb) 참조Word2Vec Algorithm과 예제  주어진 문장을 Tokenization한 후, 사전(Vocabulary) 생성하고 단어별 one-hot 벡터 생성          Sentence: “I study math.”      Vocabulary: {“I”, “study”, “math”}      Input: “study” [0, 1, 0]      Output: “math” [0, 0, 1]        Sliding Window 기법을 이용해 앞 뒤로 나타난 단어들의 쌍들로 학습 데이터 구성.[img. Window size가 1인 Sliding Window 기법의 그림]      2 layer 구성의 심층 신경망을 통해 word embedding 한다.                  Input layer, output layer 차원 수 : vocabulary 단어 수(one-hot vector 차원)                    hidden layer 차원 수 : hyper parameter(word embedding 차원 수)                  [img. 2 layer 신경망의 word embedding 원리]          input vector “study”의 경우, [0,1,0]의 벡터를 가지고 있으며, hidden layer의 차원수가 2라고 가정할 때.      linear transform matrix W~1~의 경우 3차원의 벡터를 2차원의 벡터로 바꿔야 하므로 2X3 차원을 가진다.                  W~1~의 input vector가 one-hot vector 이므로 내적을 구하기 보단 one-hot vector의 인덱스에 해당하는 W~1~의 Column을 가져오는 형식으로 계산한다.                    linear transform matrix W~2~의 경우 다시 3차원의 벡터를 가져야 하므로 3X2 차원을 가진다.                  softmax 함수를 통과시키기 전의 이상적인 logic값은 ground-truth의 내적값은 $\\infin$, 그 이외의 내적 값은 $-\\infin$이 되어야 결과값도 one-hot vector의 형태로 나온다.                          softmax 함수를 통과시켜 word embedding 값을 가져온다.    이를 통하여 의미론적 관점에서 단어간의 유사도를 알 수 있는 word vector를 구할 수 있다.[img. 단어 간의 관계가 비슷하면 두 단어의 벡터의 방향도 유사하다.]  https://ronxin.github.io/wevi/ 에서 Word2Vec을 시연해볼 수 있다.  https://word2vec.kr/search/ 에서 한국어 Word2Vec 결과값을 알아볼 수 있다.  Word2Vec으로 문맥에 어색한 단어를  찾아내는 Intrusion Detection 또한 가능하다.  Word2Vec을 이용해 다음과 같은 분야에 활용해 성능을 향상시킬 수 있다.          기계번역      PoS tagging      고유명사 태깅      감정 분석      Image Captioning      기타 등등      GloVe: Another Word Embedding Model      Global Vectors for Word Representation    Word2Vec과 함께 많이 쓰이는 Word Embedding 방법  카운트 기반 방법론(LSA)과 예측 기반의 방법론(Word2Vec) 두 가지를 모두 사용하는 방법론          (입력어의 임베딩 벡터와 출력어의 임베딩 벡터의 내적값)과 (윈도우에서 두 단어 i, j의 동시 출연 빈도에 log를 씌운 것)을 loss 함수로써 fitting하여 word embedding 값을 구하는 방식      $u_i$: 입력어의 임베딩 벡터, $v_j$: 출력어의 임베딩 벡터, $P_{ij}$: 윈도우 기반 두 단어 i, j의 동시 등장 빈도      \\[J(\\theta)=\\frac{1}{2}\\sum^w_{i,j=1}f(P_{ij})(u_{i}^Tv_j-logP_{ij})^2\\]​\t\t[math. GloVe의 손실함수]  윈도우 기반 동시 등장 빈도($P_{ij}$)는 전체 단어 집합 들의 단어들이 윈도우 크기 내에서 단어가 등장한 횟수를 의미하며, 보통 전체 단어들의 등장 빈도를 행렬로 다음과 같이 표현한다.          “I like deep learning”, “I like NLP”, “I enjoy flying” 세 문장이 주어지고 윈도우 크기가 1일 때,                  카운트      I      like      enjoy      deep      learning      NLP      flying                  I      0      2      1      0      0      0      0              like      2      0      0      1      0      1      0              enjoy      1      0      0      0      0      0      1              deep      0      1      0      0      1      0      0              learning      0      0      0      1      0      0      0              NLP      0      1      0      0      0      0      0              flying      0      0      1      0      0      0      0      [fig. 예제의 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix) https://wikidocs.net/22885]  중복 되는 계산이 적어 상대적으로 빠를 수 있고, 적은 데이터로도 성능이 좋다.(실제 성능은 비등비등)  https://nlp.stanford.edu/projects/glove/ 오픈소스 glove 모델Recurrent Neural Networks(RNNs)Basics of Recurrent Neural Networks(RNNs)[img. RNN의 구조, 좌측은 Rolled RNN, 우측은 UnRolled RNN이다.]      Sequence 입력 벡터 X과 이전 time step의 RNN 모듈에서 계산한 $h_{t-1}$을 입력으로 받아, 현재의 $h_t$를 출력하는 구조.                            수식과 구조          구성                                                \\(h_{t-1}:\\ 이전\\ hidden-state\\ vector\\\\x_t:\\ 해당 모듈의\\ input\\ vector\\\\h_t:\\ 생성된\\ hidden-sate\\ vector\\\\\\cdot 최종\\ output(y_t)\\ 출력시\\ h_t를\\ 바탕으로 계산  \\\\f_W:\\ 파라미터\\ W를\\ 포함한\\ RNN 함수\\\\y_t:\\ 해당\\ 모듈의\\ output\\ vecotr\\)                      [fig. RNN에서의 hidden state 계산과 구성 요소]        매 time step 마다 같은 함수와 Parameter 구성을 공유함  \\[h_t = f_W(h_{t-1},x_t)\\\\\\downarrow\\\\h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)\\\\y_t =W_{hy}h_t\\][math. RNN의 자세한 $f_W$함수]      Hidden State Vector($h_t$)의 차원수는 Hyper parameter이다.          즉, 개발자 판단하에 주어져야 함.        [img. $W$의 차원 수는 $h_{t-1}$ 차원수 X ($h_{t-1}$ 차원수 + $x_t$ 차원수)가 된다.]        마지막으로 output $y_t$의 경우 binary classification이면 1차원으로 바꾼 뒤 sigmoid 함수를 이용하며, Multi Class classification이면 n개의 차원으로 바꾼 뒤 SoftMax Layer를 통과 시킨다.  Types of RNNs[img. 여러 종류의 RNN 구조]      one-to-one 구조          키, 몸무게, 나이를 통해 저혈압, 정상혈압, 고혈압을 판단      Time Step이 존재하지 않음            one-to-many 구조          Input이 첫 time step에 한번, 출력은 매번,      나머지 Input은 무의미한 같은 차원의 zero vector가 들어간다.      Image Captioning            many-to-one 구조                  Input은 매번, Output은 마지막 한번                    감정 분석                  sequence-to-sequence 구조 1          Input이 일부 time step에, output이 또 일부 time step에 존재      기계번역            sequence-to-sequence 구조 2          Input과 Output이 매번 존재      video classification per frame      Character-level Language Model  Language Model이란, 문자열이나 단어를 입력받고, 그 다음에 나올 문자나 단어를 출력하는 모델RNN 예시(hello Language Model)  사전(Vocabulary) 구축 및 one-hot vector 생성  사전(Vocabulary)          주어진 문장에서 정보 단위(단어 또는 여기서는 글자)들이 유일하게 이루어진 사전을 구축한다.      예시의 “hello”의 경우 “[h, e, l, o]”라는 Vocabulary가 구축됨.        one-hot vector 생성          index 부분이 1인 사전의 갯수 만큼의 dimension을 가지는 one-hot vector 생성      마지막 “o”의 경우 그 뒤로 예측해야할 필요 없으므로 생성하지 않아도 좋다.      [img. hello의 one-hot vector]      선형결합과 비선형 함수 tanh를 이용한 $h_t$ 학습        [img. $h_t$의 dimension이 3이라고 가정하고 학습]          $h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t+b)$ b: bias      최초의 RNN 모델의 경우 $h_{t-1}$입력이 없으므로 h와 같은 차원의 zero vector를 채워준다.      자세한 방법은 위의 RNN의 구조 참조            output 출력  [img. output vector 출력]  각 RNN 모듈의 output은 hidden state($h_t$)에 $W_{hy}$를 곱한 후 bias를 더해서 구하게 된다.          $Logit = W_{hy}h_t+b$        이후 Output을 softmax 함수에 통과 시켜 가장 큰 값을 결과값으로 확정한다.  이후 ground-truth(실제 값)의 one-hot vector와 비교하여 loss를 줄이는 방향으로 학습시킨다.      학습이 끝난 뒤 Test inference 시행                  최초의 Time step에만 입력을 넣어주고, 이후 Time step 부터는 이전 모듈의 output을 Input으로 넣어주어 추론한다.                    다음날 주식을 예측하는 RNN 모델이 그 다음날, 다음 다음날에도 예측 가능한 이유            [img. 출력을 다음 모듈의 input으로 넣어주는 Inference 구조]RNN 예제[img. 셰익스피어 희곡 생성][img. 영화 대본 생성][img. C언어 code 생성]Backpropagation through time (BPTT)  모든 RNN Time step에 대해 Backpropagtion 또는 ForwardPropagtion을 진행하는 것은 성능상 한계가 있다.[img. RNN에서의 Forward와 backpropagation]  Truncation : 전체를 propagation 하면 너무 느리므로 구간 별(chunks of the sequence)로 잘라서 한다.[img. Truncation  : 구간별로 잘라서 propagation]  Hidden state Vector($h_t$)는  이전 처리 결과, 문맥 등의 필요한 정보를 포함하고 있다.          이러한 Hidden State Vector의 변화를 분석하는 것으로 RNN의 특성을 알 수 있다.      [img. Hidden state의 변화를 시각화한 것.]Vanishing/Exploding Gradient Problem in RNN  RNN에는 Long-term problem과 Vanishing/Exploding Gradient Problem을 가지고 있다.          가중치가 계속 곱해져서 Gradient가 기하급수적으로 커지거나 작아지는 문제                  math      propagation                  $h_t = tanh(w_{xh}x_t+w_{hh}h_{t-1}+b), t=1,2,3\\For\\ w_{hh}=3,w_{xh}=2,b=1\\h_3=tanh(2x_3+3h_2+1)\\h_2=tanh(2x_2+3h_1+1)\\h_1=tanh(2x_1+3h_0+1)\\\\dots\\h3=tanh(2x_3+3tanh(2x_2+3h_1+1)+1)$            [fig. hidden state 중첩에 따른 $W_{hh}$의 중첩 ][gif. LSTM과 RNN의 cell 진행에 따른 gradient 감소 비교]Long Short-Term Memory(LSTM) and Gated Recurrent Unit(GRU)  기존의 RNN 모델에 비해 Vanishing/Exploding gradient 문제를 해결하고, 성능 상에 더 좋은 결과를 보인다.Long Short-Term Memory(LSTM)\\[img. LSTM 구조의 도식화]  정보를 좀 더 오래 남기게 하고 기존의 RNN 모델을 개선한 모델.  Vanishing/Exploding  Gradient Problem, Long-Term dependency 문제를 해결한 모델  Hidden state vector($h_t$)를 기억 소자로 보고, 단기 기억을 길게 기억하도록 개선하여 LSTM이라고 이름 붙임.  이전 Time step에서 넘어오는 정보(Cell state($C_t$),  Hidden state($h_{t-1}$) )가 2개이며, 총 Input은 3개이다.          ${C_t, h_t}= LSTM(x_t,C_{t-1},h_{t-1})$      $C_t$ : Cell state vector, 핵심 정보      $h_{t-1}$: Hidden state vector, Cell state 정보를 필요한 것만 Filtering 한 정보.      Long short Term Memory의 구성과 동작 과정  입력을 받은 후 선형 변환 후 나온 결과물에 4개의 activation 함수를 통과 시켜 gate값들을 만든다.  이렇게 나온 gate 들은 Cell state 및 Hidden State를 계산할 때의 중간 결과물 역할을 한다.            LSTM 구성      수식                        $\\begin{pmatrix}i \\f\\o\\g \\end{pmatrix}=\\begin{pmatrix}\\sigma \\\\sigma\\\\sigma\\tanh \\end{pmatrix}W\\begin{pmatrix}h_{t-1}\\x_t\\end{pmatrix}\\c_t=f\\odot c_{t-1}+i\\odot g\\h_t=o\\odot tanh(c_t)$      [fig. LSTM 내부의 연산, $x_t, h_t$의 dimension을 h라고 가정]  i: Input gate, Whether to write to cell  f: Forget gate, Whether to erase cell  o: Output gate, How much to reveal cell          sigmoid를 통해 나오는 Input gate, Forget gate, Output gate는 0~1사이를 가지며 다른 벡터와 곱해져 0~1 사이의 일부로 축소해주는 역할을 함(gate)        g: Gate gate, How much to write to cellGate 들의 구체적인 예시  gate들은 이전의 $c_{t-1}$을 적절하게 가공하는 역할을 함  Forget gate            수식      도식                  $f_t=\\sigma(W_f \\cdot [h_{t-1},x_t]+b_f)$            [fig. forget gate 수식과 그림]  forget gate는 sigmoid 함수를 통해 $h_{t-1}$과 $x_t$의 일부 데이터를 축소하는 역할  Gate gate &amp; Input gate            수식      도식                  $i_t=\\sigma(W_i\\cdot [h_{t-1},x_t]+bi)\\ (input\\ gate) \\\\widetilde{C_t}=tanh(W_c\\cdot [h_{t-1},x_t]+b_C)\\  (gate\\ gate)\\C_t=f_t\\cdot C_{t-1}+i_t\\cdot \\widetilde{C_t}\\  (input\\ gate \\times gate\\ gate)$            [fig. Input gate와 Gate gate의 수식과 그림]  Input gate와 Gate gate의 결과물을 곱한 뒤, 해당 결과물을 Forget gate와 이전 Cell state를 곱한 것을 더하여 현재의 Cell state를 만들게 된다.          2번의 선형변환를 거친 현재 입력정보($h_{t-1}, x_t$)를 이전 Cell state와 합쳐 새로운 state를 만드다는 의미이다.        Output gate            수식      도식                  $o_t=\\sigma(W_o[h_{t-1},x_t]+b_o) \\ h_t=o_t\\cdot tanh(C_t)$            [fig. Output gate의 수식과 그림]  생성한 Output gate의 결과를 tanh함수를 통과시켜, 현재 필요한 정보만 filtering한 뒤, 현재 Cell state와 곱하여  Hidden state를 만든다.  tanh 함수를 미분할 시 미분값이 sigmoid에 비해 커서 vanishing/exploding gradient problem을 해결할 수 있다.Gated Recurrent Unit(GRU)[img. GRU 구조]  LSTM을 간소화하여 더욱 적은 메모리 요구량과 빠른 계산 시간을 가능하게 한 모델  LSTM과 달리 Cell state가 존재 하지 않고, hidden state 벡터 하나만 다음 Time step으로 보낸다.      LSTM과 더불어 많이 사용되며 성능상 뒤지지 않으면서 빠르게 계산이 가능하다.    GRU의 $h_{t-1}$이 LSTM의 Cell state 이전 time step 들의 정보와 결과값을 함께 포함한다.          $z_t=\\sigma(W_z\\cdot [h_{t-1},x_t])$ (Input gate)      $r_t=\\sigma(W_r\\cdot [h_{t-1},x_t])$      $\\widetilde{h_t}=tanh(W\\cdot[r_t\\cdot h_{t-1},x_t])$ (LSTM의 Gate gate 역할)      $h_t=(1-z_t)\\cdot h_{t-1}+z_t\\cdot \\widetilde{h_t}$      c.f) $C_t=f_t\\cdot C_{t-1}+i_t\\cdot \\widetilde{C_t}$ in LSTM,                  Input gate와 Forget gate의 대신인 1 - Input gate한 값으로 hidden state를 구한다.(가중 평균의 형태)          내부적으로 1개의 gate로 통합하면서 계산량을 줄임                    Backpropagation in GRU[img. GRU propagation]  곱셈이 아니라 덧셈으로 연산해서 Vanishing/Exploding gradient problem을 해결함          $h_t=(1-z_t)\\cdot h_{t-1}+z_t\\cdot \\widetilde{h_t}$        Long term dependency 문제 해결Summary on RNN/LSTM/GRU  RNN은 다양한 길이를 가질 수 있는 Sequence data에 특화된 유연한 형태의 딥러닝 모델 구조  Vanilla RNN은 간단한 구조지만 학습시 문제가 많고 학습이 잘 안된다.  LSTM과 GRU는 long term problem과 vanishing\\exploding gradient 문제를 해결했음  중첩되는 가중치를 곱셈이 아닌 덧셈으로 처리하여 문제 해결Sequence to Sequence with AttentionSeq2Seq &amp; Encoder-decoder &amp; attentionSequence to Sequence &amp; Encoder-decodeSequence to Sequence는 Sequence data를 입력으로 받은 후 Sequence data를 출력하는 many to many 모델을 말한다.자연어 처리, 기계 번역 등이 이에 해당한다.[img. 챗봇의 LSTM Encoder Decoder 예시 ]Encoder-Decoder 모델이 대표적이며, Encoder는 입력 문장을 받아 처리하는 모델이며, Decoder는 출력 문장을 처리하며, 서로 가중치를 공유하지 않는다.Encoder의 입력 처리 정보가 담긴 마지막 $h_t$(hidden state)는 Decoder 모델의 첫번째 Input $h_t$(hidden state) vector가 된다.Decoder는 첫번째 Input으로 Start Token(미리 정의된 특수 문자, )과 마지막 hidden state을 입력 받은 뒤, 순차적으로 결과를 출력하고 해당 결과들Output Token, $h_t$)은 다음 Time step의 Input이 된다.Decoder의 출력이 끝나면 End Token(미리 정의된 특수 문자2, )을 내보내게 된다.Attention기존의 Seq2Seq model의 단점은 다음과 같다.  Hidden state($h_t$)의 차원이 고정되어 있어, 여러 정보를 우겨넣어야 하는 점  Time step이 지나갈 수록 내포하고 있는 정보가 소실 되거나 유실되는 점이러한 문제를 해결하기 위해 Input Sequence를 거꾸로 주는 방법 등이 제시되었다.이를 해결하기 위해 Attention 모듈의 아이디어는​\t마지막 Hidden state($h_t$) 하나만 전달해주는 것이 아니라, 매 Time step에서 계산한 $h_0\\dots h_t$까지 전체적으로 Decoder에 전해준 뒤, Decoder의 각 Time step에서 선별적으로 필요한 Hidden state를 가져간다.[img. Attention 모듈 동작 과정]자세한 Attention 모듈 동작 과정기계번역 예시를 통해 Attention 모듈 동작 과정을 알아보자.[img. Attention 모듈 예시 1~5 ]  Encoder 모듈이 모든 과정을 끝내고 마지막 Hidden state와 Start Token을 Decoder의 첫번째 모듈이 Input으로 받는다, 이때, Encoder 각 Time step에서의 hidden state($h_0,\\dots,h_{t-1}$)들 또한 기억되어 있다.  첫번째 Decoder Hidden state의 결과값 $h_{y0}$를 Encoder의 각 Time step의 Hidden state들과 내적하여 Attention scores를 구한다.          예시를 들자면 무작위의 정수 값이 나온다.{7,1,-1,2}      이때 Attention scores를 구하는 방법은 내적뿐만 아니라 다른 방법도 있다. 아래의 Attention Mechanism 참조        해당 Attention Scores를 softmax 함수를 통과시켜 Attention distribution으로 바꿔준다.          합이 1인 같은 차원의 결과값이 나온다. {0.83,0.05,0.02,0.1}      이 값은 일종의 어떤 가중치이며, 어느 시점의 hidden state를 얼만큼 참조해야하는가? 의 의미를 가진다.        Attention distribution의 가중 평균을 구하여 하나의 Encoder Hidden state vector, Attention output을 구한다.          Attention output은 Attention distribution 만큼 정보를 사용해 만든 정보이며, Context vector라고도 부른다.      2번부터 4번까지 과정을 실시하는 부분을 Attention module이라 부른다.        이렇게 나온 Attention Output과 원본 Output를 Concatenation하여 예측 값($\\hat{y_1}$)을 출력한다.[img. Input을 이전 예측값이 아니라 실제값에서 가져오면 Teacher forcing이다. ]  1번부터 5번까지의 과정을 다음 Time step에서도 반복한다.          이때, 이전 예측 결과값을 Input으로 넣어주지 않고, Ground Truth에서 가져온 실제값을 넣어주는 사진에서의 방식을 Teacher forcing이라고 한다.      첫 예측부터 틀리고, 틀린 예측 값을 다음 Input으로 넣어주면 무더기로 틀리면서 시간낭비를 하기 때문에 보통은 초반 학습에는 Teacher forcing을 이용하고, 예측 정확도가 올라가면 이전 예측값을 넣어주는 원래 방식으로 돌린다.      다양한 Attention 메커니즘\\[score(h_t,\\overline{h}_s)=\\begin{cases}h_t^\\top \\overline{h}_s&amp;dot\\\\h_t^\\top W_a\\overline{h}_s&amp;general\\\\v_a^\\top \\tanh(W_a[h_t;\\overline{h}_s]) &amp; concat\\end{cases}\\\\\\overline{h}_s : Encoder에서의\\ Hidden\\ state,\\\\ h_t^\\top: Decoder\\ Hidden\\ state가\\ 행렬\\ 연산을\\ 위해\\ 행과\\ 열이\\ 뒤바뀐\\ 형태\\\\ (a \\times b \\rightarrow b \\times a)\\][math. attention의 score를 구하는 3가지 방법 ]  Dot ($h_t^\\top \\overline{h}_s$)          기본적인 방식, 두 Hidden state를 내적한다.        General($h_t^\\top W_a\\overline{h}_s$)          학습가능한 파라미터 $W_a$를 넣어 score 계산 방법 또한 Back propagation을 통해 학습되게 한다.        Concat($v_a^\\top \\tanh(W_a[h_t;\\overline{h}_s])$, Bahdanau attention)          두 벡터를 Concatenation하여 하나의 벡터로 만들고, $W_a$와 tanh 함수, Scalar 값으로 바꿔줄 벡터 $v_a^\\top$을 통하여 작은 Multi Layer Network를 만들어 학습되게 한다.      General이나 Concat 방식 처럼 학습 가능한 파라미터가 포함되면 다음 그림과 같은 Propagation을 진행되게 된다.[img. Attention 모듈이 포함된 model의 propagation 진행]Attention의 장점  hidden기계 번역 분야(NMT)에서 성능 향상  bottleneck(병목) 문제 해결          Bottleneck 현상: 고정된 크기의 벡터에 너무 많은 벡터 정보를 압축시키면서 정보가 손실, 변형 되며 성능이 악화되는 현상        Vanishing gradient Problem 완화          Back Propagation 시, 먼 거리를 거치지 않고, 특정 Time step에 도달하므로 가중치 중첩이 적다.        가중치의 변경 해석을 통해 예측방법을 해석할 수 있게 해줌.[img. x축 프랑스 단어, y축 해석한 영단어를 토대로 얼마나 정보를 참조했는 가를 그래프로 표현 가능]Beam search  자연어 처리를 위한 Seq2Seq 모델의 Test에서 더욱 좋은 성능을 나오게 하는 알고리즘Decoding 결과값 예측 방법 및 문제점Decoder가 예측값을 생성할 때, 3가지 방법이 있다.  Greedy decoding(탐욕 알고리즘)가장 확률이 높은 예측을 예측값으로 바로 내보내는 방법이다.가장 빠르다는 장점이 있지만, 한번 예측이 틀리면 그 뒤로 정정할 방법이 없다.즉, 가장 높은 확률의 단어 뒤로는 가장 낮은 확률인 틀린 단어 들만 존재할 경우를 방지할 수 없다.  Exhaustive search(완전 탐색)레퍼런스 길이가 T인 문장에 단어들의 정답 확률을 $y_0,\\dots,y_{t-1}$이라 할 때, \\(P(y|x)=P(y_1|x)P(y_2|y_1,x)P(y_3|y_2,y_1,x)\\dots P(y_T|y_1,\\dots,y_{T-1},x)=\\prod^T_1P(y_t|y_1,\\dots,y_{t-1},x)\\)[math. 레퍼런스 길이가 T인 문장의 정답 확률  ]            $P(y      x)$가 가장 높은 값이 되도록 하기위해, 가능한 모든 단어쌍을 확인해보는 방법이 있다.      하지만 이 방법의 경우 사전(Vocabulary size)가 V이고, 문장의 길이가 t라고 할때 무려 $O(V^t)$의 시간이 걸리며, 성능상 불가능한 경우가 많다.위 두가지 방법을 절충안 방안이 세번째 방안인  Beam search가장 확률이 높은 beam size k개 만큼의 가지치기를 하여 가장 확률이 높은 단어(또는 hypothese)을 선택하는 방법이다.\\(score(y_1,\\dots,y_t)=logP_{LM}(y_1,\\dots,y_t|x)=\\sum_{i=1}^tlogP_{LM}(y_i|y_1,\\dots,y_{i-1},x)\\)[math. 가치치기로 생성된 경우의 수의 확률을 구하는 수식]  0~1 사이인 확률 값에 log 함수를 씌워 덧셈으로 계산하게 하여 계산 용이 + 너무 작은 수로 수렴하는 것 막음  또한 hypothesis의 확률 값이 0~1사이 이므로 음수들이 나오게 되며, 이값들의 합이 가장 큰 값이 가장 좋은 값이다.beam size인 k를 조절하여 원하는 성능에 타협할 수 있다는 장점이 있지만, 이 방법은 최적의 결과를 보장하지 않는다는 단점이 있다.Beam search의 예시k가 2일 때, Reference가 “ he hit me with a pie\" 인 문장의 경우[img. Beam Searching 과정]  [he, hit, me]까지 진행한 -2.5가 최고 점수임을 확인할 수 있다.Beam search의 종료와 점수 평가Greedy decoding(algorithm)의 경우  토큰이 나올 경우, 예측의 종료임을 알 수 있었다.하지만 beam search의 경우, k가 1 초과일 경우, 가지치기로 가 나오지 않는 가지(hypothesis)가 계속 뻗게 된다.그러므로, 예측의 종료를 위해 최대 Time step(또는 문장의 길이) T를 정하고, 그 이상 부터는 예측 하지 않거나, 또는 n개의 종료된 가지(hypothesis), 즉 n개의  토큰이 나올때 까지만 예측을 진행하게 한다.  여기서 T와 n은 predefined, 즉 미리 정의해 줘야한다.  종료될 때까지,  토큰이 나와 종료된 가지는 따로 마련한 저장공간에 점수와 내용을 저장해 놓고, 종료된 후, 점수를 비교하게 된다.이때, 그저 점수를 비교하면, 상대적으로 문장의 길이가 짧은 경우가 유리하게 되므로, 문장의 길이로 나누어 주어, 전체 단어의 평균 확률이 높은 가지를 고르게 한다.\\(score(y_1,\\dots,y_t)=\\frac{1}{t}\\sum_{i=1}^tlogP_{LM}(y_i|y_1,\\dots,y_{i-1},x)\\\\where\\ t = number\\ of\\ hypothese\\)[math. 길이 Normalize가 적용된 score 계산법]BLEU score  자연어 생성 결과의 품질의 척도를 구하는 방법에 대해 알아보자.단순히 문장의 Index끼리 비교를 하면, 문장의 길이가 다를 경우 0점으로 평가될 수 있다.                   문장                  Reference      I love you baby, and it’s a quite alright              Predicted      oh, I love you baby, and it’s a quite alright      [fig. “oh” 한 단어가 들어가 Index가 뒤로 밀린 경우의 문장]이를 위해 단순비교 이외의 평가방법들을 사용해야 한다.정밀도(precision), 재현율(recall), 조화평균(F-measure)주어진 문장이Reference: Half of my heart is in Havana ooh na naPredicted: Half as my heart is in Obama ooh na, 일때,\\(precision=\\frac{\\#(correct\\ words)}{length\\_of\\_prediction}=\\frac{7}{9}=78\\%\\\\recall=\\frac{\\#(correct\\ words)}{length\\_of\\_reference}=\\frac{7}{10}=70\\%\\\\F-measure=\\frac{precision\\times recall}{\\frac{1}{2}(precision+recall)}=\\frac{0.78\\times0.7}{0.5\\times(0.78+0.7)}=73.78\\%\\)[math. 주어진 문장에 대한 정밀도, 재현율, 조화평균 ]  정밀도(precision)는 검색된 결과들 중 관련 있는 것으로 분류된 결과물의 비율이고, 재현율(recall)은 관련 있는 것으로 분류된 항목들 중 실제 검색된 항목들의 비율이다.  산술 평균 $\\geq$ 기하 평균 $\\geq$ 조화 평균이 성립하므로, 오류에 좀더 가중을 주기 위해 조화평균을 사용한다.          산술 평균 : (a + b) / 2      기하 평균: $(a*b)^\\frac{1}{2}$      조화 평균: $\\frac{1}{\\frac{\\frac{1}{a}+\\frac{1}{b}}{2}}$      하지만 이 척도는 Sequence data의 순서의 오류를 고려하지 않아 부적절하다.예를 들자면, 주어진 문장이Reference: Half of my heart is in Havana ooh na naModel 1 Predicted: Half as my heart is in Obama ooh na, Model 2 Predicted: Havana na in heart my is Half ooh of na, 일때,            Metric      Model 1      Model 2                  Precision      78%      100%              Recall      70%      100%              F-measure      73.78%      100%      [fig. 세가지 척도로 평가 시 잘못되는 예시]적절하지 못한 결과를 보여줌을 알 수 있다.BLEU scoreBiLingual Evaluation understudy(BLEU)는 자연어 처리 결과를 평가하기 위해 만들어졌다.\\(BLEU=min(1,\\frac{length\\_of\\_prediction}{length\\_of\\_reference})(\\prod^4_{i=1}precision_i)^\\frac{1}{4}\\)[math.  BLEU 계산 수식]  기하평균을 이용하여 조화평균 보다는 오류에 관대하게 하였다.  N-gram overlap을 이용하여 단어의 순서 또한 평가에 반영하게 하였다.  recall 대신 precision을 평가에 사용하는 이유는, 기계 번역 등에서는  단어의 수, 문장의 길이 등이 정확히 맞지 않아도 올바른 결과인 경우가 있기 때문에, reference의 길이에 강요받지 않기 위해서 이다.  ex) 나는 정말 니가 많이 좋아 , 난 정말 니가  좋아 :arrow_right: 길이가 다르지만 둘다 옳은 번역이다.  문장의 길이가 짧은 경우 의미를 모두 담지 않은 경우가 있으므로 brevity penalty를 주지만, 그렇다고 해서 결과값이 길수록 점수가 높아지는 것을 막기위해 min 함수를 씌워 1이 최대값으로 주게 하였다.BLEU 계산 예시주어진 문장이Reference: Half of my heart is in Havana ooh na naModel 1 Predicted: Half as my heart is in Obama ooh na, Model 2 Predicted: Havana na in heart my is Half ooh of na, 일때,            Metric      Model 1      Model 2                  Precision (1-gram)      7/9      10/10              Precision (2-gram)      4/8      0/9              Precision (3-gram)      2/7      0/8              Precision (4-gram)      1/6      0/7              Brevity penalty      9/10      10/10              BLEU      $0.9\\times (1/54)^{\\frac{1}{4}} \\approx 33\\%$      0      [fig. BLEU 계산 예시]Tansformer  RNN을 사용하지 않고 attention 만으로 Sequetial data를 입력 받고 예측하는 모델Transform introduction[img. Transform 모델 구조]Transform과 Attention 모델은 같은 논문에서 처음 소개되었음.*[img. 기존의 RNN]기존의 외방향 RNN은  다음과 같은 문제가 있었다.  전달되는 hidden State가 쌓일 수록 병목 현상(bottle neck) 때문에 멀리 있는 과거 정보(context)가 변형되거나 소실 되는 문제  반대 방향 Back propagation 시 Vanishing/Exploding gradient Problem을 가지고 있었다.  Sequential data 순서 반대 방향(즉, 미래에 나올)의 정보는 해당 Time step에서 참고할 수 없음.[img. Bi-directional RNN]양방향 RNN(Bi-directional RNN)은 별개의 다른 방향의 RNN 모델 2개를 생성하여, 각 Time step에 2개의 Hidden state를 Concat하여 2배의 Dimension을 가진 Vector를 가지는 대표적인 Encoding Vector를 형성한다.[img. Traonsform attention 구조에서 context vector를 구하는 과정]Encoder와 Decoder 내부에서는 앞서 배웠던 Attention 모듈과 비슷한 Self-attention 모듈을 통하여 Output vector를 형성하게 된다.앞선 Attention 모듈과의 차이점은 Self-attention은 Decoder에서 Encoder의 hidden state를 자기 자신의 Input vector와 사용한 것과 다르게 자기 자신의 Input Vector만으로 Output vector를 형성한다는 점이다.이때 Self Attention 구조가 아닌, 단순 Input Vector의 내적으로 구성시, 자기 자신의 Input에만 과도하게 가중치를 부여하는 단점이 있다고 한다.Self-Attention 과정은 다음과 같다.  각자 Embedding된 Input Vector들($X_1,\\dots,X_t$)로 이루어진 행렬 X를 $W^Q$와 내적하여 Query Vector 들로 이루어진 행렬 Q를 구한다.  같은 행렬 X를 $W^K$와 내적하여 Key Vector로 이루어진 행렬 K를 구한다.      같은 행렬 X를 $W^V$와 내적하여 Values Vector로 이루어진 행렬 V를 구한다.        Q와 $K^T$의 내적한 결과에 Softmax 함수를 처리 한 뒤,  V를 내적하여 Context Vector들로 이루어진 행렬  Z를 구한다.                  Z는 Value Vector들의 Weighted Sums            $K^T$는 Q와 내적하기 위해 K 행렬의 차원 을 Transform 한것. $(a\\times b)\\rightarrow(b\\times a)$      이때 Q와 K의 차원 수($d_k)$는 같아야 하고, V의 차원수($d_v$)는 달라도 된다.                  하지만 보통은 편의를 위해 일부러 차원수를 모두 같게 한다.                    그저 내적만 하지 않고, k의 차원수(=q의 차원수)의 루트값으로 나눠주는 방법도 있다. 아래 Scaled Dot-Product Attention 참조      이 과정을 수식으로 표현하면 다음과 같다.\\(A(q,K,V)=\\sum_i\\frac{\\exp(q\\cdot k_i)}{\\sum_j\\exp(q\\cdot k_j)}v_i\\\\A(Q,K,V)=softmax(QK^T)V\\\\where\\ Q: query\\ vector가\\ 모인\\ 행렬, K^T: K의\\ 차원을\\ Transform\\)[math. Dot-Product Attention의 수식과 간단한 버전]|  || :———————————————————-: || $(|Q|\\times d_k)\\cdot s(d_k\\times |K|)\\cdot(|V|\\times d_v)=(|Q|\\times d_v)$ |[fig. Self-attention 과정을 그림으로 표현한 것]  softmax 함수의 기본 수행은 Row-Wise이다.| 수식                                         | 도식                                                         || ——————————————– | ———————————————————— || $A(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ |  |[fig. Scaled Dot-Product Attention ]이때 k의 차원수가 커지면 Q와 K의 내적값의 분산값이 커지게 되고, 이렇게 분산값이 큰 Vector에 Softmax 함수를 사용하면 결과로 확률분포 또한 극단적으로 나오게 되며, 이렇게 극단적으로 나온 확률분포는 Vanishing Gradient Problem을 일으킨다.이를 막기 위해 내적값들에 softmax 함수 이전에 $\\sqrt{d_k}$로 나누어 분산을 줄이는 방법을 Scaled Dot-Product Attention이라고 한다.Transformer(cont’d)Multi-Head Attention  Self-Attention 모듈을 좀더 유연하게 확장한 모듈            수식      도식                  $MultiHead(Q,K,V)=Concat(head_1,\\dots,head_k)W^O\\where\\ head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$            [fig. Multi-Head Attention]Multi-Head Attention은 앞서사용한 Scaled Dot-Product를 구하는 과정을 각기 다른 여러 Parameter($W^Q_i,W^K_i,W^V_i$)을 이용해 여러 겹으로 진행한 모델이다.위의 Scaled Dot-product Attention의 $W^Q, W^K, W^V$와 다른 듯?[img. 각 헤드별 Attention 결과]이때 각 층을 Attention Head라고 하며, 각기 다른 Output Vector $Z_i$가 나오게 된다.[img. 최종 Ouput Vector Z의 도출]각 Attention Head에 나온 Output Vector를 1개의 다차원 벡터로 Concat 한 뒤, 추가로 선형 변환하여 최종 Output Vector Z를 구하게 된다.| Layer Type                 | Complexity per Layer   | Sequential Operations | Maximum Path Length || ————————– | ———————- | ——————— | ——————- || Self-Attention             | $O(n^2\\cdot d)$        | $O(1)$                | $O(1)$              || Recurrent                  | $O(n\\cdot d^2)$        | $O(n)$                | $O(n)$              || Convolutional              | $O(k\\cdot n\\cdot d^2)$ | $O(1)$                | $O(\\log_k(n))$      || Self-Attention(restricted) | $O(r\\cdot n\\cdot d)$   | $O(1)$                | $O(n/r)$            |  n : 입력 Sequence data 길이  d :  hyper parameter 차원  k : CNN kernel Size  r : 최대 이웃거리 (restricted self-attention)[fig. 모델에 따른 계산 복잡도 도표]각 층에서의 연산에서 Self-Attention 구조는 Recurrent와 다르게 Query vector와 Key vector의 내적을 구하는 과정에서 O($n^2\\cdot d$)만큼의 성능이 걸리고 Recurrent는 각 input마다 순차적으로 일을 처리하므로 O($n\\cdot d^2$)만큼의 시간이 걸린다.일반적으로 n은 입력 데이터에 따라 유동적으로 변하므로 Self-Attention 구조의 변동성이 더 크다고 할 수 있지만, 이전 Hidden state가 Input으로 주어져야 다음 Hidden state를 계산할 수 있어 $O(n)$의 시간이 드는 RNN과 달리 Self-Attention 구조는 동시에 진행할 수 있어, 컴퓨터 코어 수만 많다면 병렬적으로 상수 시간 내에 진행할 수 있어 빠르다.또한 Back propagation 진행시, 정보 접근 시에도 무조건 순차적으로 진행되는 RNN과 달리 곧바로 파리미터에 접근할 수 있어서 Self-Attention 구조가 더욱 빠르다.Self-Attention Block과 Residual Connection[img. 하나의 Self-Attention block]이러한 Attention 구조의 공통적인 Attention 과정과 후처리 부분을 블록화하여 하나의 모듈처럼 사용하곤 한다. (정확히는 Encoder 부분)  크게 Feed-forward 부분과 Multi-head attention 두 구조로 나뉜다.이때 CNN에서 본 것과 같이  Input 값을  Attention 처리하지 않고 Skip하여 완성된 Output Vector값에 행렬합을 해주는 것을 Residual connection이라고 하며 한 Block에 2번 처리된다.\\(LayerNorm(x+sublayer(x))\\)[math. Residual connection 수식]CNN과 같은 장점인 Vanishing Gradient Problem을 해결할 수 있다.이때 중요한 점은 행렬간의 덧셈이 성립되기 위해 Attention 처리된 Output Vector는 원본 Input과 같은 차원을 가져야 한다는 점이다.Layer Normalization학습 안정화와 성능 향상을 위해 Layer Normalization을 한다.크게 2단계로 나뉘는데,  Vector들의 평균과 분산값을 0과 1로 바꾸는 Normalization 과정  학습가능한 Parameter를 집어넣기 위해 파라미터가 포함된 선형 함수에 집어넣는 Affine Transformation 과정으로 이루어져있다.Residual Connection과 같이 2번 시행된다.\\(\\mu^l=\\frac{1}{H}\\sum^H_{i=1}a^l_i,\\ \\ \\sigma^l=\\sqrt{\\frac{1}{H}\\sum^H_{i=1}(a_i^l-\\mu^l)^2},\\ \\ h_i=f(\\frac{g_i}{\\sigma_i}(a_i-\\mu_i)+b_i)\\\\\\mu : 평균,\\ \\sigma : 표준\\ 편차,\\ h_i:Result\\ Vector\\)[math. Layer Normalization의 수식][img. Layer Normalization 이외의 방법들][img. Layer Normalization 예시]Positional EncodingRNN과 달리 Attention은 Hidden state가 순차적으로 주어지지 않고, 모든 hidden state를 동시에 이용할 수 있다.이 때문에 병렬 처리, Gradient 문제, 병목 문제 해결 등 여러 장점을 가지고 있지만, 하나의 모순이 있다.바로, Sequential data의 순서를 학습과정 중에 고려하지 않는다는 점이다.  RNN의 경우, 접근할 수 있는 hidden state는 바로 이전 hidden state이므로, 각기 output이 주어진 정보가 다르므로 순차성에 따른 구별이 간다.(뒤로 갈수록 hidden state에 많은 정보가 담기니까)  하지만 Attention은 동시에 모든 hidden state 또는 Input에 접근하므로 구별이 없다.(구별하게 되는 hidden state 또는 Input별 가중치는 Input이 주어진 뒤로 계산된다.)이를 해결하기 위해 Input 값의 위치에 따라 Vector에 구별이 가는 특별한 값을 더해주는 방법이 Positional Encoding이다.\\(PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{model}})\\\\PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{model}})\\\\\\)[math. Positional Encoding 수식][img. Positional Encoding 그래프]sin함수와 cos함수 주기의 차이성, 또, 같은 함수 순번에도 주기를 다르게 주어 순서에 따라 특별한 값을 Input vector에 더해준다.보통 맨처음 Input 시점에 한번만 진행된다.Warm-up Learning Rate SchedulerGradient Descent, Adam등의 과정에서 Hyper parameter인 Learning Rate 값을 고정하지 않고 변동을 주는 방법이다.\\(learning\\ rate = d^{-0.5}_{model}\\cdot \\min(\\#step^{-0.5},\\#step\\cdot warmup\\_steps^{-1.5})\\)[math. Learning rate 변경 수식 ][img. Learning rate 변경의 예시]경험적으로 위와 같은 그래프를 사용한다.High-Level View, VisualizationEncoder에 Attention Block을 쌓아서 High-Level를 만들 수 있어서, Visualization을 통해 볼 수 있다Decoder의 Multi-Head Attention에서 Encoder에서 가져온 Key 행렬과 Value 행렬에 준 가중치를 layer별로 확인하여, 모델의 추론을 Visualization할 수 있다.[img. 각 layer 별 참조한 단어에 대한 Visualization]Decoder &amp; Masked Self-Attention[img. Encoder-Decoder 구조]Decoder 부분에서는 2번의 Self-Attention을 진행하게된다.      Masked decoder 부분          단순 추론 뿐만 아니라 Query 행렬과 Key 행렬의 내적 연산 Softmax output의 일부분을 마스킹한 뒤, 다시 Normalization 하여 내보내는 부분.      이를 통해 아직 추론하지 않아 접근할 수 없는(생성되지 않은) 이후의 Sequential data에 가중치를 주는 것을 막는다.            Encoder-Decoder 부분          Key 행렬과 Value 행렬을 Encoder에서 가져와서 Decoder에서 보내준 Query 행렬과 Attention 하는 부분.            [img. Query 행렬과 Key 행렬 내적의 결과]        [img. 마스킹 ]  **[gif. 첫 토큰이 주어졌을 때, Masked Self-Attention 과정]**[img. Transformer와 성능 비교]낮은 성능처럼 보이지만 BLEU 기준의 결과이므로, 비슷한 결과물들 또한 점수가 낮게 나오므로 실제로는 상용으로 쓰는 수준의 성능이다.Self-supervised Pre-training ModelsSelf-supervised(자기지도 학습)  Task를 풀기 위해 자동으로 데이터를 라벨링하는 학습Pre-training(사전훈련)  여러 Task로 Transfer Learning 될 수 있도록, 하나의 Task로 모델의 Parameter를 훈련시키는 것,Fine Tuning(미세 조정)  미리 개발, 훈련된 모델을 용도에 맞게 파리미터, Layer 구조 등을 변경하는 것Transfer Learning(전이 학습)  미리 개발, 훈련된 모델을 용도에 맞게 나의 데이터셋 등으로 재학습시키는 과정Self-Supervised Pre-Training ModelsTransfer Learning, Self-supervised Learning, Transformer를 사용해 NLP에서 압도적인 성능을 보여준 두 모델을 알아보자.  Transformer 모델은 좋은 성능으로 NLP 분야 뿐만 아니라 여러 분야에서 쓰이고 있다.  최근에는 Self-attentio 층을 24층 이상으로 쌓은 뒤 self-supervised learning framework에서 학습 후, Transfer learning 형태로 fine tuning한다.  natural language generation에서 greedy decoding 수준에서 벗어나지 못하고 있다.GPT-1[img. GPT-1 모델]Pretrained model의 시초격, OPEN-AI에서 개발. Transformer 모델의 구조 특히 Decoder 구조를 따름다양한 special token을 통해 fine-tuning간의 transfer learning을 효율적으로 바꿈  Start 뿐만 아니라, Delimiter, Extranction Token 등 다양한 Special Token을 통하여 여러 문제 해결 가능여러 자연어 처리(classification, similarity,  entailment 등)를 큰 변화 없이 처리 가능한 통합적인 모델12개의 self-attention decoder-only transformer 모델층으로 이루어짐여러 방안으로 사용할 때는 미리 여러 데이터로 학습된 GPT-1 모델(pre-training)을 transfer learning의 형태로 사용 시에는 후반의 일부 layer을 용도에 맞게 교체하고, Learning rate를 적게주어 조금만 학습 시키는 방식(fine-tuning)으로 사용.[img. GPT-1 성능 결과]BERT가장 널리 쓰이는 NLP Pre-trained 모델, Encoder 구조만 활용GPT는 단방향의 Masked Attention 구조를 활용하는 반면, BERT는 아래의 MLM을 활용하여 양방향 Self-attention 구조이다.[img. 다양한 모델들 비교]  ELMO: Transformer 이전에 LSTM 기반 Encoder로 Pre-trained 된 모델          최근의 모델들은 ELMO에서 LSTM Encoder를 대체한 형태      기존의 모델들은 앞에 나온 단어 부터 순차적으로 문맥을 판단하지만, 실제 독해에서는 앞 뒤 모두의 문맥을 파악해야 한다.Masked Language Model(MLM): BERT에서의 Pre-Training  학습 방법 형태, 문장에서 일부 단어를 [MASK] 토큰으로 바꾼 뒤, 앞뒤 문맥을 파악하여 맞추는 형식으로 학습  어느 정도 비율의 단어들을 마스킹할 것인가?는 Hyperparameter이다.  보통 15%정도로 시작하며 너무 많이 마스킹 하면 문맥이 제대로 파악하지 못해 학습이 안되고, 너무 적게 마스킹하면 Training이 오래 걸린다.[img. MLM 예시]단, Mask된 단어는 그대로 마스킹된 채로 학습하면 실제 데이터는 마스킹되어있지 않기 때문에 괴리가 있는 결과가 나오기때문에, 실제로는 마스킹하기로 한 단어의 80%는 마스킹, 10%는 임의의 다른 단어로 교체, 10%는 그대로 두는 형식으로 학습시킨다.Next Sentence Prediction (NSP)문장 간의 관계에 대한 예측하여 학습하는 것, BERT의 Pre-Training 학습 방법 중 하나  어떤 문장이 특정 문장 다음 문장으로 올만한 문장인가? 연관있는 문장인가? 아니면 그냥 랜덤 문장인가?  아래와 같이 data를 생성하여 교육 시킨다.Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]Label = IsNextInput = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]Label = NotNext[code. NSP Input 예제 ]  [CLS] 토큰 : GPT의 Extractor 토큰, 문장의 예측을 위한 선언용?  [SEP] 토큰 : 문장 사이에 구별을 위한 토큰, 문장의 끝을 알림  Label : 포함된 두 문장의 관계에 대한 LabelSummary구조는 Transform 구조와 유사하며,BERT BASE의 경우 SELF-ATTENTION LAYER 12개, ATTENTION HEAD 12개,  인코딩 벡터 차원 수는 768개이며, 경량화되어 있으며,BERT LARGET의 경우 SELF-ATTENTION LAYER 24개, ATTENTION HEAD 16개,  인코딩 벡터 차원 수는 1024개INPUT SEQUENCE의 경우      단어를 SUBWORD라는 단위로 잘개 쪼개는 WordPiece embeddings 라는 방법을 사용한다.(ex) Pretraining -&gt; pre, training)        앞서 설명했던 Poisitional Encoding 방법을 변형한 미리 학습한 최적화된 값(Learned positional embedding)을 이용한다.    [CLS](Classification embedding), [SEP](Packed sentence embedding) TOKEN  Segment Embedding[img. Segment embedding의 사용]  일종의 문장별로 구분하기 위한 소속 문장 속성이 추가된 Embedding  Token + Segment + Position Vector를 서로 더해준다.[img. BERT의 Transfer Learning을 위한 Fine-Tuning 과정]  Input Layer와 Output Layer을 달리하여 특정 Downstream Task를 위한 모델로 만들 수 있다.  Masked token의 prediction을 위한 Layer를 제거한 후, 우리 Task를 위한 Layer로 바꾼 뒤, 기학습된 Transfomer encoder의 parameter들은 작은 Learning rate를 사용하여 조금만 학습이 되게 한다.                   BERT      GPT                  Training-data size      BookCorpus + Wikipedia(2,500M words)      BookCorpus(800M words)              Training special tokens during training      [SEP],[CLS], sentence A/B embedding during pre-training      -              Batch size      128,000 words      32,000 words              Task-specific fine-tuning      task-specific fine-tuning learning rate(task 별로 다름)      5e-5 고정                     Masked token를 통해 앞뒤 모든 단어 접근 가능      오직 이미 나온 단어만 접근 가능      [fig. BERT vs GPT][img. GLUE data, BERT 성능 비교]Machine Reading Comprehension(MRC)란, 기계독해하여 질의응답(Question Answering)하는 TaskDaniel and Sandra journyed to the office, Then they went to the garden. Sandra and John travelled to the kitchen. After that they moved to the hallway.q: where is daniel?a: garden[code. MRC 예제]SQuAD 같은 MRC 데이터셋에서 BERT 모델은 언제나 순위권에 많이 존재한다.  SQuAD 1.1  질문과 데이터셋을 sentence로 생각하고 [CLS] 토큰으로 붙인 후,  단어들을 Fully connected layer를 통해 스칼라 값을 얻어낸 후, 답변이 존재하는 문구의 시작 위치 단어와 끝 위치 단어를 찾아내는 방식  SQuAD 2.0 : BERT 변형 모델이 많이 존재  toekn 0 CLS 토큰을 no answer 로 사용 하여 답이 없는 경우도 방지  On SWAG  질문 + 선택지들을 [CLS] 토큰과 함께 각각 concat한 뒤, 나오는 Encoding vector를 Fully connected layer을 통과해서 나온 Scalar 값으로 판단.  Ablation Study[img. Model Size vs 성능]  Model의 Parameter 크기가 클수록 성능이 좋아진다.Advanced Self-supervised Pre-training Models  좀더 경량화, 고도화된 Pre-training ModelGPT-2[img. GPT-2 결과 예시]GPT-1과 구조가 크게 다르지 않으며, layer가 더 싶고 training data가 더 많아진 형태  일부 Layer normlization의 위치 이동  residual layer의 weight를 residual layer의 수 루트값에 반비례하게 줄여줌.(=영향력 약화)  다음 단어를 예측하는 방법으로 학습시킴, dataset 퀼리티를 늘리기 위해 text들을 걸렀음  Reddit이라는 커뮤니티에서 좋아요가 3개 이상인 외부링크의 document를 수집  전처리로 Bytpe pair encoding 적용, subword 수준의 word embedding.          구현 방법은 [NLP-Assignment]Byte_Pair_Encoding,ipynb 참조      down-stream task에서 zero-shot setting, 추가 parameter나 구조 변경없이 사용 가능decaNLP가 동기가 됨. : 모든 language task는 Question Answering으로 통합할 수 있다.  기계 번역: 이 문장의 한국어 버전은 무엇인가?  문서 요약: 이 문서의 요점은 무엇인가?  감정 분석: 그래서 화자는 이 물건을 좋아하는가? 싫어하는가?  이 원리를 통해 zero-shot setting, 별다른 모델변경, 학습(fine-tuning) 없이 down-stream task 가능,GPT-2 예시[img. GPT-2 문서 요약]별다른 fine-tuning 없이 문서 마지막에 TL;DR:(Too long, didn’t read) 이란 단어를 추가하면 다음에 요약한 문장을 생성해줌.[img. GPT-2 기계 번역]  문장에 wrote in French: 라고 붙이자 불어로 번역해줌GPT-3[img. GTP-3 parameter 수 비교]GPT-2에 비해 모델 구조 등에 특별한 변화보다, Parameter 수, Dataset, Batch size를 비교할 수 없을 정도로 크게 바꿔줌96 Attention layers, Batch size of 3.2M[img. shot]Zero-shot setting으로도 충분히 downstream-task가 가능하지만 one-shot, Few-shot 처럼 example을 줌으로 더욱 성능을 향상할 수 있다.  Zero-shot: Question answering 기반으로 문장을 주고 번역  One-shot: 하나의 번역 예시를 보여주고 번역  Few-shot: 여러 개의 번역 예시를 보여주고 번역  zero-shot으로도 model parameter의 수가 커질 수록 성능이 증가하지만,  shot이 늘어날 수록 급격하게 성능이 좋아짐.[img. shot별 성능 향상]ALBERT경량화된 BERT, Lite BERT라는 의미점점 모델이 거대해지면서 메모리의 한계와 느려진 학습 속도를 해결하기 위해, 다음과 같은 방법을 이용했다.      Factorized Embedding Parametrization        [img. Factorized Embedding Parametraiztion]    Residual connection은 skip하며 layer 사이마다 더해지므로 일정크기의 dimension의 유지가 강제된다.    이 dimension이 너무 작으면, 정보가 많이 안담기며, 너무 크게 잡으면  Parameter 수가 늘어나고, 시간이 너무 오래걸린다.    ALBERT에서는 embedding의 차원 크기를 줄이면서 정보의 소실을 줄이고, Residual 합이 가능한 방법을 쓴다.        [img. BERT VS ALBERT]    ​\t- V : Vocabulary size, H : Hidden-state dimension, E : Word embedding dimension          ALBERT는 작은 크기의 word embedding에 추가로 dimension을 늘려주는 layer을 추가하여 Residual net 이전에 차원 크기를 불려서 해결했다.  (Row rank matrix factorization)            Cross-layer Parameter Sharing                  Multihead-self-attention 구조는 각각 head마다 학습시켜야할 선형변환 행렬($W_Q,W_V,W_K,W_o \\times $ head 수 등)들이 존재하는데, 이를 head마다 구분하지 말고 공유하는 하나의 행렬로 바꾸는 구조를 의미                    Shared-FFN : feed-forward network parameter만 공유                    Shared-attention : attention layer의 parameter 공유                    All-shared: 위 두개 전부 공유                [img. share 했을 시에 성능 비교, Parameter는 크게 줄어들고, 성능은 조금 하락]                  Sentence Order Prediction          BERT의 Next Sentence Prediction task가 비효율적이라 판단하여 Masked Language Model만 실시      대신 같은 문서에서 2개의 문장의 순서를 뒤바꾸거나 그대로 둔 뒤, 정상적인 순서인지 맞추는 학습(Sentence Order Prediction)을 실시함.                  BERT의 NSP는 다른 문서에서 가져온 Negative sample의 경우, 너무 쉽고, 같은 문서에서 가져온 Postive sample의 경우 문맥이 아니라, 단어의 빈도 등으로 맞추는 등의 문제가 있었음                          [img. 기존 BERT와 비교시, 오히려 SOP의 성능이 가장 높다.]  [img. GLUE 성능 비교 결과]  ALBERT 성능이 좋다.ELECTRAEfficiently Learning an Encoder that Classifies Token Replacements Accurately의 준말BERT, GPT의 학습 방식과 달리 Generoator란 단어 복원기를 통해 Masking된 단어를 다시 복원한 뒤,  그 결과를 ELECTRA의 Discriminator를 통하여 원본과 비교하여, 복원된 단어인지, 원래 masking 안된 원본인지 구별하는 방법(GAN, +generative adversal network)이다.학습은 Generator, Discriminaotr 둘다, 실제 test 에는 Discriminator만 사용[img. ELECTRA의 학습 방법][img. ELECTRA와 다른 모델 비교]  학습 시킬수록 다른 모델에 비해 성능이 좋다는 것을 알 수 있다.Light-weight Models기존의 모델들이 너무 무거워서 가볍게 만들기 위한 가성비 모델, 임베딩 머신, 스마트 폰 등에 사용  DistillBERT          Hugging face의 논문      Teacher-student 모델                  Teacher model은 파라미터, 레이어 수가 비교적 더크며 student가 Teacher의 결과(softmax 결과)을 모방하도록 함                      TinyBERT          Teacher-student model과 비슷하나 parameter와 hidden state 등, 중간 결과 까지도 비슷하게 하도록 모방하는 모델      Fusing Knowledge Graph into Language Model  주어진 문장 뿐만 아니라 상식 또는 외부지식(Knowledge Graph)에서 지식을 가져와 처리하는 방법  ERNIE  KagNET"
  }
  , 
  
  "/articles/AI/NLP/%ED%95%9C%EA%B5%AD%EC%96%B4%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8%20%EB%8B%A4%EB%A3%A8%EA%B8%B0-KLUE.html": {
    title: "한국어 언어 모델 다루기-KLUE",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/NLP/%ED%95%9C%EA%B5%AD%EC%96%B4%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8%20%EB%8B%A4%EB%A3%A8%EA%B8%B0-KLUE.html",
    tags: ["AI","NLP","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueKLUE(한국어 언어 모델 학습 및 다중 과제 튜닝)  Naver AI Boostcamp의 KLUE 강의를 정리한 내용입니다.01. 인공지능과 자연어 처리인공지능과 자연어 처리에 대하여인공지능 : 인간의 지능이 가지는 학습, 추리, 적응, 논증 따위의 기능을 갖춘 컴퓨터 시스템ELIZA (1966) : 기계에 지능이 있는지 판별할 수 있는 튜링 테스트(이미테이션 게임)을 적용할 수 있는 최초의 Human-like AI자연어 처리에는 문서 분류, 기계 독해, 챗봇, 소설 생성, 의도 파악, 감성 분석 등의 응용 분야가 있다.인간의 자연어 처리 (대화)의 경우  화자는 자연어 형태로 객체를 인코딩  메세지의 전송  청자는 본인 지식을 바탕으로 자연어를 객체로 디코딩하는 과정을 거친다.비슷하게 컴퓨터의 자연어 처리는  Encoder는 벡터의  형태로 자연어를 인코딩  메세지의 전송  Decoder는 벡터를 자연어로 디코딩하는 과정을 거친다.즉, 자연어 처리는 컴퓨터를 이용하여 인간 언어의 이해, 생성 및 분석을 다루는 인공 지능 기술대부분의 자연어 처리 문제는 분류 문제로 생각할 수 있으며, 분류를 위해 데이터를 수학적으로 표현되어야 한다.이를 위해 가장 먼저,  분류 대상의 특징 (Feature)을 파악(Feature extraction)  특징을 기준으로 분류 대상을 그래프 위에 표현 뒤, 대상들의 경계를 수학적으로 나눔(Classification)  새로 들어온 데이터를 기존 Classification을 통해 그룹 파악 가능과거에는 위의 과정을 사람이 직접 했지만 이제는 컴퓨터가 스스로 Feature extraction과 Classification 하는 것이 기계학습의 핵심이다.자연어를 좌표 평면 위에 표시하는 방법으로 one-hot encoding 방식이 있지만 단어 벡터가 sparse 해지므로 해당 방식은 의미를 가지지 못한다.Word2Vec 알고리즘은 자연어의 의미를 벡터 공간에 임베딩하여, 단어의 의미를 포함  비슷한 의미의 단어는 비슷한 Word2Vec 벡터를 가진다.          예를 들어, ‘개’와 ‘고양이’는 ‘먹는다’ 보다 비슷한 벡터를 가진다.      subword information : 서울시는 서울을 포함하는 단어이다.  Word2Vec은 이러한 것을 무시함Out of Vocabulary(OOV) : 학습하지 않은 단어는 전혀 예측할 수 없음.Skip-gram 방식이라는 주변부의 단어를 예측하는 방식으로 학습한다.위의 단점을 보완하기 위해 FastText라는 새로운 임베딩 방법이 존재  Facebook에서 공개, word2vec과 유사하나 단어를 n-gram으로 나누어 학습 수행  n-gram 2-5일시, “assumption” = {as,ss,su, …, ass,ssu,sum, ump,…,assumption}  별도의 n-gram vector를 형성하며, 입력 단어가 vocabulary 사전에 있을 경우 word2vec과 비슷하게 return 하지만, OOV일 경우 n-gram vector 들의 합산을 returnFastText는 Word2Vec에 비해 오탈자, OOV, 등장 회수가 적은 학습 단어에 대해 강세단, Word2Vec이나 FastText 같은 word embedding 방식은 동형어, 다의어 등에 대해서 embedding 성능이 좋지 못하며, 주변 단어를 통해 학습이 이루어지므로 문맥 고려가 불가능하다는 단점이 있다.딥러닝 기반의 자연어 처리와 언어 모델모델(model)이란, 어떤 상황이나 물체 등 연구 대상 주제를 도면이나 사진 등 화상을 사용하거나 수식이나 악보와 같은 기호를 사용하여 표현한 것.  일기예보 모델, 데이터 모델, 비즈니스 모델, 물리 모델 등이 존재자연 법칙을 컴퓨터로 모사함으로써 시뮬레이션 가능, 미래의 state를 올바르게 예측하는 방식으로 모델 학습이 가능Markove 기반의 언어 모델, 혹은 마코프 체인 모델(Markov Chain Model)은 초기 언어 모델로, 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산최근의 딥러닝 기반의 언어 모델은 해당 확률을 최대로 하도록 네트워크를 학습하며, RNN (Recurrent Neural Network) 기반의 언어 모델이 그러하다.RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이룸(directed cycle)이전 state 정보가 다음 state를 예측하는데 사용됨으로써, 시계열 데이터 처리에 특화마지막 출력은 앞선 단어들의 문맥을 고려해 만들어진 최종 출력 vector (Context vector라고 함)이며, 이 위에 classification layer를 붙이면 문장 분류를 위한 신경망 모델이 됨.Seq2Seq(Sequence to Sequence)RNN 구조를 통해 Context Vector를 획득하는 Encoder와 획득된 Context vector를 입력으로 출력을 예측하는 Decoder layer를 이용한 RNN 구조Seq2Seq 구조는 모든 자연어처리 task에 적용가능하다.단, RNN의 경우 입력 sequence의 길이가 매우 길 경우, 앞단의 단어의 정보는 희석이 되며, 고정된 context vector 사이즈로 인해 긴 sequence에 대한 정보를 함축하기 어려움또한, 모든 token이 영향을 미치니, 중요하지 않은 token 도 영향을 줌이를 방지하기 위해 Attention이 개발됨Attention문맥에 따라 동적으로 할당되는 encode의 Attention weight로 인한 dynamic context vector를 획득이를 통해 Seq2Seq의 encoder, decoder 성능을 비약적으로 향상시킴, 단 여전히 느림Self-attention 구조의 경우, hidden state가 순차적으로 RNN에 전해지는 것이 아니라 모든 RNN이 서로의 output에 영향을 주게끔 설계되어 더욱 빠르다.이를 응용한 구조가 Transformer 구조이며, 각자 따로 RNN 구조를 가지던 Seq2Seq model과 달리, 하나의 네트워크를 공유함.이 Transformer 구조로 인해 다양한 구조와 언어 모델이 생성됨02. 자연어 전처리자연어 전처리전처리 : 원시 데이터(raw data)를 기계 학습 모델이 학습하는데 적합하게 만드는 프로세스Task의 성능을 확실하게 올릴 수 있는 방법자연어 처리 Task 의 단계는 다음과 같다.1. Task 설계2. 필요 데이터 수집3. 통계학적 분석 : 아웃라이어 제거(Token 너무 길거나 짧거나), 빈도 확인(사전(dictionary) 정의)4. 전처리 : 개행문자, 특수문자, 공백, 중복 표현, 이메일, 링크, 제목, 불용어, 조사, 문장 분리 제거5. Tagging : Lable 붙여주기6. Tokenizing : 자연어를 어떤 단위로 살펴 볼 것인가? 어절? 형태소? WordPiece?7. 모델 설계8. 모델 구현9. 성능 평가10. 완료Python의 upper, lower, strip, split 등의 string 관련 함수로 전처리를 쉽게 할 수 있다.자연어 토크나이징토큰화(Tokenizing)란?주어진 데이터를 토큰(Token)이라 불리는 단위로 나누는 작업토큰이 되는 기준은 어절, 단어, 형태소, 음절, 자소 등 다양할 수 있음한국어 토큰화의 특징영어는 합성어(New york)와 줄임말(It’s) 같은 예외를 제외하고, 띄어쓰기를 기준으로 토큰화하면 잘 작동한다.하지만 한국어는 조사나 어미를 붙여 말을 만드는 교착어(ex)그, 그가, 그는, 그를)이므로 어절이 의미를 가지는 최소단위인 형태소로 분리함ex) 안녕하세요 -&gt; 안녕/NNG, 하/XSA, 세/EP, 요/ECBERT 언어모델BERT 언어모델 소개이미지에서 Autoencoder는 encoder단에서 이미지의 정보를 요약된 형태로 바꾼 뒤, Decoder를 통해 이미지의 대략적인 특징을 알 수 있게 복원한다.이때 중간 layer의 Compressed Data 부분의 정보를 가져오면 이미지의 context vector 값(즉, 이미지의 대표 특징)을 알아볼 수 있다.마찬가지로 BERT는 Transformer의 Encoder, Decoder 기능을 이용할 뿐만 아니라 기존의 Input 정보에 마스킹(Masking)을 추가하여 Decoder의 복원을 어렵게하여 더욱 학습이 잘되게 한다.GPT-1이 평범한 Transformer을 이용하는 것을 시작으로 BERT, GPT-2는 마스킹 대신 언어의 일부를 자른 뒤, 나머지 부분을 복원하는 방식으로 진행된다.BERT 모델(Base의 경우)는 12개의 Transformer layer가 all-to-all로 연결된 형태이며, 이때 Input으로 2개의 문장이 &lt;SEP&gt; 토큰으로 구별된 채로 입력된다.output으로 Class Label의 경우 BERT가 판단한, 두 문장의 관계(2문장은 1문장 다음에 올만한 문장인가? 아니면 관계없는 별개읜 문장인가?)를 판단한 Class Label이 있으며, 이렇게 나온 Vector들이 두 문장들의 특성을 잘 표현하는 벡터라고 판단하고 Classification layer을 얹어 Pretraining한다.Corpus 양이 상당히 많으며, 빈도수를 기준으로 학습한다.(WordPiece tokenizing)학습은 기존의 연결된 2문장을 절반의 확률로 랜덤한 다른 문장으로 바꿔 학습하며, 이때 15% 확률로 마스킹 또한 포함한다.마스킹을 할때에는 또 80% 확률로 Mask 토큰, 10%로 랜덤한 단어로 변경, 10% 마스킹하지않는 것으로 진행된다.다양한 실험으로 성능을 증명하였으며, 다음과 같다.BERT의 구조를 조금 바꿈으로 여러가지 분류를 시행할 수 있다.144개 국어를 커버할 수 있는 BERT Multi-lingual pretrained model로 많은 Task를 할 수 있다.이때, Tokenizing 방식을 바꾸거나 Input에 토큰을 추가로 사용하는 방법으로 성능을 향상시킬 수 있으므로, 고민이 필요하다.BERT Pre-Training특정 도메인 특화 Task의 경우, PreTrained 모델을 재학습 시키면, 성능이 더욱 향상한다.BERT의 학습 단계는 다음과 같다.  Tokenizer 생성  데이터셋 확보 :          데이터셋 생성 : 데이터를 입력가능한 꼴로 바꾸는 것, 토큰 등의 삽입, 마스킹 등      데이터로더 생성: 입력방법을 결정      개인정보, 저작권 등을 유의해서 생성하자.        Next sentence prediction(NSP)  Masking05. BERT 언어모델 기반의 단일 문장 분류1. KLUE 데이터셋 소개한국어 자연어 이해 벤치마크(Korean Language Understanding Evaluation, KLUE)문장분류, 관계 추출, 문장 유사도, 자연어 추론, 개체명 인식, 품사 태깅, 질의 응답, 목적형 대화, 의존 구문 분석 등의 task 유형이 존재이때, 의존 구문 분석은 단어들 사이의 관계를 분석하는 task 이다.지배소는 의미의 중심이 되는 요소를 의미하며, 의존소는 지배소가 갖는 의미를 보완, 수식 해주는 요소이다.어순과 생략이 자유로운 한국어와 같은 언어에서 주로 연구 된다.지배소는 항상 의존소 보다 뒤에 있는 후위언어 이며, 지배소는 하나만 가지며, 교차 의존 구조(서로 지배 또는 의존하는 구조)는 없다.Sequence labeling 방식으로 처리 단계를 나눈다.  앞 어절에 의존소가 없고  다음 어절이 지배소인 어절을 삭제하는 방법으로 의존 관계를 생성이를 통하여 자연어 형태를 그래프로 구조화해서 표현이 가능하며, 그 그래프를 통해 대상에 대한 정보 추출이 가능(누가 충무공인가? 이순신은 어디 사람인가?)2. 단일 문장 분류 task 소개주어진 문장이 어떤 종류의 범주에 속하는지를 구분하는 task  감정 분석(Sentiment Analysis), 2. 주제 라벨링(Topic Labeling), 언어감지(Language Detection), 의도 분류(Intent Classification) 등에 사용됨구체적으로는 혐오 발언 분류, 기업 모니터링, 대용량 문서 분류, VoC, 번역기, 챗봇 등…kor_hate : 한국의 혐오 표현에 대한 데이터셋kor_sarcasm : 한국의 비꼰 표현의 데이터셋kor_sae : 예/아니오로 답변 가능한 질문 중 금지, 요구 , 강한 요구, 육하원칙 분류kor_3i4k : 평서문, 질문, 명령 등을 분류3. 단일 문장 분류 모델 학습BERT의 [CLS] token의 vector를 classification 하는 Dense layer 사용BERT 언어모델 기반의 두 문장 관계 분류두 문장 관계 분류 task 소개주어진 2개의 문장에 대해, 두 문장의 자연어 추론과 의미론적인 유사성을 측정하는 taskNatural Language Inference (NLI)      언어 모델이 자연어의 맥락을 이해할 수 있는지 검증하는 task        전제문장(Premise)과 가설문장(Hypothesis)을 Entailment(함의), Contradiction(모순), Neutral(중립)으로 분류  Semantic text pair  두 문장의 의미가 서로 같은 문장인지 검증하는 task  ex) 무협 소설 추천 해주세요 != 판타지 소설 추천해주세요  ex) Gmail과 네이버 메일 중 뭐 쓸래? == 두개 중에 골라줘 네이버랑 지메일IRQA(Information Retrieval Question and Answering) 챗봇 시스템 구조문장 토큰 분류주어진 문장의 각 token이 어떤 범주에 속하는지 분류하는 taskNamed Entity Recognition(NER, 개체명 인식)개체명 인식은 문맥을 파악해서 인명, 기관명, 지명 등과 같은 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구(개체) 등을 인식하는 과정을 의미함.동음이의어, 문맥 차이 등에 의해 문맥에 따라 개체명이 달라질 수 있음Part-of-speech tagging (POS TAGGING, 품사 태깅)품사란 단어를 문법적 성질의 공통성에 따라 언어학자들이 몇 갈래로 묶어 놓은 것,품사 태깅은 주어진 문장의 각 성분에 대하여 가장 알맞는 품사를 태깅하는 것형태소 태깅 또한 POS Tagging임.pororo library를 이용하면 한국어 Task를 쉽게 해결할 수 있음.kor_ner문장 토큰 분류를 위한 한국어 NER 데이터셋한국해양대학교 자연어 처리 연구실에서 공개했으며, 일반적으로 pos tagging도 함께 존재Entity tag는 [BIO] Tagging으로 이루어져 있으며 B는 개체명의 시작(Begin), I는 내부(Inside), O는 다루지 않는 개체명(Outside)을 의미함ex) B-PER : 인물 개체명의 시작, I-PER : 인물명 개체명의 내부 분을 뜻함BERT multilingual 모델 활용의 경우, WordPiece tokenizer가 잘못 판단할 수도 있으므로 음절단위로 tag를 매핑해주는 것을 추천(애시당초 음절단위로 학습했다고 함.)GPT 언어 모델Generation Pre-trained Transformer의 약자, Genereation 분야에 강점을 가지고 있음BERT와 달리 단일 방향으로 주어진 단어 다음에 가장 올 확률이 높은 단어를 예측함.Transformer의 Encoder를 활용한 BERT와 달리 Decoder를 활용함.순차적으로 단어를 예측하여 문장을 생성시기적으로 BERT보다 일찍 나왔으며, 기존의 문장의 Context vector를 생성하고 이를 통해 분류하는 성능이 뛰어남덕분에 적은 양의 데이터에서도 높은 분류 성능을 나타내며, 다양한 자연어 task에서 SOTA 달성, Pre-train 언어 모델의 새 지평을 엶특정 task를 위해 fine-tuning된 모델은 다른 task에서 사용 불가, GPT는 이를 막기 위해 fine-tunnig을 하지 않아도 다중의 task를 수행할 수 있게 만들려 했다.GPT2는 GPT1에서 조금의 decoder layer 변경과 커다란 parameter 수 증가의 차이를 가지며, 최신의 GPT3는 100배정도 데이터셋과 parameter 수를 늘렸으며, initializer와 구조를 조금 바꿨다.Awesome GPT-3에서 성능확인 가능Weight update가 없으므로, 새로운 지식에 대한 학습이 없으며, 시기에 따라 달라지는 문제, 최신의 정보가 필요한 문제는 대응 불가즉, GPT-3에 사용한 데이터셋의 지식만 가지고 있음모델 사이즈가 너무 크고, 멀티 모달 정보 활용이 필요함."
  }
  , 
  
  "/articles/AI/Structured_Data/DKT%20%EA%B8%B0%EB%B3%B8.html": {
    title: "DKT 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/Structured_Data/DKT%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","정형데이터","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true심층 지식 탐색(Deep Knowledge Tracing, DKT) 기본  Naver AI boostcamp DKT 강의를 정리한 내용입니다.DKT 이해 및 DKT Trend 소개DKT Task 이해DKT (DEEP KNOWLEDGE TRACING) : 딥러닝을 이용하는 지식 상태 추적Question과 Response로 이루어진 문제 풀이 정보를 통해 다음 지식상태(주로 문제를 풀 수 있는가?)를 예측하는 방식으로 진행된다.  즉, 주어진 문제를 맞췄는 지 틀렸는지 알아보는 Binary Classification 문제이기도 하다.지식 상태는 계속 변화하므로 지속적으로 추적해야 한다.보통 문제와 풀이 결과를 Train set으로,마지막 문제의 풀이 결과가 masking 되있는 문제들과 풀이결과가 Test set으로 주어진다.문제 풀이 정보(데이터)가 추가될 수록 학생의 지식 상태를 더 정확히 예측 가능.데이터가 적을 수록 오버피팅 현상이 쉽게 일어난다.Metric 이해AUC/ACC(Area under the roc curve/Accuracy)보통 예측의 결과는 float 형태로 나오며, 0.5(Threshold)를 기준으로 정답 여부(1,0)를 결정한다.Confusion Matrix(혼동행렬)의 이해Predicted : 모델의 예측값Actual: 실제 값Accuracy: 전체 중 예측값과 맞는 비율Precision(PPV, Positive predictive value) : 모델이 맞다고 예측한 비율 중 실제 맞은 비율Recall,Sensitivity (True positive rate(TPR)): 실제 1인 비율 중에 모델이 1이라고 한 비율Specificity : 실제 0인 비율 중에 모델이 0이라고 한 비율F1 score : Prescision과 Recall의 절충안, 동시에 고려함.다만 위의 metric 들은 Threshold에 영향을 받게됨(여기서는 0.5)AUC(Area under the roc curve)그래프의 면적이 커질수록 성능이 더 좋아진다.AUC 값의 범위는 0~1이며, 랜덤하게 0과 1을 넣은 경우 0.5이다.AUC는 척도 불면, 절대 값이 아니라 예측이 얼마나 잘 평가되는지 측정하는 것이며(예측값들의 절대적인 크기와 관계없음),분류 임계값 불변, 어떤 분류 임계값이 선택되었는지와 상관없이 모델의 예측 품질을 측정할 수 있다. (Threshold 관계 없음)단, 단점들로,척도 불변이 항상 이상적이지 않을 수 있다. 예를 들어, 0.9 이상의 값이 중요할 경우 AUC로 측정 불가분류 임계값 불변이 항상 이상적이지 않다. 예를 들어 허위 양성(FP) 최소화가 더욱 중요한 경우(중요한 메일이 지워지면 안되는 스팸메일 분류 등) 이럴 때는 AUC가 유용한 측정항목이 아니다.imbalanced data에서는 accuracy 보다는 낫지만, AUC가 비교적 높게 측정되는 경향이 있다.(단, Test data가 동일할 경우, 상대적인 성능 비교는 가능하다)FPR은 Specificity를 의미하며, TPR은 Recall을 의미한다.결과값에 따라 다음과 같은 방법으로 ROC curve를 그릴 수 있다.위와 같이 Threshold 지점을 중심으로 겹치는 부분 (=예측이 틀린 부분)이 적을수록 ROC Curve의 면적이 넓어지고, 성능이 좋다는 의미이다.DKT History 및 TrendML, DL, Transformer, GNN 등의 DKT의 트랜드가 발전해 왔다.[1강 참고 자료, History of deep knowledge tracing 참조]DKT Data Exploratory Data AnalysisDKT Datset EDA에 대한 예시i-Scream 데이터 분석i-Scream edu에서 제공하는 Datasetfeature로 userID, assessmentItemID, testId, answerCode, Timestamp, KnowledgeTag로 이루어짐.DKT에서 보통 하나의 행을 Interaction이라고 부름userID  사용자 별 고유번호, 총 7442명의 고유한 사용자 존재assessmentItemID  사용자가 푼 문항의 일련 번호, 총 9454개의 고유한 문항이 존재  총 10자리로 구성, 첫자리는 항상 알파멧 A, 그다음 6자리는 시험지 번호, 마지막 3자리는 시험지 내 문항의 번호로 구성  ex) A030071005testId  사용자가 푼 문항이 포함된 시험지의 일련 번호, 총 1537개의 고유한 시험지가 존재  총 10자리로 구성, 첫 자리는 항상 알파멧 A, 그 다음 9자리 중 앞의 3자리와 끝의 3자리가 시험지 번호, 가운데 3자리는 모두 000  앞의 3자리 중 가운데 자리는 1~9값을 가지며 이를 대분류로 사용 가능  ex) A030000071answerCode  사용자가 문항을 맞았는 지 여부를 담은 이진 데이터, 0은 틀림, 1은 맞음  전체 Interaction에 대해 65.45%가 정답을 맞춤, 즉 조금 불균형한 데이터셋Timestamp  사용자가 Interaction을 시작한 시간 정보, 시간 간격을 통해 문제를 푸는 시간을 가늠할 수 있음.KnowledgeTag  문항 당 하나씩 배정되는 태그, 일종의 중분류  총 912개의 고유 태그 존재기술 통계량 분석기술 통계량?  일반적으로 데이터를 살펴볼 때, 가장 먼저 살펴보는 것은 기술 통계량입니다.  보통 데이터 자체의 정보를 수치로 요약, 단순화하는 것을 목적으로 하며  우리가 잘 알고 있는 평균, 중앙값, 최대/최소와 같은 값들을 찾아내고, EDA 과정에서는 이들을 유의미하게 시각화하는 작업을 거침  분석은 최종 목표인 정답률과 연관 지어 진행하는 것이 유리다음은 I-scream dataset의 특성 별 빈도 분석 종합이다.다음은 I-scream dataset의 특성 별 정답률 분석 종합이다.위와 같은 단순 기술 통계량을 넘어서, 얻어낸 특성과 정답률 사이의 관계를 분석해야 하며, 이때, 여러 지식과 경험이 있으면 좋다.예를 들어, 문제를 많이 푼 사람이 문제를 더 잘 맞추는가?, 좀더 자주 나오는 태그의 문제의 정답률이 높은가?, 문항을 푸는데 걸린 시간과 정답률의 관계는 어떠한가?문항을 더 많이 푼 학생이 문제를 더 잘맞추는 경향이 있다.문항을 풀수록 한 학생의 정답률이 늘어나는 경향이 있는가?에 대한 그래프이다. 주로 초반에 잘 푼 학생은 점점 감소하며, 반대의 경우 점점 증가한다.전반적으로 증가하는 추세이다.이외에도 같은 시험지나 태그의 문제를 연달아 풀면 정답률이 오르는가? 등을 생각해볼 수 있다.Hands on EDA[Lab. ]Sequence 모델링정형데이터에는 Titanic 처럼 Time과 관계없는 Non-Sequential Data와, Transaction처럼 시간의 순서가 존재하는 Sequential Data가 존재한다.이때, Sequential Data를 Time을 통합하고 특정 feature에 맞춰 집계하거나 그대로 둔채로 추가 feature를 생성하는 방식으로 Feature Engineering이 가능하다.예를 들어, 문제, 시험, 또는 사람 별로 집계한 뒤, 정답 확률 feature를 추가할 수 있다.이러한 feature들은 hyperparameter 처럼 추가, 삭제를 통해 모델의 성능을 확인할 수 있다.이때, 단순히 이벤트의 행 단위로 개수를 세지 않고, aggregation 기준을 중심으로 split 해야된다.그 이후, feature와 hyperparameter를 바꿔가면서 성능의 차이를 알아보며 feature를 결정한다.import torchimporth torch.nn as nn# Size: [batch_size, seq_len, input_size or num_of_features]input = torch.randn(3, 5, 4)lstm = nn.LSTM(input_size=4, hidden_size=2, batch_first=True)output, h = lstm(input)output.size() # =&amp;#38;#62; torch.Size([3, 5, 2]), batch_size, seq_len, hidden_size)LSTM 구조의 Sequece input은 다음과 같이 이루어진다.batch size(dataset chunk 한 크기), seq_len(sequence의 길이), input_size(4 차원 embedding) or num_of_features의 3차원 벡터가 들어간 뒤,batch_size, seq_len, hiddensize(hyperparameter)의 output이 나온다.feature의 수에 따라 input size가 변하는 예시를 보자면 위와 같다. config = BertConfig( 3, # vocab_size, not used hidden_size = 4, num_attention_heads=1 )  # Size: [batch_size, seq_len, input_size] input = torch.randn(3, 5, 4) # Size: [batch_size, seq_len] mask = torch.randn(3, 5)  transformer = BertModel(config) encoded_layers = transformer(inputs_embeds=input, attention_mask=mask) sequence_output = encoded_layers[0] sequence_output.size() #=&amp;#38;#62; torch.Size([batch_size, seq_len, input_size]) Transformer의 input과 output 또한 크게 다르지 않지만, masking의 차원이 1차원 적다.Transformer + 연속형, 범주형 조합의 input의 경우, embedding layer의 설정에 따라 input size가 다르다.범주형은 연속형과 다르게 인코딩을 통해 vector를 뽑아내야 한다.Embedding은 일종의 Lookup Table을 만드는 것으로, 이 Lookup Table 또한 학습을 통해 결정된다.이런식으로 Embedding된 값들은 concat되어 hidden_size를 만든다.이때 concat되는 feature들의 차원이 Linear를 통해 hidden size에 맞게 줄어든다.DKT의 경우, Transformer구조를 활용 시 보통, 사용자 단위로 Sequence를 생성한 뒤, 각각 train input으로 넣어준다.DKT의 경우 마지막 문제의 정답여부를 맞추는 Task 이므로 보통 Padding을 앞에 추가하여 뒷부분을 맞춘다.Sequence Data 문제 정의에 맞는 Transformer Architecture 설계Transformer 구조는 다양한 Sequence 데이터에서 강점을 보이지만, 많은 양의 데이터와 연산량을 요구하며, 종종 상황에 맞게 변형해서 사용하거나, 아예 다른 모델을 사용해야 하는 경우도 있다.inductive bias : 특정 목적에 맞게 설계된 모델들(CNN, RNN)의 경우 input의 형태에 따라 bias가 생긴다, 즉, 적절하지 못한 input의 경우 성능이 나쁘다.(CNN에 Sequential input을 넣어준다던가)Transfomer의 경우, inductive bias가 존재하지 않지만, 그만큼 데이터가 많이 필요하다.이러한 Transformer를 개조하기 위해 Trasformer architecture의 변형을 알아보자.Data Science Bowl3~5세 들의 기초수학 학습을 위해 개념을 정확히 배웠는 지 맞추는 것이 대회의 목표과거 데이터를 바탕으로 앞으로 어떻게 풀지 class 4(바로 맞춤, 한번 틀리고 맞춤, 여러번 틀리고맞춤, 못맞춤)개를 통해 예측학습 진행 시간, 학습 종류(영상물, 게임, 활동, 평가 등), 게임플레이 세계관과 사용 정보 등이 기록되어 주어진다.이 때, Transformer 구조가 널리 사용되지 않던 시절이였고, 자원과 데이터양이 한정되어있었지만, 한 유저가 Transformer-Encoder 모델인 BERT로 3위를 차지 하였으며,  서로 다른 범주형/연속형 데이터들을 어떻게 임베딩 했는가,  BERT를 어떻게 활용했는가가 주안점이였다.서로 다른 범주형/연속형 Emedding은 다음과 같은 방법을 통하여 임베딩 했으며,위와 같이 Transformer 구조를 깊게 쌓아 마지막 Transformer 구조의 output 값을 softmax하여 classification 한다.이때 마지막 layer 부분 마지막 Trnasformer를 제외한 연하게 칠해진 Transfomer 구조의 output은 사용되지 않으며 Loss에 의해 Backpropagation에 업데이트 되지 않는다.Riid!토익 시험에 대비하여 공부한 학생들의 학습 과정을 모아둔 데이터로, 최종적으로 한 학생이 마지막에 푼 문항을 맞출지 틀리지 맞추는 대회이며, i-Scream 데이터와 매우 흡사하다.다만 데이터셋이 아주 많으며, 강의를 보는 interaction 데이터와 단순히 답을 맞췄는가 아닌가가 아닌, 사용자가 어떤 답을 냈는가와 오답 정리를 했는지도 포함되어 있음.이때, 너무 데이터가 많아서, 임베딩된 2개의 Sequence를 하나로 이어 붙인 후, Sequence의 길이를 반으로 줄이는 대신, 하나의 임베딩 차원을 2배로 늘려 학습시켜 시간 복잡도를 줄임Predicting Molecular Properties분자의 여러 정보들을 통해 원자 간 결합 상수를 찾는 대회분자내 원자 간 결합 정보, 원자 간 가림막 효과, 분자의 에너지 상태, 분자 내 원자의 전하 상태, 결합 상수 세부 정보 등이 데이터로 주어짐LGBM이나 Grpah NN을 통해 접근한 팀도 많음분자 별로 가능한 원자 조합들에 대해 모든 scalar_coupling_constant를 구해야 하므로, 위치가 중요하지 않은 Sequence Data로 볼 수 있으며, 한 분자를 Total Sequence, 원자 쌍의 하나의 Sequence로 본다면, 위 그림 처럼 원자 쌍 순서는 다르지만 결과가 똑같이 나와야함Sequence 안에서 모든 token이 다른 모든 token을 참조하며, Positional Embedding을 통해 위치정보를 반영하는 방식인 Transformer 구조가 적절하다.  즉, 위치 관계가 상관없으므로 Positional Embedding을 안주면 됨 (Permutation Invariant Transformer)이때, 분자 별로 원자쌍이 135개 이므로 Sequence Length는 총 135개 SC(Scaling constant)이며,두 원자의 정보들과 둘 사이의 관계정보 까지 임베딩 하여, 각 원자의 전하, 위치, 원자 번호, 원자 사이의 거리, 원자 결합 종류가 embedding된 vector를 input으로 사용최종적으로 예측해야하는 scaling constant(SC)가 Fc, sd, pso, dso의 합으로 이루어 져있으므로, SC를 예측하는 Transformer와 Fc, sd, pso, dso를 각각 에측하는 두 종류의 결과의 평균을 통하여 예측으로 제출Mechanisms of Actions (MoA)약물 투여시, 어떤 화학 반응이 일어나는지 예측하는 대회로, 투여한 약물의 종류, 양, 시간, 약물 합성방식, 투여 받은 사람의 유전자 발현 종류(772 features), 세포 생존 능력(cell viability) 등의 데이터를 제공함.Sequence로 묶을 수 있는 데이터가 없고, Feature 수가 너무 많고, 예측 해야할 class의 수가 207개 임에 비해, 데이터는 2만 3천개 밖에 존재하지 않아, Transformer 구조가 잘 작동하지 않았다고 함.이를 위해 위 그림과 같은 CNN 모델을 사용했다.  다수의 Feature를 가진 유전 정보와 세포 생존 정보를 PCA(Principal component analysis)를 통해 50차원, 15차원의 벡터로 만듦  기존의 feature와 concatenate하여 추가적인 feature로 생성  위 결과를 Linear에 통과시켜 더 큰 차원의 1차원 벡터로 변환,          Linear feature ordering을 통하여 차원을 늘려주어, 활용가능한 충분한 Pixel의 양을 생성,      생성된 데이터 안에서 feature를 최적의 정렬을 학습하는 효과      각 벡터의 원소가 가지는 의미를 동일하게 만듦        이를 짧은 길이의 여러 채널을 가지는 1D 데이터로 변환  이 데이터를 Conv1D Architecture에 통과시켜 최종결과 생성          이때, 커널 사이즈는 n X embedding size 인경우가 많다. (뒷 부분이 embedding size가 아니면 한 feature의 일부 embedding 만 가져가므로)      위 성능이 단일 모델 기준으로 가장 성능이 좋았다.Kaggle Riiid Competition Winner’s Solution 탐색Feature Engineering공통적인 FEFeature Engineering의 접근 방법에는 2가지가 있다.  Bottom-UpData 기반 방식,1) EDA를 통해 특징을 살피고,2) 해당 특징을 Test Data를 통해 검증 뒤,3) 이를 통해 새로운 feature를 만들어 내고, CV(Cross Validation) 상승을 확인  Time, group에 따른 K-fold Validation을 시행해보고, 오르지 않을때 까지(틀리지 않을때 까지) 시도.4) model 생성한 후, hyperparameter를 찾는 방식예를 들어, 정규분포와 일부 다른 지점을 찾아, 해당 부분은 feature로 생성  Top-Down가설(Hypothesis), domain 지식 기반 컨설팅 방법론(Logical thinking)가설-구현-검증으로 이루어져 있으며,Feature Extraction 시,1) 데이터에 대한 질문 &amp; 가설2) 데이터를 시각화하고, 변환하고, 모델링하여 가설에 대한 답을 탐색(구현-성능평가)3) 찾는 과정에서 배운 것들을 토대로, 다시 가설을 다듬고 또 다른 가설 생성위 두 방식을 같이 사용하는것이 Best,이후, 정형 데이터의 경우, Feature의 Numerical, Categorical 종류를 구분한 후, 각 종류의 특징에 따른 EDA를 해본다.예를 들어, 숫자형의 경우, 평균, 범위, 첨도 등을 알아보며,범주형의 경우, Missing value, value 별 Count, percent 최빈도 값 등을 알아보자.Target과의 상관관계를  Bar plot, hsit plot 등을 그려 알아볼 수 있다.Riiid의 경우1) 문항을 푸는 패턴으로..이전에 푼 문제인가?, 혹시 정답을 한 번호로 찍었는가?를 알 수 있다.아쉽게도 i-Scream 데이터는 선택지에 대한 정보가 없음.2) 사용자가 문항을 푸는 데 걸린 평균 시간으로…오래 걸렸을 경우, 맞춘 학생의 평균 시간과 틀린 학생의 평균 시간을 Feature로 주어 활용할 수 있다.3) 사용자 정답률 추이로…최근 정답률로, 앞으로 문항들의 정답 여부를 구할 수 있다.  최근 정답률이 낮아지면,  현재 푸는 문항들은 잘 모른다는 의미이므로, 마찬가지로 줄어들 것이다.4) 이미 푼 문제가 다시 등장하는 경우…맞췄거나, 틀렸어도 다시 복습했을 확률이 있으므로, 정답률이 올라갈 수 있음.5) 문항, 시험지, 태그의 평균 정답률로 …쉬운 문항, 시험지, 태그의 경우 정답률이 올라갈 수 있다.또한, 사용자가 푸는 문제에 대한 정보(문항의 정답률, 문항이 가진 태그의 정답률)가 많을 수록 활용 하기 쉽다.      문항-태그 정보 에서 content2vec,    사용자-문항 정보로 SVD, LDA, item2vec  문항을 특징화하는 IRT, ELO등의 implicit 한 정보를 활용할 수 있다.Data LeakageFeature를 넣어서 결과가 좋게 나오면 적용해도 되는 것일까?해당 문항의 평균 정답률 Feature를 생각해보자.평균 정답률은 validation이나, test dataset을 제외하고, 계산하게 된다.즉, 전체 데이터셋의 정답률은 실제 정답률과 다를 수 도 있다.과거 현업에서는 예를 들어, 5월 1일 ~ 8일 데이터는 train dataset, 8일부터 ~10일 데이터는  validation set, 11일 부터 15일 까지는 test dataset으로 주는 등, 시간을 고려하지 않고 주어 올바르지 못한 결과를 주는 경우가 많았다. (Inductive bias 문제?)하지만 최근에는 time series api를 이용해, inference 시, 한 row가 진행될 때마다, update하므로 문제가 없다.다양한 방법을 통한 문항 공유의 Feature 뽑아내기추천 시스템에서 많이 사용되는 Matrix Factorization 방식으로 사용자의 벡터와 문항의 벡터를 만들 수 있음. (최근에는 Factorization Machine을 많이 사용함.)Riid, i-Scream 데이터의 경우, 문제를 푼 사용자와 사용자가 푼 문항을 통해 user-item 행렬을 만들어 진행 가능.혹은 유사한 방법으로 선형대수학에서 Singular Value Decomposition (SVD)를 활용할 수 있음.난이도의 이론인 ELO, IRT(Item Response Theory) 또한 활용 가능하다.이는 학생과 문항 별로 고유한 특성이 있다는 가정을 하는 이론이다.      학생은 잠재능력이 있고, 각 문항은 학생의 잠재 능력을 받아 문항을 맞출 확률을 반환하는 고유 함수를 가지고 있다고 가정.        만약 학생의 잠재능력과 문항 별 모수를 안다면, 전체 학생의 모든 문제를 맞출 확률을 모두 알 수 있다는 이론.  이때, 문항이 가진 고유 함수는 다음과 같이 정의됨.\\(\\phi(\\theta;\\beta)=c+\\frac{1-c}{1+e^{-(\\theta-\\beta)}}\\\\\\phi : 학생의\\ 고유\\ 능력,\\ \\beta:\\ 문항\\ 별\\ 함수의\\ 모수,\\\\c:\\ 무작위로\\ 찍을\\ 시\\ 맞출\\ 확률(사지선다\\ 시, 0.25)\\)IRT(Item Response Theory)에서는 여기에 더 많은 가정을 넣어 문항 별 함수를 다양하게 만들 수 있음.Riiid 에서는 간단하게 $\\theta$와 $\\beta$를 간단하게 추정하는 방법을 사용할 수도 있다.  전체 학생의 $\\theta$와 전체 문항의 $\\beta$를 0으로 초기화한다.  아래 수식에 맞춰서 $\\theta$와 $\\beta$를 업데이터, (correct는 0/1의 binary 정답 여부)\\[\\theta_{n+1}\\leftarrow \\theta_n + \\eta_{\\theta_n}*(correct-\\phi(\\theta_n;\\beta_n))\\\\\\beta_{n+1}\\leftarrow \\beta_n + \\eta_{\\beta_n}*(correct-\\phi(\\theta_n;\\beta_n))\\]  이 과정을 전체 데이터에 대해 반복해 최종값을 찾음  구한 이 값들을 통해 test 데이터 내의 학생 별 문항에 대한 정답률을 구함.Continuous Embedding일반적으로 연속형 데이터는 임베딩 데이터와 달리 Embedding 하지 않고 집어넣는다.범주형 데이터의 경우, 임베딩 행렬의 한 열을 사용하는 형태이며, 연속형은 그럴 수 없으므로,  임베딩 대신, 주어진 연속형 데이터 값에 가중을 더 두고, 그 주변 값들에 더 작은 가중을 주어, 이 임베딩 행렬의 특정 열들을 가중합한 벡터를 임베딩으로 사용함.예를 들어 1~100까지 임베딩 해 놓은뒤, 50을 임베딩하려 할때, (50의 임베딩값*0.45) + (49의 임베딩값*0.18)+ (51의 임베딩값*0.18)+(48의 임베딩값*0.09)+(52의 임베딩값*0.9) …대략적인 정규분포를 통하여 사용한다.Last Query Transformer RNN일반적으로,  LGBM, DNN 같은 Machine Learing의 경우,          많은 Feature Engineering을 통해, 다량의 Feature를 필요로 하고, 유의미한 것을 찾아내야 한다.        Transformer 같은 Deep Learning의 경우,          알아서 Feature를 찾아주므로, FE를 적게 사용하고 아주 많은 양의 데이터를 요구로 하고, sequence의 길이의 제곱에 비례한 시간 복잡도를 가지므로 부담스럽다.      Tabular data(정형 데이터)에서는 여전히 FE가 많이 필요, 보통의 경우에도 FE를 통해 성능을 올릴 수 있음.      Resolving deficitsRiid의 1등 솔루션인 Last Query Transformer RNN은, 위 두 가지 문제를 모두 해결한 방법으로 1등을 차지.특징으로,      다수의 Feature를 사용하지 않음, 대신 sequence 길이를 늘림(시간 복잡도가 증가하는 문제를 아래로 해결).          5개의 feature 만 사용, 다른 상위권 모델의 경우 70~80개 사용            마지막 Query만 사용하여 시간 복잡도를 낮춤              일반적으로 $n \\times m$ 행렬과  $m \\times l$ 행렬의 곱에 대한 시간 복잡도는 $O(nml)$이다.      Transformer에서 Query, Key, Value에 대한 행렬 Q, K, V가 각각 (L, d)로 주어져 있고, 우리가 계산하는 Attention Score의 계산식은 다음과 같다.    \\[Att(Q, K, V) = \\mathcal{softmax}\\frac{QK^T}{\\sqrt{d}}*V\\\\Scaled\\ dot\\ attention : 유사도\\ 구할시\\ dot\\ 연산\\ 활용\\]          시간 복잡도가 $O(L^2d)$로 변한다.      추가적으로 마지막 Query만 사용한다면, Q 행렬의 차원이 (L, d)에서 (1, d)로 줄어든다.      즉, 최종적으로 $O(Ld)$로 줄어든다.                문제 간 특징을 Transformer로 파악하고, 일련의 Sequece 사이 특징들을 LSTM을 활용해 뽑아낸 뒤, 마지막 DNN을 통해 Sequence 별 정답을 예측    Positional embedding과 look-ahead mask를 제외하여 순서와 관계없이 입력 간의 관계를 파악하게 함  그 뒤, Sequential 특성 파악을 위해 LSTM 활용  이를 통해, Encoder 수(=Layer 수)와 Sequence length를 증가시켜 성능이 향상 됨.  BERT 모델에 비해 3배 이상의 sequence length를 가짐(512 vs 1728)ML Pipeline[DKT-8]ML_Pipeline.ipynb 참조Model Serving모델 서빙의 종류On-device ServingCloud-based Serving웹서버를 활용한 모델 서빙HTTP 통신웹 서버 구축MLflow를 활용한 모델 서빙MLflow예시 시스템End to End Project실제 현업과 Competition의 비교문제정의 3요소input(Data_X, DataType)Output(Data_Y, 예측해야 할 값)Metric(평가 지표)WorkflowWorkflow란?워크 플로우 관리Apache Airflow를 활용한 워크 플로우 관리Airflow는 Workflow를 프로그래밍 방식으로 작성, 예약 및 모니터링하는 플랫폼으로, python을이용 한 워크 플로우 관리 툴이다.  Airbnb -&gt; Apache 로 프로젝트 넘어감Airflow는 크게Webserver, Scheduler, Worker, Meta DB로 이루어져 있다.토이 프로젝트 소개"
  }
  , 
  
  "/articles/AI/Structured_Data/%EC%A0%95%ED%98%95%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%B6%84%EB%A5%98%20%EA%B8%B0%EB%B3%B8.html": {
    title: "정형 데이터 분류 기본",
    date: " Dec 14, 2022 ",
    url: "/articles/AI/Structured_Data/%EC%A0%95%ED%98%95%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%B6%84%EB%A5%98%20%EA%B8%B0%EB%B3%B8.html",
    tags: ["AI","정형데이터","요약"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true정형 데이터 분류 기본  Naver AI Boostcamp의 정형 데이터 분류 강의를 정리한 내용입니다.정형 데이터와 데이터의 이해정형 데이터란, 엑셀 파일이나 관계형 데이터베이스의 테이블에 담을 수 있는 데이터로 행(row)과 열(column)으로 표현 가능한 데이터. 하나의 행은 하나의 데이터 인스턴스를 나타내고, 각 열은 데이터의 피처를 나타냄비정형 데이터란 이미지, 비디오, 음성, 자연어 등의 정제되지 않은 데이터정형 데이터는 범용적인 데이터이며, 가장 기본적이 데이터로 필수적이다.정형 데이터의 분석을 위해서 문제를 이해해야 하며, 어떻게 문제를 풀지 고민해야 한다.  예를 들어 Input 데이터를 Aggregation 형태로 넣어주거나, TimeSeries로 넣어주거나, 양쪽 둘다 넣어주기          Aggregation: ex) 사람 하나당 하나의 row를 가지는 형식            Train, Valid, Test 셋 나누기    적절한 평가지표를 선정해야 함.분류(Classification) : 예측해야할 대상의 개수가 정해져 있는 문제 (ex) 신용카드 정상 거래 분류)회귀(Regression) : 예측해야할 대상이 연속적인 숫자인 문제 (ex) 일기 예보 기온 예측)평가지표(Evaluation Metric) : 분류, 회귀 머신러닝 문제의 성능을 평가할 지표분류 문제의 평가지표를 알기 위해서 예측 경우를 정의한 Confusion Matrix에 대해서 알아야 한다.TN : False를 False로 예측FP : False를 True로 예측FN: True를 False로 예측TP: True를 True로 예측분류 문제의 평가 지표들  Accuracy (정확도): (TP + TN)/(TP+TN+FP+FN)  전체 비율 중 올바르게 분류한 비율  불균형 데이터에는 비적합함.          100명 환자 중 1명의 암환자를 찾는 데이터에서 전부 그냥 환자가 아니라고 해도 99%의 정확도를 가진다.        Precision(정밀도): TP / (TP + FP)  모델이 True로 예측한 값 중 실제 True인 경우의 비율  Negetive 데이터가 중요한 경우 사용          스팸 메일 분류(일반 메일은 스팸 메일로 분류해서 삭제하면 큰일남)        Recall(재현율) : TP / (TP + FN)      실제 값이 True인 값 중 모델이 True라고 한 경우의 비율        Positive 데이터가 중요한 경우 사용          종양의 종류 판단 시, 양성 종양이냐 악성 종양이나 분류      ROC(Receiver operating characteristics, 수신자 조작 특성)  TPR(True Positive Ratio)$=\\frac{TP}{TP+FN}$을 Y축, FPR(False Positive Ratio)$=\\frac{FP}{FP+TN}$을 X축으로 하여 모델의 임계값을 바꿔가면서 그린 곡선AUC(Area Under Curve)ROC의 면적을 표시한 것, 0과 1사이의 값을 가짐,1일수록 성능이 좋음.EDAEDA(Exploratory Data Analysis, 탐색적 데이터 분석)데이터를 탐색하고 가설을 세우고 증명하는 과정, 주어진 문제를 데이터를 통해 해결하기 위해 필요하다.때문에 적절한 전처리, 방법론들을 선택하고, 가설과 의문을 세운뒤 검증, 입증하여 해결한다.EDA의 과정  Data에 대한 가설 혹은 의문  시각화 혹은 통계량, 모델링을 통한 가설 검정  위의 결론을 통해 다시 새로운 가설 혹은 문제 해결데이터 종류, 도메인, 사용 모델 등에 따라 EDA의 방향성과 방법이 달라지며, 정해진 답이 존재하지 않음때문에 EDA 과정에서 수많은 활동과 과정을 거쳐야 하며, 이를 방향성 없이 풀어나가는 것은 비효율적이므로, 다음 두가지를 기본 개요로 놓고 시작한다.      개발 변수의 분포(Variation)        변수 간의 분포와 관계(Covariation)  Titanic data를 이용한 EDA의 예시이름, 성별, 나이, 좌석 등을 이용해 생존 여부를 판단하는 TaskAge, SibSp, Parch, Fare 등은 연속형 변수이며, Age의 경우 포아송 분포를 따르고 있다.Pclass, sex, Embarked 등은 각 값이 구분 이외에는 의미가 없는 범주형이며,  상식적으로 female과 male은 동일해야하고, Pclass의 경우 높은 등급이 더적어야 하므로, 데이터 불균형이나 특별한 상황을 가정할 수 있다.변수 간 그래프를 통해 변수의 관계를 알 수 있다. 위의 경우 당시 타이타닉에서 여성이 남성보다 생존하도록 배려를 받았음을 알 수 있다.기존 변수에서 다른 변수를 추출해서 변수간의 관계를 비교할 수 도 있다.위의 경우 이름에서 결혼 여부를 추출하여 생존여부를 따져 보자 미혼 여성이 생존자가 많다는 것을 알 수 있다.하지만 생존비율은 결혼한 여성이 더욱 많이 살아남았다.또한 기존의 변수 여러개를 이용해 새로운 변수 또한 만들 수 있다.혼자 탑승한 사람은 생존율이 좀더 낮다.데이터 전처리(Preprocessing)데이터 전처리는 머신러닝 모델에 데이터를 입력하기 위해 데이터를 처리하는 과정EDA, 모델, 목적에 따라 방법이 달라지게 된다.연속형, 범주형 처리연속형 데이터 처리Scaling 방법데이터의 단위 혹은 분포를 변경하는 방법, 선형기반의 모델(선형회귀, 딥러닝 등)인 경우 변수들 간의 스케일을 맞추는 것이 필수적1) Scaling만 하기$$\\left{\\begin{matrix}\\begin{alignat*}{0}  Min\\ Max\\ Scaling\\ :\\mathbf{X}_{new}=\\frac{\\mathbf{X}_i-\\min(\\mathbf{X})}{\\max(\\mathbf{X})-\\min(\\mathbf{X})}\\  Standard\\ Scaling:X_{new}=\\frac{x-\\mu}{\\sigma}\\  Robust\\ Scaling:X_{scale}=\\frac{x_i-x_{med}}{x_{75}-x_{25}}\\end{alignat*}\\end{matrix}\\right.$$Scaling에는 다양한 방법이 존재하며, 대표적으로  Min Max Scaling현재 값에 최소값을 뺀 뒤, 최대값과 최소값으로 나눔  Standard Scaling현재 값을 평균으로 빼고, 표준편차로 나눔  Robust Scaling : 이상치에 영향을 가장 덜 받음현재 값을 중위 값으로 빼고 IQR(이상치 부분 참조)로 나눠서 계산스케일에 따라  계수 값에 따른 모델의 해석과 모델 성능에 영향이 간다.2) Scaling + Distribution(분포 변경) + Binning 하기  Log transformation변수의 분포가 치우친 경우, Log Transformation으로 정규분포와 비스무리하게 바꿔줄 수 있음exponential Transformation의 경우 정반대의 결과를 냄  Quantile transformation어떤 분포가 나와도 Uniform(모두 동일한 높이의 그래프), 또는 정규 분포로 바꿔줌값을 나열한 뒤, 값의 분위 수에 따라 변환해줌이러한 분포 변화를 통해 상관관계를 변경할 수 있다.Binning연속형 변수를 범주형 변수로 바꾸는 방법,위와 같은 분포를 다봉분포라고 하는데 봉우리 마다 범주를 매길 수 있다.Overfitting 방지에 탁월함, 선형 모델의 해석이 용이함범주형 데이터 처리  출처: categorical 데이터, 연속형 데이터보다 더욱 주의해야함.보통 문자열로 이루어져 있지만, 이를 학습에 사용하기 위해 수치형 변수로 바꾸곤 함.  One hot encoding변수를 1과 0으로 나눈 후, 해당 값이 존재하면 1 아니면 0으로 놓는 방법모델이 변수의 의미를 정확하게 파악할 수 있다는 장점이 있다.단, 변수 갯수가 엄청나게 많아지면 sparse하고 크기가 커지게 된다.차원의 저주 문제, 메모리 문제 야기함  Label encoding컬럼의 수는 1개로 유지하되, 각각의 값이 다른 의미를 가진 값을 부여해줌위의 One hot encoding의 단점을 보완.단, 모델이 이 숫자의 크기, 순서를 중요하게 생각하고 학습할 수 있음특히 선형 모델에서는 순서와 상관없는 변수의 경우, Label encoding은 사용하면 안됨      Frequency encoding      해당 변수의 출현빈도를 변수의 값으로 사용하는 방법  Target encoding각각의 변수의 target 변수의 평균으로 encoding 하는 방식Frequency encoding와 Target encoding은 순서와 크기가 중요한 경우이므로 encoding 값이 의미가 있어 학습이 되어도 상관 없다는 장점.하지만, 다른 변수가 같은 값을 가질 수 있다는 단점, 미래에 새로 등장한 값의 경우 encoding할 수 없다는 단점, Target encoding의 경우 overfitting이 생길 수 있음  (Entity) Embedding문자열같은 경우 내용이 길어 수치화 하기 힘들며, Word2Vec과 같은 알고리즘으로 수치화할 수 있다.결측치 처리결측치는 위와 같이 NaN, 즉 측정하지 못하거나 손실 등의 여러 이유로 값이 존재하지 않는 경우를 의미한다.pattern(결측치의 패턴 파악)위와 같이 (흰색의 비어있는 부분이 결측치) 결측치의 생성은 랜덤한 경우와 일정한 패턴이 있는 경우로 나눠진다.보통은 위와 같이 결측치의 패턴이 존재하지 않는 경우가 대부분이며 이때는 단변량이나 다변량 결측치 삽입을 사용해야한다.위와 같이 패턴이 존재하는 경우 패턴을 찾아낸 뒤, 결측치를 채울 방법을 고려해야 한다.  A3에서 결측치가 존재한 경우 A4, A5,A6 모두 결측치가 존재함  A1은 결측치가 없음 등결측치가 존재하는 데이터의 나머지 변수와 비교해서  채워넣을 수 있다.Univariate(단변량 결측치 삽입)하나의 변수가 가진 결측치를 해당 변수를 통해서 채우는 방법1) 데이터 포인트 제거결측치가 있는 Row를 제거하거나, 변수 자체(Column)을 제거하는 방법모델의 사용 데이터가 줄어드므로, 데이터가 적으면 좋지 않은 방법, 예측해야할 값(y)가 결측치일 경우, 삭제가 불가능한 경우도 있음.2) 평균값 삽입변수의 평균 값을 삽입하는 방법원래 좌측 상단의 그래프처럼 1:1의 정비례 관계의 그래프에서 중간 50 부분에 결측치가 생겨서, 이를 평균값인 50을 넣어주게 되면 비례관계와 분포가 바뀌게 된다.3) 중위값 삽입4) 상수값 삽입Univariate 방법은 모두 결측치가 너무 많으면 문제가 생길 수 있다.Multivariate(다변량 결측치 삽입)결측치 값을 머신러닝을 통해서 구하거나, 나머지 변수값을 비교하여 가장 비슷한 결측치 없는 Row의 값을 넣어주는 방법이 있다.      회귀분석 : x1을 x2….Xn으로 예측, x2로 x1….xn 예측의 반복    KNN nearest : 결측치가 존재하는 샘플과 가장 유사한 샘플을 구함  합리적 접근법단, 결측치가 많아서 시행이 불가능하거나, 데이터가 너무 많아 시간이 너무 오래걸리는 경우도 있다.좌측 파란색이 결측치 채우기 전, 주황색이 결측치 채운 후의 상관관계이다.우측 변수의 빈도 변화단변량 보다는 훨씬 나은 결과를 보임을 알 수 있다.합리적 접근법다른 곳에서 데이터를 구하거나, 다른 변수를 통해서 구할 수 있다.이상치 처리일반적인 데이터와 크게 다른 경우를 의미한다.위의 경우 상단의 두 그래프는 좌(이상치가 있는 경우), 우 (이상치 제거한 경우)이며, 좌측은 x,y가 상관이 있어보이지만, 우측을 보면 x,y와 관계없이 랜덤하게 흩뿌려져있다.아래의 그래프의 경우도 이상치 몇개로 인하여 선형모델이 학습한 기울기가 바뀌어 버린다.이상치를 잘못 처리하면 커다란 영향이 간다.tree 모델은 이상치에 덜 영향을 받긴한다.이상치 탐색 방법 예시 두가지로, Z Score, IQR이 존재한다.이상치 탐색 후,  이상치를 보는 관점에 따라 처리방법이 달라진다.  정성적인 측면 : 이상치의 발생 이유는? 이상치의 의미는? 이를 통해 처리 방법을 고민ex) 아파트 거래가 vs 아파트 건축연도에 대한 그래프에서, 보통 신축일 수록 비싼데, 60년된 은마아파트가 비싼 이유는 무엇인가? =&gt; 아파트의 위치, 재개발 여부 등에 따라 가격이 바뀔 수 있으므로,  위치, 재개발 여부 변수를 추가하여 이상치를 이상치가 아니게 만드는 방식으로 제거 가능  성능적인 측면 : Train Test Distribution, 만약 Train과 Test 둘다 이상치가 존재하는데, Trian set에서만 이상치를 없애버리면, 예측력이 떨어지게 됨.파랑색 점이 Train, 노랑색 점이 Test, 빨간색 점이 Z Score로 구한 이상치일 때,이상치 점들이 Test 점들과 겹치지 않는 경우가 많으므로, 제거하면 모델 성능이 올라간다.머신 러닝 개념Underfitting &amp; OverfittingUnderfitting은 너무 데이터셋에 맞지 않는 예측곡선을 그린 경우,Overfitting은 너무 데이터셋에 맞는 예측곡선을 그린 경우이다.이론상 우리가 모든 데이터를 이용해 데이터셋을 만들거나 분포가 완전 동일하다면, Overfitting된 그래프가 최적이지만, 실제로는 일부분만 사용하므로 적절하지 않다.Overfitting 여부를 확인하기 위해 Validation dataset을 따로 두어, Train Accuracy와 Validation Accuracy를 비교하여 격차가 벌어지기 시작하는 점 이후로 는 Overfitting 되가는 것이다.Underfitting 현상을 방지하기 위해서는1) 더 많은 데이터로 더 오래 학습2) feature를 더욱 많이 반영3) Variance가 높은 모델 활용Overfitting 현상을 방지하기 위해서는 Regularization(정규화) 방법이 있으며모델이 Noise에도 크게 반응하지 않게 규제하는 방법이다.Regularization에는 다음으로 나뉜다.1) 정형 데이터에서 사용 불가능한 방법  Noise robustness  Label smoothing  Batch normalization2) 정형데이터에서 사용 가능한 방법  Early stoppingTraining data와 비교하여 Validation 성능이 떨어지려할 때 멈추는 방법  Parameter norm penaltynoise에 민감하지 않게 모델을 만들 수 있음L1 : Lasso 페널티L2 : Ridge? 페널티페널티 계수를 조절해서 overfitting을 줄일 수 있다.하이퍼파라미터로도 사용할 수 있다.  Data augmentation원본 데이터를 변형하여 데이터를 증강시켜 사용하는 방법SMOTE : 정형데이터에서의 데이터 증강방법  주로 Imbalance한 데이터들을 대상으로 근처의 데이터를 생성하여 데이터 증강  Dropout  가지치기, 무작위로 노드의 연결을 끊어버려 일부 feature만 사용하여 모델을 생성정형 데이터에서는 Tree model 등에서 사용할 수 있으며, 모두 사용하는 것이 아닌 랜덤 샘플링 등이 예시이다.Validation strategyValidation strategy는 데이터셋을 각 Train, Validation, Test 세가지로 나누어 학습시키는 방법이다.먼저 Test 데이터셋은 프로젝트 결과와 직결되는 데이터셋이므로 설정을 신경써서 해야한다.  최대한 전체 데이터셋을 대표할 수 있는 데이터셋이어야 함,          데이터를 바꿀 때 마다 성능이 달라짐      즉, 왠만하면 안바꿔야 함      Validation dataset은 내가 만든 모델을 Test 데이터셋에 적용하기 전에 모델의 성능을 파악하기 위한 데이터셋이며, 이때 파악한 성능으로 Early stop이나 기타 Regularization 기법을 활용 가능  테스트 데이터셋과 최대한 비슷하게, 즉 전체 데이터셋을 대표할 수 있게 구성해야함.          테스트 데이터셋을 알 수 없는 경우(미래에 있거나, competition이라 가려져 있거나), 전체 데이터셋을 대표할 수 있게 구성해야 한다.      예를 들어, 모래먼지가 자주 부는 지방에서 object detection =&gt; Validation set에서 모래먼지가 낀 듯한 데이터의 비율이 좀더 많아야함.      Train dataset은 모델이 보고 학습하는 데이터로, 보통 Validation dataset과 Test dataset을 정한 뒤 남은 것을 사용하나, 이때, noise 데이터를 포함하냐 안하냐가 중요하다.Hold-out validationHold-out validation은 하나의 고정된 Validation set을 지정하는 방식이다.Random sampling랜덤으로 Validation을 정하는 방법으로 class가 imbalance하게 될 수 있으므로 잘 사용하지 않는다.단, 데이터셋이 아주 방대하면 Test dataset과 비슷하게 나오기도 한다.Stratified split각각의 label의 비율을 train과 validation set이 비슷하게 만드는 방식이다.보통 Train과 Validation은 8:2 비율이며, 데이터가 많으면 9:1, Test를 많이 하면 7:3까지 늘리기도 한다.Cross ValidationTrain과 Validation set을 여러 부분에서 가져와 학습시킨 모델들을 앙상블을 통해 계산하는 방법, 특히 K-fold cross validation이라고 함.위의 두 샘플링 방법, 랜덤과 Stratified 둘다 사용할 수 있다.Lable의 비율을 맞추고 k개로 나누는 방법, 많이 사용함.Group(label이 혼재되어 있음)들을 하나의 덩어리로 생각하고 같은 fold에는 들어가지 않도록 Split 함, Group의 수는 언제나 fold의 수 k보다 커야한다.시계열 데이터셋의 경우 시간을 기준으로 validation set을 나누어, 미래 데이터가 과거 데이터에 영향을 받지 않도록 하는 방법이 있음앞쪽 fold 일수록 data가 적어짐Reproducibility(재연성)모델의 학습 성능을 다시 학습하여 재연하기 위해서 랜덤성을 제거해줘야한다.이를 위해 랜덤 seed를 고정해줘야 한다.Machine learning workflow트리 모델 소개What is tree model트리 모델이란, 칼럼(feature) 값들을 어떠한 기준으로 Group을 나누어 목적에 맞는 의사결정을 만드는 방법질문으로 yes or no로 descision을 내리는 의사결정나무를 생성하여 만든다.Decision Tree, Random Forest, AdaBoost, GBM, XGBoost, LightGBM, CatBoost 순으로 발전해 왔다.Bagging &amp; Boosting트리 모델을 만들때 사용한 방법Decision Tree 여러개를 이용하여 모델을 생성하며, Training data의 활용법에 따라 Bagging과 Boosting으로 나뉜다.  Bagging      랜덤 포레스트의 생성 방법    데이터셋을 샘플링하여 모델을 만들어 나가는 방법          Bootstrap + Aggregation으로 이루어진다      Bootstrap: Data를 여러번 sampling, Aggregation : 종합(ensemble)        샘플링한 데이터 셋을 하나로 하나의 Decision Tree 생성  생성한 Decision Tree의 Decision들을 취합(aggregation)하여 하나의 Deicision 생성  BoostingTree model with hyper-parameter"
  }
  , 
  
  "/articles/computer_science/OS/Linux%20%EA%B3%84%EC%A0%95%20%EB%B0%8F%20%EA%B6%8C%ED%95%9C.html": {
    title: "Linux 계정 및 권한",
    date: " Dec 14, 2022 ",
    url: "/articles/computer_science/OS/Linux%20%EA%B3%84%EC%A0%95%20%EB%B0%8F%20%EA%B6%8C%ED%95%9C.html",
    tags: ["HIDE","OS","LINUX"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueLinux 사용자와 권한에 대하여사용자 계정(user)리눅스는 시스템 보안 기능 중 하나로 서로 다른 권한을 가진 여러 사용자 계정을 만드는게 가능한 다중 사용자 시스템이다.cat /etc/passwd를 이용해 모든 사용자의 관리 정보를 출력해볼 수 있다.cat /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologin각 줄의 관리 정보의 구성은 다음과 같다.사용자명:암호(x로 마스킹됨):고유 UID:그룹 ID:계정 설명(보통, 사용자명):홈 디렉토리:사용자가 사용하는 셸(터미널)title: 추가 계정 정보들의 위치암호는 /etc/shadows 파일에 별도로 저장되어 있다.사용 가능한 셸 종류는 /etc/shells에 저장되어 있고, 기본은 /bin/bash이다.사용자는 크게 root(super), system, unprivileged로 나뉜다.root 유저모든 권한을 행사 가능한 관리자 계정, 계성 생성, 권한 부여, 비밀번호 변경 등이 가능하다.유저 아이디(UID)가 0으로 고정되어 있다. 즉, 관리 정보 맨 위에 존재한다. 만약 UID 0 값을 바꾸면 더이상 모든 권한을 가지지 않게되며, 0을 가진 계정이 root 계정의 권한을 가진다.system 유저Linux가 설치되면서 자동으로 생성되며, 주로 시스템 관리와 사용시 필요하다. 대략 관리 정보 40번째까지 존재한다.각자 필요한 권한만 가지고 있으며, 일반적인 방법으로는 로그인이 불가능하다.unprivileged 유저생성되어 우리가 직접 사용하는 계정으로, 관리자에게 권한을 부여 받아 시스템을 이용한다.OS 종류에 따라 100~499번(Redhat)까지, 100~999번(Debian)까지 할당 된다.사용자 그룹(Group)모든 사용자는 그룹을 통해 관리할 수 있다. 이때, 사용자는 반드시 적어도 하나의 그룹에 속해있어야 한다.그룹 단위로 파일접근 권한 설정과 프로세스 관리가 수행되므로 이를 통해 권한을 주고 뺏을 수 있다.그룹에 대한 정보는 /etc/group 파일에 저장되어 있다.처음 사용자 생성시, 기본적으로 자신의 UID와 동일한 GID(그룹 아이디)를 가진다.사용자 권한권한 Commandsudo실전 : AWS EC2 권한"
  }
  , 
  
  "/articles/computer_science/algorithm/Python%20for%20Algorithms-DP&Merge.html": {
    title: "Python for Algorithms-DP&amp;Merge",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/algorithm/Python%20for%20Algorithms-DP&amp;Merge.html",
    tags: ["알고리즘","PYTHON","HIDE"],
    content: "min_depth: 2max_depth: 3varied_style: truePython for Algorithms-DP동적 계획법 (DP)전체적으로  완전 탐색 구현  메모이제이션 구현을 통해 풀어낸다.최적화 문제 풀이 레시피최적 부분 구조인 문제의 경우 DP를 이용해 최적의 결과를 얻을 수 있다.  최적 부분 구조 : 각 부분 문제의 최적해만 있으면 전체 문제의 최적해를 쉽게 얻어 낼 수 있는 구조          최적의 답을 구하는 완전 탐색 알고리즘 설계      일부 선택된 부분만의 최적의 답을 구하는 부분 문제로 바꾸기      재귀 호출 시 넘길 입력 인자를 선택              완적한 최적 부분 구조일 경우 완전히 입력을 없앨 수도 있으며, 입력을 줄어들 수록 많은 부분 문제가 줄어들어 효율적이다.                  메모이제이션 구현                    입력이 문자열, 배열 등 저장에 비효율적인 구조의 경우, 효율적인 정수형 등으로 바꿀 수 있도록 변환해보자.      재귀적 동적 계획법반복적 동적 계획법"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20ALG.html": {
    title: "CS 질문 정리 - ALG",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20ALG.html",
    tags: ["HIDE","CS","알고리즘"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리 - ALG자료구조그래프 알고리즘순회수학기타"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20BE.html": {
    title: "CS 질문 정리 - BE",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20BE.html",
    tags: ["HIDE","CS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리 - BECommonSSR, CSR?Virtual DOM?Browser?Rendering 과정?MVC 패턴?ReactVue"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20DB.html": {
    title: "CS 질문 정리 - DB",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20DB.html",
    tags: ["HIDE","CS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리DB 시스템각 DB 시스템의 장단점SQL 구문OptimizerOptimizer에 대하여NoSQL트랜잭션과 인덱스인덱스에 대해서트랜잭션에 대해서ACID격리 수준아키텍처"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20DevOps.html": {
    title: "CS 질문 정리 - DevOps",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20DevOps.html",
    tags: ["HIDE","CS","DEVOPS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리 - DevOpsTDDDockerK8S"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20FE.html": {
    title: "CS 질문 정리 - FE",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20FE.html",
    tags: ["HIDE","CS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리 - FECommonSSR, CSR?Virtual DOM?Browser?Rendering 과정?ReactVue"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20Network.html": {
    title: "CS 질문 정리 - Network",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20Network.html",
    tags: ["HIDE","CS","NETWORK"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리응용 계층DNS 과정전달 계층TCP와 UDP연결 계층물리 계층네트워크 보안SSL/TLS 과정대칭키 비대칭키암호화 알고리즘"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20OS.html": {
    title: "CS 질문 정리 - OS",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20OS.html",
    tags: ["HIDE","CS","OS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리프로세스와 스레드데드락메모리Linux"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20PL.html": {
    title: "CS 질문 정리 - PL",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20-%20PL.html",
    tags: ["HIDE","CS"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리객체 지향형 프로그래밍제어자(Modifier)클래스나 변수, 메서드 선언 시 앞에 함께 선언되어 부가적인 의미를 부여한다.  접근 제한자(Access Modifier)객체 지향형 프로그래밍 언어들에서 클래스 내부 또는 외부에서 메소드와 필드의 접근을 제어하여 캡슐화를 구현하기 위한 수단이다.주로 Java로 설명하므로 자세한 건, Java을 참조  접근 제한자 이외  자바의 경우 주로 static, final, abstract를 이용한다.                  static : 클래스가 인스턴스화하지 않고도 접근 가능하며, 모든 인스턴스가 값을 공유한다.  클래스가 메모리에 로딩될 시 생성되며, 메모리의 위치가 고정되게 된다.  static 메서드의 경우, 각 인스턴스의 인스턴스 멤버를 사용할 수 없게 된다.                    final: 클래스의 경우 확장(상속) 불가능, 메소드와 변수의 경우 최초 선언 이후로, 재정의, 변경 불가능하게 제한한다.  보통 static final로 같이 사용하여 다른 언어에서의 Const처럼 상수로 사용한다.                    abstract:  클래스와 메서드에만 사용, 메서드의 선언부만 작성하고 실제 수행 내용은 오버라이딩 해서 사용한다.  이를 추상 메서드라 하며, 추상 메서드가 있는 클래스의 앞에 같이 선언하여 추상 클래스로 만들어지면, 해당 클래스는 인스턴스화될 수 없고 상속만 받을 수 있다.              그 외에는 native(자바가 아닌 언어로 구현 뒤, 자바에서 사용 시), transient(직렬화 시, 직렬화 대상 제외), snchronized(스레드 동기화용), volatile(메인 메모리에 변수 저장), strictfp(부동 소수점 계산 제한) 등이 존재하며, 자바 이외의 언어는 추가로 extern, inline, *const 등이 존재한다.  선언형 프로그래밍함수형 프로그래밍JavaJava의 접근 제한자클래스와 지역 변수를 제외하고 모든 메서드, 필드, 생성자는 폐쇄성 오름차순으로 Public, Protected, Default, Private이 존재한다.  Public: 언제 어디서나 접근 가능  Protected: 같은 패키지 소속이거나 자식 객체만 접근 가능  Default: 같은 패키지에 소속된 객체만 접근 가능, 아무 선언 안할 시 기본값  Private: 클래스 내부에서만 접근 가능,          접근 불가능한 private 생성자를 이용해 인스턴스 생성 또한 클래스 내부의 다른 퍼블릭 메소드로 진행하도록 강제하여 싱글톤 패턴을 만드는데 사용할 수 있다.      title: Class 접근 제한자는 Public과 Default만 있으며, Public의 경우, 다른 패지키에서 해당 클래스를 사용 가능하다. 지역변수는 언제나 Default 상태이다.Java 상속과 인터페이스Javascript비동기이벤트 루프프로미스C-family메모리의 동적 할당C 언어의 경우, 런타임 중에 사용할 메모리 공간을 힙에 할당하기 위해 malloc 함수를 이용할 수 있다.다른 프로그래밍 언어는 자동으로 동적 할당이 가능하지만 C언어는 메모리에 대한 완전한 제어를 가지는 대신 관리 책임 또한 지어야 한다.title: malloc 함수void* malloc(size_t size);size : 동적으로 할당할 메모리의 크기return : 할당된 메모리의 주소, 할당 실패시 NULL  리턴이 void 형이므로 주소를 얻어오기 위해 int*처럼 캐스팅하여 값을 받아와야 한다.이후 사용이 끝난 메모리는 free 함수를 이용해 해제해 줘야 한다.title: free 함수void free(void *ptr);ptr: 해제하고자 하는 메모리의 포인터  해제해주지 않을 경우 해당 메모리 공간을 계속 차지하는 메모리 누수 현상이 일어난다.  다른 프로그래밍 언어는 가비지콜렉션을 통해 자동으로 해제해준다.title: 동적 할당 예시#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; // or &lt;malloc.h&gt;void main(){\tint* arr;\tarr = (int*)malloc(sizeof(int) * 3); // 3 길이의 int 배열 크기 만큼 동적 할당 \tarr[0] = 1;\tarr[1] = 2;\tarr[3] = 3;\tfor (int i = 0; i &lt; 3; i++){\t\tprintf(\"%d\", arr[i]);\t}\tfree(arr); // 사용한 메모리 해제}title: 만약 해제한(free) 메모리 또 해제하면? =&gt; DFBchar * myString = malloc(sizeof(char)*STRING_BUFFER_SIZE);free(myString); free(myString); // all realloc이를 double free bug(DFB)라고 하며, 보통 crash나 메모리 오염의 위험이 있다.이를 막기위해  다음과 같이 해제한 뒤 NULL을 지정해주면 좋다.free(myString);myString = NULL;title: 왜 DFB가 나타나는 가?간단히 말하자면, 두 번 해제된 메모리 공간은 덮어씌어질 위험이 생긴다.블로그 글 참조title: 메모리 누수란?ㅣPython"
  }
  , 
  
  "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88.html": {
    title: "CS 질문 정리 시리즈",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/etc/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88/CS%20%EC%A7%88%EB%AC%B8%20%EC%A0%95%EB%A6%AC%20%EC%8B%9C%EB%A6%AC%EC%A6%88.html",
    tags: ["CS","SERIES","JOB"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueCS 질문 정리 시리즈면접을 위한 CS 전공 지식 노트(주홍철 저, 길벗) 라는 책을 읽고 추가적인 질문들을 추가한 글즉, 위 책과 함께 보면 좋다. 저 책에서 다룬 부분은 되도록 다루지 않기 때문이다.위 책에서 그러하듯, 질문 위주가 아니라 개념 위주로 다룬다. 웹 개발에 관련된 내용이 대부분이다.CS 질문 정리 - FECS 질문 정리 - BECS 질문 정리 - DevOpsCS 질문 정리 - PLCS 질문 정리 - ALGCS 질문 정리 - OSCS 질문 정리 - NetworkCS 질문 정리 - DB"
  }
  , 
  
  "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80-%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%84%A4%EA%B3%84.html": {
    title: "어떻게 코딩할 것인가-데이터 설계",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80-%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%84%A4%EA%B3%84.html",
    tags: ["HIDE","CRUDE","ETC"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true데이터 설계Edx 강의 How to Code 시리즈를 정리한 내용입니다.데이터 설계는 함수 설계보다 프로그램에 직접적인 영향을 끼치진 않지만, 함수 설계에 크나큰 영향을 끼친다.  함수는 데이터를 다루는 방법이기 때문데이터는 사람이 이해 가능한 문제 영역의 정보(problem domain information)를 프로그램이 이해 가능하게 표현(represent)한 것이다.이러한 데이터는 개발자가 보았을 때, 다시 문제 영역의 정보로 해석(interpret)할 수 있어야 한다.  예를 들어, enum = {1, 2, 3}은 데이터이며, 사람이 이해하는 apple, orange, strawberry는 문제 영역 정보이다.  1은 데이터이며, apple의 표현이며, apple이라는 문제 영역 정보로 해석할 수 있다.정보의 구조는 데이터의 구조를 정하며, 데이터의 구조는 템플릿의 구조를 정하고 템플릿의 구조는 함수의 구조를 정하고, 이러한 함수가 모여 전체 프로그램을 설계하므로 중요하다.데이터 정의(Data Definition)란?데이터 정의는 정보와 데이터의 표현/해석 관계를 수립하는 것이다. 다음과 같은 내용을 정해야 한다.  데이터가 새로운 타입으로 어떻게 대치되는가?  정보를 어떻게 데이터로 표현하고, 데이터를 어떻게 정보로 해석하는가?  데이터를 다루기 위한 템플릿title: 데이터 정의 예시# Data definitions:{: #data-definitions}# TLColor is one of:{: #tlcolor-is-one-of}#  - 0{: #0}#  - 1{: #1}#  - 2{: #2}# interp. 0 means red, 1 yellow, 2 green               {: #interp-0-means-red-1-yellow-2-green}def FnForTlcolor(c):\tif c == 0:\t\tpass\telif c == 1:\t\tpass\telif c == 2:\t\tpass데이터 정의를 통해 함수 설계에 다음과 같은 영향을 끼친다.  데이터 입력과 출력 도메인을 제한  테스트를 만드는데 도움을 줄 수 있음  템플릿을 만드는데 도움을 줄 수 있음title: 데이터 정의에 도움을 받은 함수 디자인# Function Design:{: #function-design}# Signature: TLColor -&gt; TLColor{: #signature-tlcolor-tlcolor}# Purpose: produce next color of traffic light{: #purpose-produce-next-color-of-traffic-light}# Stub : def nextColor_stub(c): return 0{: #stub-def-nextcolor-stub-c-return-0}# Tests {: #tests}# 테스트를 만드는데 도움을 줄 수 있음, 데이터 입력과 출력 도메인을 제한을 통해 테스트 생성print(nextColor(0) == 2)print(nextColor(1) == 0)print(nextColor(2) == 1)# Template from TLColor {: #template-from-tlcolor}# 템플릿을 만드는데 도움을 줄 수 있음# function{: #function}def nextColor(c):\tif c == 0:\t\treturn 2\telif c == 1:\t\treturn 0\telif c == 2:\t\treturn 1데이터 종류 별 설계 과정데이터의 종류에 따른 여러 설계 방법과 함수 설계의 예시를 알아보자.  예시에서는 하나의 데이터 설계에 하나의 함수 설계가 대치되므로 매우 비효율적이게 보이겠지만, 실제로는 하나의 데이터 설계 이후 여러 함수가 공유하므로 훨씬 낫다.데이터의 종류에 따라 정의한 결과물들을 이용해 함수 설계를 하므로, 데이터 별 정의는 함수 설계에 직교성을 가진다.  쉽게 말해 사용하는 데이터에 따라 함수 설계 방법이 크게 바뀐다는 의미모든 설계 과정은 다음과 같은 공통 과정을 거친다.  정보의 내재 구조 파악          데이터를 어떤 타입으로 표현해야할 것인가? ex) atomic, Interval, Itemization        가능한 구조 정의(복합 데이터의 경우에만 시행)  데이터 타입 정의          타입명과 데이터의 구성 방법을 기술        정보와 데이터 간의 상호 해석/표현 방법 정의  데이터에 대한 예시들 기술  데이터를 해석/표현하는 하나의 인자를 가진 함수 템플릿 정의          함수의 템플릿을 만드는데 지대한 영향을 끼침      템플릿을 이용해서 꼭 테스트해서 오류를 조기에 찾아내자      Data Driven Template을 참조하여 구현한다.      title: Data Driven Template(DDT)Type of datacond question (if applicable)Body or cond answer (if applicable)Atomic Non-DistinctNumberStringBooleanImageinterval like Number[0, 10)etc.Appropriate predicatetype(x)==inttype(x)==strtype(x)== type(True)type(x)==Image(and (&lt;= 0 x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(&lt; x 10))etc.Expression that operates on the parameter.(... x)Atomic Distinct Value\"red\"falseemptyetc.Appropriate predicatex==\"red\"x==falselen(x)==0etc.Since value is distinct, parameter does not appear.(...)One OfenumerationsitemizationsCond with one clause per subclass of one of.(cond [&lt;question1&gt; &lt;answer1&gt;] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;question2&gt; &lt;answer2&gt;])Where each question and answer expression is formed by following the rule in the question or answer column of this table for the corresponding case. A detailed derivation of a template for a one-of type appears below.It is permissible to use else for the last question for itemizations and large enumerations. Normal enumerations should not use else.Note that in a mixed data itemization, such as;; Measurement is one of: ;; - Number[-10, 0) ;; - true ;; - Number(0, 10]the cond questions must be guarded with an appropriate type predicate. In particular, the first cond question for Measurement must be(and (number? m) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(&lt;= -10 m) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(&lt; m 0))where the call to number? guards the calls to &lt;= and &lt;. This will protect &lt;= and &lt; from ever receiving true as an argument.CompoundPositionFireworkBallconsetc.Predicate from structure(posn? x)(firework? x)(ball? x)(cons? x) (often just else)etc.All selectors.(... (posn-x x) (posn-y x))(... (firework-y x) (firework-color x))(... (ball-x x) (ball-dx x))(... (first x) (rest x))etc.Then consider the result type of each selector call and wrap the accessor expression appropriately using the table with that type. So for example, if after adding all the selectors you have:(... (game-ball g) ;produces Ball &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(game-paddle g)) ;produces PaddleThen, because both Ball and Paddle are non-primitive types (types that you yourself defined in a data definition) the reference rule (immediately below) says that you should add calls to those types' template functions as follows: (... (fn-for-ball (game-ball g)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(fn-for-paddle (game-paddle g)))Other Non-Primitive Type ReferencePredicate, usually from structure definition(firework? x)(person? x)Call to other type's template function(fn-for-firework x)(fn-for-person x)Self ReferenceForm natural recursion with call to this type's template function:(fn-for-los (rest los))Mutual ReferenceNote: form and group all templates in mutual reference cycle together.Call to other type's template function:(fn-for-lod (dir-subdirs d) (fn-for-dir (first lod))  데이터 템플릿 내부의 비교 구문과 로직 구현 부분을 참조하여 넣으면 된다.원자 데이터(Atomic)더 이상 의미를 나눌 수 없는 데이터에 적용하며 기본 내장된 원시 타입들을 이용해 만드는 비원시 타입 데이터이다.  예를 들어 Vancouver, Boston, Homead는 각기의 의미를 가지고 있으며 추가로 나누는 것이 불가능하며 원시 타입인 String을 이용해 만들었다.title: 원자 데이터 정의 예시# Vancouver, Boston, Homead 같은 도시명들을 원자형으로 표현하고자 한다. {: #vancouver-boston-homead-같은-도시명들을-원자형으로-표현하고자-한다}# 1. 정보 내재 구조 파악# CityName is String {: #cityname-is-string}# 3. 데이터 타입 정의# interp. the name of a city {: #interp-the-name-of-a-city}# 4. 해석/표현 방법 정의           # 5. 데이터에 대한 예시 기술{: #5-데이터에-대한-예시-기술}CT1 = \"Vancouver\"CT2 = \"Boston\"#6. 데이터를 해석/표현하는 함수 템플릿 정의# def fnForCityname(cn):{: #def-fnforcityname-cn}#\t# ...cn#\tpass# Template rules used: {: #template-rules-used}# Data Driven Template을 참조하여 정의한다.# - Atomic non-distinct: String {: #atomic-non-distinct-string}# 우리는 CityName을 Atomic non-distinct라고 정의함.  함수 템플릿 내의 ...은 만약 데이터가 구분형(distinct)이라면 ...으로 충분하고, 비구분형(non-distinct)라면 ...n 같이 명시해줘야 한다.  구분형에서는 해당하는 조건문 블락에 들어가면 해당 데이터를 더이상 안쓰기 때문이다.title: 원자 데이터 정의를 사용한 함수 정의 예시# 최고의 도시의 이름일 경우 참을 돌려주는 함수 만들기{: #최고의-도시의-이름일-경우-참을-돌려주는-함수-만들기}# Signature: CityName -&gt; Boolean{: #signature-cityname-boolean}# Purpose: produce true if the given city is the best in the world(Karazakarak).{: #purpose-produce-true-if-the-given-city-is-the-best-in-the-world-karazakarak}# Stub: {: #stub}def isBestCity_stub(cn):\treturn false# Tests: 특별 케이스, 결과 데이터, 입력 데이터를 고려하여 설정{: #tests-특별-케이스-결과-데이터-입력-데이터를-고려하여-설정}print(isBestCity_stub(\"Skavenblight\") == False)print(isBestCity_stub(\"Karazakarak\") == True)# Template: CityName 데이터 설계시 만든 템플릿 함수를 개명하여 사용 {: #template-cityname-데이터-설계시-만든-템플릿-함수를-개명하여-사용}# def isBestCity(cn):{: #def-isbestcity-cn}#\t# ...cn#\tpass# Function{: #function}def isBestCity(cn):\treturn cn == \"Karazakarak\" #...cn 부분이 cn을 이용한다는 점에서 힌트를 얻어 만든 함수 \t  함수 구현 시, 데이터 정의 템플릿을 활용할 때 무조건 데이터 템플릿의 모든 부분을 활용할 필요는 없다.범위 데이터(Interval)특정한 범위가 존재하는 데이터를 명시하는데 사용, 나중에 등장할 [[#항목형 데이터(Itemization)|항목형 데이터]]에 부분 데이터로 등장하는 경우가 많다.title: 범위 데이터 정의 예시: 1~32번까지 존재하는 영화 좌석 번호를 표시할 데이터# 1. type comment : interval{: #1-type-comment-interval}# SeatNum is Integer[1, 32] {: #seatnum-is-integer-1-32}# interp. seat numbers in a row, 1 and 32 are aisle seats{: #interp-seat-numbers-in-a-row-1-and-32-are-aisle-seats}# data examples {: #data-examples}# 주로 범위의 처음과 끝, 중간과 특수한 부분을 테스트SN1 = 1 # aisleSN2 = 12 # middleSN3 = 32 # aisle# data template{: #data-template}def fNForSeatNum(sn):\t#(... sn)\tpass# Template rules used: at Data Drive Templates page{: #template-rules-used-at-data-drive-templates-page}# - atomic non-distinct: Integer[1, 32]{: #atomic-non-distinct-integer-1-32}  ”[” : 해당 값 포함(이상, 이하), “(” : 해당 값 미포함(미만, 초과) ex) [1, 32) = 1~31title: 범위 데이터 함수 예시: 끝단에 있는 영화 좌석 여부를 알려주는 함수# Functions:{: #functions}# Signature: SeatNum -&gt; Boolean{: #signature-seatnum-boolean}# Purpose: produce true if the given seat number is on the aisle{: #purpose-produce-true-if-the-given-seat-number-is-on-the-aisle}# tests{: #tests}print(isAisle(1)==True)print(isAisle(16)==False)print(isAisle(32)==True)# def isAisle(sn) return false {: #def-isaisle-sn-return-false}#stub# &lt;use template from SeatNum&gt; {: #use-template-from-seatnum}# 범위 데이터에서 정의한 템플릿 사용def isAisle(sn)\treturn sn == 1 or sn == 32  함수 테스트 작성 시, 범위의 극단값들과 중간 값, 특수 값 등을 고려하고 작성하자.열거형 데이터(Enumeration)구분되는 유한 개의 항목으로 표현할 수 있는 데이터, 숫자, 이미지, 문자 등으로 나타낼 수 있다.  ex) 요일, 성별 등, 원자 데이터와 달리 들어올 데이터가 예상 가능한 표현 내에 있다.title: 열거형 데이터 정의 예시: 학생의 시험 점수 표시 데이터(A, B, C)# A, B, C =&gt; consists of a fixed number of disinct items {: #a-b-c-consists-of-a-fixed-number-of-disinct-items}# 2.type comment : enumeration{: #2-type-comment-enumeration}# LetterGrade is one of:{: #lettergrade-is-one-of}# - \"A\" {: #a}# - \"B\" {: #b}# - \"C\" {: #c}# 3.interp. the letter grade in a course{: #3-interp-the-letter-grade-in-a-course}# &lt;examples are redundant for enumerations&gt; 4. 예시, 보통 열거형 데이터 전부를 집어넣어 보므로 생략{: #examples-are-redundant-for-enumerations-4-예시-보통-열거형-데이터-전부를-집어넣어-보므로-생략}# def fNForLetterGrade(lg): 5. Template for data{: #def-fnforlettergrade-lg-5-template-for-data}#\t if Q:#\t\tA#    elif Q:{: #elif-q}#\t\tA#    else:{: #else}#\t\tA# if Q A를 DDT를 참고하여 아래처럼 바꿔주면 된다.        {: #if-q-a를-ddt를-참고하여-아래처럼-바꿔주면-된다}def fNForLetterGrade(lg): # 5-2. Template for data after Data Driven Table\tif lg == \"A\":\t\t# ...\t\tpass\telif lg == \"B\":\t\t# ...\t\tpass\telse:\t\t# ...\t\tpass# Template rules used:{: #template-rules-used}# - one of: 3 cases{: #one-of-3-cases}# - atomic distinct value: \"A\"{: #atomic-distinct-value-a}# - atomic distinct value: \"B\"{: #atomic-distinct-value-b}# - atomic distinct value: \"C\"{: #atomic-distinct-value-c}title: 열거형 데이터 함수 예시: 학생의 시험 점수 한 단계 올려주기 함수# Functions:{: #functions}# Signature: LetterGrade -&gt; LetterGrade{: #signature-lettergrade-lettergrade}# Purpose: produce next highest letter grade (no change for A){: #purpose-produce-next-highest-letter-grade-no-change-for-a}# def bumpUp(lg): return \"A\" {: #def-bumpup-lg-return-a}#stub# 테스트 갯수는 열거형 항목 갯수 만큼{: #테스트-갯수는-열거형-항목-갯수-만큼}print(bumpUp(\"A\")==\"A\")print(bumpUp(\"B\")==\"A\")print(bumpUp(\"C\")==\"B\")#&lt;use template from LetterGrade&gt;def bumpUp(lg):\tif (lg == \"A\"):\t\treturn \"A\"\telif (lg == \"B\"):\t\treturn \"A\"    elif (lg == \"C\")\t    return \"B\"  열거형의 경우 항목 수 이상 만큼 테스트를 하는게 이상적이다.          항목 수 보다 많은 경우 :      하지만, 일정 항목만 테스트 해도 코드의 모든 부분을 커버할 수 있다면 적게하는 경우도 있다.(Whitebox Test)      항목형 데이터(Itemization)일부 데이터가 구분되는 데이터가 아닌 다른 부차적 데이터로 이루어진 데이터title: 항목형 데이터 정의 예시: 새해 카운트다운 데이터# itemization: {: #itemization}# CountDown is one of:{: #countdown-is-one-of}#  - false{: #false}#  - Natural[1, 10]{: #natural-1-10}#  - \"complete\" ;; true로 놓으면 숫자도 true이므로 문자열로 표시{: #complete-true로-놓으면-숫자도-true이므로-문자열로-표시}# interp. false means not yest started{: #interp-false-means-not-yest-started}# Natural[1, 10] means countdown is running and how many seconds left{: #natural-1-10-means-countdown-is-running-and-how-many-seconds-left}# \"complete\" means countdown is over{: #complete-means-countdown-is-over}# Examples:{: #examples}CD1 = falseCD2 = 10 # just started runningCD3 = 1 # almost overCD4 = \"complete\"# Template{: #template}def fnForCountdown(c):\tif c==False (\t\t#...\t\tpass    elif type(c)==int and c &gt;= 1 and c &lt;= 10:        # 숫자 인지 확인(guard)하지 않으면 &lt; 같은 부등호를 쓰면 string=\"complete\"가 들어오면 오류.\t    #...c # 이후 c의 숫자에 따라 추가로 구현이 다르므로 body에 c를 포함해야 함.\t    pass    else: # 입력을 CountDown으로 한정하므로 나머지는 굳이 비교 안해도 됨\t    #...\t    pass#template rules used:# - one of: 3 cases{: #one-of-3-cases}# - atomic distinict: false{: #atomic-distinict-false}# - atomic non-distinct: Natural[1, 10]{: #atomic-non-distinct-natural-1-10}# - atomic distnict: \"complete\"{: #atomic-distnict-complete}title: 항목형 데이터 함수 예시: 새해 카운트 다운 함수# Functions:{: #functions}# Signature: Countdown -&gt; Image{: #signature-countdown-image}# Purpose: produce nice image of current state of countdown{: #purpose-produce-nice-image-of-current-state-of-countdown}# Stub: def countdownToImage(c) return \"blank\"{: #stub-def-countdowntoimage-c-return-blank}print(countdownToImage(False) == \"blank\")print(countdownToImage(1) == \"1\")print(countdownToImage(5) == \"5\")print(countdownToImage(10) == \"10\")print(countdownToImage(\"complete\") == \"Happy New Year!\")# &lt;use template from Countdown&gt;{: #use-template-from-countdown}def countdownToImage(c):\tif c == False:\t\treturn \"blank\"\telif type(int) == int and c &gt;= 1 and c &lt;= 10:\t\treturn f\"{c}\"\telse:\t\treturn \"Happy New Year!\"  열거형 데이터가 포함된 부분도 테스트해줘야 한다.복합 데이터(Compound data)하나 이상의 같거나 다른 형태의 데이터들과 짝을 이루는 데이터 구조title: 복합 데이터 정의 예시: 하키 선수 명단class Player:\tdef __init__(self, fn, ln):\t\tself.fn = fn\t\tself.ln = ln# Player is Player(fn:string, ln:string){: #player-is-player-fn-string-ln-string}# interp. Player is a hocky player with{: #interp-player-is-a-hocky-player-with}# - fn is the first name{: #fn-is-the-first-name}# - ln is the last name{: #ln-is-the-last-name}P1 = Player(\"Bobby\", \"Orr\")P2 = Player(\"Wayne\", \"Gretzky\")def fnForPlayer(p):\t#...(p.fn, p.ln)\tpass# Template rules used:{: #template-rules-used}# - Compound: 2 fields{: #compound-2-fields}# - atomic non-distinct: String{: #atomic-non-distinct-string}# 만약 서로 다른 형태의 데이터들이 짝지었다면{: #만약-서로-다른-형태의-데이터들이-짝지었다면}# 여기에 추가로 정의하자 (ex) atomic distinct: false){: #여기에-추가로-정의하자-ex-atomic-distinct-false}title: 복합 데이터 함수 예시: 사전순 하키선수 이름 함수# Signature: Player Player -&gt; String{: #signature-player-player-string}# Purpose: produce first name of player with the earliest last name in alphabetical order.{: #purpose-produce-first-name-of-player-with-the-earliest-last-name-in-alphabetical-order}# Stub:{: #stub}# def longerName(p1:Player, p2:Player):{: #def-longername-p1-player-p2-player}#\treturn p1.fn#Tests:print(longerName(P1, P2)==\"Wayne\")#Templates:def longerName_stub(p1, p2):\t# ...(p1.fn, p1.ln, p2.fn, p2.ln)\tpass def longerName(p1, p2):\treturn p1.fn if p1.ln &lt; p2.ln else p2.fn  함수 템플릿 부분을 잘 보자List 데이터(List data)하나 이상의 타입의 데이터들을 동적인 수만큼 할당 가능한 데이터 구조강의에서는 재귀를 이용해 동적할당을 처리하지만, 대부분의 프로그래밍 언어에서는 그러하지 않으므로 변경했다.title: 리스트 데이터 정의 예시: 퀴디치 팀 명단ListOfString = []# ListOfString is one of:{: #listofstring-is-one-of}# - empty list{: #empty-list}# - List of String{: #list-of-string}# interp. a list of strings{: #interp-a-list-of-strings}LOS1 = []LOS2 = [\"Mcgill\", \"UBC\"]def fnForLos(los):\tif len(los)==0:\t\t#...\t\tpass\telse:\t\tfor e in los:\t\t\t#...e\t\t\tpass# Template rules used:{: #template-rules-used}# - one if: 2 cases{: #one-if-2-cases}# - atomic distinct: empty{: #atomic-distinct-empty}# - compound: n fields of string{: #compound-n-fields-of-string}여러 종류의 데이터 정의가 필요한 경우두 개 이상의 데이터가 참조 관계 등으로 인해 정의되어야 할 경우title: 참조 관계의 두 데이터 정의 예시 : 학교별 등록금 정보# Data definition:{: #data-definition}class School:\tdef __init__(self, name, tuition):\t\tself.name = name\t\tself.tuition = tuition# Shcool is School(String, Integer[0,]){: #shcool-is-school-string-integer-0}# interp. name is the school's name, tuition is interational student's tuition in USD{: #interp-name-is-the-school-s-name-tuition-is-interational-student-s-tuition-in-usd}S1 = School(\"School1\", 27797)S2 = School(\"School2\", 23300)S3 = School(\"School3\", 28500)def fnForSchool(s):\t#(...(school-name s)(school-tuition s))\tpass # this template is helper funtion for fn-for-los{: #this-template-is-helper-funtion-for-fn-for-los}# Template rules used:{: #template-rules-used}# - compound: School(String, Integer[0,]){: #compound-school-string-integer-0}# ListOfSchool is one of:{: #listofschool-is-one-of}# - empty{: #empty}# - compound: n fields of School {: #compound-n-fields-of-school}# reference from school# interp. a list of schools{: #interp-a-list-of-schools}LOS1 = []LOS2 = [S1, S2, S3]def fnForLos(los):\tif len(los)==0: \t\t#... \t\tpass \telse: \t\tfor e in los: \t\t\t#...fnForSchool(e) # mutual recursion from mutual-reference\t\t\tpass\t\t\t# Template rules used:{: #template-rules-used}# - one of 2 cases:{: #one-of-2-cases}# - atomic distinct : empty{: #atomic-distinct-empty}# - compound: n fields of School{: #compound-n-fields-of-school}# - Mutual reference: School {: #mutual-reference-school}# other reference재귀형 데이터데이터 내부에 자신과 같은 구조가 마찬가지로 포함된 데이터 구조title: 재귀형 데이터 정의 예시: 재귀 구조 숫자 리스트# Data definition:{: #data-definition}# ListOfNumber is one of the{: #listofnumber-is-one-of-the}# - empty{: #empty}# - [Integer] + ListOfNumber {: #integer-listofnumber}# 데이터 설계 중 재귀 구조 발견# interp. an arbitrary number of Numbers{: #interp-an-arbitrary-number-of-numbers}def fnForLon(lon):\tif len(lon)==0:\t\t#...\t\tpass\telse:\t\t#...lon[0]\t\t#...fnForLon(lon[1:])\t\tpass"
  }
  , 
  
  "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80.html": {
    title: "어떻게 코딩할 것인가",
    date: " Dec 28, 2022 ",
    url: "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80.html",
    tags: ["CRUDE","ETC","OSSU","PL"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true어떻게 코딩할 것인가?Edx 강의 How to Code 시리즈를 정리한 내용입니다.  좋은 프로그램이란?          잘 테스트된 여러 조각의 유기적인 작은 프로그램으로 이루어져 있으며 쉽게 추가, 수정 할 수 있으며, 원하는 목적을 잘 수행하는 프로그램        좋은 프로그램을 만들기 어려운 이유     - 프로그램의 목적을 정의하기 모호하거나 계속 바뀌는 경우가 많음     - 어려운 문제 하나를 쉬운 문제 여러개로 정확히 나누기 힘듦“좋은 프로그래밍 설계란, 애매모호하고 어설프게 나누어진 문제들을 잘 구성된 하나의 솔루션으로 바꾸는 과정이다.”어떻게 코딩할 것인가-함수 설계어떻게 코딩할 것인가-데이터 설계어떻게 코딩할 것인가-세계 설계스페이스 인베이더-세계 설계 예시 프로젝트"
  }
  , 
  
  "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20Ruby&Jekyll.html": {
    title: "Subbrain 개발기 - Ruby&amp;Jekyll",
    date: " Dec 30, 2022 ",
    url: "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20Ruby&amp;Jekyll.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSubbrain 개발기 - Ruby &amp; Jekyll개요Ruby는 가독성과 생산성에 치중한 고수준 인터프리터 언어로, 함수형, 절차형, 객체형 프로그래밍을 지원하며, 웹 프레임워크인 ruby on rails가 유명하다Jekyll은 작성한 문서 파일들을 간단히 정적 블로그 사이트로 바꿔주는 Ruby와 Gem 기반 정적 사이트 생성 툴이다. 주로 github page와 연동되어 개발자 블로그로 많이 사용된다.사용하기 쉽고 간단하며, 은근히 많은 사람들이 블로그를 만들기 위해 사용하여, 기술 스택으로 채용하게 되었다.하지만 둘 다 내가 이번 프로젝트에서 처음 접해보았으며, Ruby의 간단한 문법과 Jekyll의 편의성에도 불구하고 학습으로 인한 개발 시간 증가의 원인 중 하나이기도 하다.이 둘을 통해 블로그 기능 중 주로 동적 요구가 필요하지 않은 것들을 만들었다.개발 기능개발 환경 설정  Ruby 설치 및 linter 설정title: Gemfile 예시gem \"webrick\", \"~&gt; 1.7\"gem \"rufo\"gem \"reek\"gem \"rubocop\"gem \"jekyll-seo-tag\"gem \"nokogiri\"Ruby를 설치한 뒤, 위와 같이 Gemfile을 설정 후, bundle install을 통해 설치 후, VScode ruby 플러그인을 설치하여 여러 환경 설정을 손쉽게 할 수 있다.  VScode task 설정title: vscode .vscode/tasks.json{  \"version\": \"2.0.0\",  \"tasks\": [    {      \"label\": \"test\",      \"dependsOn\":[\"jekyll\", \"webpack\", \"sass\", \"tsc\"]    },    {      \"label\": \"jekyll\",      \"command\": \"bundle exec jekyll serve\",      \"type\": \"shell\",      \"options\": {        \"cwd\": \"./\"      },      \"presentation\": {        \"reveal\": \"always\",        \"panel\": \"new\"      }    },    {      \"label\": \"webpack\",      \"command\": \"node_modules/.bin/webpack\",      \"type\": \"shell\",      \"options\": {        \"cwd\": \"./\"      },      \"presentation\": {        \"reveal\": \"always\",        \"panel\": \"new\"      },      \"problemMatcher\": []    },    {      \"label\": \"sass\",      \"command\": \"sass -w _sass:assets/css\",      \"type\": \"shell\",      \"options\": {        \"cwd\": \"./\"      },      \"presentation\": {        \"reveal\": \"always\",        \"panel\": \"new\"      },      \"problemMatcher\": []    },    {      \"label\": \"tsc\",      \"command\": \"npx tsc -w\",      \"type\": \"shell\",      \"options\": {        \"cwd\": \"./\"      },      \"presentation\": {        \"reveal\": \"always\",        \"panel\": \"new\"      },      \"problemMatcher\": []    }  ]}프로젝트 개발 시 나는 다음과 같은 커맨드들을 일일이 입력해야 했다.  bundle exec jekyll serve : Gem에 명시된 라이브러리들과 함께 Jekyll 개발 모드로 실행  node_modules/.bin/webpack -w: 자바스크립트 파일 변경 시 자동 번들링  sass -w _sass:assets/css: 작성한 sass 파일 변경시 css 파일로 자동 컴파일  npx tsc -w: 작성한 타입스크립트 파일 변경 시 자동 자바스크립트로 컴파일이는 매우 귀찮은 일이였고, 위와 같이 VScode의 기능을 활용해 커스텀 Task를 생성해 ctrl + p Tasks: Run task 명령을 통해 한꺼번에 실행시킬 수 있게 만들었다.프로젝트 파일 구조Jekyll의 기본 폴더 구조를 기본으로 몇몇 폴더와 다른 툴들을 위한 파일이 추가되었다.title: 프로젝트 파일 구조 개요subbrain/---folders↓--- |-assets/ : css, 이미지, 자바스크립트 등 여러 정적 파일이 저장되어 있는 폴더 |-blog/ : 라우터 인덱싱, 정적 페이지들의 html 파일들 저장소 |-node_modules/ : javascript가 사용하는 패키지 |-scripts/ : Typescript 파일들 |-_articles/ : 작성한 마크다운 파일들이 카테고리 별 폴더 구조로 나뉘어 저장 |-_data/ : 생성된 블로그 포스트들의 정보(태그 정보, 카테고리 정보 등)가 json형식으로 저장 |-_etcs/ : 기타 임시 코드 저장용 폴더 |-_includes/ : 재사용 가능한 컴포넌트들(헤더, 푸터, 드로워 등) |-_layouts/ : 페이지 템플릿들(옵시디언 마크다운 블로그 포스트 글 등) |-_plugins/ : 커스텀 루비 플러그인 파일들, 주로 블로그 포스트의 전처리와 후처리 구현 |-_sass/ : css로 컴파일되 assets 폴더로 옮겨짐 |-_site/ : 정적 사이트 생성 결과물 ---files↓--- |-tsconfig.json : Typescript 설정 파일 |-webpack.config.js : Webpack 설정 파일 |-package.json : Javascript 패키지 정보 |-Gemfile : Ruby 패키지 정보 |-index.html : 메인 페이지 |-.eslintrc.json : Typescript linter 설정 파일 |-_config.yml : Jekyll 설정 파일  _가 앞에 붙은 폴더나 _config.yml에서 제외 목록에 들어간 폴더는 정적 사이트 생성시 제외됨title: 일부 폴더 소개      assets 폴더 : css, 이미지, 자바스크립트 등 여러 정적 파일이 저장되어 있는 폴더  정적 사이트 프로젝트이므로 규모가 크다.  _sass 폴더에서 css 파일을, scripts 폴더에서 JS 파일을 컴파일하여 이곳에서 저장한다.        _articles 폴더 : 작성한 마크다운 파일들이 카테고리 별 폴더 구조로 나뉘어 저장  기존의 _post 폴더의 동작을 모방하여, Jekyll이 이들의 변화를 감지하고 블로그 포스트를 생성하도록 구현함    _data 폴더 : 생성된 블로그 포스트들의 정보(태그 정보, 카테고리 정보 등)가 json 형식으로 저장  Jekyll이 자동으로 변화를 업데이트하게끔 구현하였다.  주로 자바스크립트와 커스텀 플러그인에서 포스트 정보를 가져오는데 사용하는 일종의 DB 역할을 한다.          Liquid의 경우 Jekyll의 site 변수에서 정보를 제공하지만, JS와 ruby에서 사용할 수 없어서 구현        _includes 폴더와 _layouts 폴더: 컴포넌트와 페이지 템플릿  파일들  liquid 태그가 포함되어 html 파일들이 존재, _layouts 내의 파일은 일종의 페이지를 위한 템플릿이고, _includes 내의 파일은 일종의 재사용 가능한 컴포넌트들이다.          예를 들어 _layouts/obsidian.html은 옵시디언 파일들을 페이지로 만들 때 공통으로 사용      _includes/drawer/drawer.html은 페이지 내 drawer를 구현할 때 liquid include 문법으로 불러와 사용한다.            blog 폴더 : 정적으로 생성되는 페이지들에 사용되는 html 파일들  일종의 라우팅 인덱스 폴더, 내 소개 페이지, 검색결과 페이지, 404 에러 페이지, 포스트 리스트 페이지가 존재        _site 폴더 : 생성된 정적 사이트 결과물  내부 파일을 gh-page 브랜치에 옮겨 배포한다.    기타 폴더(_sass 폴더, scripts 폴더, node_modules 폴더)는 해당 도메인에서 설명하겠다.Obsidian &lt;=&gt; Jekyll 포스트 구조title: Obsidian 이란?Jekyll에서 기본적으로 마크다운 파일을 HTML 파일로 바꾸어주는 기능을 지원한다.  커스텀 id 설정과 Hard wrap 기능을 지원하기 위해 Kramdown 마크다운 변환으로 설정을 바꾸어 주었다.추가적으로 내가 원하는 기능을 구현하기 위해 Ruby 플러그인을 직접 구현하였다.  직접 구현한 이유는 학습 + 관련 라이브러리 없음 + gh page에서 플러그인 지원 안함title: Obsidian &lt;=&gt; Jekyll 포스트 아키텍처 구현  변환 전, md 파일에 영향을 줘야하는 기능은 Jekyll에서 지원하는 Generator 클래스를 상속하여 구현  변환 후, html 파일에 영향을 줘야하는 기능은 Liquid에서 지원하는 Custom Filter 등록 기능으로 구현전처리 구현: Custom Generator 기반title: 전처리 플러그인 파일 구조전처리 레이어는 ruby 플러그인으로 구현되었으며, 추가로 레이어 내부에 각 전처리 함수들이 계층 구조로 이루어져 있다.Jekyll 측에서 각 마크다운 파일을 처리할 때 사용하는 Generator  클래스를 상속받아 구현하였다.title: Preprocessor.rbrequire_relative './layouts/obsidian/preprocess_obsidian'require_relative './common/modules/preprocess_frontmatter'require_relative './common/preprocess_common'module Preprocessor  class ArticleConverter &lt; Jekyll::Generator    include PreprocessObsidian    include PreprocessFrontmatter    include PreprocessCommon    def generate(site)      changed = register_articles(site.collections['articles'])      clear_categories if changed      site.collections['articles'].docs.map do |article|        result = preprocess_common(article, changed)        result = preprocess_obsidian(site, result) if result['layout'].upcase == 'OBSIDIAN'        result      end      create_category_pages(site).each do |page|        site.pages &lt;&lt; page      end    end  endend  가장 먼저 새로 등록된 파일들의 정보를 _data 폴더에 업데이트한다.(register_articles)  블로그 포스트의 정보(태그, 사용하는 레이아웃)에 따라 처리한다.  주로 옵시디언에서만 지원하는 마크다운 기능들을 랜더링하기 위한 정보들을 추출하는 함수들이다.  폴더 구조에 따라 변형되는 게시판 카테고리 구조 페이지 또한 이곳에서 구현된다.  마크다운 파일은 DOM 구조가 없으므로 대부분 정규 표현식을 통해 요소를 검색하고 수정한다.구현하면서 정규표현식을 정말 많이 공부할 수 있었다. 기존에는 정규 표현식을 정말 가독성이 떨어지고 복잡하다고 생각하여 초보적인 단계까지만 학습했지만, 이제 캡처 그룹부터 룩 어헤드까지 사용할 수 있게 되었다.후처리 구현 : Custom Liquid Filter 기반title: 후처리 플러그인 파일 구조후처리 레이어 또한 ruby 플러그인으로 구현되었으며, Jekyll의 layout 기능을 이용해서 필요한 후처리 함수만 Liquid Custom Filter를 통해 처리되도록 구현했다.title: layout 후처리 필터 예시...&lt;div class=\"content-section\"&gt;  {{content | postprocess_obsidian}}&lt;/div&gt;...주로, 콜아웃이나 TOC, 위키 링크 등의 실제 뷰를 HTML 태그로 구현하는 기능을 만들었다.title: postprocess_obsidian.rbrequire 'liquid'require 'nokogiri'require_relative './modules/postprocess_toc'require_relative './modules/postprocess_callout'require_relative './modules/postprocess_wikilink'  module Jekyll  module PostprocessObsidian    include PostprocessToc    include PostprocessCallout    include PostprocessWikilink    def postprocess_obsidian(str)      html = Nokogiri.HTML5(str)      html = convert_noneng_custom_id(html)      html = convert_toc(html)      html = html.to_html      html = convert_callout(html)      html = convert_wikilink(html)      html    end  endendLiquid::Template.register_filter(Jekyll::PostprocessObsidian)개발 초반에는 DOM 트리 탐색 및 변경을 정규표현식으로 처리했으나, 후반에는 Nokogiri라는 HTML DOM 기반 parser 라이브러리를 이용하여 구현했다.  다만 일부 경우, 오히려 가독성이 나빠지는 경우가 있어 그대로 정규 표현식 기반으로 구현한 기능도 존재한다.회고Rubytitle: 내 Ruby 코드 예시def create_category_page_recursive(site, categories, data) # python과 유사하다.  result = data['categories'].keys.inject([]) do |memo, sub_category| # 다양한 loop문 표현    memo + create_category_page_recursive(site, categories + [sub_category], data['categories'][sub_category])  end  if categories.empty? # 함수형 프로그래밍, 파라미터 없을 경우 생략 가능한 \"()\"    result # return을 생략 가능  else    [CategoryPage.new(site, categories, data)] + result  endend  Ruby는 파이썬과 상당히 비슷하게 사용자의 가독성과 편의성을 상당히 신경쓴 것이 느껴졌다.          데이터 타입을 선언할 필요가 없음      함수의 return을 생략 가능      .empty?, .nil? 등 직관적이고 함수형 프로그래밍이 가능        단, 오히려 이점이 오히려 호불호 갈린 경우도 많다.          함수에 파라미터를 넘기지 않을 때는 괄호를 생략할 수 있음, 그럴 경우 해당 객체의 변수를 읽은건지, 함수를 호출한 건지 보기만해서는 알 수 없음.      해당 함수가 return 값이 존재하는 함수인지, 아닌지도 알기 힘들다.      마치 자바스크립트 루프문들 처럼 같은 코드를 여러 방식으로 표현할 수 있다는 점      unless 처럼 단순히 if의 반대의 의미를 가진 표현도 있다.      title: Rubocop Linter 경고문 예시  Ruby의 Linter 겸 Formatter인 Rubocop을 적용했었는데, 내 짧은 프로그래밍 인생 중, 가장 훌륭한 Linter였다.          코드 스타일, 네이밍 컨벤션, 주석 여부 같은 기본 기능      같은 동작의 더 좋은 표현까지 추천      ABCSize, 순환 복잡도 같은 각종 코드 복잡도까지 지원      귀찮으면 버튼 하나로 이를 자동으로 수정 (코드 표현까지도!)      VScode와의 호환성이 좋고, 설정도 쉬웠으5며, Reek 같은 다른 린터와 동시에 사용가능.      설명이 잘되어 있고, 문서화가 잘되어있다.      title: Rubocop CyclomaticComplexity 문서 중 일부문서를 읽으면서, 좋은 코드에 대해 많이 배울 수 있었다.물론 어쩌면 내가 다른 언어의 Linter를 잘못쓰는 것일지도 모르지만  regex를 이용하기 전에 진즉에 Nokogiri를 썼으면 시간을 꽤나 아꼈을 것 같다.  제대로 설계를 안하고 써서 그런지, 코드 가독성이나 재사용성이 조금 아쉽다.  Ruby on rails를 함께 사용했으면 경험에 큰 도움이 됬을 것 같지만, 내가 Ruby를 더 파볼 일은 없을 것 같다.Jekyll  확실히 다양한 기능을 클릭 몇번으로 적용 가능하고, 가벼운 블로그로써 손색이 없었지만, 나의 기능에 대한 욕심이 이를 망쳤다.          오히려, 기존 기능을 수정하여 사용하느라 힘들었다.      예를 들어, 기본 값인 post 콜렉션을 이용하다 파일들의 제목을 이상하게 바꿔야하는 (date-sluggedtitle.md) 제한이 마음에 안들어 새로운 콜렉션으로 바꾸는 과정에서 모든 파일들의 내용과 제목을 손봐줘야 했다.      정적 사이트에 동적 기능을 넣을 생각을 한 시점에서 이상함을 느껴야 했다.        github page에서 보안상의 이유로 일부 플러그인을 제외하고 지원하지 않았던 점 또한 치명적이었다.          지원되는 플러그인 찾기 -&gt; netlify 등의 다른 무료 배포 서비스 찾아보기 -&gt; 자바스크립트, 루비, 템플릿 언어 등으로 기능 직접 구현 -&gt; 커스텀 플러그인 지원을 위해 github action 건드려보기      위 과정의 시간이 오래 걸렸다.        하지만, 만약 나의 경우와 정반대로, 별 다른 커스터마이징 없이 블로깅하려면, 굳이 Jekyll을 쓰는 것 보다는 Tistory나 네이버 블로그, velog 같은 상용 블로그를 쓰는게 더 나은것 같다.          댓글, 조회수, 추천 등의 동적인 기능이 가능하며, 광고 등을 쉽게 적용할 수 있기 때문        Liquid 템플릿 언어는 include를 이용한 코드 재사용이 쉬웠고, 각종 필터와 함수들이 꽤 편리했지만, 여전히 별로였다. React의 JSX와 컴포넌트가 너무 그리웠다.          jinja2 같은 다른 템플릿 언어와의 차별점은 잘 모르겠다.      "
  }
  , 
  
  "/articles/etc/etcs/Obsidian.html": {
    title: "Obsidian",
    date: " Jan 3, 2023 ",
    url: "/articles/etc/etcs/Obsidian.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueObsidian무료로 이용 가능한 마크다운 작성 툴개요나는 기록할 때 notion이나 one note보다 마크다운 파일을 좀 더 선호하는데 이유는 다음과 같다.  오프라인으로 활용 가능  무료로 이용가능한 경우가 많음  Github README 등, 범용성이 좋음  HTML 등을 이용한 기능 확장이 편함    장단점    typora, marktext 등 여러 마크다운 툴을 이용해보았지만 Obsidian은 다음과 같은 장점으로 사용하게 되었다.    무료로 이용가능  wysiwyg 에디팅, 그래프 뷰 등 다양한 기본 기능 지원          wysiwyg(What you see is What you get): 실제 마크다운 렌더링 결과를 보면서 동시에 작성 가능        다양한 커스텀 플러그인을 지원  사용법이 익숙하다.대신 Obsidian은 오픈소스가 아니며, 성능이 무겁고, 필수적으로 저장소 폴더로 설정해야만 마크다운 파일을 볼 수 있다는 단점이 있다."
  }
  , 
  
  "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80-%EC%84%B8%EA%B3%84%20%EC%84%A4%EA%B3%84.html": {
    title: "어떻게 코딩할 것인가-세계 설계",
    date: " Jan 4, 2023 ",
    url: "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80-%EC%84%B8%EA%B3%84%20%EC%84%A4%EA%B3%84.html",
    tags: ["HIDE","CRUDE","ETC"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true세계 설계Edx 강의 How to Code 시리즈를 정리한 내용입니다.이곳에서의 세계는 곧 프로그램, 예시로 반응성 프로그램을 의미하는듯 하다.하지만 세계 설계를 다른 프로그램에 적용하여 사용할 수 있다.빅뱅 함수 ? 상태를 읽어 프로그램을 동작시키는 함수변화될 세계 상태와 세계 상태를 바꾸고 상호작용할 핸들러 함수들을 인수로 받아 해당 세계 상태 시 해당 함수들을 실행하게 하는 함수  세계 상태의 예시 : 시간(tick)  행해질 옵션 함수 예시 : 화면 표시, 세계 상태 변경, 키 입력 시 행동세계 설계 과정세계 설계는 두 가지 단계로 이루어져 있다.  도메인 분석 단계 : 다이어그램 등으로 설계할 세계에 대한 분석을 통해 모델을 형성하는 단계          프로그램 시나리오 작성      상수 정보 식별      변수 정보 식별      빅뱅 함수 정의        프로그램 구현 단계: 실제 설계와 구현이 이루어 지는 단계          상수 정보 입력      데이터 정의      함수 정의                  메인 함수 정의(빅뱅 함수 정의)          메인 함수 인자인 위시리스트 항목 정의                    위시리스트 항목 디버깅 및 테스트      도메인 분석 단계\t1. 프로그램 시나리오 작성 : 고양이가 시간이 지나며 화면을 가로지르는 프로그램\t2. 상수 정보 식별 : 고양이 사진, 화면 크기, 고양이 사진 y 위치 등은 바뀌지 않음\t3. 변수 정보 식별 및 정의: 고양이 사진 x 위치는 시간이 지나며 바뀜\t4. 빅뱅 함수 정의 : 시간을 세계 상태로 받아 고양이 사진을 적절한 위치에 맞게 배치하는 옵션 함수 이용프로그램 구현 단계템플릿? 함수, 프로그램을 구체적으로 작성하기 이전에 불변할 기본적인 구조와 알고리즘, 정보 등을 표시한 것  템플릿을 이용해 디자인 과정을 여러 처리 과정으로 나누며 이러한 여러 처리 과정을 각 함수마다 실행하게 하여 큰 문제를 잘게 쪼갤 수 있게 한다.title: 세계 설계 템플릿추적성 : 도메인 분석 단계에서 분석했던 모델 정보가 반영되도록 템플릿을 설계하자.  사람들이 사용하는 프로그램은 언제나 변화를 필요로 하므로 문서화와 좋은 추적성을 가진 코드가 중요  상수명 등을 일치시키자.# import libraries{: #import-libraries}# My world program  (make this more specific){: #my-world-program-make-this-more-specific}# ================={: #}# Constants:{: #constants}# ================={: #}# Data definitions:{: #data-definitions}# WS is ... (give WS a better name){: #ws-is-give-ws-a-better-name}# ================={: #}# Functions:{: #functions}# WS -&gt; WS{: #ws-ws}# start the world with ...{: #start-the-world-with}# def main(ws):\treturn BigBang(\t\tws, # ws\t\tonTick=tock, # WS -&gt; WS\t\ttoDraw=render, # WS -&gt; Image\t\t# stopWhen=..., # WS -&gt; Boolean\t\t# onMouse=..., # WS Integer Integer MouseEvent -&gt; WS\t\t# onKey=... # WS KeyEvent -&gt; WS\t)# WS -&gt; WS{: #ws-ws}# produce the next ...{: #produce-the-next}# !!!{: #}def tock(ws):\t#...\tpass# WS -&gt; Image{: #ws-image}# render ... {: #render}# !!!{: #}def render(ws)\t#...\tpasstitle: 세계 설계 템플릿 예시import Imageimport BigBang# 고양이가 스크린을 가로지르는 프로그램{: #고양이가-스크린을-가로지르는-프로그램}# - 추적성을 고려하여 도메인 분석 단계를 참조하여 작성하기{: #추적성을-고려하여-도메인-분석-단계를-참조하여-작성하기}# ================={: #}# Constants: {: #constants}# 상수 정보 입력 단계WIDTH = 600HEIGHT = 400# 단일 지점 제어(single point of control){: #단일-지점-제어-single-point-of-control}# ex) HEIGHT가 바뀌면 자동으로 CTR-Y도 바뀌게 HEIGHT를 이용해 상수 정의{: #ex-height가-바뀌면-자동으로-ctr-y도-바뀌게-height를-이용해-상수-정의}CTR_Y = HEIGHT/2 MTS = Image(WIDTH, HEIGHT)CAT = Image(\"Cat\")# ================={: #}# Data definitions: {: #data-definitions}# 데이터 정의 단계# Cat is Number{: #cat-is-number}# interp. x position of the cat in screen coordinates{: #interp-x-position-of-the-cat-in-screen-coordinates}C1 = 0 # left edgeC2 = WIDTH/2 # middleC3 = WIDTH # right edge# Template{: #template}# def fnForCat(c):{: #def-fnforcat-c}# \t{: #\t}# ...c# \tpass{: #\tpass}# Template rules used:{: #template-rules-used}# - atomic non-distinct: Number{: #atomic-non-distinct-number}# ================={: #}# Functions: {: #functions}# 함수 정의 단계# CAT -&gt; CAT{: #cat-cat}# start the world with (main 0){: #start-the-world-with-main-0}# def main(c):\treturn BigBang(\t\tc, # ws\t\tonTick=advanceCat, # CAT -&gt; CAT\t\ttoDraw=render # CAT -&gt; Image\t)# 핸들러 함수 정의{: #핸들러-함수-정의}# CAT -&gt; CAT{: #cat-cat}# produce the next cat, by advancing it 1 pixel to right{: #produce-the-next-cat-by-advancing-it-1-pixel-to-right}# !!!{: #}def advanceCat(c):\treturn 0# CAT -&gt; Image{: #cat-image}# render the cat image at appropriate place on MTS{: #render-the-cat-image-at-appropriate-place-on-mts}# !!!{: #}def render(c):\treturn MTS# 위시리스트 구현은 아래 참조{: #위시리스트-구현은-아래-참조}프로그램 수정보통 프로그램들은 간단한 모델을 만든 뒤, 프로그램 구현 후, 부족한 점이나 보완할 점을 수정하며 복잡한 시스템으로 발전시키는 방식으로 만드므로, 프로그램 수정은 아주 중요하다.도메인 분석 단계의 모델을 수정 후, 다른 단계들 또한 다시 수정하면 된다.  ex) 고양이 SPEED 조정 기능 추가 -&gt; Constant SPEED 추가 후, 위시리스트 재구성 뒤 함수 구현"
  }
  , 
  
  "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80-%ED%95%A8%EC%88%98%20%EC%84%A4%EA%B3%84.html": {
    title: "어떻게 코딩할 것인가-함수 설계",
    date: " Jan 4, 2023 ",
    url: "/articles/computer_science/OSSU/HowToCode/%EC%96%B4%EB%96%BB%EA%B2%8C%20%EC%BD%94%EB%94%A9%ED%95%A0%20%EA%B2%83%EC%9D%B8%EA%B0%80-%ED%95%A8%EC%88%98%20%EC%84%A4%EA%B3%84.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true함수 설계Edx 강의 How to Code 시리즈를 정리한 내용입니다.함수 설계 과정을 숙달하고 이해하면 어렵고 고차원의 함수를 구현할 때 큰 도움이 된다.함수 설계는 폭포수 모델로 구현되지 않았으며 애매모호하거나 틀릴 경우 이전 단계로 돌아가서 수정해도 된다.  물론 그렇다고 과정을 대충 생략하면 안된다. 각 단계는 다음 단계를 설계하는데 도움이 되게끔 설계되었다.가끔 단계의 순번을 바꾸는 방식이 유용할 때도 있지만 4번의 구현 단계는 절대로 처음으로 오면 안된다.  4번 함수 구현 자체가 이 설계의 목적이기 때문자주, 그리고 빨리 함수를 실행시켜 조기에 잘못을 찾아 고치는게 좋다. 여러 잘못이 쌓인다면 무엇이 잘못인지 모르게 되기 때문이다.title: 사소한 가독성 팁강의에서 강사님이 나는 별로 세세하게 따지지 않았었던 인덴트와 순서가 상관없는 데이터의 순서 통일 등을 통해 가독성을 올릴 수 있었다.# data 순서가 다음과 같다면{: #data-순서가-다음과-같다면}data = [3, 1, 2]# 테스트 순서도 굳이 다음과 같이 맞춰주기{: #테스트-순서도-굳이-다음과-같이-맞춰주기}print(fn(3)==3) print(fn(1)==1)print(fn(2)==2)함수 설계 과정함수 디자인에 앞서 그림이나 시나리오 등을 작성하며 테스트용, 그리고 함수 내에 사용할 목적의 상수 등을 미리 정의할 수 있다.title: 함수 디자인 예시# 문자열의 길이가 5보다 작은지 알아보는 함수를 만들자!{: #문자열의-길이가-5보다-작은지-알아보는-함수를-만들자}# 1. signature, purpose, stub 설계{: #1-signature-purpose-stub-설계}# signature: (string =&gt; boolean){: #signature-string-boolean}# purpose: 문자열을 입력받아 해당 문자열의 길이를 비교해 5보다 작은지(X) =&gt; 5 미만인지(O) 여부를 참, 거짓으로 알려주는 함수{: #purpose-문자열을-입력받아-해당-문자열의-길이를-비교해-5보다-작은지-x-5-미만인지-o-여부를-참-거짓으로-알려주는-함수}# stub : 테스트를 위한 임시 함수{: #stub-테스트를-위한-임시-함수}def isLessThan5_stub(string):\treturn True\t# 2. 테스트 설계{: #2-테스트-설계}print(isLessThan5_stub(\"\") == True) # 경계값 확인print(isLessThan5_stub(\"stub\") == True) # True 반환 커버리지print(isLessThan5_stub(\"apple\") == False) # 경계 조건print(isLessThan5_stub(\"computer\") == False) # False 반환 커버리지# 3. 함수 템플릿 설계{: #3-함수-템플릿-설계}# def isLessThan5(string):{: #def-islessthan5-string}# ...string {: #string}# ...= 무언가를 할거다는 의미# 4. 함수 설계{: #4-함수-설계}def isLessThan5(string):\treturn len(string) &lt; 5# 5. 테스트 결과 확인{: #5-테스트-결과-확인}print(isLessThan5(\"\") == True) # 통과print(isLessThan5(\"stub\") == True) # 통과print(isLessThan5(\"apple\") == False) # 통과print(isLessThan5(\"computer\") == False) # 통과# 만약 틀렸을 경우, 위 1~4 과정 중 애매모호하거나 잘못된 부분 확인하기{: #만약-틀렸을-경우-위-1~4-과정-중-애매모호하거나-잘못된-부분-확인하기}1. 시그니처, 목적, 스텁, 생성      시그니처 : 인풋과 아웃풋에 대한 정보, 특히 데이터 타입,      - 자세해야 한다. ex) Number (X) Float (O)      - ex) 두 수 더하기 함수의 시그니처 (Integer Integer -&gt; Integer)        목적은 해당 함수의 사용하는 목적과 입력값과 결과값의 의미          단순 입력과 출력이 아니라 각각 데이터의 의미가 들어가면 좋다. 애매모호한 기준과 단어를 쓰면 안됨.      다른 함수와 비교해 비슷한 일을 하는지 비교해보고, 비슷하다면 다른 함수를 이용할 수 없는지 알아보자.      title: 목적의 예시ex) 두 수가 주어지고 두 수를 더한 값이 출력 (X)ex) 두 사람의 사과 갯수가 들어가 두 사람이 가진 사과들의 총합을 출력 (O) =&gt; 데이터의 의미ex) 해당 수가 큰 값인지 여부를 출력 (X)ex) 해당 수가 100보다 큰 수인지 여부를 출력 (O) =&gt; 기준이 확실  스텁 : 앞으로 작성할 테스트들을 오류없이 돌려보기 위해 임시로 적당한 결과를 내놓는 함수          입력값과 출력값의 데이터 타입만 맞추고 실제 값은 아무값이나 주어지면 된다.      시그니처와 맞는 타입, 네이밍 컨벤션 지키기, 좋은 함수명 중요      ex) def add(a, b): return 0      2. 테스트 커버리지와 입력 값들을 고려한 테스트 실행 코드 작성  테스트의 수는 테스트 커버리지와 각기 다른 입력값들의 구간 대표값과 경계값을 모두 고려할 수 있을 만큼 해야한다.          코드 커버리지: 테스트가 함수 내부의 코드의 일부를 확인하는 정도                  주로 분기문처럼 조건에 따라 동작하지 않을 수 있는 코드가 있는 경우, 테스트의 수를 고려해야 한다.          ex)  (a &gt; 3) if (a &gt; 0) else (a &lt; -2)의 경우, -3, -1 , 1, 4의 a값을 넣을 경우 모든 부분을 커버한다.          이때 a가 가질 수 있는 각 구간의 값들 -3, -1, 1, 4를 구간 대표값이라고 한다.                    경계 값 또는 경계 조건 (boundary condition): 각 구간 사이 기준에 해당하거나 특수한 값(0, -1, 1) 등 성질이 바뀌는 값들을 의미                  주어진 두 사각형 중, 넓은 사각형을 돌려주는 함수, 그렇다면 두 넓이가 같다면?          실수로 고려하지 못하는 경우가 많으며 새로 발견되면, 1번 과정도 수정해야 되는 경우가 많음                    title: 테스트 커버리지 고려 예시첫번째 이미지가 두번째 이미지보다 길이와 너비 둘다 큰가? 함수의 경우(너비 : 더 작은 경우, 같은 경우, 큰 경우) $\\times$ (높이 : 더 작은 경우, 같은 경우, 큰 경우)가 필요하므로 경계값과 대표값을 포함해 최소 9개의 테스트가 있어야 한다.  테스트를 통해 함수의 목적, 필요한 상수와 데이터, 함수에 필요한 인자 등을 미리 깨달을 수 있게 하며, 이를 통해 함수 설계를 쉽게 한다.3. 템플릿 만들기  네이밍 컨벤션과 주어진 파라미터 지키기, 보통 ...(파라미터들의 나열로 표현)  함수들을 구현할 때 템플릿을 복사하여 구현한다.  템플릿을 완성하면 1번의  stub은 주석 처리해야 한다.          ex) def add(a, b): ...a ...b        함수의 템플릿은 데이터 설계의 정보를 기반으로 만들면 쉽다.title: 데이터 설계와 함수 템플릿의 상관 관계  보다시피 데이터 정의 과정의 참조과정을 통해 함수의 구조, 도움 함수(helper function) 등의 필요성과 위치 등을 짐작할 수 있으며, 이를 통해 함수를 쉽게 구현할 수 있다.만약, 여러 타입의 인자가 주어진다면, 데이터 타입에 따라 템플릿을 변형해야 한다.예를 들어, 두개의 compound type이 주어지면 2개의 중첩된 조건문의 템플릿이 나타날 수 있다.아래 예시는 compound type의 템플릿 조건 안에 atomic distinct type의 템플릿이 추가로 결합되었다.title: 재귀구조와 empty를 가진 compound type t와 atomic distinct type k를 인자로 받는 함수 템플릿def lookup-key(t k):\tif len(t) == 0:\t\t# ...k\t\tpass\telse:        # (...k (node-key t)    ;Integer        #     (node-val t)    ;String\t    # (lookup-key (node-l t) (...k))        # (lookup-key (node-r t) (...k))        pass4. 함수 구현하기  군더더기 없이 오류없이 잘 구현하였는가?          군더더기는 주로 사용하지 않는 변수, 쓸모 없는 코드, 쓸모 없는 주석을 의미        도움 함수(helper function) : 함수 내부의 복잡한 로직을 추가로 구현하는 함수, 보통 데이터 정의 과정에서 들어난다.          위의 데이터 설계와 함수 템플릿의 상관 관계 그림의 fn-for-school 함수 같은 함수를 의미      5. 테스트와 디버깅하기  고려할 사항들          모든 테스트가 예상 결과값을 통과했는가?      함수 내 모든 코드들을 테스트하는가?      목적에 맞는 결과를 내놓는가?        만약, 오류가 생기면, 테스트가 잘못됬는지, 함수 구현이 잘못되었는지 꼭 확인하자!  빠르게, 자주 테스트하여 잘못을 조기에 발견하자!이후 함수를 완성하면  함수의 설명 부분을 다시 작성해보자. 여러 로직이 겹치는 경우 보조함수로 바꿔야 한다.  시그니처와 스텁은 더이상 의미없으므로 지워도 된다.  테스트는 경우에 따라 재활용하거나 후에 디버깅 원인 분선시 필요할 수 있으므로 옮기거나 잘 정리하자복합 함수 설계함수 내부에 또 다른 함수를 포함한 함수를 설계하기 위한 방법 소개재귀 함수 설계자기 자신 함수를 함수 로직 내에 이용하는 함수title: 재귀 함수 구현 시 유의점  데이터 설계 시 재귀 구조가 나타난다면, 함수 구현 시에도 재귀 구조가 나타난다.  기필코 기저 사례(Base Case)가 존재해야 하며, 도달 가능해야 한다.          그렇지 않으면 무한 루프가 일어난다.        로직 구현시, 테스트 생성 및 구현 시 기저 사례를 맨 처음 처리할 것.          구현이 간단한 경우가 많고, 재귀 함수 구현을 위해 꼭 필요하므로 빠르게 디버깅 하기 위함        재귀 사례는 결국 기저 사례로 향하는지 알기 위해, 내부 재귀함수의 인자가 기저 사례로 수렴하도록 변경되는지 확인해야 함.  재귀 함수의 결과값이 항상 특정 조건을 만족한다는 가정을 충족해야 한다.(Invariant) 아래 예시 참조  보조 함수와 다르게 새로 테스트를 작성할 필요가 없다.title: 재귀 함수와 재귀 데이터 설계 예시 : 짝수 필터링사실, 많은 경우 재귀로 처리하는 것보다 반복문으로 처리하는 경우가 성능 상으로 좋은 경우가 많다.# Data definition:{: #data-definition}# ListOfNumber is one of the{: #listofnumber-is-one-of-the}# - empty{: #empty}# - [Integer] + ListOfNumber {: #integer-listofnumber}# 데이터 설계 중 재귀 구조 발견# interp. an arbitrary number of Numbers{: #interp-an-arbitrary-number-of-numbers}def fnForLon(lon):\tif len(lon)==0:\t\t#...\t\tpass\telse:\t\t#...lon[0]\t\t#...fnForLon(lon[1:])\t\tpass# Function:{: #function}# ListOfNumber -&gt; ListOfNumber{: #listofnumber-listofnumber}# filter even Number from ListOfNumber.{: #filter-even-number-from-listofnumber}# Invariants : All the results of evenFilter should contain only odd number.{: #invariants-all-the-results-of-evenfilter-should-contain-only-odd-number}# Stub{: #stub}# def evenFilter(lon):{: #def-evenfilter-lon}#\treturn []\tprint(evenFilter([0])==[])print(evenFilter([4,3,2,1])==[3, 1])# &lt;Template From fnForLon&gt;{: #template-from-fnforlon}def evenFilter(lon):\tif len(lon)==0:\t\treturn []\telse:\t\treturn ([lon[0]] if lon[0]%2 else []) + evenFilter(lon[1:]) # 이후 evenFilter는 모두 홀수만 포함된다는 가정을 충족해야 한다. (Invariant)불변성(Invariants)복합 함수의 경우 어떤 함수의 결과물이 항상 특정 조건을 만족한다는 가정이 충족해야 할 필요가 있다.이를 불변성이라고 하며, 이를 엄격히 지키는지 확인하기 위해 로직을 수학적으로 증명하고, 테스트를 세세하게 짜내어야 한다.불변성이 깨지면 가정이 깨지므로 원하는 결과가 나오지 않는다.불변성은 재귀나 복합 함수 뿐만 아니라 반복문의 반복 과정에서도 사용된다.예시로 동적 계획법의 점화식이나 이중 탐색 트리의 좌측 자식은 언제나 현재 노드의 값보다 작고, 우측 자식은 언제나 현재 노드의 값보다 큼 등이 있다.보조 함수 설계보조 함수(helper function)는 자기 자신이 아닌 다른 함수로, 복잡한 함수의 일부 로직을 담당하는 함수이다.다음과 같은 경우에 보조 함수를 설계해야 한다.  함수 설계 중 설계 내용이 새로운 지식 도메인으로 바뀜(즉, 두가지 이상의 일을 함)  데이터 설계 중 재귀 구조가 발견됨  함수 해석 부분 구현 시, 내용이 길고 복잡함title: 의미없는 보조함수의 추가는 오히려 구현을 복잡하게 만들 수 있다.보조 함수 설계시, 각 보조 함수의 기능과 데이터를 따로 설계하고, 테스트 해야 함.  이때, 복잡한 구조일 수록 내부의 보조 함수를 염두에 두고 최대한의 커버지리가 가능한 테스트를 설계해야 한다.  하지만, 반대로 보조 함수는 자신을 참조하는 부모 함수를 염두에 두지 않고 자신의 기능만 신경쓰면 된다.보조 함수는 자신을 위한 또 다른 보조 함수를 가질 수 있다.title: 보조 함수 의존 관계아래 그림처럼 특정 함수의 테스트와 설계의 끝은 해당 함수 내부에서 의존하는 보조 함수들의 설계와 테스트가 전부 끝나야 가능하다.따라서, 각 보조 함수를 일단 위시리스트 항목으로 만들어 두고 상위 함수를 먼저 구현할 수 있다.title: 위시리스트 항목(wish-list entry)?시그니처, 목적, !!!와 스텁으로 이루어진 완성되지 못한 함수스텁과 다른 점은 처음에는 스텁처럼 생겼지만 언젠가는 추가로 테스트와 내부 로직을 제대로 구현한다는 점이다.!!!, TODO:등을 통해 나중에 꼭 구현하도록 표시해둔다.# Number -&gt; Number{: #number-number}# 고양이를 우측으로 1 픽셀 옮긴 좌표를 출력{: #고양이를-우측으로-1-픽셀-옮긴-좌표를-출력}# !!! or TODO:{: #or-todo}def advanceCat(c):\treturn 0\ttitle: 구현 완료된 위시 리스트 항목# 구현 완료된 위시 리스트 항목{: #구현-완료된-위시-리스트-항목}# CAT -&gt; CAT{: #cat-cat}# produce the next cat, by advancing it 1 pixel to right{: #produce-the-next-cat-by-advancing-it-1-pixel-to-right}# Tests:{: #tests}print(advanceCat(3)==4)# def advanceCat(c): return 1 {: #def-advancecat-c-return-1}# stub# &lt;use template from Cat&gt;{: #use-template-from-cat}def advanceCat(c):\treturn c+1# CAT -&gt; Image{: #cat-image}# render the cat image at appropriate place on MTS{: #render-the-cat-image-at-appropriate-place-on-mts}print(render(4)==Image(\"cat\", 4, CTR_Y, MTS))# def render(c): return MTS {: #def-render-c-return-mts}# stub# &lt;use template from Cat&gt;{: #use-template-from-cat}def render(c):\treturn Image(\"cat\", c, CTR_Y, MTS)"
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8.html": {
    title: "Spring5 입문",
    date: " Jan 4, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8.html",
    tags: ["WEB","SPRING","BE","SUMMARY"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.스프링이란?스프링은 자바 웹 어플리케이션 개발 프레임워크로, JSP, MyBatis, JPA 등 다양한 기술과 함께 전자정부 표준 프레임워크, 포털, 쇼핑, 금융 등 많은 서비스에 이용된다.Spring 특징  의존성 주입(Dependency Injection: DI) 지원  AOP(Aspect-Oriented Programming) 지원  MVC 웹 프레임워크 제공  JDBC, JPA 연동, 선언적 트랜잭션 처리 등 DB 연동 지원스프링 개념Spring5 입문-스프링 설정과 의존 관리 툴Spring5 입문-의존성 주입(DI)  Spring5 입문-의존 자동 주입    Spring5 입문-빈(Bean)과 컨테이너    Spring5 입문-컴포넌트 스캔(Component Scan)    Spring5 입문-AOP    Spring5 입문-DB 연동  MVC 패턴을 이용한 웹 개발Spring5 입문-MVC 개념과 설정Spring5 입문-컨트롤러와 뷰 구현웹 어플리케이션"
  }
  , 
  
  "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20Sass.html": {
    title: "Subbrain 개발기 - Sass",
    date: " Jan 5, 2023 ",
    url: "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20Sass.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSubbrain 개발기 - Sass개요Sass(Syntatically Awesome Stylesheet)는 CSS의 확장 언어로, 함수, 변수, 오버라이딩 등을 지원하여 더욱 효율적인 CSS 개발을 가능케 한다.Jekyll의 Dependency로 Ruby Sass가 자동으로 설치되어 바로 사용가능하므로, 경험해보는 겸에 사용해보았다.엄밀히 말하면 .scss와 .sass로 나뉘며, 그중 기본적으로 사용되고 더욱 많이 사용된다는 .scss를 사용했다.  .scss는 좀더 원본 CSS와 비슷하게 문법에 ;와 {}을 사용하며 .sass는 인덴트로 구분한다.개발 세부파일 구조title: _sass 폴더 구조Sass의 최초 구성은 Jekyll에서 제공하는 템플릿 테마인 minima에서 가져왔다.이후, 이를 바탕으로 내가 추가적인 .scss 파일 작성 및 구조 재구성을 통해서 현재의 프로젝트 구조가 되었다.  /common/ 폴더는 사이트 전체에서 사용하는 공통적인 컴포넌트들의 CSS 파일들이다. 검색바, 리다이렉션 메시지 등이 존재한다.  /obsidian/ 폴더는 마크다운 포스트 페이지에 사용되는 컴포넌트들의 CSS 파일들이다. 이미지, 콜아웃, 링크 워닝들이 존재한다.  /theme/ 폴더는 코드 강조, 전체적인 UI 요소 CSS, 기본값 CSS 파일 등 사이트 전체에서 존재해야 하는 스타일들이 존재한다.  theme.scss 파일은 다른 파일들이 공통적으로 사용할 변수와 @mixin을 정의해 놓았다.  main.scss 파일은 /theme/ 폴더 내의 파일들을 하나로 묶어 헤더에 등록한다.회고사실 Sass에 대해 경험과 학습 시간이 전혀 없었지만, 사용법이 직관적이여서 금방 사용할 수 있었다.&amp; 문법의 경우, 개인적으로 그닥 애용하지 않는 편이 좋은 것 같다. 자식 컴포넌트와 헷갈려 가독성이 나빠지는것 같다.살짝 의구심이 드는 구조로 @import 구문을 이용해 중앙에서 선언한 상수와 함수를 불러와 사용하면, 작성시 편리하고 가독성이 좋았지만, 실제 CSS로 컴파일 되면서 여러 중복 코드가 css 파일 내에 생성되었다. 이럴 경우 웹 컨텐츠 요청 속도에 영향이 가진 않을까 생각이 들고, 이를 개선할 구조가 있는지 알아봐야겠다.반응형 웹을 위한 CSS와 호환성을 위한 CSS 등을 따로 다른 파일 등으로 나뉠 수 있게 구성하거나 mixin, extend로 쉽게 구현할 수 있으면 좋을 것 같다.Sass를 사용하면서 생기는 대부분의 문제는 Sass가 아니라 CSS에 대한 지식이 적어서 생기는 경우가 많았다. CSS도 따로 공부할 수 있으면 좋겠다.확실히 기존의 CSS 보다 코드가 줄어들고 가독성 좋게 변한다. 앞으로도 계속 사용할 것 같다."
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%8A%A4%ED%94%84%EB%A7%81%20%EC%84%A4%EC%A0%95%EA%B3%BC%20%EC%9D%98%EC%A1%B4%20%EA%B4%80%EB%A6%AC%20%ED%88%B4.html": {
    title: "스프링 설정과 의존 관리 툴",
    date: " Jan 6, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%8A%A4%ED%94%84%EB%A7%81%20%EC%84%A4%EC%A0%95%EA%B3%BC%20%EC%9D%98%EC%A1%B4%20%EA%B4%80%EB%A6%AC%20%ED%88%B4.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true스프링 설정과 의존 관리 툴title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.스프링5 설정 순서  JDK 설치 및 환경 변수 설정  의존 관리 툴(메이븐, 그래이들) 설치 및 환경 변수 설정  IDE 이클립스 설치  스프링 의존과 의존 관리 툴 플러그인 설치의존 관리 툴 : 메이븐(Maven)메이븐은 프로젝트 관리 및 빌드 툴이다.메이븐을 설치하고 스프링 프로젝트를 만드려면 다음과 같은 순서를 따르자.프로젝트 폴더 생성 및 pom.xml 설정POM 파일은 크게 다음과 같은 정보를 포함하고 있다.  프로젝트 정보: 프로젝트의 이름, 개발자의 목록, 라이센스 등의 정보 기술  빌드 설정: 소스, 리소스, 라이프 사이클별 실행할 플러그인 등 빌드와 관련된 설정 기술  빌드 환경: 사용자 환경별로 달라질 수 있는 프로파일 정보를 기술  POM 연관 정보: 의존 모듈, 상위 프로젝트, 포함하고 있는 하위 모듈 기술title: 프로젝트 폴더 설정과 pom.xml 예시  프로젝트명/src/main/java폴더를 만든 뒤, cd 프로젝트명/ 로 작업 영역을 옮긴다.  프로젝트 폴더 내에서 아래 pom.xml을 복사하거나 mvn archetype:generate을 이용해 가벼운 설정 이후 생성할 수 있다.&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;회사명&lt;/groupId&gt;    &lt;artifactId&gt;프로젝트ID&lt;/artifactId&gt;    &lt;version&gt;버전 정보(ex) 0.0.1-SNAPSHOT)&lt;/version&gt;    &lt;packaging&gt;패키징 방법(ex)jar)&lt;/packaging&gt;    &lt;name&gt;프로젝트명&lt;/name&gt;\t&lt;description&gt;프로젝트 설명&lt;/description&gt;    &lt;properties&gt;        &lt;java.version&gt;자바 버전 ex)17.0&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;             &lt;artifactId&gt;spring-context&lt;/artifactId&gt;            &lt;version&gt;의존 버전 (ex)6.0.3)&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;&lt;!--'의존 &lt;scope&gt;'에 설명--&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;                &lt;!--&lt;configuration&gt;\t                플러그인에 필요한 설정                &lt;/configuration&gt;--&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;이후 mvn compile을 통해 실행 결과를 확인 해보자.추가로 의존 모듈을 추가하고 싶으면 search.maven.org에서 검색하여 추가할 수 있다.의존 &lt;scope&gt;의 예시  compile: 컴파일 시 필요한 모듈, 테스트 및 런타임에도 클래스패스에 모듈 포함, 기본값  runtime: 런타임에만 필요, 배포시에도 포함됨 (ex) JDBC 드라이버)  provided: 컴파일 시 필요하지만 실제 런타임 때 컨테이너 같은 것에서 기본으로 제공하므로 배포시 제외됨(ex) 서블릿)  test: 테스트 코드를 컴파일 시 필요, 테스트시에만 클래스패스에 포함, 배포시 제외 (ex) Mock)의존 모듈 및 플러그인 설치title: 의존 모듈과 플러그인의 차이  의존 모듈은 프로젝트에서 필요로 하는 jar 파일로, classpath에 추가된다  플러그인은 메이븐이 빌드하는 과정에 사용되는 jar파일로 classpath에 추가되지 않음          메이븐은 플러그인의 기능들을 이용해 빌드한다.      필요한 플러그인이나 의존 모듈들은 빌드, 컴파일, 테스트 등의 동작 시, 메이븐 중앙 리포지토리에서 다운로드 받은 뒤 로컬 리포지토리에 저장되어 사용된다.  메이븐 중앙 리포지토리 url : http://repo1.maven.org/maven2  로컬 리포지토리 저장 경로 : [USER_HOME]/.m2/repository/[groupId]/[artifactId]/[version]/[artifactId]-[version].jar이때, 메이븐이 자동으로 각 모듈과 플러그인들의 하위 의존들을 자동으로 설치해준다.\\  이를 의존 전이(Transitive Dependencies)라고 한다.메이븐 라이프사이클(Lifecycle)라이프사이클은 메이븐이 프로젝트를 빌드하는 과정을 의미하며, 각 과정의 일부까지만 명령어를 통해 실행할 수 있다.크게 clean, build, site의 세가지 라이프 사이클을 제공하며, 각 라이프 사이클은 추가로 단계(phase)로 나뉜다.  clean 라이프 사이클 : 빌드 시 생성되었던 산출물을 삭제          이전 산출물 : 업데이트 이전 리소스나 의존 모듈 등이 존재        build(default) 라이프 사이클 : default 값, 프로젝트 패키징 및 배포 절차, 핵심 절차          무려 도합 23개의 단계가 존재하며 일부 주요 단계만 설명한다.                  validate : 프로젝트 상태, 필요 정보 체크          compile : 소스 코드 컴파일 및 클래스 출력 폴더에 클래스 생성          test : 테스트 실행          package : 컴파일 코드와 리소스 파일들을 war, jar 등으로 패키지 수행          install : 패키지를 로컬 저장소에 설치          deploy : 생성 패키지를 원격 저장소에 배포                      site 라이프 사이클 : 프로젝트 문서화 절차          사이트 문서 자동 생성, 배포 등의 절차      라이프 사이클의 특정 단계 실행하려면 다음과 같이 실행하며, 앞선 모든 단계들을 실행한다.mvn testmvn deploymvn surefire:test # 특정 플러그인의 단계만 실행하기, 앞선 단계 실행 X  각 플러그인의 목적 혹은 기능을 골(goal)이라고 표현한다.그래이들(Gradle)XML 대신 Groovy 스크립트를 이용한 동적인 빌드를 지원하며, 스프링과 같은 의존성 주입 방식으로 모듈을 설치하며, 전체적으로 성능은 더욱 좋지만, 아직까진 메이븐이 더 잘 사용된다.title: 프로젝트폴더/build.gradle 파일 예시apply plugin: 'java' // 자바 플러그인 사용sourceCompatibility = 17.0 // 현재 프로젝트 자바 버전targetCompatibility = 17.0 // 디플로이 시 자바 버전compileJava.options.encoding = \"UTF-8\" // 인코딩repositories {\tmavenCentral() // 메이븐 중앙 레포지토리에서 의존 설치}dependencies {\timplementation 'org.springframework:spring-context:6.0.3'} // spring-context 6.0.3버전 의존을 컴파일 시 설치wrapper { // gradle이 없어도 의존을 설치할 수 있게 gradlew 생성\tgradleVersion = '7.6' // 현재 gradle 버전}  위 예시 파일을 만든 후 gradle wrapper 명령어를 실행하면 gradlew 파일이 생성되어 gradle을 설치하지 않아도 명령어를 실행할 수 있다.          ex) gradlew compileJava      "
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%9D%98%EC%A1%B4%EC%84%B1%20%EC%A3%BC%EC%9E%85(DI).html": {
    title: "Spring5 입문-의존성 주입(DI)",
    date: " Jan 6, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%9D%98%EC%A1%B4%EC%84%B1%20%EC%A3%BC%EC%9E%85(DI).html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-의존성 주입(DI)title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.스프링은 다양한 객체에 대한 의존성 주입을 지원하는 조립기의 역할을 하는 객체 컨테이너다.  아래 내용을 참고해 각 용어에 대한 이해를 하고 위 문장을 다시 이해해보자!의존(Dependency)의존성 주입을 통해 의존에 의해 생기는 문제점들을 해결할 수 있다.  의존하고 있는 객체 코드 변경 시, 의존성 주입을 담당 객체 코드만 바꾸면 되므로 수정 최소화  객체를 싱글톤으로 하나만 이용하여 자원 사용을 최소화      객체를 중앙 관리하여 무결성 등의 문제가 줄어듦    의존(Dependency) : 변경에 의해 영향을 받는 관계 (ex) 상속 관계, 특정 객체의 메소드, 변수를 사용하는 다른 객체 관계)          예를 들어 부모 클래스는 자식 클래스의 변경에 영향을 받지 않지만, 자식 클래스는 부모 클래스에 변경에 영향을 받으므로 자식은 부모에 의존하는 관계이다.        의존성 주입(Dependency Injection) : 어떤 객체가 의존하는 객체를 직접 생성하는 대신 의존 객체를 대신 생성한 객체로 부터 전달받는 방식          직접 의존성 주입을 코딩하지 않고 스프링에서 자동으로 지원한다. Spring5 입문-의존 자동 주입        조립기(assembler) : 의존 객체를 생성 혹은 검색한 뒤 전달해주는 클래스, 두 객체를 조립하는 것처럼 보여 조립기라고 함.title: 간단한 의존성 주입과 조립기 예시Assembler assembler = new Assembler(); // 조립기MemberDao memberDao = assembler.getMemberDao(); // 객체 주입을 위한 생성 혹은 기존 객체 가져오기(싱글톤 패턴)ChangePwService pwSvc = new ChangePwService();  // new ChangePwService(memberDao) 생성자 방식 객체 주입pwdSvc.setMemberDao(memberDao;// pwSvc에 setter 방식 객체 주입설정 객체title:  스프링 설정 클래스 생성 예시(src/main/java/config/AppCtx.java)package config;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;import spring.ChangePasswordService;import spring.MemberDao;@Import({AppConf2.class, AppConf3.class}) // 다른 설정 클래스 정보를 가져와 합침@Configuration // 스프링 설정 클래스 임을 표시public class AppCtx {\t\tprivate Something something = new Something(); // 빈 아닌 객체도 존재 가능, 단, 의존성 주입 및 관리, 검색 기능을 사용할 수 없다.\t\t@Autowired\tprivate MemberPrinter printer; // @Autowired 어노테이션을 통한 자동 주입 \t// 아래 memberDao 처럼 메서드를 정의해줄 필요 없다.\t\t@Bean// 스프링 빈 생성을 위한 메서드임을 표시\tpublic MemberDao memberDao() {\t\treturn new MemberDao();\t}\t@Bean\tpublic ChangePasswordService changePwdSvc() {\t\tChangePasswordService pwdSvc = new ChangePasswordService();\t\tpwdSvc.setMemberDao(memberDao()); // 설정 클래스 내부의 다른 객체를 사용할 때 평범하게 생성자를 이용하면 됨. (이유 아래 설명)\t\treturn pwdSvc;\t}}  설정 클래스 : 스프링 컨테이너 생성을 위한 설정과 빈 객체 정보를 포함한 클래스          코드가 길 경우 여러 클래스로 나눌 수 있다. 위와 같이 @Import를 사용하거나 스프링 컨테이너 선언 시 여러 설정 클래스를 지정해주면 됨.        @Configuration: 설정 클래스 지정 어노테이션, 설정을 통해 생성 객체와 의존 주입 대상을 정함.엄밀히 말하면 설정 클래스 내부의 객체들을 싱글톤 패턴으로 바꾸기 위해 위 설정 클래스를 상속한 새로운 클래스를 만들어 사용한다.이를 통해 평범한 생성자를 구현했던 등록한 객체들 또한 싱글톤 패턴이 보장되는 빈 객체로 바꾼다."
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%9D%98%EC%A1%B4%20%EC%9E%90%EB%8F%99%20%EC%A3%BC%EC%9E%85.html": {
    title: "Spring5 입문-의존 자동 주입",
    date: " Jan 9, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%9D%98%EC%A1%B4%20%EC%9E%90%EB%8F%99%20%EC%A3%BC%EC%9E%85.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-의존 자동 주입title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.@Autowired@Autowired는 스프링이 해당 타입에 일치하는 @Bean 객체를 찾아 주입해주는 어노테이션이다.  같은 클래스나 심지어 자식 클래스를 내놓는 다른 메소드가 두개 이상일 경우에도 오류가 난다. 이럴 때는 @Qualifier를 통해 선택하면 된다.  만약, @Autowired와 수동 할당을 동시에 사용하면 @Autowired가 우선으로 덮어씌운다.title: @Autowired의 사용례import org.springframework.beans.factory.annotation.Autowired;public class ChangePasswordService {\t@Autowired // ChangePasswordService 인스턴스 생성시 스프링이 직접 주입\tprivate MemberDao memberDao;\t\t//...\t// @Autowired에 의해 더 이상 쓸모없어진 메서드\t// public void setMemberDao(MemberDao memberDao) {\t//\tthis.memberDao = memberDao;\t// }}title: @Autowired의 다른 사용례아래와 같이 세터 메서드에 대신 사용해도 된다.import org.springframework.beans.factory.annotation.Autowired;public class MemberInfoPrinter {\tprivate MemberDao memDao;\t@Autowired\tprivate MemberPrinter printer;\t//@Autowired에 의해 더이상 생성자에 객체를 넘겨줄 필요가 없음\tpublic MemberListPrinter(\t\t//MemberDao memberDao, MemberPrinter printer\t\t) {\t\t\t//this.memberDao = memberDao;\t\t//this.printer = printer;\t\t}\t@Autowired\tpublic void setMemberDao(MemberDao memberDao) {\t\tthis.memDao = memberDao;\t}\t\t// public void setPrinter(MemberPrinter printer) {\t// \tthis.printer = printer;\t// }}일치하는 클래스의 빈 객체가 설정 클래스 내에 없으면 오류가 난다. 만약 의도적으로 빈 객체를 지정하지 않아도 되게 만드려면 다음과 같이 3개의 방법이 있다.title: Nullable 빈 객체가 필요한 경우 예시private DateTimeFormatter dateTimeFormatter;public void print(Member member) {\tif (dateTimeFormatter == null) { // 빈 객체가 없어도 되는 로직\t\t//..\t} else { // 빈 객체가 있어야 하는 로직\t\t//...\t}}@Autowired // autowired에 의해 오류 발생public void setDateFormatter(DateTimeFormatter dateTimeFormatter) {\tthis.dateTimeFormatter = dateTimeFormatter;}title: @Autowired(required=false) 방법다른 방법들과 다르게 해당하는 빈 객체가 존재하지 않다면 세터 메서드가 실행되지 않기 때문에 아래와 같이 기본 생성자를 통해 기본 값을 주고 싶은 경우 사용하면 좋다.@Autowired(require=false)private DateTimeFormatter dateTimeFormatter; // 혹은@Autowired(require=false)public void setDateFormatter(DateTimeFormatter dateTimeFormatter) {\tthis.dateTimeFormatter = dateTimeFormatter; }public MemberPrinter() {\tdateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyy년 MM월 dd일\");}title:  Java8 이상에서Optional 방법Optional 객체가 대신 전달되며 이를 이용할 수 있음.@Autowiredpublic void setDateFormatter(Optional&lt;DateTimeFormatter&gt; formatterOpt) {\tif (formatterOpt.isPresent()){\t\tthis.dateTimeFormatter = formatterOpt.get(); \t} else {\t\tthis.dateTimeFormatter = null;\t}}// 혹은@Autowiredprivate Optional&lt;DateTimeFormatter&gt; formatterOpt;title: @Nullable 방법단, 이 방법은 다른 방법 require=false와 다르게 setDateFormatter 메서드가 호출이 된다.따라서 해당 변수에는 null 값이 무조건 들어가게 된다.import org.springframework.lang.Nullable;@Autowiredpublic void setDateFormatter(@Nullable DateTimeFormatter dateTimeFormatter) {\tthis.dateTimeFormatter = dateTimeFormatter; }@Autowired@Nullableprivate DateTimeFormatter dateTimeFormatter; 빈 객체 자동 주입은 가장 자주 사용하는 @Autowired 이외에도 @Resource, @Inject등이 존재한다.@Qualifier동일 클래스 혹은 자식 클래스 등 할당 가능한 여러 빈 객체 중 자동 주입 대상 빈을 한정하는데 사용하는 어노테이션title: @Qualifier 사용법 1:  @Bean에서 명칭 설정import org.springframework.context.annotation.Bean;import org.springframework.beans.factory.annotation.Qualifier;@Bean@Qualifier(\"printer\")public MemberPrinter memberPrinter1() {\treturn new MemberPrinter();}@Beanpublic MemberSummaryPrinter memberPrinter2() { // MemberPrinter를 상속받은 객체\treturn new MemberSummaryPrinter();}title: @Qualifier 사용법 2: @Autowired에서 명칭으로 지정만약 사용하려는 빈 객체의 @Qualifier가 없다면 해당 메소드명을 한정자로 사용할 수 있다.import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;@Autowired@Qualifier(\"printer\")public void setMemberPrinter(MemberPrinter printer) {\tthis.printer = printer;}@Autowired @Qualifier(\"memberPrinter2\") // 자식 객체인 MemberSummaryPrinter 객체가 주입됨public void setMemberPrinter(MemberPrinter printer) { \tthis.printer = printer; }"
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EB%B9%88(Bean)%EA%B3%BC%20%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88.html": {
    title: "Spring5 입문-빈(Bean)과 컨테이너",
    date: " Jan 9, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EB%B9%88(Bean)%EA%B3%BC%20%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true빈(Bean)과 컨테이너title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.컨테이너객체 컨테이너 : 스프링에서 빈(Bean) 객체의 생성, 초기화, 보관, 제거 등을 관리하는 ApplicationContext 인터페이스를 구현한 객체.title: ApplicationContext 인터페이스 클래스 그래프  BeanFactory는 최상위 인터페이스로 객체 생성과 검색에 대한 기능 정의  ApplicationContext 인터페이스는 메시지, 프로필/환경 변수 등을 처리  AbstractApplicationContext 클래스는 close()  메서드를 정의해 컨테이너 종료 가능  AnnotationConfigApplicationContext는 자바 어노테이션을 이용해 클래스로 부터 객체 설정 정보 가져옴  나머지는 xml, groovy 문법으로 가져온다.title: 스프링 컨테이너 선언// ...import org.springframework.context.annotation.AnnotationConfigApplicationContext;private static ApplicationContext ctx = new AnnotationConfigApplicationContext(AppCtx.class); // 스프링 컨테이너 초기화// AppCtx = 컨테이너 인스턴스, 설정 클래스가 여러개일 경우 (AppCtx.class, AppCtx2.class) 처럼 여러개 지정 가능 MemberRegisterService regSvc = ctx.getBean(\"memberRegSvc\", MemberRegisterService.class); // 컨테이너의 객체 생성 혹은 검색ctx.close();// 컨테이너 종료스프링 컨테이너는 초기화와 종료에 의해 다음과 같이 빈 객체의 라이프 사이클을 관리한다.  컨테이너 초기화 -&gt; 빈 객체 생성, 의존 주입, 초기화  컨테이너 종료 -&gt; 빈 객체 소멸빈(Bean)  빈(Bean) 객체 : 스프링이 생성, 관리, 검색 가능한 객체, 빈 객체를 생성하는 메소드명으로 구분하며 검색하여 단 하나의 인스턴스만 생성한다.    빈 등록 방법    빈 객체를 등록하는 방법은 직접 설정 클래스 내부 선언을 통한 수동 등록과 컴포넌트 스캔 두가지가 있다.  title: 설정 클래스 내부에서 빈 객체를 생성하는 메서드 선언import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import spring.MemberDao;public class AppCtx{\t@Autowired \tprivate MemberPrinter printer; // @Autowired 어노테이션을 통한 자동 주입 // 아래 memberDao 처럼 메서드를 정의해줄 필요 없다. \t@Bean // 스프링 빈 생성을 위한 메서드임을 표시 \tpublic MemberDao memberDao(){\t\treturn new MemberDao();\t}}  Spring5 입문-컴포넌트 스캔(Component Scan)    빈 생성 및 검색 방법    이후 알아볼 컨테이너 인스턴스를 통해 빈 객체를 생성 혹은 검색할 수 있다.    빈 객체 자동 주입은 가장 자주 사용하는 @Autowired 말고도 @Resource, @Inject등이 존재한다.  자세한 내용은 Spring5 입문-의존 자동 주입 참조title: .getBean() 메서드private static ApplicationContext ctx = new AnnotationConfigApplicationContext(AppCtx.class);VersionPrinter versionPrinter = ctx.getBean(\"versionPrinter\", VersionPrinter.class);.getBean() : 빈 객체를 생성하는 메소드명과 클래스를 인자로 빈 객체를 생성하거나, 이전에 생성했던 빈 객체를 가져온다.  만약 메서드 리턴과 클래스가 일치하지 않거나 오타가 있다면 오류가 난다.  만약 해당 클래스의 빈 객체 종류가 단 하나만 존재한다면, 클래스만 넣어도 된다.빈 객체의 라이프 사이클빈 객체는 스프링 컨테이너에 의해 관리되며 크게 생성, 의존 주입, 초기화, 소멸로 나뉜다.  생성 : 컨테이너 생성 직후 실행          빈 객체 싱글톤, 혹은 설정에 따라 프로토타입 패턴으로 생성      title: 빈 객체의 Scope 설정아래와 같이 @Scope를 통해 prototype 범위의 빈을 지정해 빈 객체를 구할 때마다 새로운 객체를 생성하게 할 수 있다.  단, prototype 빈은 컨테이너 종료 시에도 소멸 메서드를 실행하지 않고, 직접 소멸하고 소멸 관련 메서드를 구현해야 한다.import org.springframework.context.annotation.Scope;@Bean@Scope(\"prototype\") // 명시하지 않거나 @Scope(\"singleton\") 일 경우 오직 하나만 생성public Client client() {    Client client = new Client();    client.setHost(\"host\");    return client;}  의존 주입                  의존 자동 주입을 통한 의존 설정              초기화          구현한 초기화 메서드 실행      title: 초기화 메서드 구현 예제InitializingBean 인터페이스의 afterPropertieSet 메서드를 구현하면 실행되며, 주로 DB 커넥션 풀 연결 생성, 채팅 클라이언트 시작 등에 사용수동으로 설정 클래스 빈 생성 메서드 내에서 부를 수 있으나, 그 경우 2번 실행되게 된다.import org.springframework.beans.factory.InitializingBean;public class Client implements InitializingBean {    @Override    public void afterPropertiesSet() throws Exception {        System.out.println(\"Client.afterPropertiesSet() 실행\");    }\tpublic void connect() {\t\tSystem.out.println(\"Client.connect() 실행\");\t}}만약, 변경 불가능한 외부 모듈 클래스, 아니면 모종의 이유로 인터페이스를 사용하고 싶지 않으면 다음과 같이 @Bean 어노테이션의 속성을 사용하면 된다.title: @Bean 속성을 이용한 초기화 메서드 구현Client 클래스 내의 connect라는 메서드를 초기화 시 실행시키게 된다.Client 클래스 내 connect 함수 구현 필요, 단, 파라미터가 존재할 경우 에러 발생@Bean(initMethod = \"connect\")public Client client() {\t//...}  소멸 : 컨테이너 close() 메서드 실행 이후 실행          특정 메서드 실행      title: 소멸 메서드 구현 예제DisposableBean 인터페이스의 destroy 메서드를 구현하면 실행되며, 주로 DB 커넥션 풀 연결 종료, 채팅 클라이언트 종료 등에 사용import org.springframework.beans.factory.DisposableBean;public class Client implements DisposableBean {    @Override    public void destroy() throws Exception {        System.out.println(\"Client.destroy() 실행\");    }\t\tpublic void close() {\t\tSystem.out.println(\"Client.close() 실행\");\t}}마찬가지로, 변경 불가능한 외부 모듈 클래스, 모종의 이유로 인터페이스를 사용하고 싶지 않으면 다음과 같이 @Bean 어노테이션의 속성을 사용하면 된다.title: @Bean 속성을 이용한 소멸 메서드 구현Client 클래스 내의 close라는 메서드를 소멸 시 실행시키게 된다.Client 클래스 내 close 함수 구현 필요. 단, 파라미터가 존재할 경우 에러 발생@Bean(destroyMethod = \"close\")public Client client() {\t//...}"
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%BB%B4%ED%8F%AC%EB%84%8C%ED%8A%B8%20%EC%8A%A4%EC%BA%94(Component%20Scan).html": {
    title: "Spring5 입문 - 컴포넌트 스캔(Component Scan)",
    date: " Jan 9, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%BB%B4%ED%8F%AC%EB%84%8C%ED%8A%B8%20%EC%8A%A4%EC%BA%94(Component%20Scan).html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-컴포넌트 스캔(Component Scan)title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.@Component@Component가 붙어있는 클래스는 컴포넌트 스캔에 의해 빈(bean)으로 등록될 수 있다.title: @Component 사용례package spring;import org.springframework.stereotype.Component;@Component // memberDao 라는 이름으로 빈 객체 등록public class MemberDao{}@Component(\"infoPrinter\") // infoPrinter 라는 이름으로 빈 객체 등록public class MemberInfoPrinter{}컴포넌트들은 패키지 별로 같은 이름이 있으면 충돌하며, 수동 등록한 빈이 있으면 수동 등록이 우선수되므로 명시적으로 이름을 바꿔주거나 네이밍 규칙을 잘 선정하자.@ComponentScan앞서 @Component를 붙인 클래스들을 스캔하려면 기존의 설정 클래스를 이용하면 된다.title: @ComponentScan 사용례import org.springframework.context.annotation.ComponentScan;@Configuration // 스프링 설정 클래스 임을 표시@ComponentScan(basePackages = {\"spring\"} // spring 패키지 내의 @Component 탐색\texcludeFilters = {\t@Filter(type = FilterType.ASPECTJ, pattern = \"spring.*Dao\"),//Dao로 끝나는 클래스 타입은 스캔에서 제외, aspectjweaver 모듈 필요\t@Filter(type = FilterType.ASSIGNABLE_TYPE, classes=MemberDao.class)//MemberDao 타입과 그 자식 클래스들 제외\t})public class AppCtx {\t//...}@ComponentScan의 스캔에 제외하려면 excludeFilters 속성에 Regex, Aspectj 등의 필터를 사용할 수 있다.\t- FilterType.ANNOTATION 필터 타입을 이용하면 특정 애노테이션을 붙인 클래스도 제외 가능추가로 다음과 같은 어노테이션이 붙은 클래스들 또한 스캔된다.  @Controller  @Service  @Repository  @Aspect  @Configuration위 어노테이션들은 @Aspect를 제외하곤 실제로 @Component 기능을 상속받은 특수 애노테이션들이다."
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-AOP.html": {
    title: "Spring5 입문-AOP",
    date: " Jan 9, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-AOP.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-AOP(Aspect Oriented Programming)title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.AOP(Aspect Oriented Programming, 기능 지향 프로그래밍)은 여러 객체에 공통으로 적용할 수 있는 기능을 분리하여 재사용성을 높여주는 프로그래밍 기법이다.이때 핵심 기능과 공통 기능을 구분해 구현하는 방법은 프록시 패턴을 이용하며, 구현 방법에 따라 세 가지 방법으로 나뉜다.  컴파일 시점에 코드에 공통 기능 삽입 -&gt; AspectJ 같은 AOP 전용 도구로 구현  클래스 로딩 시점에 바이트 코드에 공통 기능 삽입 -&gt; AspectJ 같은 AOP 전용 도구로 구현  런타임에 프록시 객체 생성해 공통 기능 삽입 -&gt; 스프링의 자동 생성 프록시 객체로 구현title: 프록시 기반 AOP우린 이 중에 3번째 방법을 알아볼 것이다.프록시 패턴아래 예시 코드와 같이 핵심 기능의 실행은 다른 객체에 위임하고 부가적인 기능을 제공하는 객체를 프록시(Proxy)라고 하며, 실제 핵심 기능을 실행하는 객체는 대상 객체라고 한다.이를 통해 공통 기능 코드를 재사용하고 수정하기 쉬우며 가독성 좋은 코드를 만들어 낼 수 있다.title: 사실은, 저자가 생각하기에는 기능 추가와 확장에 초점이 맞춰져 있는 데코레이터 패턴이 좀 더 가깝다고 한다. 프록시 패턴은 좀 더 접근 제어와 보안에 관련됨프록시 패턴 예시title: 프록시 객체와 대상 객체 예시public interface Printer {\tpublic String print(String words);}public class TargetObject implements Printer{\t@Override\t\tpublic String print(String words) {\t\t\tSystem.out.printf(\"TargetObject.print()\");\t\treturn result;\t\t}}public class ProxyObject implements Printer {\t\tprivate Printer delegate;\t\tpublic ProxyObject(Printer delegate) {\t\tthis.delegate = delegate;\t}\t@Override\tpublic String print(String words) {\t\t\tSystem.out.printf(\"Before TargetObject.print()\");\t\t\tString result = delegate.print(words);\t\tSystem.out.printf(\"After TargetObject.print()\");\t\t\t\treturn result;\t}}title: 프록시 패턴 예시public class MainProxy {\tpublic static void main(String[] args) {\t\tProxyObject proxy = new ProxyObject(new TargetObject());\t\tSystem.out.println(\"%s\", proxy.print(\"result\"));\t}}/* console:Before TargetObject.print()TargetObject.print()After TargetObject.print()result*/AOP 구현스프링의 aspectjweaver 의존을 추가하면 AOP를 쉽게 구현할 수 있다.스프링 AOP는 프록시 객체를 자동으로 생성해 주며, 다음과 같은 용어가 존재한다.  Aspect : 공통으로 적용할 기능, 메서드, 예시로 트랜잭션, 보안, 캐시, 성능 모니터링 등이 있음  Advice : Aspect을 적용할 시점, 메서드 전(Before), 후(After), 정상 작동 후(After Returning), 오류 후(After Throwing) 등이 존재하지만 모든 시점에 자유롭게 적용가능한 Around Adivce를 가장 자주 사용한다.  Joinpoint: Advice를 적용 가능한 부분, 메서드, 필드 값 변경 등이 존재하지만 스프링은 메서드에만 적용 가능하다.  Pointcut: 해당 프록시 객체를 적용할 클래스, 빈 객체 등의 범위를 지정  Weaving: 이러한 AOP를 특정 객체에 설정해주는 행위를 의미Aspect 클래스 구현스프링 AOP에서 가장 자주 사용되는 Around Advice를 기준으로 다음과 같이 구현하면 된다.  Aspect로 사용할 클래스에 @Aspect 어노테이션 적용  @Pointcut 어노테이션으로 공통 기능을 적용할 객체를 지정  공통 기능 구현 메서드에 @Around 어노테이션 적용하여 대상 메서드 전후 실행에 관련한 로직 작성title: Aspect 클래스 구현package aspect; import java.util.Arrays;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.Signature;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.springframework.core.annotation.Order;@Aspect //프록시 객체 구현을 위한 Aspect 객체로 지정@Order(1) // 여러 AOP 적용시 메서드를 감쌀 우선순위, 낮을수록 먼저 감싼다.public class ExeTimeAspect {    @Pointcut(\"execution(public * chap07..*(..))\") // chap07 패키지 내의 모든 퍼블릭 메서드들이 대상    private void publicTarget() {    }    @Around(\"publicTarget()\") // 아래 Around Adivce 기능은 publicTarget에서 설정한 범위에만 적용된다.    public Object measure(ProceedingJoinPoint joinPoint) throws Throwable { // 공통 기능 구현 메서드, 리턴 타입은 모든 값을 아우를 수 있게 원시 타입인 Object        long start = System.nanoTime();        try {            Object result = joinPoint.proceed(); // 적용 대상 메서드 실행, ProceedingJoinPoint에 대해서 아래 쪽에 설명            return result;        } finally {            long finish = System.nanoTime();            Signature sig = joinPoint.getSignature();            System.out.printf(\"%s.%s(%s) 실행 시간 : %d ns\\n\",                    joinPoint.getTarget().getClass().getSimpleName(), // 적용 대상 메서드 시그니처 정보                    sig.getName(), Arrays.toString(joinPoint.getArgs()),                    (finish - start));        }    }}title: execution 명시자pointcut 메서드 설정을 위한 범위 지정을 위해 사용됨@Around 어노테이션 안에 직접 명시해도 범위가 지정되지만, 위 예시와 같이 PublicTarget() 처럼 @Pointcut 메서드를 지정해주면 다른 패키지에서 임포트하거나 @Around(\"패키지명.Aspect객체.publicTarget()\")처럼 재활용 가능execution(수식어패턴? 리턴타입패턴 클래스이름패턴?메서드이름패턴(파라미터패턴))*: 모든 값.. : 0개 이상title: ProceedingJoinPoint 객체의 인터페이스  proceed(): 지정 메서드가 실행되는 시점을 의미, 메서드 결과값을 리턴하며, 이를 다시 리턴해줘야 함.  getSignature(): 호출되는 메서드에 대한 시그니처 객체          리턴 값인 Signature 객체로 부터 메서드명(getName()), 시그니처 정보(toLongString()) 등을 구함        getTarget(): 대상 객체 인스턴스  getArgs(): 대상 메서드 파리미터Aspect 클래스 적용이후 완성한 클래스를 다음과 같이 설정 클래스에 등록하면 적용된다.title: AppCtx 설정 클래스 예시//...import org.springframework.context.annotation.EnableAspectJAutoProxy;@Configuration@EnableAspectJAutoProxy // 필수 Enable류 어노테이션, AOP 객체 사용을 위한 설정 빈 객체들을 자동으로 등록해줌public class AppCtx {    @Bean    public ExeTimeAspect exeTimeAspect() { // 우리가 만든 Aspect        return new ExeTimeAspect();    }\t@Bean\tpublic Calculator calculator() {\t\treturn new RecCalculator();\t}//...이후 다음과 같이 빈 객체를 가져와 클래스를 확인해보면 자동 생성된 프록시 객체가 생성되있음을 알 수 있다.title: AOP 대상이 된 빈 객체 실행예Calculator cal = ctx.getBean(\"calculator\", Calculator.class); long fiveFact = cal.factorial(5);System.out.println(\"cal.factorial(5) = \" + fiveFact);System.out.println(cal.getClass().getName()); // com.sum.proxy.%Proxy17 =&gt; RecCalculator 클래스가 아니라 자동 생성된 프록시 객체ctx.close();/* console:RecCalculator.factorial([5]) 실행 시간: 50201 nscal.factorial(5)=120com.sum.proxy.%Proxy17*/이때 %Proxy17의 타입은 메서드의 리턴 타입인 Calculator를 상속받아 생성됨title: 만약 원본 클래스인 RecCalculator 클래스를 상속받아 프록시 객체를 이용해 생성하고 싶다면어노테이션 @EnableAspectJAutoProxy(proxyTargetClass = true)로 설정하면 메서드 리턴타입이 아닌, 실제 결과값의 타입으로 설정된다."
  }
  , 
  
  "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20webpack,%20eslint,%20etc.html": {
    title: "Subbrain 개발기 - webpack, eslint, etc",
    date: " Jan 11, 2023 ",
    url: "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20webpack,%20eslint,%20etc.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSubbrain 개발기 - Webpack, Eslint, etc개요Webpack은 자바스크립트 환경의 정적 모듈 번들러로, 자바스크립트, CSS, JSON, 이미지 파일 등을 하나의 번들 스크립트로 묶어준다.이를 통해 필요 데이터 압축, 요청 메시지 수 감소, 코드 난독화, 고립된 코드 확인 등의 장점을 가진다.Eslint는 자바스크립트 정적 코드 분석기로, 설정을 통해 코드의 일관성을 지키고 버그를 줄이는데 사용할 수 있다.Rubocop은 루비 정적 코드 분석기 + 코드 포멧터이다.Prettier는 다양한 포맷팅을 지원하는 코드 포멧터로, 설정한 코드 스타일에 맞춰 자동으로 코드를 변경한다.Husky는 Git에 커밋하기 전에 커밋 메시지 린팅, 테스트 실행, 코드 실행 등을 진행해주는 패키지이다.개발 편의성과 기술 스택 습득을 위해 사용해 보았다. 소규모의 개인 프로젝트에서는 조금 의미가 퇴색되지만, 나중에 커다란 팀 프로젝트에서 큰 힘을 발휘할 수 있을 것 같다.개발 기능title: 🛠️ 변경 예정아무리 생각해도 잘 못쓰고 있는것 같아… 다시 정리해야할 것 같아…회고아무래도 webpack을 제외한 기능들은 VScode에서 편리하게 지원하므로 제대로 된 구현을 하지 않은 느낌이다.학습을 위해서라도 체계적으로 정리하고 수정할 필요성을 느꼈다."
  }
  , 
  
  "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20Typescript.html": {
    title: "Subbrain 개발기 - Typescript",
    date: " Jan 11, 2023 ",
    url: "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0%20-%20Typescript.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSubbrain 개발기 - Typescript개요타입스크립트는 정적 타입 체크 등의 기능이 추가된 자바스크립트의 슈퍼셋 언어이다. 이를 통해 타입 불일치 등에 의한 버그를 줄이고 타입 정의를 통해 데이터 정의를 확실히 할 수 있다.주로 드로워와 카테고리 구조 여닫기, 검색 결과 페이지 생성, 검색 창에 태그 생성, 드로워 애니메이션 적용 등의 동적인 기능들을 개발하는데 사용했다.타입스크립트를 이용해 “그나마” 체계적이고 버그가 적은 코드를 만들 수 있었다.검색과 관련된 기능은 이곳이 아닌 브라우저 기반 검색 페이지에서 설명한다.개발 기능Typescript configtitle: tsconfig.json{  \"compilerOptions\": {    \"target\": \"es2021\",    \"moduleResolution\": \"node\",    \"noImplicitAny\": true,    \"removeComments\": true,    \"preserveConstEnums\": true,    \"esModuleInterop\": true,    \"resolveJsonModule\": true,    \"sourceMap\": true,    \"strict\": true,    \"rootDir\": \"scripts\",    \"outDir\": \"assets/scripts\",    \"typeRoots\": [\"node_modules/@types\", \"scripts/types\"]  },  \"include\": [\"scripts/**/*\"],  \"exclude\": [\"node_modules\", \"**/*.spec.ts\"]}  기본값에서 크게 달라진 점은 없으며, JSON 처리와 결과물 설정을 Jekyll 폴더 구조의 assets 폴더로 옮기도록 하여 런타임에 불러올 수 있도록 하였다.프로젝트 파일 구조title: Typescript 파일 구조  사용 용도별로 폴더를 나누어 엔트리 포인트 역할을 할 index.ts를 가지고 있으며, components 폴더에서 기능을 구현한다.예외적으로  types 폴더는 데이터 타입들에 대한 정의  utils 폴더는 공통으로 사용하는 함수 코드를 가지고 있다.회고생성된 JS 코드와 TS 코드를 연결해주는 Sourcemap 파일들이 보기 싫었지만, 덕분에 초기에 이를 통하여 브라우저에서 디버깅을 쉽게할 수 있었다.  하지만 Webpack이 번들링하면서 디버깅이 어렵게 되어, 나중에 Webpack을 개발 환경과 빌드 환경으로 나눌 필요가 있을 것 같다.파일 구조는 도대체 어떻게 정해야 하는가?평소에 React하고만 쓰다보니 코드가 엉망이다.정적 사이트에 동적인 기능을 많이 많들다 보니 비대해져 성능이 조금 나빠진 것 같다."
  }
  , 
  
  "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/%EA%B0%9C%EC%9D%B8%20%EB%B8%94%EB%A1%9C%EA%B7%B8%20Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0.html": {
    title: "개인 블로그 Subbrain 개발기",
    date: " Jan 12, 2023 ",
    url: "/articles/etc/Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0/%EA%B0%9C%EC%9D%B8%20%EB%B8%94%EB%A1%9C%EA%B7%B8%20Subbrain%20%EA%B0%9C%EB%B0%9C%EA%B8%B0.html",
    tags: ["PJT","ETC"],
    content: "나의 Subbrain 개발기title: Subbrain의 예시개요프로젝트 명: Subbrain 개인 지식 기반 정적 블로그개발 팀 : 혼자개발 기간: 2022. 08. 17 ~ 2022. 12. 27 (개발 중단), 일 평균 2시간 투자URL: https://roadvirushn.github.io/Trello: https://trello.com/b/CJ5hI8Iz/til-%EB%A6%AC%EB%89%B4%EC%96%BCSubbrain의 시작title: Subbrain의 메인 페이지Subbrain은 어딘가에 사는 늙은 개발자 지망생이 만든 IT 블로그 겸 개인 지식 기반 시스템이다.Subbrain은 보조 뇌🧠라는 의미로, 나의 모자란 기억력을 보조하고, 저작물을 저장, 공개 하기 위한 용도로 개발되었다.title: 사실은…원래의 프로젝트명은 대형 SNS _Twitter_의 명칭을 패러디한 Twathut (머저리의 헛간)이었다.엉뚱한 발명만 하는 한 머저리 그림자와 그의 작업장 겸 헛간을 형상화한 로고는 이 때문이다.👈🏽 이 캐릭터의 이름이 Twat이었으나 단순히 영어로 “머저리”라는 뜻 뿐만 아니라 “여성의 음부”를 의미한다는 것을 깨닫고 급히 바꿨다.심지어 개발 중의 가제는 너글의 디지털 정원이었다.이 프로젝트는 다음과 같은 필요성으로 시작되었다.  기존의 지식 정리 방법이 남이 보기에 극히 더럽고 비효율적이여서 이를 개선하고자.  나의 지식을 원하는 자에게 이를 숨김없이 나눠주기 위함. (특히, 다 잊어버렸을 미래의 나)  집도 차도 직장도 없는데 인터넷 한 귀퉁이에 내 홈페이지 하나 정도는 있어도 되지 않나 생각해서 😭따라서 Subbrain은 다음과 같은 조건을 중점으로 개발되었다.  지식을 정리하는데 기존에 사용하던 마크다운 파일 형식과 옵시디언 툴의 기능을 지원할 것  서버 비용이 적거나 무료로 사용할 수 있으면서 다른 유저들을 위해 적절한 접근성을 가질 것  내가 원하는 기능을 손쉽게 커스터마이징하여 구현할 수 있을 것Subbrain 기술 스택title: Subbrain 아키텍처원하는 기능을 구현하기 위해 다음과 같은 기술 스택이 선정되었다.  Github page: 깃헙에서 제공하는 무료 정적 사이트 배포 서비스          상업적 목적이 아닌 이상 무료로 사용할 수 있으며, CDN, DNS, 배포 설정을 자동으로 해주므로 거렁뱅이인 나에게 이상적이다.        Jekyll: Ruby 기반의 정적 사이트 생성 툴          Github page 공식 문서의 적용 예시로 나와있음 + 많이들 사용하길래 사용        Obsidian: 커스터마이징 가능하고, 무료로 사용 가능하며, 인터넷 연결이 필요없는 마크다운 작성 툴          내가 애용하는 툴이며, 이곳에서 한번 정리한 TIL 들을 블로그를 위해 한번 더 정리해야 한다는 점에서 불편함을 느꼈다.        Javascript          분명 Jekyll의 기본 기능만으로는 만족하지 못할 것이라 예상하였다.        Sass: CSS 확장 언어          기본 템플릿(minimal)을 바탕으로 직접 원하는 템플릿을 효율적으로 구현하기 위해      여러 기업에서 CSS를 좀더 효율적으로 활용하기 위해 쓰는 것 같아 선정하였다.      프로젝트를 진행하면서 다음 기술 스택들이 추가되었다.  Typescript: 자바스크립트 타입 오류나는게 짜증나서 + 기술 스택 학습을 위해  Webpack: 자바스크립트와 기타 여러 정적 어셋들의 번들링 툴          자바스크립트 모듈 및 라이브러리 임포트 순서 설정 등에 어려움을 느낀 후로 도입했다.      알고보니 진즉에 필수로 적용했어야 했다.        Github Action: github에서 지원하는 CICD 툴          루비 커스텀 플러그인을 보안상의 이유로 github page에서 지원하지 않자 이를 우회하기 위해 조금 건드리게 됬다.      솔직히 필요한 문제 해결만 한 뒤로 깊게 배우지 않았으며, 앞으로도 깊게 배울 생각이 없다.      Subbrain의 기능Subbrain 블로그는 다른 블로그와 달리 다음과 같은 차별화된 기능을 가진다.  Obsidian-Jekyll 연동 포스트 생성 Obsidian으로 생성한 md 파일을 프로젝트 폴더로 옮기기만 하면 그대로 블로그의 포스트로 생성됨.  파일 구조 기반 카테고리 사이트맵 생성, Github Repository 기반 포스트 등록파일 구조 형식으로 정리한 마크다운 파일들이 그대로 카테고리 항목과 포스트로 생성되도록 하는 기능, 서비스 배포는 Github 레포지토리에 변경 사항을 푸시하면 자동으로 진행된다.  Obsidian의 커스텀 플러그인 기능이 포함된 마크다운 지원콜아웃, Mathjax 문법, Link 미리보기, 외부 링크 경고 등 순정 markdown에서 지원하지 않는 Obsidian의 기능을 지원  브라우저 기반 검색 기능정적 사이트는 보통 WAS와 DB가 존재하지 않아 동적인 검색 기능을 지원하기 힘들지만, 지식을 저장하고 공유하는 블로그 입장에서 검색 기능은 필수적이다. 이를 위해 자바스크립트 기반의 검색 기능을 구현하였다.  커스터마이징된 기능과 템플릿  개폐 가능하며, 카테고리 항목, 최근 조회 포스트 등의 기능을 가진 사이드바 추가, 스크롤바, 태그 등의 스타일을 변경Subbrain의 끝Subbrain은 2022년 12월 27일을 기점으로 개발이 중단된다. 이유는 다음과 같다.      재사용이 힘들고 표준화가 되어있지 않은 코드  여태껏 진행해왔던 프로젝트들은 주로 프레임워크를 이용해 개발하는 경우가 많아 컴포넌트화 및 객체화 등에 소홀히 하였고 마치 절차지향적인 코드가 생성되었다.        Jekyll의 한계  Jekyll은 말그래도 간단한 정적 블로그를 생성하기 위해 만들어졌으며, 내가 원하던 기능들을 구현하는 데에 한계가 많았다. 직접 구현한 기능들은 막대한 개발 비용 요구와 나쁜 성능을 가지고 있었다.        단촐할 것으로 예상한 설계와 달리 점점 방대해지는 프로젝트  처음에 간단하게 기술 스택을 지정한 이유는 이 프로젝트가 이렇게 긴 시간으로, 그리고 고도화된 기능을 구현하게 될 것이라고 생각하지 않았기 때문이다. 애시당초 검색엔진, 댓글 등의 동적 기능을 고려하면서 동시에 정적 사이트 프로젝트를 설계하는 것은 커다란 모순인 것 같다.        그래서 이건 블로그인가? 개인 지식 기반 시스템인가?  블로그는 남에게 내 글을 읽게 하는 곳이므로, 누가 보더라도 흥미롭고 유익하게 글을 써야 한다.   개인 지식 기반 시스템(PKMS)은 나의 지식을 정리하고 저장하는 곳이므로,일부 정보는 생략과 축약을 통해 없애고, 필요한 정보를 위주로 필요 정보만 나타내야 하며, 이 둘은 서로 상충된다.        현자타임  새로운 서비스? 기존의 다른 포트폴리오를 정리하고 개선할 시간도 부족하지 않은가?   어떤 회사가 Jekyll과 Ruby 언어를 사용하는 프로젝트에 관심을 가지겠는가? 그것도 ruby on rails 없이?   사람들이 이 프로젝트를 흥미롭게 생각할 것 인가?        전혀 힙(hip)하지 않음  막상 공들여 만들어낸 결과물은 기존의 tistory나 네이버 블로그에 비해 조악하였고 전혀 차별화되지 않았다. 그렇다면 학습 이외에 내가 이 서비스를 계속 개발해야할 이유가 있는가?  Subbrain은 한동안 현재 상태로 계속 사용될 것이며, 가끔 실험 등을 위해 조금씩만 개선될 것이다.이후, 개인 블로그 개발 프로젝트 시즌 2 mospOS(가제)의 개발이 완료된 후 어딘가에 처박힐 예정이다.기술 스택 별 회고Subbrain 개발기 - Ruby&amp;JekyllSubbrain 개발기 - TypescriptSubbrain 개발기 - SassSubbrain 개발기 - webpack, eslint, etcSubbrain의 교훈 및 회고title: “IT 프로젝트의 성공률은 약 30%” - 보스턴 컨설팅 그룹  아쉬운 점들                  예상보다 느린 개발 진척과 차별화되지 못한 결과물  처음에는 모든 개발을 14 영업일 이내에 끝낼 생각이였지만, 대략 그것보다 두배 가량의 시간이 걸렸으며, 점점 추가된 계획까지 포함한다면 아마 또 그것의 배가 들었을 것이다.  결과로 나온 블로그는 객관적인 입장에서 기존의 무료 상용 블로그에 비해 떨어진다. 이 모든 일은 다음 항목들이 원인이라고 생각한다.                    잘 설계된 계획과 이를 지키는 일을 아주 중요하다.  원 계획은 훨씬 간소한 일정과 간단한 기능만을 포함한 블로그였지만, 점점 거창한 기능과 사용해보지 않은 기술들을 추가로 고려하면서 일정이 눈덩이처럼 불어나버렸다.   또한, 솔로 프로젝트이므로 의사소통에 문제가 적을 것이라 생각하여 문서화를 소홀히 했다.  👉 다음부터는 정식적인 문서 작성과 체계화된 설계를 필두로 하여 개발을 진행할 것이다.  예를 들어 Trello의 사용법을 좀더 정교화하고 엄격히 한다거나, figma 목업, 유즈케이스 다이어그램, 클래스 다이어그램 같은 UML 도입, Sprint 등의 도입을 고려해볼 것이다.                    얻은 기술 스택이 생각보다 도움이 될것 같지 않음.  러닝 커브와 트랜드를 고려하지 않고 재미있어 보이는 기술 스택을 위주로 골라 학습에 많은 시간이 걸렸고, 앞으로 사용할 일도 많이 없을 것 같다.  👉 다음부터는 기획은 창의적이되, 기술은 검증되고 인기있는 것을 사용하도록 하자.  현재 예상되는 블로그 시즌 2의 기술 스택은 Next.js, Express.js를 사용하는 동적 사이트가 될 것 같다.              좋았던 점들                  새로 얻은 기술 스택들과 교훈 그리고 보람  Jekyll, Webpack, Typescript, Regex, Github Action, Ruby, Sass 등 내가 공부만 해보고 제대로 단독으로 사용해본적 없는 기술들을 사용해볼 수 있었다. 앞으로 사용 여부와 관계없이 나에게 많은 깨달음과 보람을 주었다.                    이전 보다 나아진 TIL 정리법  상용 블로그에 비할바 못되지만 기존의 Github 레포지토리에 정리되지 않은 채로 우르르 올리던 때보다 훨씬 체계적이고 보여줄만한 개인 지식 기반 시스템이 완성되었다.                    부족하지만 내 손으로, 내 의지로 만든 내 보금자리  이전까지 해본 프로젝트는 취직을 위해, 팀원을 위해, 평가를 위해, 학습을 위해 등 철저히 목적지향성이었지만, 이번 프로젝트는 순수히 나의 지적 욕구와 필요성에 의해 개발되었으며, 홀로 힘으로 만들어냈다. 내가 어째서 개발을 시작하게 되었는지 다시 떠올릴 수 있었던 좋은 기회였다.            … 그리고 드디어 인터넷 한 귀퉁이에 나의 공간이 생겼다.이 곳은 앞으로 진행될 나의 세계 정복 계획의 교두보가 될 것이다. 👽"
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-DB%20%EC%97%B0%EB%8F%99.html": {
    title: "Spring5 입문-DB 연동",
    date: " Jan 14, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-DB%20%EC%97%B0%EB%8F%99.html",
    tags: ["WEB","SPRING","BE","SUMMARY"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-DB 연동title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.의존 모듈 추가 및 DB 설정  Mysql를 기준으로 진행되며 Maven이나 Gradle을 이용해 다음과 같은 의존 모듈을 추가하자.          spring-jdbc : JdbcTemplate, 스프링 트랜잭션 기능 등의 기능을 제공      tomcat-jdbc : DB 커넥션 풀 기능을 제공      mysql-connector-java : MySQL 연결에 필요한 JDBC 드라이버를 제공        Mysql은 공식 레퍼런스를 이용해 설치하거나 Docker를 이용해 생성하자.  이후 DB 서버에 접속해 직접 SQL 구문을 이용해 DB, 사용자, 테이블 생성 및 권한 설정을 한다.title: sql문 예시create user 'spring5'@'localhost' identified by 'spring5';create database spring5fs character set=utf8;grant all privileges on spring5fs.* to 'spring5'@'localhost';create table spring5fs.MEMBER (\tID int auto_increment primary key,\tEMAIL varchar(255),\tPASSWORD varchar(100),\tNAME varchar(100),\tREGDATE datetime,\tunique key (EMAIL)) engine=InnoDB character set = utf8;  DB와 통신할 때 사용하는 메서드를 모아놓은 객체인 DAO(Data Access Object) 또한 정의해줘야 한다.title: /src/main/java/spring/MemberDao.java추가로 설정 클래스로 하단의 memberDao를 빈으로 추가하자.package spring;import java.util.Collection;public class MemberDao {\tpublic Member selectByEmail(String email) {\t\treturn null;\t}\t\tpublic void insert(Member member) {}\tpublic void update(Member member) {}\tpublic Collection&lt;Member&gt; selectAll() {\t\treturn null;\t}}DataSource 설정JDBC API는 DriverManager 객체 혹은 DataSource 객체를 통해 커넥션 풀에서 연결을 가져올 수 있다.title: 커넥션 풀(Connection pool)이란?DBMS 연결을 생성하는 시간을 줄이고, 발생하는 부하를 제한하기 위해 일정 개수의 DB 커넥션을 미리 생성해두고 이를 사용시 마다 대여, 반납하는 방식각 커넥션은 사용 중인 활성 상태, 대기 중인 유휴 상태가 존재한다.Tomcat JDBC Datasource 클래스모든 Datasource 클래스는 javax.sql.Datasource를 구현해야 하며,  우리는 Tomcat JDBC 모듈의 Datasource 클래스 이용할 것이다.title: /src/main/java/config/AppCtx.java먼저 DataSource를 스프링 빈으로 등록하고 주입을 받도록 한다.이외의 DB 설정은 다음이 있다.  커넥션 할당 시 검사(setTestOnBorrow)  커넥션 유효 쿼리 지정(setValidationQuery(\"select 1\"))  최소 커넥션 갯수(setMinIdle)  최대 연결 할당 대기 시간(setMaxWait(default=30초))  유휴 연결 제거 대기 시간(setMinEvictableIdleTimeMillis(default=60초))package config;import org.apache.tomcat.jdbc.pool.DataSource;//...@Configurationpublic class DbConfig {\t@Bean(destroyMethod=\"close\") // DataSource 객체 소멸 시 close 메서드(커넥션 풀 비우기)를 실행\tpublic DataSource dataSource() {\t\tDataSource ds = new DataSource(); // 객체 생성\t\tds.setDriverClassName(\"com.mysql.jdbc.Driver\"); // JDBC Mysql 드라이버 사용\t\tds.setUrl(\"jdbc:mysql://localhost/spring5fs?characterEncoding=utf8\"); // URL 지정\t\tds.setUsername(\"spring5\"); // 유저명 설정\t\tds.setPassword(\"spring5\"); // 패스워드 설정\t\tds.setInitialSize(2); // 초기 커넥션 개수 지정\t\tds.setMaxActive(10); // 최대 커넥션 개수 지정\t\tds.setTestWhileIdle(true); // 유휴 상태 연결 주기적으로 검사\t\tds.setMinEvictableIdleTimeMillis(1000*60*3); // 최소 유휴 시간 3분\t\tds.setTimeBetweenEvictionRunsMillis(10*1000); // 10초 주기 검사\t\treturn ds;\t}}위와 같이 설정하면 아래와 같이 연결을 가져와 사용할 수 있다.title: /src/main/java/dbquery/DbQuery.javapackage dbquery;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import javax.sql.DataSource;public class DbQuery {    private DataSource dataSource;    public DbQuery(DataSource dataSource) {        this.dataSource = dataSource;    }    public int count() {        Connection conn = null;        try {            conn = dataSource.getConnection(); // 연결 가져오기            try (Statement stmt = conn.createStatement(); // sql문 생성 준비를 위한 Statement                    ResultSet rs = stmt.executeQuery(\"select count(*) from MEMBER\")) {                rs.next();                 return rs.getInt(1); // 결과 값인 ResultSEt             }        } catch (SQLException e) {            throw new RuntimeException(e);        } finally {            if (conn != null)                try {                    conn.close();                } catch (SQLException e) {                }        }    }}JdbcTemplateSpring은 JDBC, JPA, MyBatis 등 여러 기술을 사용할 수 있지만 이번에는 JdbcTemplate를 사용하는 방법을 배워보자.JdbcTemplate 클래스를 이용하면 켐플릿 메서드 패턴과 전략 패턴을 이용하여 기존 JDB API의 구조적 반복을 줄이고 재사용성을 늘릴 수 있다.예시는 다음과 같다.title: JDBC API 연동 코드Member member;Connection conn = null;PreparedStatement pstmt = null;ResultSet rs = null;try {\tconn = DriverManager.getConnection(\"jdbc:mysql://localhost/spring5fs\", \"spring5\", \"spring5\");\t// ------↑↑-----반복되는 코드------↑↑-----\tpstmt = conn.prepareStatement(\"select * from MEMBER where EMAIL = ?\");\tpstmt.setString(1, email);\trs = pstmt.executeQuery();\tif (rs.next()) {\t\tmember = new Member(rs.getString(\"EMAIL\"), \t\t\trs.getString(\"PASSWORD\"),\t\t\trs.getString(\"NAME\"),\t\t\trs.getTimestamp(\"REGDATE\"));\t\tmember.setId(rs.getLong(\"ID\"));\t\treturn member;\t} else {\t\treturn null;\t}\t// ------↓↓-----반복되는 코드------↓↓-----} catch (SQLException e) {\te.printStackTrace();\tthrow e;} finally {\tif (rs != null)\t\ttry { rs.close(); } catch (SQLException e2) {}\tif (pstmt != null)\t\ttry { pstmt.close(); } catch (SQLException e1) {}\tif (conn != null)\t\ttry { conn.close(); } catch (SQLException e) {}}title: Jdbc Template 코드List&lt;Member&gt; results = jdbcTemplate.query(\t\"select * from MEMBER where EMAIL = ?\",\tnew RowMapper&lt;Member&gt;(){\t\t@Override\t\tpublic Member mapRow(ResultSet rs, int rowNum) throws SQLException {\t\t\tMember member = new Member(rs.getString(\"EMAIL\"),\t\t\t\trs.getString(\"PASSWORD\"),\t\t\t\trs.getString(\"NAME\"),\t\t\t\trs.getTimestamp(\"REGDATE\"));\t\t\tmember.setId(rs.getLong(\"ID\"));\t\t\treturn member;\t\t}\t},\temail);return results.isEmpty() ? null : results.get(0);기존의 JDBC 코드에 비해 훨씬 줄어든다는 점을 알 수 있다.JdbcTemplate를 사용하면 짧은 코드로 손쉽게 쿼리를 실행할 수 있다.JdbcTemplate 생성jdbc.core.JdbcTemplate 객체를 생성하고 dataSource 객체를 할당해 준 뒤, 해당 DAO를 빈 객체로 등록해주자.title: MemberDao 전문package spring;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List; import java.sql.Timestamp;import javax.sql.DataSource;import org.springframework.jdbc.core.JdbcTemplate; // JdbcTemplate 임포트import org.springframework.jdbc.core.PreparedStatementCreator;import org.springframework.jdbc.core.RowMapper;import org.springframework.jdbc.support.GeneratedKeyHolder;import org.springframework.jdbc.support.KeyHolder;public class MemberDao {    private JdbcTemplate jdbcTemplate;    public MemberDao(DataSource dataSource) { // 생성자로 JdbcTemplate와 데이터베이스 설정        this.jdbcTemplate = new JdbcTemplate(dataSource); // 설정 클래스 추가 시 datasource를 건네줘야 함    }    public Member selectByEmail(String email) {        List&lt;Member&gt; results = jdbcTemplate.query( // query 메서드를 통한 조회                \"select * from MEMBER where EMAIL = ?\",                new RowMapper&lt;Member&gt;() {                    @Override                    public Member mapRow(ResultSet rs, int rowNum) throws SQLException {                        Member member = new Member(                                rs.getString(\"EMAIL\"),                                rs.getString(\"PASSWORD\"),                                rs.getString(\"NAME\"),                                rs.getTimestamp(\"REGDATE\").toLocalDateTime());                        member.setId(rs.getLong(\"ID\"));                        return member;                    }                }, email);        return results.isEmpty() ? null : results.get(0);    }\tpublic void insert(Member member) {        KeyHolder keyHolder = new GeneratedKeyHolder();        jdbcTemplate.update(new PreparedStatementCreator() { // update 메서드로 insert도 진행            @Override            public PreparedStatement createPreparedStatement(Connection con)                    throws SQLException {                // 파라미터로 전달받은 Connection을 이용해서 PreparedStatement 생성                PreparedStatement pstmt = con.prepareStatement(                        \"insert into MEMBER (EMAIL, PASSWORD, NAME, REGDATE) \" +                        \"values (?, ?, ?, ?)\",                        new String[] { \"ID\" });\t\t\t\t// 인덱스 파라미터 값 설정                pstmt.setString(1, member.getEmail());                pstmt.setString(2, member.getPassword());                pstmt.setString(3, member.getName());                pstmt.setTimestamp(4,                        Timestamp.valueOf(member.getRegisterDateTime()));\t\t\t\t// 생성한 PreparedStatement 객체 리턴                return pstmt;            }        }, keyHolder);\t\tNumber keyValue = keyHolder.getKey();        member.setId(keyValue.longValue());    }\tpublic void update(Member member) {        jdbcTemplate.update(                \"update MEMBER set NAME = ?, PASSWORD = ? where EMAIL = ?\",                member.getName(), member.getPassword(), member.getEmail());    }    public List&lt;Member&gt; selectAll() {        List&lt;Member&gt; results = jdbcTemplate.query(\"select * from MEMBER\",                (ResultSet rs, int rowNum) -&gt; { // 람다식을 이용한 더욱 짧은 구현, 앞선 조회 쿼리에 사용한 RowMapper와 같은 내용이므로 재사용으로 생략도 가능하다.                     Member member = new Member(                            rs.getString(\"EMAIL\"),                            rs.getString(\"PASSWORD\"),                            rs.getString(\"NAME\"),                            rs.getTimestamp(\"REGDATE\").toLocalDateTime());                    member.setId(rs.getLong(\"ID\"));                    return member;                });        return results;    }    public int count() {        Integer count = jdbcTemplate.queryForObject( // queryforObject를 이용한 조회, 주로 결과 값이 하나일 경우에 사용                \"select count(*) from MEMBER\", Integer.class);        return count;    }}DAO를 빈 객체로 등록하기@Configuration@EnableTransactionManagementpublic class AppCtx {    @Bean    public MemberDao memberDao() {        return new MemberDao(dataSource());    }}JdbcTemplate 조회 쿼리 생성 실행RowMapper 인터페이스쿼리 실행 결과를 자바 객체로 변환할 때 사용하는 인터페이스로, 이용 시 mapRow() 메서드를 직접 구현해야 한다.title: RowMapper 인터페이스package org.springframework.jdbc.core.RowMapper;public interface RowMapper&lt;T&gt; {\tT mapRow(ResultSet rs, int rowNum) throws SQLException;}SQL 실행 결과 ResultSet에서 한 행 씩, 데이터를 읽어와 자바 객체로 변환한다. (즉, 나오는 결과가 배열 형태여야 한다.)title: select sql을 위한 RowMapper 구현 예시List&lt;Member&gt; results = jdbcTemplate.query( // query 메서드를 통한 조회 \t\"select * from MEMBER where EMAIL = ?\", \tnew RowMapper&lt;Member&gt;() { //임의 클래스를 이용한 즉석 오버라이딩\t\t@Override \t\tpublic Member mapRow(ResultSet rs, int rowNum) throws SQLException {\t\t\tMember member = new Member( \t\t\t\trs.getString(\"EMAIL\"), \t\t\t\trs.getString(\"PASSWORD\"),\t\t\t\trs.getString(\"NAME\"), \t\t\t\trs.getTimestamp(\"REGDATE\").toLocalDateTime());\t\t\tmember.setId(rs.getLong(\"ID\"));\t\t\treturn member;\t\t}\t}, email);람다식으로 더욱 짧게 구현할 수도 있다.List&lt;Member&gt; results = jdbcTemplate.query(\t\"select * from MEMBER where EMAIL = ?\", \t(ResultSet rs, int rowNum) -&gt; {\t\tMember member = new Member( \t\t\trs.getString(\"EMAIL\"), \t\t\trs.getString(\"PASSWORD\"),\t\t\trs.getString(\"NAME\"), \t\t\trs.getTimestamp(\"REGDATE\").toLocalDateTime());\t\tmember.setId(rs.getLong(\"ID\"));\t\treturn member;\t}, email);만약 같은 mapRow 로직을 가진다면, 구현한 RowMapper를 재활용할 수도 있다.query() 메서드query() 메서드는 sql 파라미터로 전달받은 쿼리를 실행하고 앞서 배운 RowMapper를 이용해 ResultSet의 결과를 자바 객체로 변환한다.title: query 메서드의 시그니처  List&lt;T&gt; query(String sql, RowMapper&lt;T&gt; rowMapper)  List&lt;T&gt; query(String sql, Object[] args, RowMapper&lt;T&gt; rowMapper)  List&lt;T&gt; query(String sql, RowMapper&lt;T&gt; rowMapper, Object... args)  List&lt;T&gt; query(PreparedStatementCreator psc, RowMapper&lt;T&gt; rowMapper) : PreparedStatementCreator는 아래 참조뒤의 인자 args의 갯수는 String sql의 인덱스 파라미터(?)의 갯수에 달려있으며,  순서대로 args의 값과 sql 내부의 ? 위치에 포맷팅된다.title: query 메서드와 인덱스 파라미터의 이용 예시String email = \"asdf@asdf.com\";String name = \"asdf\";List&lt;Member&gt; results = jdbcTemplate.query(\t\"select * from MEMBER where EMAIL = ? and NAME = ?\",\tnew MemberRowMapper(), // RowMapper 재활용\temail, name);// s0ql 결과는 select * from MEMBER where EMAIL = \"asdf@asdf.com\" and NAME = \"asdf\"만약 결과가 없다면 길이가 0인 List를 리턴하며, 결과가 오직 하나라면 길이가 1인 List를 리턴한다.따라서 만약, 단 하나의 결과만 기대하는 쿼리의 경우,return results.isEmpty() ? null : results.get(0); 처럼 리스트 길이가 0이면 결과 없음, 또는 첫번째 항목을 가져와야 하거나, 다음에 소개될 queryForObject() 메서드를 이용하면 된다.queryForObject() 메서드title: queryForObject() 메서드 예시query() 메서드와 같이 인덱스 파라미터를 활용할 수 있다.double avg = queryForObject(\t\"select avg(height) from FURNITURE where TYPE=? and STATUS=?\",\tDouble.class, // Row mapper 대신 결과값의 타입만 지정해주면 OK\t100, \"S\");만약 원하는 쿼리의 결과가 하나뿐일 경우 -&gt; 결과 값의 타입을 원하는 방식으로 받을 수 있고, 간단하게 구현 가능한 queryForObject() 메서드를 추천만약, 쿼리 결과가 2개 이상이거나 0개이면 오류를 일으킨다.title: queroyForObject()의 메서드  T queryForObject(String sql, Class&lt;T&gt; requiredType)  T queryForObject(String sql, Class&lt;T&gt; requiredType, Object... args)  T queryForObject(String sql, RowMapper&lt;T&gt; rowMapper)  T queryForObject(String sql, RowMapper&lt;T&gt; rowMapper, Object... args)query() 메서드처럼 rowMapper를 그대로 사용할 수 있으며, 이때는 리스트가 아닌 rowMapper의 리턴 타입으로 되돌려 준다.JdbcTemplate 변경 쿼리 생성 실행SQL의 INSERT, UPDATE, DELETE 쿼리 등은 update() 메서드를 이용하여 구현한다.title: update() 메서드 사용  int update(String sql)  int update(String sql, Object... args)  int update(PreparedStatementCreator psc)역시나 인덱스 파라미터(?)를 지원한다.  int update(PreparedStatementCreator psc, KeyHolder generatedKeyHolder)추가로 아래에 소개될 PreparedStatement와 KeyHolder를 이용할 수 있다.title: update() 메서드 예시jdbcTemplate.update(\t\"udpate MEMBER set NAME = ?, PASSWORD = ? where EMAIL = ?\",\tmember.getName(), member.getPassword(), member.getEmail());PreparedStatement 인터페이스 이용애매모호한 ? 인덱스 파라미터 대신, PreparedStatement를 이용할 수도 있다.마찬가지로 PrepareStatementCreator 인터페이스의 createPreparedSatement를 오버라이딩하여 구현하며, 다음이 예시이다.title: 임의 클래스를 이용한 PreparedStatement 이용 예시import java.sql.PreparedStatement;jdbcTemplate.update(new PreparedStatementCreator() {\t@Override\tpublic PreparedStatement createPreparedStatement(Connection con) throws SQLException {\t\t// 파라미터로 전달받은 Connection을 이용해서 PreparedStatement 생성\t\tPreparedStatement pstmt = con.prepareStatement(\t\t\t\"insert into MEMBER (EMAIL, PASSWORD, NAME, REGDATE) values (?, ?, ?, ?)\");\t\t// 인덱스 파라미터의 값 설정\t\tpstmt.setString(1, member.getEmail());\t\tpstmt.setString(2, member.getPassword());\t\tpstmt.setString(3, member.getName());\t\tpstmt.setTimestamp(4, Timestamp.valueOf(member.getRegisterDateTime()));\t\t// 생성한 PreparedStatement 객체 리턴\t\treturn pstmt;\t}});이 방식은 앞서 배웠던 query() 메서드에서도 사용 가능하다.KeyHolder를 이용한 자동 생성 키값 구하기많은 테이블에서 행 추가 시, 주요 키인 ID를 자동으로 증가시키는 방식으로 이용하므로, 코드 상에서 새로 생성된 행에 대한 ID를 알 수 없다.  update() 메서드의 결과 값은 변경된 행의 갯수만 리턴한다.따라서 쿼리 후 생성된 값에 대한 키 값을 구하는 방법으로 keyHolder를 이용할 수 있다.title: Keyholder 구현 예시import org.springframework.jdbc.support.GeneratedKeyHolder;import org.springframework.jdbc.support.KeyHolder;public void insert(Member member) {\tKeyHolder keyHolder = new GeneratedKeyHolder(); //자동 생성된 키 값 구해주는 KeyHolder 구현 클래스    jdbcTemplate.update(new PreparedStatementCreator() {        @Override        public PreparedStatement createPreparedStatement(Connection con) throws SQLException {            PreparedStatement pstmt = con.prepareStatement(                    \"insert into MEMBER (EMAIL, PASSWORD, NAME, REGDATE) \" +                    \"values (?, ?, ?, ?)\",                    new String[] { \"ID\" }); // preparedStatement의 두번째 인자로 구할 키 값의 이름 명시                                 pstmt.setString(1, member.getEmail());            pstmt.setString(2, member.getPassword());            pstmt.setString(3, member.getName());            pstmt.setTimestamp(4,                    Timestamp.valueOf(member.getRegisterDateTime()));\t\t\t// 생성한 PreparedStatement 객체 리턴            return pstmt;        }    }, keyHolder);\tNumber keyValue = keyHolder.getKey(); // 구한 키값을 알려주는 메서드    member.setId(keyValue.longValue()); // 적절한 타입으로 캐스팅}스프링 익셉션 변환 처리DB를 다루는 것은 민감하고 중요하며 복잡한 사안이므로 상당히 많은 오류를 보게될 것이다.  인증/인가 관련 오류 (ex) CannotGetJdbcConnectionException)  SQL 구문 에러 (ex) BadSqlGrammarException)  db 설정 관련 에러 (ex) CannotGetJdbcConnectionException)이를 차분히 읽고 문제 발생 원인을 찾을 수 있다.스프링은 다음과 같이 익셉션을 처리한다.  JDBC, Hibernate, JPA 등 여러 연동 기술들의 각기 다른 익셉션 발생  -&gt;  스프링은 각각의 익셉션을 동일한 DataAccessException으로 변환          이를 통해 연동 기술에 관계없이 동일하게 익셉션 처리 가능        -&gt; DataAccessException를 상속받은 구체적인 하위 타입으로 변환 (ex) DuplicateKeyExcetion, QueryTimeoutException)          이를 통해 이름만으로 문제 원인 유추 가능      DataAccessException은 RuntimeException을 상속받고 있으므로, 스프링에서 자동으로 오류 처리를 해준다.따라서 기존의 JDBC에서는 반드시 try~catch를 이용한 익셉션 처리를 포함해야 구현 가능하지만, 스프링에서는 필요한 경우에만 익셉션 처리를 구현해줄 수 있다.트랜잭션 처리 : @TransactionalJDBC에서는 트랜잭션을 하나의 작업단위로 커밋하거나 롤백하려면 아래와 같이 직접 commit과 rollback 하여 진행했다.  코드 누락, 반복, 관리 힘듬 등의 문제가 있음title: 기존의 JDBC 트랜잭션 구현Connection conn = null;try {\tconn = DriverManager.getConnection(jdbcUrl, user, pw);\tconn.setAutoCommit(false); // 트랜잭션 범위 시작\t//... 기타 쿼리\tconn.commit(); // 트랜잭션 범위 종료 및 커밋} catch(SQLException ex) {\tif (conn != null)\t\t// 트랜잭션 범위 종료 : 롤백\t\ttry { conn.rollabck(); } catch (SQLException e) {}} finally {\tif (conn != null)\t\ttry { conn.close(); } catch (SQLException e) {}}스프링의 @Transactional 어노테이션을 이용하면 손쉽게 위와 같은 코드를 아래처럼 줄일 수 있다.title: 스프링의 @Transactional을 이용한 트랜잭션 처리import org.springframework.transaction.annotation.Transactional;@Transactionalpublic void changePassword(String email, String oldPwd, String newPwd) {\tMember member = memberDao.selectByEmail(eamil);\tif (member == null)\t\tthrow new MemberNotFoundException();\tmember.changePassword(oldPwd, newPwd);\tmemberDao.update(member);}@Transactional 어노테이션이 붙은 메서드는 동일한 트랜잭션 범위 내에 실행되며, 실패시 전부 롤백 된다.@Transactional 메서드 내부의 다른 일반 메서드 과정 내의 오류에도 롤백되며, 일반 메서드 내의 DB 변화도 롤백된다.@Transactional와 스프링 설정법@Transactional 기능을 사용하라면 다음과 같이 두 개의 설정이 필요하다.  설정 클래스에 플랫폼 트랜잭션 매니저(PlatformTransactionManager) 빈 설정  @Transactional 어노테이션 활성화 설정플랫폼 트랜잭션 매니저(PlatformTransactionManager) 빈 설정스프링 설정 클래스에 다음과 같은 내용을 추가해야 한다.title: 스프링 설정 클래스 예시(AppCtx)  @EnableTransactionManagement: @Transactional 어노테이션이 붙은 메서드를 트랜잭션 범위에서 실행, 빈이 직접 트랜잭션 적용          프록시 기반이므로 AOP에서 배웠던 order, proxyTargetClass 속성을 설정 가능하다.        PlatformTransactionManager: 구현 기술에 관계없이 동일한 방식으로 트랜잭션 처리를 위한 인터페이스, 이용할 datasource를 지정해야 함.import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.transaction.PlatformTransactionManager;import org.springframework.transaction.annotation.EnableTransactionManagement;@Configuration@EnableTransactionManagement // 플랫폼 트랜잭션 매니저 사용함을 의미public class AppCtx {    @Bean    public PlatformTransactionManager transactionManager() { // 트랜잭션매니저 등록        DataSourceTransactionManager tm = new DataSourceTransactionManager();        tm.setDataSource(dataSource());        return tm;    }        @Bean     public ChangePasswordService changePwdSvc() { // @Transactional이 적용된 트랜잭션 메서드가 존재하는 서비스 객체 등록        ChangePasswordService pwdSvc = new ChangePasswordService();        pwdSvc.setMemberDao(memberDao());        return pwdSvc;    }}@Transactional 어노테이션 활성화 설정title: ChangePasswordService 객체 예시package spring;import org.springframework.transaction.annotation.Transactional;public class ChangePasswordService {    private MemberDao memberDao;        @Transactional // 이제 changePassword 메서드는 트랜잭션 범위 내에서 실행됨    public void changePassword(String email, String oldPwd, String newPwd) {        Member member = memberDao.selectByEmail(email);        if (member == null)            throw new MemberNotFoundException();        member.changePassword(oldPwd, newPwd);        memberDao.update(member);    }    public void setMemberDao(MemberDao memberDao) {        this.memberDao = memberDao;    }}이제 원할 때, 위의 서비스를 불러와 트랜잭션을 사용할 수 있다.프록시를 이용한 커밋, 롤백 처리스프링의 @Transactional 어노테이션은 앞서 배웠던 AOP를 이용해 트랜잭션을 처리한다. 즉 프록시를 이용한다.  스프링이 설정 클래스에서 @EnableTransactionManagement 태그를 감지  @Transactional 어노테이션이 적용된 빈 객체를 찾음  적절한 프록시 객체 생성 뒤 메서드 이용 시 아래와 같이 프록시를 통해 트랜잭션 적용title: 프록시 트랜잭션 예시      트랜잭션 성공 시 프록시 커밋 동작        트랜잭션 실패 시 프록시 롤백 동작  롤백은 RuntimeException이 일어날때만 생성되므로 SQLException 같이 다른 타입의 에러를 처리하려면 다음과 같이 설정하면 된다.title: @Transactional의 에러 처리 추가 예시  반대로 noRollbackFor 속성을 지정해주면 해당하는 익셉션에 롤백하지 않는다.@Transactional(rollbackFor = {SQLException.class, IOException.class})public void someMethod(){\t//...}@Transactional의 주요 속성과 전파  value: 트랜잭션 관리할 PlatformTransactionManager 빈의 이름 지정, 없으면 해당 타입의 빈을 자동으로 찾음  isolation: 트랜잭션 격리 레벨, READ_UNCOMMITTED, READ_COMMITTED, REPEATABLE_READ, SERIALIZABLE 등이 존재, 기본값은 Isolation.DEFAULT  timeout: 트랜잭션 제한 시간 지정, 초단위이며 기본값 -1인 경우 DB의 설정을 따름  propagation: 트랜잭션 전파 타입 지정, 기본값은 Propagation.REQUIRED          기본 값이면 메서드 수행 시 이미 진행중인 트랜잭션이 존재하면 해당 트랜잭션에 병합      title: 트랜잭션 전파란?@Transactional이 적용된 메서드 내부에 또 다른 @Transactional 메서드가 존재하면, 트랜잭션 전파(transaction propagation) 속성에 따라 다른 동작을 한다.예를 들어 Propagation.MANDATORY는 이미 진행중인 트랜잭션이 있다면 에러를 일으키며, Propagation.REQUIRES_NEW은 항상 새로운 트래잭션으로 덮어씌운다.  이외에도 SUPPORTS, NOT_SUPPORTED, NEVER, NESTED 등이 존재로깅 처리트랜잭션 처리, 오류, 관련 로그 메시지를 보기 위해서 일일이 print하기 보다는 Log4j2나 Logback 등의 외부 패키지를 이용할 수 있다.  엄밀히 말하면 스프링 자체 로깅 모듈인 spring-jcl이 위의 로깅 모듈을 인식하여 사용한다.메이븐이나 그래들을 이용해 Logback을 의존 모듈로 추가하고, 클래스 패스에 설정파일을 위치시키기 위해 src/main/resources 폴더를 만들어 설정 xml을 추가하고, 메이븐, 그래들로 프로젝트를 업데이트하면 된다.title: Logback 설정 xml 예시 (main/resources/logback.xml)&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt;    &lt;appender name=\"stdout\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;        &lt;encoder&gt;            &lt;pattern&gt;%d %5p %c{2} - %m%n&lt;/pattern&gt;        &lt;/encoder&gt;    &lt;/appender&gt;    &lt;root level=\"INFO\"&gt;        &lt;appender-ref ref=\"stdout\" /&gt;    &lt;/root&gt;    &lt;logger name=\"org.springframework.jdbc\" level=\"DEBUG\" /&gt; &lt;!--로그 메시지 상세 보기 설정--&gt;&lt;/configuration&gt;이후 터미널에서 다음 예시 같은 핵심 로그를 확인할 수 있다.title: Logback 로그 예시날짜 시간 DEBUG o.s.j.d DataSourceTransactionManager - Initiating transaction commit날짜 시간 DEBUG o.s.j.d DataSourceTransactionManager - Committing JDBC transaction on Connection[ProxyConnection...]날짜 시간 DEBUG o.s.j.d DataSourceTransactionManager - Initiating transaction rollback날짜 시간 DEBUG o.s.j.d DataSourceTransactionManager - Rolling back JDBC transaction on Connection[ProxyConnection...]"
  }
  , 
  
  "/articles/computer_science/OSSU/HowToCode/%EC%8A%A4%ED%8E%98%EC%9D%B4%EC%8A%A4%20%EC%9D%B8%EB%B2%A0%EC%9D%B4%EB%8D%94-%EC%84%B8%EA%B3%84%20%EC%84%A4%EA%B3%84%20%EC%98%88%EC%8B%9C%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.html": {
    title: "스페이스 인베이더-세계 설계 예시 프로젝트",
    date: " Jan 19, 2023 ",
    url: "/articles/computer_science/OSSU/HowToCode/%EC%8A%A4%ED%8E%98%EC%9D%B4%EC%8A%A4%20%EC%9D%B8%EB%B2%A0%EC%9D%B4%EB%8D%94-%EC%84%B8%EA%B3%84%20%EC%84%A4%EA%B3%84%20%EC%98%88%EC%8B%9C%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: true어떻게 코딩할 것인가-세계 설계 예시 프로젝트Edx 강의 How to Code 시리즈를 정리한 내용입니다.시연목표간단한 스페이스 인베이터 형식의 게임을 만드는 프로젝트였다.완벽한 게임을 만드는 것은 아니며 기초적인 기능만 몇개 구현하도록 되어있다.자세한 명세는 해당 How to Code에서 확인할 수 있다.코드(김 주의)  혼자서 BSL로 Dr.Racket 에디터에서 작성됨(2023.01.19)  대략 600줄, 상수부터 데이터, 함수, 세계 설계까지 전부 포함  104 개의 테스트 코드 작성  28 개의 함수 작성title: 간단한 SPACE INVADER(require 2htdp/universe)(require 2htdp/image);; Space Invaders;; Constants:(define WIDTH  300)(define HEIGHT 500)(define INVADER-X-SPEED 1.5)  ;speeds (not velocities) in pixels per tick(define INVADER-Y-SPEED 1.5)(define TANK-SPEED 2)(define MISSILE-SPEED 10)(define HIT-RANGE 10)(define INVADE-RATE 100)(define BACKGROUND (place-image (above (text \"ROADVIRUSHN'S\" 24 \"lightgreen\") (text \"HTC 1 Final PJT\" 24 \"lightgreen\") (text \"space = shot\" 20 \"gray\") (text \" left, right = move\" 20 \"gray\")) (/ WIDTH 2) 100 (empty-scene WIDTH HEIGHT)))(define INVADER  (overlay/xy (ellipse 10 15 \"outline\" \"blue\")              ;cockpit cover              -5 6              (ellipse 20 10 \"solid\"   \"blue\")))            ;saucer(define TANK  (overlay/xy (overlay (ellipse 28 8 \"solid\" \"black\")       ;tread center                       (ellipse 30 10 \"solid\" \"green\"))     ;tread outline              5 -14              (above (rectangle 5 10 \"solid\" \"black\")       ;gun                     (rectangle 20 10 \"solid\" \"black\"))))   ;main body(define TANK-HEIGHT/2 (/ (image-height TANK) 2))(define MISSILE (ellipse 5 15 \"solid\" \"red\"))(define TICK-PER-SEC 28);; Data Definitions:(define-struct game (invaders missiles tank time));; Game is (make-game  (listof Invader) (listof Missile) Tank Number);; interp. the current state of a space invaders game;;         with the current invaders, missiles, tank position, and current time.;; Game constants defined below Missile data definition#;(define (fn-for-game s)  (... (game-time s)       (fn-for-loinvader (game-invaders s))       (fn-for-lom (game-missiles s))       (fn-for-tank (game-tank s))));; Template rules used:;;  - compound :4 field;;    - atomic non-distinct: time(Natural);;    - reference: invaders field is listof Invader;;    - reference: missiles field is listof Missile;;    - reference: tank field is Tank(define-struct tank (x dir));; Tank is (make-tank Number Integer[-1, 1]);; interp. the tank location is x, HEIGHT - TANK-HEIGHT/2 in screen coordinates;;         the tank moves TANK-SPEED pixels per clock tick left if dir -1, right if dir 1(define T0 (make-tank (/ WIDTH 2) 1))   ;center going right(define T1 (make-tank 50 1))            ;going right(define T2 (make-tank 50 -1))           ;going left(define T4 (make-tank (/ WIDTH 2) 0))   ;idle#;(define (fn-for-tank t)  (... (tank-x t) (tank-dir t)));; Template rules used:;;  - compound :2 field;;    - atomic non-distinct: x(Natural);;    - atomic non-distinct: dir(Natural)(define-struct invader (x y dir));; Invader is (make-invader Number Number Number[-1,1]));; interp. the invader is at (x, y) in screen coordinates;;         the invader along x by (dir * INVADER-X-SPEED) pixels per clock tick(define I1 (make-invader 150 100 1))           ;not landed, moving right(define I2 (make-invader 150 HEIGHT -1))       ;exactly landed, moving left(define I3 (make-invader 150 (+ HEIGHT 10) 1)) ;&gt; landed, moving right(define I4 (make-invader (/ WIDTH 2) 0 1))#;(define (fn-for-invader invader)  (... (invader-x invader) (invader-y invader) (invader-dx invader)))(define-struct missile (x y));; Missile is (make-missile Number Number);; interp. the missile's location is x y in screen coordinates(define M1 (make-missile 150 300))                       ;not hit I1(define M2 (make-missile (invader-x I1) (+ (invader-y I1) 10)))  ;exactly hit I1(define M3 (make-missile (invader-x I1) (+ (invader-y I1)  5)))  ;&gt; hit I1#;(define (fn-for-missile m)  (... (missile-x m) (missile-y m)))(define G0 (make-game empty empty T0 0))(define G1 (make-game empty empty T1 0))(define G2 (make-game (list I1) (list M1) T1 0))(define G3 (make-game (list I1 I2) (list M1 M2) T1 0))(define G4 (make-game empty empty T4 0)); ==================================; World:;; game -&gt; game;; start the world with (main G4);; (define (main game)  (big-bang game                   ; game    (on-tick   tock-game)     ; game -&gt; game    (to-draw   render-game)   ; game -&gt; Image    (on-key    key-game)))    ; game KeyEvent -&gt; game;; tock-game;; game -&gt; game;; produce the next game status.;; Add single Invader every 5 secs = 140 tick.;; Add two Invaders every  15 secs = 420 tick.; (define (tock-game game) (make-game empty empty (make-tank (/ WIDTH 2) 0) 0)) ; stub(check-expect (tock-game (make-game empty empty (make-tank (/ WIDTH 2) 0) 0)) (make-game empty empty (make-tank (/ WIDTH 2) 0) 1))(check-expect (tock-game (make-game empty empty (make-tank (/ WIDTH 2) 0) 140))  (make-game (cons (make-invader (/ WIDTH 2) 0 1) empty) empty (make-tank (/ WIDTH 2) 0) 141))(check-expect (tock-game (make-game (cons (make-invader (/ WIDTH 2) 0 1) empty) empty (make-tank (/ WIDTH 2) 0) 141))  (make-game (cons (make-invader (+ (/ WIDTH 2) INVADER-X-SPEED) INVADER-Y-SPEED 1) empty) empty (make-tank (/ WIDTH 2) 0) 142));; &lt;Template from game&gt;(define (tock-game s)  (make-game    (next-invader s)   (next-missile s)   (next-tank (game-tank s))   (next-game-time (game-time s))));; next-game-time;; number -&gt; number;; get next game time(define (next-game-time s) (+ s 1))(check-expect (next-game-time 0) 1);; next-tank;; Tank -&gt; Tank;; get next Tank status; (define (next-tank t) (make-tank (/ WIDTH 2) 0)) ;stub;; Tests:(check-expect (next-tank (make-tank (/ WIDTH 2) 0)) (make-tank (/ WIDTH 2) 0)) ; no move(check-expect (next-tank (make-tank (/ WIDTH 2) 1)) (make-tank (+ (/ WIDTH 2) (* TANK-SPEED 1)) 1)) ; right(check-expect (next-tank (make-tank (/ WIDTH 2) -1)) (make-tank (+ (/ WIDTH 2) (* TANK-SPEED -1)) -1)) ; left(check-expect (next-tank (make-tank 0 -1)) (make-tank 0 0)) ; left when left edge(check-expect (next-tank (make-tank WIDTH 1)) (make-tank WIDTH 0)) ; right when right edge(check-expect (next-tank (make-tank 0 1)) (make-tank (+ 0 (* TANK-SPEED 1)) 1)) ; right when left edge(check-expect (next-tank (make-tank WIDTH -1)) (make-tank (+ WIDTH (* TANK-SPEED -1)) -1)) ; left when right edge;; &lt;Template from Tank&gt;(define (next-tank t)  (cond [(over-x-edge? (next-tank-pos (tank-x t) (tank-dir t))) (make-tank (tank-x t) 0)]        [else (make-tank (next-tank-pos (tank-x t) (tank-dir t)) (tank-dir t))]));; next-tank-pos;; number number -&gt; number;; get next tank position; (define (next-tank-pos x t) x) ;stub;; Tests:(check-expect (next-tank-pos (/ WIDTH 2) 0) (/ WIDTH 2))(check-expect (next-tank-pos (/ WIDTH 2) 1) (+ TANK-SPEED (/ WIDTH 2)))(check-expect (next-tank-pos (/ WIDTH 2) -1) (+ (* TANK-SPEED -1) (/ WIDTH 2)))(define (next-tank-pos x t)  (+ (* TANK-SPEED t) x));; Template rules used:;; - compound: 2 fields;;  - atomic nondistinct : x;;  - atomic nondistinct : d;; over-x-edge?;; number -&gt; boolean;; check if x position is over the edge.; (define (over-x-edge? x) false) ; stub;;Tests:(check-expect (over-x-edge? (+ WIDTH 1)) true)(check-expect (over-x-edge? (- WIDTH 1)) false)(check-expect (over-x-edge? -1) true)(check-expect (over-x-edge? 1) false)(check-expect (over-x-edge? 0) false)(check-expect (over-x-edge? WIDTH) false)(define (over-x-edge? x) (or (&gt; x WIDTH) (&lt; x 0)));; Template rules used:;;  - atomic nondistinct : x;; next-invader;; game -&gt; ListOfInvader;; get next invader status from game status.;(define (next-invader s) empty); stub;;Tests:(check-expect (next-invader (make-game empty empty (make-tank (/ WIDTH 2) 0) 0)) empty) ;; default(check-expect (next-invader (make-game empty empty (make-tank (/ WIDTH 2) 0) 140)) (cons I4 empty)) ;; create first invader(check-expect (next-invader (make-game empty empty (make-tank (/ WIDTH 2) 0) 420)) (cons (make-invader (- (/ WIDTH 2) 20) 0 1) (cons (make-invader (+ (/ WIDTH 2) 20) 0 1) empty))) ;; create two invader(check-expect (next-invader               (make-game (cons I4 empty) empty (make-tank (/ WIDTH 2) 0) 160))              (cons (make-invader (+ (/ WIDTH 2) (* INVADER-X-SPEED 1)) (+ 0 INVADER-Y-SPEED) 1) empty)) ;; move invader(check-expect (next-invader               (make-game (cons (make-invader (+ (/ WIDTH 2) 30) 20 -1) (cons I4 empty)) empty (make-tank (/ WIDTH 2) 0) 160))              (cons (make-invader (+ (+ (/ WIDTH 2) 30) (* INVADER-X-SPEED -1) ) (+ 20 INVADER-Y-SPEED) -1) (cons (make-invader (+ (/ WIDTH 2) (* INVADER-X-SPEED 1) ) (+ 0 INVADER-Y-SPEED) 1) empty))) ;; move invaders(check-expect (next-invader               (make-game (cons (make-invader 0 0 -1) empty) empty (make-tank (/ WIDTH 2) 0) 160))              (cons (make-invader (+ 0 (* INVADER-X-SPEED 1)) (+ 0 INVADER-Y-SPEED) 1) empty)) ;; bounce invader to right(check-expect (next-invader               (make-game (cons (make-invader WIDTH 0 1) empty) empty (make-tank (/ WIDTH 2) 0) 160))              (cons (make-invader (+ WIDTH (* INVADER-X-SPEED -1)) (+ 0 INVADER-Y-SPEED) -1) empty)) ;; bounce invader to left(check-expect (next-invader               (make-game (cons (make-invader (/ WIDTH 2) 50 1) empty) (cons (make-missile (/ WIDTH 2) 50) empty) (make-tank (/ WIDTH 2) 0) 160))              empty);; hit invader exactly(check-expect (next-invader               (make-game (cons (make-invader (/ WIDTH 2) 50 1) empty) (cons (make-missile (/ WIDTH 2) (- 50 HIT-RANGE)) empty) (make-tank (/ WIDTH 2) 0) 160))              empty);; hit invader slightly-y(check-expect (next-invader               (make-game (cons (make-invader (/ WIDTH 2) 50 1) empty) (cons (make-missile (+ (/ WIDTH 2) HIT-RANGE) 50) empty) (make-tank (/ WIDTH 2) 0) 160))              empty);; hit invader slightly-x;; &lt;Template from game&gt;(define (next-invader s)  (generate-invader   (move-invader    (destroy-invader     (game-invaders s) (game-missiles s)))   (game-time s)));; destroy-invader;; ListOfInvader ListOfMissile -&gt; ListOfInvader;; destroy hitted invaders; (define (destroy-invader loi lom) empty) ;stub;; Tests(check-expect (destroy-invader empty empty) empty) ; default(check-expect (destroy-invader (cons I4 empty) (cons (make-missile (/ WIDTH 2) 250) empty)) (cons I4 empty)) ; nothing(check-expect (destroy-invader (cons I4 empty) (cons (make-missile (/ WIDTH 2) 0) empty)) empty) ; hitted(check-expect (destroy-invader (cons I4 empty) (cons (make-missile (/ WIDTH 2) HIT-RANGE) empty)) empty) ; hitted2(check-expect (destroy-invader (cons I4 empty) (cons (make-missile (- (/ WIDTH 2) HIT-RANGE) 0) empty)) empty) ; hitted x direction(define (destroy-invader loi lom)  (cond [(or (empty? loi) (empty? lom)) loi]        [else (if (check-missile-list-collision lom (first loi))                  (destroy-invader (rest loi) lom)                  (cons (first loi) (destroy-invader (rest loi) lom)))]));; Template rules used:;; - one of: 2 cases;;   - empty;;   - (cons invader loi);; - reference: check-missile-list-collision;; check-missile-list-collision;; ListOfMissile invader -&gt; boolean;; check list of missile hit given invader.; (define (check-missile-list-collision lom i) false) ; stub;; Tests:(check-expect (check-missile-list-collision empty I4) false) ; no missiles(check-expect (check-missile-list-collision (cons (make-missile 200 200) empty) I4) false) ; no hit(check-expect (check-missile-list-collision (cons (make-missile (/ WIDTH 2) 0) empty) I4) true) ; hit(define (check-missile-list-collision lom i)  (cond [(empty? lom) false]        [else (if (check-collision                   (missile-x (first lom)) (missile-y (first lom)) (invader-x i) (invader-y i) HIT-RANGE)                  true                  (check-missile-list-collision (rest lom) i))]));; Template rules used:;; - compound: 2 fields;;   - one of: 2 cases;;     - atomic-distinct: empty;;     - (cons missile listOfMissile);;   - invader: 2fields;; - self-reference: lom has type ListOfMissile;; check-collision;; number number number number number -&gt; boolean;; Check given position x1, y1, x2, y2 are collided within hit range.;; (define (check-collision x1 y1 x2 y2 r) false) ; stub(check-expect (check-collision 0 0 10 10 10) false); out of hit range(check-expect (check-collision 0 0 5 5 10) true); collided(check-expect (check-collision 0 0 10 0 10) true); edge case(check-expect (check-collision 0 0 3 4 5) true); edge case 2(check-expect (check-collision 0 0 0 0 1) true)(define (check-collision x1 y1 x2 y2 r)  (if (&gt;= (* r r) (+(*(- x2 x1)(- x2 x1)) (*(- y2 y1)(- y2 y1))))      true      false));; generate-invader;; ListOfInvader number -&gt; ListOfInvader;; generate invader by time.; (define (generate-invader loi t) empty) ;stub;;Tests(check-expect (generate-invader empty 140) (cons I4 empty)) ;; create first ufo(check-expect (generate-invader empty 420) (cons (make-invader (- (/ WIDTH 2) 20) 0 1) (cons (make-invader (+ (/ WIDTH 2) 20) 0 1) empty))) ;; create two ufos(check-expect (generate-invader (cons (make-invader (/ WIDTH 2) 100 1) empty) 280) (cons I4 (cons (make-invader (/ WIDTH 2) 100 1) empty))) ;; create duplicate ufo(define (generate-invader loi t)  (cond [(and (&gt; t 0) (= (modulo t 420) 0)) (cons (make-invader (- (/ WIDTH 2) 20) 0 1) (cons (make-invader (+ (/ WIDTH 2) 20) 0 1) loi))]        [(and (&gt; t 0) (= (modulo t 140) 0)) (cons I4 loi)]        [else loi]));; Template rules used:;; - interval t;; - compound: 2cases;;   - atomic distinct: empty;;   - (cons invader loi);; - self-reference: loi;; move-invader;; ListOfInvader -&gt; ListOfInvader;; move all the invader in the invader list.;; (define (move-invader loi) empty);stub;;Tests:(check-expect (move-invader empty) empty)(check-expect (move-invader (cons (make-invader (/ WIDTH 2) 50 1) empty)) (cons (make-invader (+ (/ WIDTH 2) (* INVADER-X-SPEED 1)) (+ 50 INVADER-Y-SPEED) 1) empty))  ;; move invader(check-expect (move-invader (cons (make-invader (+ 20 (/ WIDTH 2)) 100 -1) (cons (make-invader (/ WIDTH 2) 50 1) empty))) (cons (make-invader (+ (+ 20 (/ WIDTH 2)) (* INVADER-X-SPEED -1)) (+ 100 INVADER-Y-SPEED)-1) (cons (make-invader (+ (/ WIDTH 2) (* INVADER-X-SPEED 1)) (+ 50 INVADER-Y-SPEED) 1) empty)))  ;; move invaders(check-expect (move-invader (cons (make-invader 0 50 -1) empty)) (cons (make-invader (+ 0 (* INVADER-X-SPEED 1)) (+ 50 INVADER-Y-SPEED) 1) empty)) ; bounce to right(check-expect (move-invader (cons (make-invader WIDTH 50 1) empty)) (cons (make-invader (+ WIDTH (* INVADER-X-SPEED -1)) (+ 50 INVADER-Y-SPEED) -1) empty)) ; bounce to left;; &lt;Template from move-missile&gt;(define (move-invader i)  (cond [(empty? i) empty]        [(over-x-edge? (next-invader-x-pos (invader-x (first i)) (invader-dir (first i))))         (cons (bounce (first i)) (move-invader (rest i)))]        [else (cons (make-invader (next-invader-x-pos (invader-x (first i)) (invader-dir (first i))) (+ (invader-y (first i)) INVADER-Y-SPEED) (invader-dir (first i)))                    (move-invader (rest i)))]));; bounce;; invader -&gt; invader;; make invader bounce; (define (bounce i) i) ;stub;; Tests:(check-expect (bounce (make-invader 0 0 -1)) (make-invader (+ 0 (* INVADER-X-SPEED 1)) (+ 0 INVADER-Y-SPEED) 1))(check-expect (bounce (make-invader WIDTH 0 1)) (make-invader (+ WIDTH (* INVADER-X-SPEED -1)) (+ 0 INVADER-Y-SPEED) -1))(check-expect (bounce (make-invader (/ INVADER-X-SPEED 2) 0 -1)) (make-invader (- (* INVADER-X-SPEED 1) (/ INVADER-X-SPEED 2)) (+ 0 INVADER-Y-SPEED) 1))(check-expect (bounce (make-invader (- WIDTH (/ INVADER-X-SPEED 2)) 0 1)) (make-invader (- WIDTH (- INVADER-X-SPEED (/ INVADER-X-SPEED 2))) (+ 0 INVADER-Y-SPEED) -1))(define (bounce i) (make-invader (get-bounced-x (invader-x i)) (+ (invader-y i) INVADER-Y-SPEED) (* (invader-dir i) -1)));; get-bounced-x;; number -&gt; number;; get x position after bounced.; (define (get-bounced-x n) n);stub;; ASSUME: position number x or WIDTH - x should be less than INVADER-X-SPEED;; Tests:(check-expect (get-bounced-x 0) INVADER-X-SPEED)(check-expect (get-bounced-x WIDTH) (- WIDTH INVADER-X-SPEED))(check-expect (get-bounced-x (/ INVADER-X-SPEED 2)) (/ INVADER-X-SPEED 2))(check-expect (get-bounced-x (- WIDTH (/ INVADER-X-SPEED 2))) (- WIDTH (/ INVADER-X-SPEED 2)))(define (get-bounced-x n) (if (&gt; INVADER-X-SPEED n)                              (- INVADER-X-SPEED n)                              (- WIDTH (- INVADER-X-SPEED (- WIDTH n)))));; next-invader-x-pos;; number number -&gt; number;; get next invader x postion to given direction.; (define (next-invader-x-pos x d) x) ;stub(define (next-invader-x-pos x d) (+ x (* INVADER-X-SPEED d)));; skip verbose jobs with short func.;; next-missile;; game -&gt; ListOfInvader;; get next missile status from game status.; (define (next-missile s) empty);stub;;Tests:(check-expect (next-missile (make-game empty empty (make-tank (/ WIDTH 2) 0) 0)) empty) ;; default(check-expect (next-missile               (make-game empty (cons (make-missile (/ WIDTH 2) 50) empty) (make-tank (/ WIDTH 2) 0) 160))              (cons (make-missile (/ WIDTH 2) (- 50 MISSILE-SPEED)) empty));; move missile(check-expect (next-missile               (make-game empty (cons (make-missile (+ (/ WIDTH 2) 20) 100) (cons (make-missile (/ WIDTH 2) 50) empty)) (make-tank (/ WIDTH 2) 0) 160))              (cons (make-missile (+ (/ WIDTH 2) 20) (- 100 MISSILE-SPEED)) (cons (make-missile (/ WIDTH 2) (- 50 MISSILE-SPEED)) empty))) ;; move missles(check-expect (next-missile               (make-game (cons (make-invader (/ WIDTH 2) 50 1) empty) (cons (make-missile (/ WIDTH 2) 50) empty) (make-tank (/ WIDTH 2) 0) 160))              empty);; hit invader exactly(check-expect (next-missile               (make-game (cons (make-invader (/ WIDTH 2) 50 1) empty) (cons (make-missile (/ WIDTH 2) (- 50 HIT-RANGE)) empty) (make-tank (/ WIDTH 2) 0) 160))              empty);; hit invader slightly-y(check-expect (next-missile               (make-game (cons (make-invader (/ WIDTH 2) 50 1) empty) (cons (make-missile (+ (/ WIDTH 2) HIT-RANGE) 50) empty) (make-tank (/ WIDTH 2) 0) 160))              empty);; hit invader slightly-x;; &lt;Template from next-invader&gt;(define (next-missile s)  (move-missile   (destroy-missile    (game-missiles s) (game-invaders s))));; destroy-missile;; ListOfMissile ListOfInvader -&gt; ListOfMissile;; destroy hitted missile; (define (destroy-missile loi lom) empty) ;stub;; &lt;Tests From destroy-invader&gt;(check-expect (destroy-missile empty empty) empty) ; default(check-expect (destroy-missile (cons (make-missile (/ WIDTH 2) 250) empty) (cons I4 empty)) (cons (make-missile (/ WIDTH 2) 250) empty)) ; nothing(check-expect (destroy-missile (cons (make-missile (/ WIDTH 2) 0) empty) (cons I4 empty)) empty) ; hitted(check-expect (destroy-missile (cons (make-missile (/ WIDTH 2) HIT-RANGE) empty) (cons I4 empty)) empty) ; hitted2(check-expect (destroy-missile (cons (make-missile (- (/ WIDTH 2) HIT-RANGE) 0) empty) (cons I4 empty)) empty) ; hitted x direction;; &lt;Template from destroy-missile&gt;(define (destroy-missile lom loi)  (cond [(or (empty? lom) (empty? loi)) lom]        [else (if (check-invader-list-collision loi (first lom))                  (destroy-missile (rest lom) loi)                  (cons (first lom) (destroy-missile (rest lom) loi)))]));; check-invader-list-collision;; ListOfInvader Missile -&gt; boolean;; check list of Invader hit given Missile.; (define (check-invader-list-collision lom i) false) ; stub;; &lt;Tests From check-missile-list-collision&gt;(check-expect (check-invader-list-collision empty (make-missile 200 200)) false) ; no invader(check-expect (check-invader-list-collision (cons I4 empty) (make-missile 200 200)) false) ; no hit(check-expect (check-invader-list-collision (cons (make-invader 200 190 1) empty) (make-missile 200 200)) true) ; hit;; &lt;Template from check-missile-list-collision&gt;(define (check-invader-list-collision loi m)  (cond [(empty? loi) false]        [else (if (check-collision (invader-x (first loi)) (invader-y (first loi)) (missile-x m) (missile-y m) HIT-RANGE)                  true                  (check-invader-list-collision (rest loi) m))]));; move-missile;; ListOfMissile -&gt; ListOfMissle;; move all the missiles in the Missile list.; (define (next-missile m) m) ; stub;;Tests:(check-expect (move-missile empty) empty)(check-expect (move-missile (cons (make-missile 0 HEIGHT) empty)) (cons (make-missile 0 (- HEIGHT MISSILE-SPEED)) empty)) (check-expect (move-missile (cons (make-missile 0 0) empty)) empty)(define (move-missile m)  (cond [(empty? m) empty]        [(over-y-edge? (next-missile-y-pos (missile-y (first m)))) (move-missile (rest m))] ;; contain help function logic for convinience.        [else (cons (make-missile (missile-x (first m)) (next-missile-y-pos (missile-y (first m)))) (move-missile (rest m)))]));; over-y-edge?;; number -&gt; boolean;; check if y position is over the edge.; (define (over-y-edge? x) false) ; stub;;Tests:(check-expect (over-y-edge? (+ HEIGHT 1)) true)(check-expect (over-y-edge? (- HEIGHT 1)) false)(check-expect (over-y-edge? -1) true)(check-expect (over-y-edge? 1) false)(check-expect (over-y-edge? 0) false)(check-expect (over-y-edge? HEIGHT) false)(define (over-y-edge? y) (or (&gt; y HEIGHT) (&lt; y 0)));; Template rules used:;;  - atomic nondistinct : y;; next-missile-y-pos;; number -&gt; number;; check if y position is over the edge.; (define (over-y-edge? x) false) ; stub;;Tests:(check-expect (next-missile-y-pos 0) (- 0 MISSILE-SPEED))(check-expect (next-missile-y-pos HEIGHT) (- HEIGHT MISSILE-SPEED))(check-expect (next-missile-y-pos (/ HEIGHT 2)) (- (/ HEIGHT 2) MISSILE-SPEED))(define (next-missile-y-pos y) (- y MISSILE-SPEED));; Template rules used:;;  - atomic nondistinct : y(define TANK-Y (- HEIGHT 15));; game -&gt; Image;; render screen by game status; (define (render-game game) BACKGROUND) ; stub;; TESTS:(check-expect (render-game G0) (place-image TANK (/ WIDTH 2) TANK-Y BACKGROUND)) ;; default(check-expect (render-game G1) (place-image TANK 50 TANK-Y BACKGROUND)) ;; moved TANK(check-expect (render-game G2) (place-image MISSILE (missile-x M1) (missile-y M1) (place-image INVADER (invader-x I1) (invader-y I1)(place-image TANK 50 TANK-Y BACKGROUND)))) ;; bullet and single invader with tank.(check-expect (render-game G3) (place-image MISSILE (missile-x M2) (missile-y M2) (place-image INVADER (invader-x I2) (invader-y I2) (place-image MISSILE (missile-x M1) (missile-y M1)(place-image INVADER (invader-x I1) (invader-y I1)(place-image TANK 50 TANK-Y BACKGROUND)))))) ;; two bullets and two invaders with tank.;; &lt;Template from game&gt;(define (render-game s)  (place-missile   (game-missiles s) (place-invader                      (game-invaders s) (place-tank                                         (game-tank s) BACKGROUND))));; place-invader;; ListOfInvader IMG -&gt; IMG;; draw invaders in the screen.; (define (place-invader loi img) BACKGROUND) ;stub;; Tests:(check-expect (place-invader empty BACKGROUND) BACKGROUND) ;; nothing(check-expect (place-invader (cons I1 empty) BACKGROUND) (place-image INVADER (invader-x I1) (invader-y I1) BACKGROUND)) ;; a invader(check-expect (place-invader (cons I2 (cons I1 empty)) BACKGROUND) (place-image INVADER (invader-x I2) (invader-y I2) (place-image INVADER (invader-x I1) (invader-y I1) BACKGROUND))) ;; two invaders;; Template:(define (place-invader loi img)  (cond [(empty? loi) img]        [else (place-invader (rest loi) (place-image INVADER (invader-x (first loi)) (invader-y (first loi)) img))]));; Template rules used:;; - compound:;;   - one of two cases:;;     - empty;;     - (cons invader loi);;   - Image;; place-missile;; ListOfMissile IMG -&gt; IMG;; draw missiles in the screen.; (define (place-missile loi img) BACKGROUND) ;stub;; Tests:(check-expect (place-missile empty BACKGROUND) BACKGROUND) ;; nothing(check-expect (place-missile (cons M1 empty) BACKGROUND) (place-image MISSILE (missile-x M1) (missile-y M1) BACKGROUND)) ;; a missile(check-expect (place-missile (cons M2 (cons M1 empty)) BACKGROUND) (place-image MISSILE (missile-x M2) (missile-y M2) (place-image MISSILE (missile-x M1) (missile-y M1) BACKGROUND))) ;; two missiles;; &lt;Template FROM place-invader&gt;(define (place-missile lom img)  (cond [(empty? lom) img]        [else (place-missile (rest lom) (place-image MISSILE (missile-x (first lom)) (missile-y (first lom)) img))]));; place-tank;; Tank IMG -&gt; IMG;; draw tank in the screen.;(define (place-tank tank img) BACKGROUND) ;stub;; Tests:(check-expect (place-tank T0 BACKGROUND) (place-image TANK (tank-x T0) TANK-Y BACKGROUND)) ;; a tank(check-expect (place-tank T1 BACKGROUND) (place-image TANK (tank-x T1) TANK-Y BACKGROUND)) ;; tank moved;; &lt;Template FROM place-invader&gt;(define (place-tank tank img)  (place-image TANK (tank-x tank) TANK-Y img));; game e -&gt; game;; produce the new game status when key inputted. ; (define (key-game game e) G0) ; stub;; Tests:(check-expect (key-game G0 \"e\") (make-game empty empty (game-tank G0) 0)) ; nothing changed(check-expect (key-game G0 \" \") (make-game empty (cons (make-missile (tank-x (game-tank G0)) TANK-Y) empty) (game-tank G0) 0)) ; shoot once(check-expect (key-game G0 \"left\") (make-game empty empty (make-tank (tank-x (game-tank G0)) -1) 0)) ; go left(check-expect (key-game G0 \"right\") (make-game empty empty (make-tank (tank-x (game-tank G0)) 1) 0)) ; go right;; &lt;Template from handle-key&gt;(define (key-game g e)  (cond [(key=? e \" \") (shot g)]        [(key=? e \"left\") (go-left g)]        [(key=? e \"right\") (go-right g)]        [else g]));; shot;; game -&gt; game;; Create missile at the tank position.;(define (shot g) g) ;stub;; Tests :(check-expect (shot G0) (make-game empty (cons (make-missile (tank-x (game-tank G0)) TANK-Y) empty) (game-tank G0) 0));;&lt;Template from game&gt;(define (shot g)  (make-game (game-invaders g) (cons (make-missile (tank-x (game-tank g)) TANK-Y) (game-missiles g)) (game-tank g) (game-time g)));; go-left;; game -&gt; game;; make tank go left, change direction to left.;(define (go-left g) g) ;stub;; Tests : (check-expect (go-left G0) (make-game empty empty (make-tank (tank-x (game-tank G0)) -1) 0)) ; go left;;&lt;Template from game&gt;(define (go-left g)  (make-game (game-invaders g) (game-missiles g) (make-tank (tank-x (game-tank g)) -1) (game-time g)));; go-right;; game -&gt; game;; make tank go right, change direction to right.;(define (go-right g) g) ;stub;; Tests : (check-expect (go-right G0) (make-game empty empty (make-tank (tank-x (game-tank G0)) 1) 0)) ; go right;;&lt;Template from game&gt;(define (go-right g)  (make-game (game-invaders g) (game-missiles g) (make-tank (tank-x (game-tank g)) 1) (game-time g)))소감  강의에서 배운 설계법을 엄격히 지키면서 진행했다. 처음에는 어색했지만 이윽고 함수 설계와 테스트 설계에 아주 익숙해졌다.  설계와 TDD의 위력을 알 수 있었다. 버그가 극도로 적고, 문서화가 동시에 진행됬으며, 프로그램에 대한 설계가 눈에 쉽게 들어왔다.          모든 함수의 테스트를 완전히 지키며 코딩하였더니, 마지막으로 이들을 조립하여 완벽한 프로그램을 완성한 뒤의 디버깅은 단 하나의 버그, 30초만에 해결됬다.      원래 였다면 수많은 버그들의 시너지로 우왕좌왕했을 것이다.        현세대의 프로그래밍 언어와 IDE가 너무나도 그리웠다.          비직관적이고 생소한 BSL                  덧셈 조차 전위 방식으로 짜야 했다. (ex) 1 + 2 (X), (+ 1 2) (O))          함수 내에 변수 할당 및 재활용 없음          랜덤한 요소가 있는 함수는 어떻게 테스팅해야하는가?                    별 기능을 지원하지 않는 Dr.Racket                  AutoComplete 기능 없음          함수 참조 확인 기능 없음          패키지 분할은 적어도, 이번 프로젝트에서 사용하지 않게 막아놔서 한 파일에 전부 작성          버그 표시 및 오타 지정, 사용하지 않는 상수 등의 표시 없음                      기존의 게임 엔진들의 기초적인 기능들이 얼마나 좋은 기능인지 알 수 있었다.          랜더링부터 스프라이트 충돌 설정까지 제로부터 전부 직접 구현해야 했다.      물론, 덕분에 아주 뿌듯했다.        넉넉하게 10~12시간 정도 걸린것 같은데, 좀더 숙달되고, 좋은 IDE와 현세대 프로그래밍 언어로 구현하면 절반 이하로도 구현할 수 있을 것 같다.          하지만 만약, 내가 이 정도의 프로그램을 기존처럼 설계없이 코드했다면 두배의 시간이 들었을 것 같다.        앞으로 알고리즘 문제나 다른 경우에도 이 설계법을 도입하고 싶고, 다음 강의가 너무 기대된다."
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-MVC%20%EA%B0%9C%EB%85%90%EA%B3%BC%20%EC%84%A4%EC%A0%95.html": {
    title: "Spring5 입문-MVC 개념과 설정",
    date: " Jan 20, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-MVC%20%EA%B0%9C%EB%85%90%EA%B3%BC%20%EC%84%A4%EC%A0%95.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-MVC 개념과 설정title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.MVC 프로젝트 생성스프링 MVC 프레임워크를 이용하기 위해서는 기존 프로젝트 생성에서 다음과 같은 점이 다르다.  파일 구조 : /webapp/ 폴더 내부에 HTML, CSS, JS, JSP 등 웹 앱을 위한 코드가 위치하게 된다.          src/main/webapp/WEB-INF 폴더 생성 :  원래 서블릿에서 필요한 라이브러리와 모듈이 위치하지만 의존 관리 툴을 이용한다면 자동으로 빌드시 처리해준다.      src/main/webapp/WEB-INF/view 폴더 : 내부에 페이지 뷰 부분을 생성할 .jsp 파일들이 위치하게 된다.      src/main/webapp/WEB-INF/web.xml 파일 : 웹 요청 처리를 위한 서블릿 설정 아래 참조      title: web.xml 예시사용할 서블릿 클랫, 설정 클래스 위치, 인코딩 설정 및 초기값 설정 등에 사용됨.&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- 웹 요청 처리를 하는 servelet 등록 --&gt;&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"    xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee             http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\"    version=\"3.1\"&gt;      &lt;servlet&gt;        &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;!--dispatcher명 등록--&gt;        &lt;servlet-class&gt;&lt;!--DispatcherServlet 등록--&gt;            org.springframework.web.servlet.DispatcherServlet        &lt;/servlet-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextClass&lt;/param-name&gt;            &lt;param-value&gt;                org.springframework.web.context.support.AnnotationConfigWebApplicationContext            &lt;/param-value&gt;&lt;!--설정을 자바 클래스를 통해 함을 의미--&gt;        &lt;/init-param&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;            &lt;param-value&gt;&lt;!--스프링 컨테이너 초기화에 이용할 설정 파일 정해주기--&gt;                config.MvcConfig                config.ControllerConfig            &lt;/param-value&gt;        &lt;/init-param&gt;        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;!--톰켓 컨테이너 구동 시 이 서블릿 함께 시작 true--&gt;    &lt;/servlet&gt;    &lt;servlet-mapping&gt;&lt;!--모든 요청을 기본적으로 이 서블릿이 처리하도록 매핑(\"/\")--&gt;        &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt;        &lt;url-pattern&gt;/&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;      &lt;filter&gt;&lt;!--인코딩 설정--&gt;        &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt;        &lt;filter-class&gt;            org.springframework.web.filter.CharacterEncodingFilter        &lt;/filter-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;encoding&lt;/param-name&gt;            &lt;param-value&gt;UTF-8&lt;/param-value&gt;        &lt;/init-param&gt;    &lt;/filter&gt;    &lt;filter-mapping&gt;        &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt;        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;    &lt;/filter-mapping&gt;&lt;/web-app&gt;  의존 관리 툴 설정 및 의존 모듈(pom.xml || build.gradle)          war 패키지 사용 설정 : 서블릿/JSP를 이용한 웹 어플리케이션 개발 시 필요한 패키징 형식      javax.servlet-api 혹은 jakarta.servlet-api : 서블릿을 이용하기 위한 모듈, 톰캣 버전에 따라 택일                  서블릿? : 웹 서버 내부에 요청을 받고 동적으로 응답해주는 자바 컴포넌트                    javax.servlet.jsp-api :  jsp를 사용하기 위한 모듈                  JSP? : HTML 코드 내부에 자바 코드를 이용해 동적 웹 페이지를 생성하는 웹 앱 도구                    jstl : JSP 내부의 탬플릿 언어를 더 읽기 쉬운 JSTL로 바꿈      spring-webmvc : MVC 프레임워크 웹 개발을 위한 모듈      title: pom.xml 예시&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0        http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;sp5&lt;/groupId&gt;    &lt;artifactId&gt;sp5-chap09&lt;/artifactId&gt;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;    &lt;packaging&gt;war&lt;/packaging&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;javax.servlet&lt;/groupId&gt;            &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt;            &lt;version&gt;3.1.0&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt;            &lt;artifactId&gt;javax.servlet.jsp-api&lt;/artifactId&gt;            &lt;version&gt;2.3.2-b02&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;javax.servlet&lt;/groupId&gt;            &lt;artifactId&gt;jstl&lt;/artifactId&gt;            &lt;version&gt;1.2&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;            &lt;version&gt;5.0.2.RELEASE&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.7.0&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                    &lt;encoding&gt;utf-8&lt;/encoding&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;title: build.gradle 예시apply plugin: 'java'apply plugin: 'war'sourceCompatibility = 17.0 // 현재 프로젝트 자바 버전targetCompatibility = 17.0 // 디플로이 시 자바 버전compileJava.options.encoding = \"UTF-8\"repositories {\tmavenCentral()}dependencies {\tprovidedCompile 'jakarta.servlet:jakarta.servlet-api:6.0.0'\t// providedCompile 'javax.servlet:javax.servlet-api:3.1.0'\tprovidedRuntime 'javax.servlet.jsp:javax.servlet.jsp-api:2.3.3'\timplementation 'javax.servlet:jstl:1.2'\timplementation 'org.springframework:spring-webmvc:6.0.4'}wrapper {\tgradleVersion = '7.6'}  이클립스 톰캣 설정          tomcat 웹서버 설치 및 이클립스 적용                  책에서는 8.0~9.0 버전을 추천하고 8.5 버전을 이용했지만, 난 10.0 버전을 사용함          Window-&gt;Preferences-&gt;Server-&gt;Runtime Environment 에서 Add 버튼으로 Apache tomcat의 설치 폴더를 지정해주면 된다.                    MVC 패턴MVC(Model-View-Controller) 패턴은 요청에 따라 컨트롤러가 사용자가 보는 뷰와 데이터가 담긴 모델을 조작하여 웹 서비스를 구성하는 방법이다.스프링의 MVC 패턴은 아래 그림과 구성 요소로 이루어진다.title: 스프링의 MVC 패턴검정색은 필수적으로 개발자가 구현해야 하는 컴포넌트, 나머지는 선택적으로 구현할 수 있는 컴포넌트이다.&lt;&lt;spring bean&gt;&gt;은 빈 객체로 생성되는 컴포넌트이다.DispatcherServlet 을 중심으로 HandlerMapping, HandlerAdapter, 컨트롤러, ViewResolver, View, JSP가 각자 역할을 수행해서 클라이언트 요청을 처리하는 구조이다.요청 과정 예시1. 요청 전송 과정클라이언트 브라우저 측에서 서버 측에 /프로젝트명/hello?name=rv URL로 Get 요청을 보내는 것을 가정하겠다.이를 앞선 web.xml 설정에 따라 생성된 DispatcherServlet가 받아들인다.title: DispatcherServlet  위 그림의 중앙에 위치한 DispatcherServlet은 MVC 구성요소의 핵심 요소로, 다른 컴포넌트 간의 요청을 받아 넘겨주는 중앙 창구 역할을 한다.  추가로, DispatcherServlet은 스프링 컨테이너를 생성하고, 해당 스프링 컨테이너는 web.xml 설정에 따른 HandlerMapping, HandlerAdapter, 컨트롤러 빈, ViewResolver 빈 객체를 생성한다.앞으로도 자주 나올 것이다.2. 요청 URL과 매칭되는 컨트롤러 검색DispatcherServlet는 해당 요청을 처리하는 컨트롤러를 직접 구하지 않고, HandlerMapping 이라는 빈 객체에게 컨트롤러를 알아오도록 시킨다.HandlerMapping은 /hello 경로로 등록된 컨트롤러 빈을 알아와 DispatcherServlet에게 리턴한다.  등록되지 않은 경로의 처리는 기본 핸들러와 HandlerMapping 우선 순위 참조title: 컨트롤러 구현 예시(main/java/chap09/HelloController.java)package chap09;// 클라리언트 요청 처리용 컨트롤러 파일import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;@Controller // 스프링 MVC 컨트롤러 클래스로 설정public class HelloController {\t@GetMapping(\"/hello\") // 메서드가 처리할 요청 경로 지정, Get 요청 한정\tpublic String hello(Model model, // 처리 결과를 뷰에 전달시 사용\t\t\t@RequestParam(value = \"name\", required = false) String name) { // HTTP 요청 파라미터 값 변환\t\tmodel.addAttribute(\"greeting\", \"안녕하세요, \" + name);//모델의 greeting 속성값을 설정\t\treturn \"hello\"; //처리 결과 뷰를 설정\t}}  @GetMapping에 적힌 내용에 따라 HandlerMapping이 검색 가능하다.  위 컨트롤러는 결과값으로 처리에 필요한 뷰 이름을 문자열로 돌려주고, 입력받은 Model 객체에 greeting 속성 값을 저장한다.  위 컨트롤러를 ModelAndView로 변환하는 것은 HandlerAdapter가 처리해야 한다.title: 컨트롤러 등록(main/java/config/ControllerConfig.java)package config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import chap09.HelloController;@Configurationpublic class ControllerConfig {\t@Bean // 컨트롤러 등록\tpublic HelloController helloController() {\t\treturn new HelloController();\t}}3. 처리 요청 ~ 6. 컨트롤러 실행 결과 ModelAndView로 변환 리턴DispatcherServlet는 받은 컨트롤러를 직접 실행하지 않고, HandlerAdapter 이라는 빈 객체에게 요청 처리를 위임한다.HandlerAdapter는 컨트롤러에 맞는 메서드를 요청에 처리하고, ModelAndView 객체로 변환해 돌려준다.title: DispatcherServlet가 직접 컨트롤러를 검색하고, 직접 컨트롤러를 실행하지 않는 이유DispatcherServlet은 컨트롤러의 요청 처리 결과로 ModelAndView 타입만 받는다.하지만 컨트롤러는 구현에 따라 여러 클래스로 존재하므로, 실행한 결과가 모두 ModelAndView가 아니다.따라서 이를 하나의 ModelAndView로 변환해줄 각기 다른 종류의HandlerAdapter가 필요하다.  마찬가지로 각기 다른 컨트롤러 종류를 검색하기 위해 각기 다른 종류의 HandleMapping이 필요하다.따라서, 만약 여러 클래스의 컨트롤러가 존재한다면 해당 타입 컨트롤러를 처리해줄 HandlerAdapter와 HandleMapping을 직접 스프링 빈으로 등록해야 한다.title: 각기 다른 클래스 컨트롤러의 예시다양한 컨트롤러의 예시로 @Controller 어노테이션 사용, 이전 버전에 사용하던 Controller 인터페이스, HttpRequestHandler 인터페이스 구현, 직접 구현한 커스텀 컨트롤러 등이 있다.7.  컨트롤러의 실행 결과를 보여줄 View 검색DispatcherServlet는 해당 ModelAndView를 응답으로 생성할 View 객체를 직접 구하지 않고, ViewResolver 이라는 빈 객체에게 View를 알아오도록 시킨다.ModelAndView 내에는 처리에 필요한 View 객체 이름이 담겨져 있고, 이를 이용해 ViewResolver는 해당 View 객체를 찾거나 생성한 뒤 DispatcherServlet에게 돌려준다. (JSP의 경우는 언제나 새로운 View 객체를 생성 !빈 객체 아님)title: ViewResolver 설정 예시 (main/java/config/MvcConfig.java)package config;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.DefaultServletHandlerConfigurer;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.ViewResolverRegistry;@Configuration@EnableWebMvc // 스프링 MVC 설정 클래스로 활성화public class MvcConfig implements WebMvcConfigurer {\t// ...\t// JSP를 이용해 컨트롤러의 실행 결과 보여주는 설정\t@Override\tpublic void configureViewResolvers(ViewResolverRegistry registry) {\t\tregistry.jsp(\"/WEB-INF/view/\", \".jsp\");\t}}  configureViewResolvers 메소드에서 registry.jsp 함수를 이용해 경로에 존재하는 모든 .jsp 파일을 뷰로 등록한다.8. View 객체에게 응답 생성 요청DispatcherServlet가 View 객체에게 응답 결과 생성을 요청하면 View가 JSP를 실행하여 응답 결과를 생성한다.title: view.jsp 예시 (main/webapp/WEB-INF/view/hello.jsp)&lt;%@ page contentType=\"text/html; charset=utf-8\" %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;\t&lt;head&gt;\t\t&lt;title&gt;Hello&lt;/title&gt;\t&lt;/head&gt;\t&lt;body&gt;\t\t인사말: ${greeting} &lt;!--controller에서 지정한 greeting 출력--&gt;\t&lt;/body&gt;&lt;/html&gt;  사용된 ${greeting}은 앞서 컨트롤러가 모델에 주입한다.  위 JSP 결과물 페이지가 응답 메시지에 담긴다.기본 핸들러와 HandlerMapping 우선 순위앞서 컨트롤러의 @***Mapping설정에 따라 컨트롤러와 뷰가 검색되도록 하였다. 그렇다면 view를 사용하지 않는 정적 파일 url 같은 경우 어떻게 처리해야 할까?이를 처리하기 위해 view를 사용하지 않는 컨트롤러를 직접 구현하거나 아래 예시처럼 기본값 핸들러를 지정해줄 수 있다.  이를 지정해주지 않으면 404 에러 페이지가 뜬다.title: ViewResolver 설정 예시 (main/java/config/MvcConfig.java)package config;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.DefaultServletHandlerConfigurer;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.ViewResolverRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configuration@EnableWebMvc // 스프링 MVC 설정 클래스로 활성화public class MvcConfig implements WebMvcConfigurer {\t@Override // WebMvcConfigurer 인터페이스의 이 메소드를 재정의하면 기본값 핸들러 사용 가능\tpublic void configureDefaultServletHandling( // \"/\" 매핑 설정, ViewResolver 설정 안된 주소에 대한 기본 설정\t\tDefaultServletHandlerConfigurer configurer) {\t\tconfigurer.enable(); \t}\t// JSP를 이용해 컨트롤러의 실행 결과 보여주는 설정\t@Override // WebMvcConfigurer 인터페이스의 이 메소드를 재정의하면 뷰 관련 설정 가능\tpublic void configureViewResolvers(ViewResolverRegistry registry) {\t\tregistry.jsp(\"/WEB-INF/view/\", \".jsp\");\t}}  DefaultServletHandlerConfigurer.enable();를 통해 DefaultServletHttpRequestHandler와 SimpleUrlHandlerMapping 빈이 추가된다.  DefaultServletHttpRequestHandler: 클라이언트의 요청을 WAS가 제공하는 디폴트 서블릿에 전달  SimpleUrlHandlerMapping:  설정된 경로를 특정 서블릿에게 처리하게 가능DefaultServletHandlerConfigurer.enable();로 적용된 SimpleUrlHandlerMapping은 우선순위가 아주 낮아,  설정 경로가 겹치는 경우, 우리가 커스텀으로 설정한 (보통 RequestMappingHandlerMapping) 핸들러가 먼저 처리하게 된다.하지만 아무런 경로가 겹치지 않는다면 DefaultServletHttpRequestHandler가 실행될 것이다.  ex) “/index.html” 경로의 경우 컨트롤러가 등록되어 있지 않으므로 WAS 서버에서 처리MVC 설정 클래스MVC 패턴을 위한 설정 클래스는 WebMvcConfigurer 인터페이스를 상속받아야 한다.title: 참고로 스프링 4.x 버전 이하는 자바가 인터페이스의 디폴트 메서드 기능이 없었으므로, 대신 WebMvcConfigurerAdpater 클래스를 상속해서 필요한 메서드만 재정의했다.해당 인터페이스는 configureDefaultServletHandling, configureViewResolvers를 포함해, configurePathMatch, addFormatters 등의 MVC 패턴 설정 관련 주요 메서드를 정의하고 있다.추가로 @EnableWebMvc를 잉요하면 자동으로 MVC 설정에 필요한 빈을 생성해 준다.@EnableWebMvc를 사용하지않고 모두 직접 설정한다면 아래와 같이 구현할 수 있다.title: @EnableWebMvc를 사용하지 않은 config 클래스package config;import java.util.HashMap;import java.util.Map;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.HttpRequestHandler;import org.springframework.web.servlet.HandlerAdapter;import org.springframework.web.servlet.HandlerMapping;import org.springframework.web.servlet.config.annotation.DefaultServletHandlerConfigurer;import org.springframework.web.servlet.config.annotation.ViewResolverRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import org.springframework.web.servlet.handler.SimpleUrlHandlerMapping;import org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter;import org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter;import org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping;import org.springframework.web.servlet.resource.DefaultServletHttpRequestHandler;import org.springframework.web.servlet.view.InternalResourceViewResolver;  @Configurationpublic class MvcConfig2 implements WebMvcConfigurer {    @Bean // @Controller 객체 처리용 handlerMapping    public HandlerMapping handlerMapping() {        RequestMappingHandlerMapping hm = new RequestMappingHandlerMapping();        hm.setOrder(0);        return hm;    }      @Bean // @Controller 객체 처리용 handlerAdapter    public HandlerAdapter handlerAdapter(){        RequestMappingHandlerAdapter ha = new RequestMappingHandlerAdapter();        return ha;    }    @Bean // 기본값 핸들러 경로 설정(모두)    public HandlerMapping simpleHandlerMapping() {        SimpleUrlHandlerMapping hm = new SimpleUrlHandlerMapping();        Map&lt;String, Object&gt; pathMap = new HashMap&lt;&gt;();        pathMap.put(\"/**\", defaultServletHandler());        hm.setUrlMap(pathMap);        return hm;    }    @Bean // 기본값 핸들러    public HttpRequestHandler defaultServletHandler(){        DefaultServletHttpRequestHandler handler = new DefaultServletHttpRequestHandler();        return handler;    }        @Bean    public HandlerAdapter requestHandlerAdapter() {        HttpRequestHandlerAdapter ha = new HttpRequestHandlerAdapter();        return ha;    }    @Bean    public ViewResolver ViewResolver() {        InternalResourceViewResolver vr = new InternalResourceViewResolver();        vr.setPrefix(\"/WEB-INF/view/\");        vr.setSuffix(\".jsp\");        return vr;    }    // InternalResourceViewResolver : prefix + 뷰이름 + suffix를 조합하는 경로의 jsp 파일을 사용하는 view 객체 리턴}더욱 불편하고 잘안쓰이는 방식이지만, 기본 설정을 알아볼 수 있으므로 눈여겨 보는게 좋다."
  }
  , 
  
  "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%BB%A8%ED%8A%B8%EB%A1%A4%EB%9F%AC%EC%99%80%20%EB%B7%B0%20%EA%B5%AC%ED%98%84.html": {
    title: "Spring5 입문-컨트롤러와 뷰 구현",
    date: " Jan 20, 2023 ",
    url: "/articles/web/backend/Spring/Spring5%20%EC%9E%85%EB%AC%B8-%EC%BB%A8%ED%8A%B8%EB%A1%A4%EB%9F%AC%EC%99%80%20%EB%B7%B0%20%EA%B5%AC%ED%98%84.html",
    tags: ["HIDE","CRUDE"],
    content: "style: numbermin_depth: 2max_depth: 3varied_style: trueSpring5 입문-컨트롤러와 뷰 구현title: 출처_초보 웹 개발자를 위한 스프링 5 프로그래밍 입문(최범균 저, 가메 출판사)_의 내용을 바탕으로 정리한 내용입니다.개발 설정은 대부분 극초기에 끝나며 개발의 대부분은 컨트롤러와 뷰의 구현이다. 컨트롤러는 특정 요청 URL을 처리하며, 뷰는 처리 결과를 HTML과 같은 형식으로 응답한다."
  }
  
  
};
</script>

<script type="module" src="assets/scripts/bundle/search.bundle.js" charset="utf-8"></script>
<script src="assets/scripts/bundle/lunr.bundle.js"></script>
<div id="query-results">
  <h2 class="article-list-heading">Query <span class="query-str"></span> <span class="query-tags"></span> Search
    Results</h2>
  <ul class="article-list">
    <h1>
      🙊 Oops! No result!
    </h1>
  </ul>
</div>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">🧠SUBBRAIN</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="a-name">🧠SUBBRAIN</li><li><a class="u-email" href="mailto:roadvirushn@gmail.com">roadvirushn@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li>
    <a href="https://github.com/RoadVirusHN"><svg class="svg-icon">
        <use xlink:href="/assets/svg/social-icons.svg#github"></use>
      </svg>
      <span class="username">RoadVirusHN</span></a>
  </li><!---->
</ul></div>

      <div class="footer-col footer-col-3">
        <p>이것이 디지털 동물의 숲이다!! 파멸편 (This is the Digital Animal Crossing!! Bad Ending.01)</p>
      </div>
    </div>

  </div>

</footer>
</body>

<script src="/assets/scripts/bundle/common.bundle.js"></script>

</html>