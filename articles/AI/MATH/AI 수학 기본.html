<!DOCTYPE html>
<html lang="kr"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>AI 수학 기본 | The Digital garden of Nurgle.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="AI 수학 기본" />
<meta property="og:locale" content="kr" />
<meta name="description" content="style: number min_depth: 2 max_depth: 3 varied_style: true" />
<meta property="og:description" content="style: number min_depth: 2 max_depth: 3 varied_style: true" />
<link rel="canonical" href="http://localhost:4000/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html" />
<meta property="og:url" content="http://localhost:4000/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html" />
<meta property="og:site_name" content="The Digital garden of Nurgle." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-14T13:41:08+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="AI 수학 기본" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-14T13:41:08+09:00","datePublished":"2022-12-14T13:41:08+09:00","description":"style: number min_depth: 2 max_depth: 3 varied_style: true","headline":"AI 수학 기본","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html"},"url":"http://localhost:4000/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="The Digital garden of Nurgle." /></head>
<div class="scrollWrapper">
  <div class="scrollbar"></div>
  <div class="progressbar"></div>
  <div class="scrollbarButton"></div>
</div>

<link rel="stylesheet" href="/assets/css/obsidian/obs-scrollbar.css" />

<!--<div class="redirection">
  <h1 class="name">Redirection for full experience.</h1>
  <br>
  Move to <br /> <a class="to" href="#">netlify url</a><br />
  <div>after <span class="counter">10</span>secs.</div>
  press <button class="cancle">here</button> to cancle.
</div>
<div class="overlay"></div>
<script type="module" src="/assets/scripts/common/components/init_redirection.js"></script>

<link rel="stylesheet" href="/assets/css/common/redirection.css" />-->

<body><header class="site-header" role="banner">

  <div class="wrapper" style="display: flex; justify-content: space-between;"><div id="header-wrapper">
    <a class="site-title" rel="author" href="/blog">The Digital garden of Nurgle.</a>

    </div><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><script src="https://unpkg.com/lunr/lunr.js"></script>
<link rel="stylesheet" href="/assets/css/common/searchbar.css" />

<form id="search-form" method="get">
  <span id="search-wrapper">
    <span id="tag-holder" ></span>
    <input type="text" id="search-box" placeholder='Prefix "#" to add Tag.' autocomplete="off">
    <span class="inner-search" >🔍</span>
  </span>
</form><a class="page-link" href="/">ABOUT ME</a><a class="page-link" href="/blog">ALL ARTICLES</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
    <link rel="stylesheet" href="/assets/css/common/drawer.css" />
<button class="drawer-button open">▶️</button>
<div id="drawer" class="close">
  <button class="drawer-button close">
    ◀️
  </button>
  <div class="drawer-content">
    <div class="my-description">
      <div class="avatar-section" style="display: flex; flex-direction: row;">

        <img src="/assets/img/common/avatar.png" alt="avatar" class="avatar">
        <div style="display: flex; flex-direction: column; margin-left: 5px;">
          <a href="/about/">
            <h3 class="name">ROADVIRUSHN</h3>
          </a>
          <div class="stack-list" style="margin: 5px 0 0 5px;">
            <a title="My github page" href="https://github.com/RoadVirusHN">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#github"></use>
  </svg>
</a>
<a title="My G-mail" href="mailto:roadvirushn@gmail.com">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#gmail"></use>
  </svg>
</a>
<a title="My Blog" href="https://luminous-bubblegum-8e9be4.netlify.app">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#blog"></use>
  </svg>
</a>
          </div>
        </div>
        <!-- <h4 class="name">(JUNSEOK YUN)</h4> -->
      </div>
      <p style="margin: 5px 0 0 0;">
        풀스택 웹🌐 개발자 지망생 🧑🏽‍💻
        <br>
        ➕ 인공지능 관심 🤖
      </p>
    </div>
      <hr>
      <div class="categories">
        <h3 style="margin: 0;"><a href="/">Categories</a></h3>
        <ul class="category-list">
  
  
  
  <li>
    <strong style="font-size: larger;">┣ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/COMPUTER_SCIENCE/" class="category-drop-down">▶</a>
      
      <span class="category-link">COMPUTER_SCIENCE</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 5
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/DATABASE/">DATABASE</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/ALGORITHM/">ALGORITHM</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 15 
            📂: 1
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/OS/">OS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            📂: 1
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/NETWORK/">NETWORK</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 8 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/ETC/">ETC</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">┣ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/WEB/" class="category-drop-down">▶</a>
      
      <span class="category-link">WEB</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 3
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/FRONTEND/">FRONTEND</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/BACKEND/">BACKEND</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 5 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          ┃  
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/CI,CD/">CI,CD</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            📂: 2
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">┣ </strong>
    <h3 style="display: inline;">
      
      <span class="category-link">ETC</span>
    </h3>
    <span style="font-size: xx-small;">
      📄: 9 
      
    </span>
    <ul class="child-category-list">
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">┗ </strong>
    <h3 style="display: inline;">
      
      <a href="/categories/AI/" class="category-drop-down">▶</a>
      
      <span class="category-link">AI</span>
    </h3>
    <span style="font-size: xx-small;">
       
      📂: 9
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/AITOOLS/">AITOOLS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 3 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/CV/">CV</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/DEEP_LEARNING/">DEEP_LEARNING</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/DATA_VIS/">DATA_VIS</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/GRAPH/">GRAPH</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/LIGHTWEIGHT/">LIGHTWEIGHT</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/MATH/">MATH</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┣ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/NLP/">NLP</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 3 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          ┗ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/STRUCTURED_DATA/">STRUCTURED_DATA</a>
        </h4>
          <span style="font-size: xx-small;">
            📄: 2 
            
          </span>
      </li>
      
    </ul>
  </li>
  
</ul>
      </div>
      <hr>
      <div class="recent-view">
        <h3 style="margin: 0;">Recent views</h3>
        <ul style="margin: 0;">
          <li>
            <strong style="color:rgb(219, 219, 12);">1 <a id="recent-1"></a></strong>
          </li>
          <li>
            2 <a id="recent-2"></a>
          </li>
          <li>
            3 <a id="recent-3"></a>
          </li>
          <li>
            4 <a id="recent-4"></a>
          </li>
          <li>
            5 <a id="recent-5" style="overflow: hidden;"></a>
          </li>
        </ul>
      </div>
    </div>
    <hr>
  <div style="height: 7vh;"></div>
</div>
    <div class="wrapper">
      <article class="article h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="article-header">
    <h1 class="article-title a-name" itemprop="name headline">AI 수학 기본</h1>
    <p class="article-meta">
      <time class="dt-published" datetime="2022-12-14T13:41:08+09:00" itemprop="datePublished">Dec 14, 2022
      </time></p>
  </header>

  <div class="article-content e-content" itemprop="articleBody">
     
  
<script>
  MathJax = {
    tex: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"],
      ],
    },
    svg: {
      fontCache: "global",  
     // scale: 1.5,
    },
    chtml: {
     // scale: 1.5,
    },
  };
</script>
<script
  type="text/javascript"
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>
 
 


<script src="/assets/scripts/bundle/obsidian.bundle.js"></script>
<link rel="stylesheet" href="/assets/css/obsidian/callout.css" />
<link rel="stylesheet" href="/assets/css/obsidian/image.css" />
<link rel="stylesheet" href="/assets/css/obsidian/link-warning.css" />
<link rel="stylesheet" href="/assets/css/obsidian/preview.css" />

<div class="content-section">
  
<ol id='markdown-toc-0'>
<li><a href='#Vector' id='markdown-toc-0-Vector'>Vector</a><ul><li><a href='#벡터란' id='markdown-toc-0-벡터란'>벡터란?</a></li> 
<li><a href='#벡터의-성질' id='markdown-toc-0-벡터의-성질'>벡터의 성질</a></li> 
<li><a href='#벡터-노름-norm-과-기하학적-성질' id='markdown-toc-0-벡터-노름-norm-과-기하학적-성질'>벡터 노름(norm)과 기하학적 성질</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#Matrix' id='markdown-toc-0-Matrix'>Matrix</a><ul><li><a href='#행렬이란' id='markdown-toc-0-행렬이란'>행렬이란?</a></li> 
<li><a href='#행렬의-이해' id='markdown-toc-0-행렬의-이해'>행렬의 이해</a></li> 
<li><a href='#행렬의-곱셈-matrix-multiplication-과-내적' id='markdown-toc-0-행렬의-곱셈-matrix-multiplication-과-내적'>행렬의 곱셈(matrix multiplication)과 내적</a></li> 
<li><a href='#역행렬의-이해' id='markdown-toc-0-역행렬의-이해'>역행렬의 이해</a></li> 
<li><a href='#행렬의-응용' id='markdown-toc-0-행렬의-응용'>행렬의 응용</a></li> 

</ul></li> 
<li><a href='#Gradient-Algorithm-경사하강법' id='markdown-toc-0-Gradient-Algorithm-경사하강법'>Gradient Algorithm(경사하강법)</a><ul><li><a href='#미분-differentiation-이란' id='markdown-toc-0-미분-differentiation-이란'>미분 (differentiation)이란?</a></li> 
<li><a href='#미분-활용' id='markdown-toc-0-미분-활용'>미분 활용</a></li> 
<li><a href='#경사하강법-Gradient-Algorithm-구현' id='markdown-toc-0-경사하강법-Gradient-Algorithm-구현'>경사하강법 (Gradient Algorithm) 구현</a></li> 
<li><a href='#그레디언트-벡터-Gradient-vector-이용한-경사하강법' id='markdown-toc-0-그레디언트-벡터-Gradient-vector-이용한-경사하강법'>그레디언트 벡터 (Gradient vector) 이용한 경사하강법</a></li> 
<li><a href='#경사하강법의-선형-회귀-적용-apply-to-linear-regression' id='markdown-toc-0-경사하강법의-선형-회귀-적용-apply-to-linear-regression'>경사하강법의 선형 회귀 적용 (apply to linear regression)</a></li> 
<li><a href='#확률적-경사하강법-stochastic-gradient-descent' id='markdown-toc-0-확률적-경사하강법-stochastic-gradient-descent'>확률적 경사하강법 (stochastic gradient descent)</a></li> 

</ul></li> 
<li><a href='#인공지능-학습의-수학적-이해' id='markdown-toc-0-인공지능-학습의-수학적-이해'>인공지능 학습의 수학적 이해</a><ul><li><a href='#신경망의-해석' id='markdown-toc-0-신경망의-해석'>신경망의 해석</a></li> 
<li><a href='#소프트맥스-연산' id='markdown-toc-0-소프트맥스-연산'>소프트맥스 연산</a></li> 
<li><a href='#활성화-함수-activation-function-와-다층-신경망-MLP' id='markdown-toc-0-활성화-함수-activation-function-와-다층-신경망-MLP'>활성화 함수(activation function)와 다층 신경망(MLP)</a></li> 
<li><a href='#역전파-backpropagation-알고리즘' id='markdown-toc-0-역전파-backpropagation-알고리즘'>역전파(backpropagation) 알고리즘</a></li> 

</ul></li> 
<li><a href='#확률론' id='markdown-toc-0-확률론'>확률론</a><ul><li><a href='#확률분포' id='markdown-toc-0-확률분포'>확률분포</a></li> 
<li><a href='#기대값' id='markdown-toc-0-기대값'>기대값</a></li> 
<li><a href='#몬테카를로-샘플링-Monte-Carlo-sampling' id='markdown-toc-0-몬테카를로-샘플링-Monte-Carlo-sampling'>몬테카를로 샘플링(Monte Carlo sampling)</a></li> 

</ul></li> 
<li><a href='#통계학' id='markdown-toc-0-통계학'>통계학</a><ul><li><a href='#모수' id='markdown-toc-0-모수'>모수</a></li> 
<li><a href='#데이터-모수-추정' id='markdown-toc-0-데이터-모수-추정'>데이터 모수 추정</a><ul>
</ul></li> 
<li><a href='#최대-가능도-maximum-likelihood-estimation-MLE-추정' id='markdown-toc-0-최대-가능도-maximum-likelihood-estimation-MLE-추정'>최대 가능도(maximum likelihood estimation, MLE) 추정</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#베이즈-통계학' id='markdown-toc-0-베이즈-통계학'>베이즈 통계학</a><ul><li><a href='#조건부-확률' id='markdown-toc-0-조건부-확률'>조건부 확률</a></li> 
<li><a href='#베이즈-정리를-통한-정보의-갱신' id='markdown-toc-0-베이즈-정리를-통한-정보의-갱신'>베이즈 정리를 통한 정보의 갱신</a></li> 
<li><a href='#조건부확률과-인과관계의-차이' id='markdown-toc-0-조건부확률과-인과관계의-차이'>조건부확률과 인과관계의 차이</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#CNN' id='markdown-toc-0-CNN'>CNN</a><ul><li><a href='#CNN-Convolution-Neural-Network-의-이해' id='markdown-toc-0-CNN-Convolution-Neural-Network-의-이해'>CNN(Convolution Neural Network)의 이해</a></li> 
<li><a href='#다차원-CNN-Convolution-Neural-Network-의-이해' id='markdown-toc-0-다차원-CNN-Convolution-Neural-Network-의-이해'>다차원 CNN(Convolution Neural Network)의 이해</a></li> 
<li><a href='#CNN의-역전파' id='markdown-toc-0-CNN의-역전파'>CNN의 역전파</a></li> 

</ul></li> 
<li><a href='#RNN' id='markdown-toc-0-RNN'>RNN</a><ul><li><a href='#시퀀스-sequence-데이터' id='markdown-toc-0-시퀀스-sequence-데이터'>시퀀스(sequence) 데이터</a><ul>
</ul></li> 
<li><a href='#RNN의-이해와-BPTT' id='markdown-toc-0-RNN의-이해와-BPTT'>RNN의 이해와 BPTT</a></li> 

</ul></li> 
</ol>
<h1 id="aimath">AIMath</h1>

<h2 id='Vector'>Vector</h2>

<h3 id='벡터란'>벡터란?</h3>

<ul>
  <li>
    <p><strong>숫자를 원소로 가지는 리스트 또는 배열</strong>을 의미</p>

    <blockquote>
      <p>벡터의 코딩상 의미</p>
    </blockquote>

    <p><img src="/assets/img/AI 수학 기본/1611570207609.png" alt="" /></p>
  </li>
  <li>
    <p>X = 열 벡터, X^T^ = 행 벡터</p>
  </li>
  <li>
    <p>수학적으로는 <strong>공간에서의 한 점, 원점으로 부터의 상대적 위치</strong>를 의미</p>

    <blockquote>
      <p>벡터의 수학적 의미</p>
    </blockquote>
  </li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611570477130.png" alt="" /></p>

<ul>
  <li>각 벡터가 가지는 행과 열의 수를 <strong>벡터의 차원</strong>이라고 한다.</li>
</ul>

<h3 id='벡터의-성질'>벡터의 성질</h3>

<p><strong>1. 벡터에 양수를 곱해주면 방향은 그대로, 길이만 변한다.</strong></p>

<ul>
  <li>
    <p>이 때 곱해주는 숫자를 <strong>스칼라곱(α)</strong>이라고 표현한다.</p>
  </li>
  <li>
    <p>스칼라곱이 음수이면 방향이 정반대 방향이 된다.</p>
  </li>
  <li>
    <p>벡터의 길이는 1보다 크면 길이가 증가, 1보다 작으면 길이가 감소한다.<br />
<strong>2. 같은 모양(같은 행과 열)을 가지면 덧셈, 뺄셈, 곱셈, 나눗셈이 가능하다.</strong><br />
    - 이때의 곱셈을 성분곱(Hadamard product)라고 한다.<br />
    - numpy array에도 적용된다.<br />
<strong>3. 벡터와 벡터의 덧셈과 뺄셈은 다른 벡터로부터 상대적 위치 이동을 표현함.</strong></p>
  </li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611572169045.png" alt="" /></p>

<h3 id='벡터-노름-norm-과-기하학적-성질'>벡터 노름(norm)과 기하학적 성질</h3>

<h4 id='벡터의-노름'>벡터의 노름</h4>

<ul>
  <li>
    <p><strong>벡터의 노름(norm)은 원점에서부터의 거리를 의미</strong></p>

    <ul>
      <li>차원의 수와 관계없이 모든 벡터는 노름을 구할 수 있다.</li>
      <li><strong>L~1~-노름은 각 성분의 변화량의 절대값의 합</strong>을 의미
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>(x,y)는</td>
                  <td>x</td>
                  <td>+</td>
                  <td>y</td>
                  <td>만큼 거리</td>
                </tr>
              </tbody>
            </table>
          </li>
        </ul>
      </li>
      <li><strong>L~2~-노름은 피타고라스 정리를 이용해 유클리드 거리</strong>를 계산
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>(x,y)는 sqrt(</td>
                  <td>x</td>
                  <td>^2^ +</td>
                  <td>y</td>
                  <td>^2^)를 의미</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li><code class="language-plaintext highlighter-rouge">np.linalg.norm</code>으로 구현 가능</li>
        </ul>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td> </td>
              <td>．</td>
              <td> </td>
              <td>기호는 노름이라고 부름</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>

\[\left\| x\right\|_1 = \sum_{i=1}^d|x_i|\\
\left\| x\right\|_2 = \sqrt{\sum_{i=1}^d|x_i|^2}\\\]
  </li>
</ul>

<blockquote>
  <p>백터 노름의 코드 구현</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">l1_norm</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_norm</span>

<span class="k">def</span> <span class="nf">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_norm</span>

</code></pre></div></div>
<h4 id='노름의-활용과-성질'>노름의 활용과 성질</h4>

<ul>
  <li>
    <p>노름의 종류를 무엇으로 적용하느냐에 따라 기하학적 성질이 달라진다.</p>

    <ul>
      <li>L~1~노름은 마름모 모양의 원을 그리며, L~2~노름은 기존의 원 모양의 원을 가진다.</li>
      <li>노름의 종류에 따라 머신러닝에서의 활용도 달라진다.</li>
    </ul>
  </li>
  <li>
    <p>노름을 이용해 <strong>두 벡터 사이의 거리를 뺄셈을 통하여 계산</strong>할 수 있다.</p>

    <p><img src="/assets/img/AI 수학 기본/1611574014374.png" alt="" /></p>

    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td> </td>
              <td>y - x</td>
              <td> </td>
              <td>=</td>
              <td> </td>
              <td>x - y</td>
              <td> </td>
              <td>인점을 이용하게 된다.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>즉 x-y 또는 y-x를 한 원점에서의 좌표를 이용해 L2 norm을 구하면 두벡터 사이의 거리가 나온다.</li>
    </ul>
  </li>
  <li>
    <p><strong>L~2~노름 한정으로 내적을 이용해 이렇게 구한 벡터사이의 거리를 이용해 각도 또한 계산</strong> 가능하다.</p>

    <p><img src="/assets/img/AI 수학 기본/1611574198533.png" alt="" /></p>

    <ul>
      <li><strong>&lt;x, y&gt; 는 내적(inner product)을 의미하며 성분 곱들의 합을 의미한다.</strong>
        <ul>
          <li>예를 들어 x = (0, 1), y = (0, 2)의 내적은  0 * 0 + 1 * 2 = 2 이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>내적을 이용한 각도 계산</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="c1"># np.inner(x, y)가 내적을 구하는 numpy 함수
</span>    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">l2_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">l2_norm</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arccos</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta</span>

</code></pre></div></div>
<h4 id='내적의-해석'>내적의 해석</h4>

<ul>
  <li>
    <p>내적은 <strong>정사영(orthogonal projection)된 벡터의 길이</strong>와 관련 있다.</p>

    <ul>
      <li>Proj(x)는 벡터 y로 정사영된 벡터 x의 그림자를 의미한다.</li>
      <li>Proj(x)의 길이는 코사인법칙에 의해</li>
    </ul>

\[\left\|x\right\|cos\theta\]

    <ul>
      <li>
        <p>가 된다.</p>

        <p><img src="/assets/img/AI 수학 기본/1611574680528.png" alt="" /></p>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>이때 **내적은 정사영의 길이를 벡터 y의 길이</td>
              <td> </td>
              <td>y</td>
              <td> </td>
              <td>만큼 조정한(곱한) 값**이다.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>내적을 이용해 유사도(similarity)를 구할 수 있다.</li>
    </ul>
  </li>
</ul>

<h2 id='Matrix'>Matrix</h2>

<h3 id='행렬이란'>행렬이란?</h3>

<ul>
  <li><strong>벡터를 원소로 가지는 2차원 배열</strong></li>
</ul>

<blockquote>
  <p>행렬의 수식 표현</p>
</blockquote>

\[X = \begin{bmatrix} 1 &amp; -2 &amp; 3 \\ 7 &amp; 5 &amp; 0 \\ -2 &amp; -1 &amp; 2\end{bmatrix}\]

<blockquote>
  <p>행렬의 코드 표현</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span> <span class="c1"># numpy에선 행(row)이 기본단위
</span>
</code></pre></div></div>
<p>\(\boldsymbol{X} = \begin{bmatrix} \boldsymbol{x_{1}} \\ \boldsymbol{x_{2}} \\ \boldsymbol{\vdots}\\ \boldsymbol{x_{n}} \end{bmatrix} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; \dots &amp; x_{1m}\\ x_{21} &amp; x_{22} &amp; \dots &amp; y_{2m}\\ \vdots &amp; \vdots &amp; &amp; \vdots\\ x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{nm}\end{bmatrix}
\begin{aligned}\boldsymbol{x_{1}}\\ \boldsymbol{x_{2}}\\ \boldsymbol{x_{7}}\end{aligned}\)</p>

<ul>
  <li>
    <p>n x m 행렬의 표현</p>
  </li>
  <li>
    <p>행렬은 <strong>행(row)과 열(column)이라는 인덱스(index)</strong>를 가집니다.</p>
  </li>
  <li>
    <p>행렬의 특정 행이나 열을 고정하면 행 벡터 또는 열 벡터라 부른다.</p>
  </li>
  <li>
    <p>전치 행렬(transpose matrix) <strong>X^T^</strong>는 행과 열의 인덱스가 바뀐 행렬을 의미함.</p>
  </li>
  <li>
    <p>벡터 또한 동일하게 행과 열이 바뀐 전치 벡터가 존재한다.</p>
  </li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611576318945.png" alt="" /></p>

<h3 id='행렬의-이해'>행렬의 이해</h3>

<p><strong>1. 첫번째 의미</strong></p>

<ul>
  <li>벡터가 공간의 한점을 의미한다면 행렬은 <strong>공간에서 여러 점들의 집합</strong>을 의미함.</li>
  <li>행렬의 행벡터 x~i~는 i번째 데이터를 의미함.</li>
  <li>행렬 x~ij~는 i번째 데이터의 j 번재 변수값을 의미함.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611576499688.png" alt="" /></p>

<ul>
  <li>벡터와 마찬가지로 <strong>같은 모양을 가지면 같은 인덱스 위치끼리 덧셈, 뺄셈. 성분곱을 계산할 수 있다.</strong></li>
  <li>벡터와 마찬가지로 <strong>스칼라곱(α) 또한 가능하다.</strong></li>
</ul>

<p><strong>2. 두번째 의미</strong></p>

<ul>
  <li>행렬은 <strong>벡터 공간에서 사용되는 연산자(operator)</strong>를 의미.</li>
  <li>행렬 곱을 통해 벡터를 다른 차원의 공간을 보낼 수 있음
    <ul>
      <li>행렬을 X벡터와 곱하면 m차원에서 n차원 벡터로 변환되어 n차원의 z벡터가 됨.</li>
      <li>이를 통해 맵핑, 디코딩 등이 가능함.</li>
      <li>이를 <strong>선형 변환(linear transform)</strong>이라고도 함.</li>
      <li>딥러닝은 선형 변환과 비선형 변환의 합성으로 이루어짐</li>
    </ul>
  </li>
  <li>패턴 추출, 데이터 압축 등에도 사용함.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611577936452.png" alt="" /></p>

<h3 id='행렬의-곱셈-matrix-multiplication-과-내적'>행렬의 곱셈(matrix multiplication)과 내적</h3>

<p><strong>1. 행렬의 곱셈(matrix multiplication)</strong></p>

<ul>
  <li>행렬 곱셈은 <strong>i번째 행벡터와 j 번째 열벡터 사이의 내적을 성분으로 가지는 행렬</strong>을 만듭니다.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611576784395.png" alt="" /></p>

<ul>
  <li>고로 행과 열의 갯수가 같아야 가능하다.</li>
</ul>

<blockquote>
  <p>행렬의 곱셈 code 구현 시</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
            	<span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            	<span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            	<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            	<span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">Y</span><span class="p">)</span> <span class="c1"># numpy에선 @ 연산으로 행렬 곱셈 계산
# array([[-8, 6],
#        	[5, 2],
#         	[-5, 1]])
</span>
</code></pre></div></div>
<p><strong>2. 행렬의 내적</strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">np.inner</code>는 <strong>i번째 행벡터와 j번째 행벡터 사이의 내적을 성분으로 가지는 행렬</strong>을 계산합니다.</li>
  <li>수학의 행렬 내적 <strong>tr(XY^T^)</strong>과 다름</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611577585251.png" alt="" /></p>
<blockquote>
  <p>행렬의 내적 code 구현 시</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
            	<span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            	<span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            	<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span> <span class="c1"># numpy에선 np.inner() 함수로 행렬 내적 계산
# array([[-5, 3],
#        	[5, 2],
#         	[-3, -1]])
</span>
</code></pre></div></div>
<h3 id='역행렬의-이해'>역행렬의 이해</h3>

<ul>
  <li><strong>행렬 A의 연산을 같은 연산으로 거꾸로 돌리는 행렬을 역행렬(Inverse matrix)라고 부르며, A^-1^라 표기</strong>한다.</li>
  <li><strong>행과 열 숫자가 같고 행렬식(determinant)가 0이 아닌 경우에만 계산 가능</strong>.</li>
</ul>

<blockquote>
  <p>역행렬과의 행렬곱의 결과</p>
</blockquote>

\[AA^{-1} = A^{-1}A = I(항등행렬)\]

<ul>
  <li><strong>항등행렬(Identity Matrix)은 곱하게 될 시 자기 자신이 나오는 행렬이다.</strong></li>
</ul>

<blockquote>
  <p>역 행렬의 코드 구현</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">Y</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span> <span class="c1"># np.linalg.inv(Y) Y 행렬의 역행렬이 리턴
# array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) # 정확히는 float으로 비슷한 값이 나온다.
</span>
</code></pre></div></div>
<ul>
  <li>역행렬을 계산할 수 없는 조건이라면 유사역행렬(pseudo-inverse) 또는 무어-펜로즈(Moore-Penrose) 역행렬 A^+^을 이용한다.</li>
</ul>

<blockquote>
  <p>유사역행렬의 성질</p>
</blockquote>

\[n \geq m 인\ 경우, \  A^+ = (A^TA)^{-1}A^T,\ A^+A = I\\  
n \leq m 인\ 경우, \  A^+ = A^T(A^TA)^{-1},\ AA^+ = I\\\]

<ul>
  <li>순서를 바꾸면 결과가 달라지므로 유사역행렬의 순서에 주의!</li>
</ul>

<blockquote>
  <p>유사 역행렬의 코드 구현</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">aray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">Y</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span> <span class="c1"># np.linalg.pinv(Y) Y 행렬의 유사역행렬이 리턴
# array([[1, 0], [0, 1]]) # 정확히는 float으로 비슷한 값이 나온다.
</span>
</code></pre></div></div>
<h3 id='행렬의-응용'>행렬의 응용</h3>

<p><strong>1. 연립방정식 풀기</strong></p>

\[a_{11}x_1 + a_{12}x_2 + \dots + a_{1m}x_{m} = b_{1}\\
a_{12}x_1 + a_{22}x_2 + \dots + a_{2m}x_{m} = b_{2}\\
\vdots\\
a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nm}x_{m} = b_{n}\\

n \leq m \ 인\ 경우:\ 식이\ 변수\ 개수보다\ 작거나\ 같아야\ 함\]

<p><em><strong>sol)</strong></em>  n이 m보다 작거나 같으면 무어-펜로즈 역행렬을 이용해 해를 하나 구할 수 있다.<br />
\(Ax = B \\
\Rightarrow x = A^+b\\
=A^T(AA^T)^{-1}b\)</p>

<p><strong>2. 선형회귀분석</strong></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">np.linalg.pinv</code>를 이용하면 데이터를 선형모델(linear model)로 해석하는 선형회귀식을 찾을 수 있다.<img src="/assets/img/AI 수학 기본/1611582591497.png" alt="" /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">sklearn</code>의 <code class="language-plaintext highlighter-rouge">LinearRegression</code>과 같은 결과를 가져옴</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Scikit Learn을 활용한 회귀분석
</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c1"># Moore-Penrose 역행렬, y절편(intercept)항을 직접 추가해야한다.
</span><span class="n">X_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span> <span class="c1"># intercept 항 추가
</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalog</span><span class="p">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X_</span><span class="p">)</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">@</span> <span class="n">beta</span>

</code></pre></div></div>
<h2 id='Gradient-Algorithm-경사하강법'>Gradient Algorithm(경사하강법)</h2>

<h3 id='미분-differentiation-이란'>미분 (differentiation)이란?</h3>

<ul>
  <li><strong>변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구.</strong></li>
  <li><strong>변화율의 극한, 최적화에서 제일 많이 사용하는 기법.</strong></li>
</ul>

\[f'(x) = \lim_{h\rightarrow0}\frac{f(x+h) - f(x)}h\]

<blockquote>
  <p>미분 코드 구현</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sympy</span> <span class="k">as</span> <span class="n">sym</span> <span class="c1"># 기호를 통해 함수를 이해하게 해줌
</span><span class="kn">from</span> <span class="nn">sympy.abc</span> <span class="kn">import</span> <span class="n">x</span>

<span class="n">sym</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">sym</span><span class="p">.</span><span class="n">poly</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#Poly(2*x + 2, x, domain=&amp;#39;zz&amp;#39;)
</span>
</code></pre></div></div>
<h3 id='미분-활용'>미분 활용</h3>
<ul>
  <li>함수 그래프의 접선의 기울기와도 같으며, 이를 통해 함수값의 증감을 알 수 있다.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/1611627291652.png" alt="" /></p>

<ul>
  <li>
    <p>이렇게 구한 기울기를 통하여 미분값을 더하거나 빼서, 고차원 공간에서도 최적화 가능</p>

    <ul>
      <li><img src="/assets/img/AI 수학 기본/1611642449114.png" alt="" /></li>
      <li>미분값을 더하면 경사상승법이라 하며, 함수의 극대값을 찾는데 사용</li>
      <li>미분값을 빼면 경사하강법이라 하며, 함수의 극소값을 찾는데 사용</li>
    </ul>
  </li>
  <li>
    <p>극소값이나 극대값에 도달하면 미분값이 0이므로 최적화가 종료됨.</p>

    <p><img src="/assets/img/AI 수학 기본/1611628210487.png" alt="" /></p>
  </li>
</ul>

<h3 id='경사하강법-Gradient-Algorithm-구현'>경사하강법 (Gradient Algorithm) 구현</h3>
<blockquote>
  <p>코드 구현 슈도 코드</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Input</span> <span class="p">:</span> <span class="n">gradeint</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">Output</span><span class="p">:</span> <span class="n">var</span>
<span class="c1"># gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료 조건
</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">init</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&amp;</span><span class="c1">#38;#62; eps): # 미분값이 정확이 0이 되는 경우는 거의 없음, 즉 아주 작은값(eps)이하가 되면 종료
</span>    <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># lr: 학습률이 높을수록 넓게 업데이트함, 너무 크거나 작으면 안됨
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span> <span class="c1"># 미분값 업데이트
</span>
</code></pre></div></div>
<p><strong>다변수 함수일 경우?</strong></p>

<ul>
  <li>벡터가 입력인 다변수 함수의 경우 <strong>편미분(partial differentiation)</strong>을 사용.</li>
</ul>

\[\partial _{x_{i}}f\left( x\right) =\lim _{h\rightarrow 0}\dfrac{f\left( x+he_{i}\right) -f\left( x\right) }{h}\\
e_{i}\ :\ i번째\ 값만 \ 1이고\ 나머지는\ 0인\ 단위벡터\]

<blockquote>
  <p>코딩을 이용한 편미분</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sympy</span> <span class="k">as</span> <span class="n">sym</span>
<span class="kn">from</span> <span class="nn">sympy.abc</span> <span class="kn">import</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="n">sym</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">sym</span><span class="p">.</span><span class="n">poly</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">sym</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
<span class="c1"># 2*x + 2*y - sin(x+2*y)
</span>
</code></pre></div></div>
<ul>
  <li>각 변수 별로 <strong>편미분을 계산한 그레이언트 벡터를 이용하여 경사하강/경사상승법</strong>에 사용 가능.</li>
</ul>

\[\partial _{x_{i}}f\left( x\right) =\lim _{h\rightarrow 0}\dfrac{f\left( x+he_{i}\right) -f\left( x\right) }{h}\\
e_{i}\ :\ i번째\ 값만 \ 1이고\ 나머지는\ 0인\ 단위벡터 \\ 
\nabla f = (\partial_{x1}f,\partial_{x2}f,\dots,\partial_{xd}f)\]

<h3 id='그레디언트-벡터-Gradient-vector-이용한-경사하강법'>그레디언트 벡터 (Gradient vector) 이용한 경사하강법</h3>

<p><img src="/assets/img/AI 수학 기본/1611629412267.png" alt="" /></p>

<ul>
  <li>∇f (x,y)는 임의의 점(x,y)에서 가장 빨리 함수값이 증가하는 방향이다</li>
  <li>그러므로  -∇f (x,y)방향으로 가면 가장 빨리 함수값이 감소하는 방향이 되며, 이를 적용해 경사하강을 한다.</li>
</ul>

<blockquote>
  <p>그레디언트 벡터가 적용된 경사하강법 슈도코드</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Input</span> <span class="p">:</span> <span class="n">gradeint</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">Output</span><span class="p">:</span> <span class="n">var</span>
<span class="c1"># gradient: 그레디언트 벡터를 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료 조건
</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">init</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&amp;</span><span class="c1">#38;#62; eps): # 벡터의 절대값 대신 노름(norm)을 계산해 종료조건 설정
</span>    <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># lr: 학습률이 높을수록 넓게 업데이트함, 너무 크거나 작으면 안됨
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span> <span class="c1"># 미분값 업데이트
</span>
</code></pre></div></div>
<h3 id='경사하강법의-선형-회귀-적용-apply-to-linear-regression'>경사하강법의 선형 회귀 적용 (apply to linear regression)</h3>

<ul>
  <li>
    <p>무어-펜로즈 행렬을 이용해서 선형회귀가 가능했지만, 경사하강법을 이용하는게 일반적이다.</p>
  </li>
  <li>
    <p>**선형회귀의 목적식은 ∥y-Xβ∥~2~ 또는 ∥y-Xβ∥~2~^2^ ** 이며, 이를 최소하하는 β를 찾는게 목적이므로 다음과 같은 그레디언트 벡터를 구해야한다.(loss : RMSE 기준)<br />
\(\nabla_\beta\left\| y - X\beta\right\|_2  = (\partial_{\beta_1}\left\| y - X\beta\right\|_2,\dots,\partial_{\beta_d}\left\| y - X\beta\right\|_2)\\
\partial_{\beta_k}\left\| y - X\beta\right\|_2 = \partial_{\beta_k}\left\{\frac{1}n\sum_{i=1}^n{\left(y_i - \sum_{j=1}^{d}X_{ij}\beta_j\right)^2}\right\}^\frac{1}2 = -\frac{X^T_{\cdot k}(y - X\beta)}{n\left\| y - X\beta\right\|_2}\\
X^T_{\cdot k} = 행렬\ X의\ k번째\ 열(column)\ 벡터를\ 전치시킨\ 것\\
즉,\\
\nabla_\beta\left\| y - X\beta\right\|_2  = (\partial_{\beta_1}\left\| y - X\beta\right\|_2,\dots,\partial_{\beta_d}\left\| y - X\beta\right\|_2) = \left( -\frac{X^T_{\cdot 1}(y - X\beta)}{n\left\| y - X\beta\right\|_2},\dots, -\frac{X^T_{\cdot d}(y - X\beta)}{n\left\| y - X\beta\right\|_2}\right)\)</p>
  </li>
  <li>
    <p>이에 목적식을 최소화하는 β를 구하는 경사하강법 알고리즘은 다음과 같다.</p>
  </li>
</ul>

\[\beta^{(t+1)}\leftarrow\beta^{(t)} - \lambda\nabla_\beta\left\| y - X\beta\right\|_2 =\beta^{(t)} + \frac{\lambda}n\frac{X^T(y - X\beta^{(t)})}{\left\| y - X\beta\right\|_2}\\\]

<ul>
  <li>목적식으로 ∥y-Xβ∥~2~ 대신 ∥y-Xβ∥~2~^2^을 최소화하면 식이 좀더 간단해진다.</li>
</ul>

\[\nabla_\beta\left\| y - X\beta\right\|_{2}^2  = (\partial_{\beta_1}\left\| y - X\beta\right\|_{2}^2,\dots,\partial_{\beta_d}\left\| y - X\beta\right\|_{2}^2) = -\frac{2}nX^T(y - X\beta) \\
\beta^{(t+1)}\leftarrow\beta^{(t)} + \frac{2\lambda}{n}X^T(y - X\beta^{(t)})\]

<blockquote>
  <p>경사하강법 기반 선형회귀 알고리즘 슈도 코드</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Input</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Output</span><span class="p">:</span> <span class="n">beta</span>
<span class="c1"># norm: L2-노름을 계산하는 함수
# lr: 학습률, T: 학습횟수 = hyperparameter
</span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>  <span class="c1"># 학습횟수 제한, 또는 이전 처럼 일정한 수준 이하로 떨어질 때까지 해도 된다.
</span>    <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">error</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># 베타 업그레이드
</span>
</code></pre></div></div>
<h3 id='확률적-경사하강법-stochastic-gradient-descent'>확률적 경사하강법 (stochastic gradient descent)</h3>

<ul>
  <li>
    <p>이론적으로 적절한 학습률과 학습횟수를 선택시, 수렴이 보장되어있다.</p>
  </li>
  <li>
    <p>하지만 비선형회귀의 경우 목적식이 볼록하지 않으므로(non-convex) 수렴이 항상 보장되지 않음</p>

    <ul>
      <li>딥러닝의 목적식은 대부분 볼록함수가 아니다, 즉 대부분 보장하지 않음</li>
      <li>아래와 같은 경우 특정 부분에 수렴했지만 함수의 최소지점이 아니다.</li>
    </ul>

    <p><img src="/assets/img/AI 수학 기본/1611640246290.png" alt="" /></p>
  </li>
</ul>

<p>​</p>

<ul>
  <li>
    <p><strong>확률적 경사하강법(SGD)은 모든 데이터를 사용해서 업데이트 하는 대신 데이터 한개 또는 일부활용하여 업데이트</strong>합니다.</p>

    <blockquote>
      <p>가정</p>
    </blockquote>

    <p><img src="/assets/img/AI 수학 기본/1611640728653.png" alt="" /></p>
  </li>
</ul>

\[\theta^{(t+1)}\leftarrow\theta^{(t)}-\widehat{\nabla _{a}L}(\theta^{(t)})\]

<ul>
  <li>만능은 아니지만 딥러닝에서 <strong>mini-batch 방식을 추가</strong>하여 일반적으로 사용함.</li>
  <li>SGD는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용
    <ul>
      <li><strong>연산량이 b/n으로 감소</strong></li>
    </ul>
  </li>
</ul>

\[\beta^{(t+1)}\leftarrow\beta^{(t)} + \frac{2\lambda}{b}X^T_{(B)}(y_{(b)} - X_{(b)}\beta^{(t)})\]

<ul>
  <li>
    <p>경사하강법은 전체 데이터를 가지고 목적식 그레디언트 벡터를 계산하는 반면, SGD는 미치배치(데이터의 일부)를 가지고 그레디언트 벡터를 계산</p>
  </li>
  <li>
    <p>그러므로 매번 다른 데이터셋을 사용하는 것과 비슷하기 때문에 함수 곡선의 모양이 미니배치마다 바뀌게 된다.</p>
  </li>
  <li>
    <p>하지만 최종적인 방향성은 유사하게 이동하게 된다.</p>

    <p><img src="/assets/img/AI 수학 기본/1611641382157.png" alt="" /></p>
  </li>
  <li>
    <p>즉 볼록모양이 아니어도 효율적으로 활용가능 하다.</p>
  </li>
</ul>

<blockquote>
  <p>경사하강법(좌) vs 확률적 경사하강법(우)</p>
</blockquote>

<p><img src="/assets/img/AI 수학 기본/1611641475679.png" alt="" /></p>

<ul>
  <li>
    <p>다만 mini-batch 사이즈를 너무 작게 잡으면 경사하강법에 비해 너무 느려진다.</p>
  </li>
  <li>
    <p>최근에는 경사하강법으로 전체 데이터를 이용해 학습시키면, 방대한 데이터셋에 의하여 메모리가 초과되므로 미니배치로 나누어서 학습하는 SGD를 사용한다.</p>
  </li>
</ul>

<h2 id='인공지능-학습의-수학적-이해'>인공지능 학습의 수학적 이해</h2>

<h3 id='신경망의-해석'>신경망의 해석</h3>

<ul>
  <li>비선형, 복잡한 모델이 대부분인 신경망은 사실 선형 모델과 비선형 함수들의 결합으로 이루어져 있다.</li>
</ul>

\[\begin{aligned}\begin{aligned}\begin{bmatrix} O_1 \\ O_2 \\ \vdots \\ O_n \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \begin{bmatrix} w_{11} &amp; w_{12} &amp; \dots &amp; w_{1p}\\  w_{21} &amp; w_{22} &amp; \dots &amp; w_{2p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ w_{d1} &amp; w_{d2} &amp; \dots &amp; w_{dp} \end{bmatrix} + \begin{bmatrix} \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\b_1&amp;b_2&amp;\dots&amp;b_p \\ b_1&amp;b_2&amp;\dots&amp;b_p \\ b_1&amp;b_2&amp;\dots&amp;b_p\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\end{bmatrix}\\
O\ \ \ \ \ \ \ \ \ \ \ \ \ \ X\ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ W \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ b\ \ \ \ \ \ \ \ \ \ \ \ \ \ \end{aligned}\\(n \times p)\ \ \ \ (n \times d)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (d\times p)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (n\times p) \ \ \ \ \ \ \ \ \ \end{aligned}\]

<p><strong>[math 0. 신경망 수식 분해]</strong></p>

<ul>
  <li>b는 각 한 행의 모든 값이 같다.</li>
</ul>

<pre><code class="language-mermaid">graph BT
	x1((x&amp;#38;#60;sub&amp;#38;#62;1&amp;#38;#60;/sub&amp;#38;#62;)) &amp;#38; x2((x&amp;#38;#60;sub&amp;#38;#62;2&amp;#38;#60;/sub&amp;#38;#62;)) &amp;#38; x((x...)) &amp;#38; xd((x&amp;#38;#60;sub&amp;#38;#62;d&amp;#38;#60;/sub&amp;#38;#62;))--&amp;#38;#62;o1((O&amp;#38;#60;sub&amp;#38;#62;1&amp;#38;#60;/sub&amp;#38;#62;)) &amp;#38; os((o...)) &amp;#38; op((O&amp;#38;#60;sub&amp;#38;#62;p&amp;#38;#60;/sub&amp;#38;#62;))

</code></pre>
<p><strong>[chart 0. 신경망 모델의 차트화]</strong></p>

<ul>
  <li>
    <p>d개의 변수로 p개의 선형모델을 만들어 p개의 잠재변수를 설명하는 모델의 도식화이다.</p>
  </li>
  <li>
    <p>화살표는 w~ij~들의 곱을 의미한다.</p>
  </li>
</ul>

<h3 id='소프트맥스-연산'>소프트맥스 연산</h3>

\[softmax(o) = softmax(Wx+b) = 
\left( \dfrac{\exp(0)}{\sum_{k=1}^p\exp(o_k) }, \dots, \dfrac{\exp(0_p)}{\sum_{k=1}^p\exp(o_k) },\right)\]

<p><strong>[math1.softmax]</strong></p>

<ul>
  <li>소프트맥스(softmax) 함수는 <em>모델의 출력을 확률로 해석할 수 있게 변환해주는 연산</em></li>
  <li>분류 문제를 풀 때 선형모델과 소프트 맥스 함수를 결합하여 예측</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">denumerator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vec</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="c1"># 각 출력의 값
</span>    <span class="c1"># 너무 큰 벡터가 들어오는 것을 막기위해 np.max(vec)을 vec에서 빼서 방지
</span>    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">denumerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="c1">#모든 출력의 값들의 합
</span>    <span class="n">val</span> <span class="o">=</span> <span class="n">denumerator</span> <span class="o">/</span> <span class="n">numerator</span>
    <span class="k">return</span> <span class="n">val</span>

</code></pre></div></div>
<p><strong>[code 1.softmax의 코드 구현]</strong></p>

<ul>
  <li>이러한 소프트맥스 함수로 벡터를 확률 벡터(각 성분의 합이 1인 벡터)로 변환할 수 있다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dim</span><span class="p">)[</span><span class="n">_</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">val</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">one_hot_encoding</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">vec_dim</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">vec_argmax</span><span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">vec_argmax</span><span class="p">,</span> <span class="n">vec_dim</span><span class="p">)</span>

</code></pre></div></div>
<p><strong>[code 1-1.one_hot 함수 구현]</strong></p>

<ul>
  <li>학습이 아닌 추론시에는 one_hot 벡터를 이용해 가장 큰 벡터를 정답으로 삼으면 되므로 softmax()를 씌울 필요 없다.(<del>one_hot(softmax(o)</del>=&gt; <strong>X</strong>, one_hot(o) =&gt; <strong>O</strong>)</li>
</ul>

<h3 id='활성화-함수-activation-function-와-다층-신경망-MLP'>활성화 함수(activation function)와 다층 신경망(MLP)</h3>

<ul>
  <li>신경망은 <em>선형모델과 활성함수(activation function)을 합성한 함수</em></li>
</ul>

\[\bold{H} = (\sigma (z_1), \dots, \sigma(z_n)), \sigma(z) = \sigma(Wx + b)\\
\sigma = 활성함수(비선형),\ z = (z_1,\dots,z_q) = 잠재벡터,\ \bold H = 새로운\ 잠재벡터 = 뉴런\]

<p><strong>[math 2. 신경망 뉴런]</strong></p>

<p><img src="/assets/img/AI 수학 기본/image-20210127165653697.png" alt="" /></p>

<p><strong>[img 2. 신경망 뉴런 도식화]</strong></p>

<ul>
  <li><em>잠재벡터들을 이용해 만든 새로운 잠재벡터들</em>,(그리고, 이 새로 만든 잠재벡터로 만들 새로운 잠재벡터들)을 <em>뉴런(neuron) 또는</em> 이라고 하며, 이러한 구조의 인공신경망을 <em>퍼셉트론(perceptron)</em>이라고 한다.
    <ul>
      <li>각 뉴런(노드) 가 가지고 있는 값은 텐서(tensor)라고 한다.</li>
    </ul>
  </li>
  <li>활성화 함수는 <em>실수값을 받아 실수값을 돌려주는 비선형(nonlinear) 함수</em>
    <ul>
      <li>비선현 근사를 하기 위해 존재</li>
    </ul>
  </li>
  <li>이로 인해 딥러닝이 선형모형과 차이를 보였으며, <em>시그모이드(sigmoid), tanh, 그리고 주로 쓰이고 있는 ReLU 함수</em> 등이 있다.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210127172913738.png" alt="" /></p>

<p><strong>[img 3. sigmoid, tanh, ReLu 함수 그래프]</strong></p>

<ul>
  <li>만약, 이렇게 구한 <em>잠재 벡터 H에서 가중치행렬 W^(2)^와 b^(2)^를 통해 다시 한번 선형 변환해서 출력하게 되면 (W^(2)^, W^(1)^)를 패러미터로 가진 2층(2-layers) 신경망</em>이 된다.</li>
</ul>

\[\bold O = \bold H\bold W^{(2)} + b^{(2)},\  \bold{H} = (\sigma (z_1), \dots, \sigma(z_n)) = \sigma(\bold Z^{(1)}),\ \sigma(z) = \sigma(W^{(1)}x + b^{(1)}) \\
\bold Z^{(1)} = \bold X\bold W^{(1)} + \bold b^{(1)}\]

<p><strong>[math 2-1. 2중 신경망]</strong></p>

<p><img src="/assets/img/AI 수학 기본/image-20210127210938621.png" alt="" /></p>

<p><strong>[img 2-1.2중 신경망의 구조]</strong></p>

<ul>
  <li>이렇게 <em>신경망이 여러층 합성된 함수를 다층(multi-layer) 퍼셉트론(MLP)</em>라고 한다.</li>
</ul>

\[\\ \bold O = \bold Z^{(L)}
\\ \vdots
\\ \bold H^{(l)} = \sigma(\bold Z^{(l)})
\\ \bold Z^{(l)} = \bold H^{(l-1)}\bold W^{(l)} + \bold b^{(l)}
\\ \vdots
\\ \bold{H^{(1)}} = \sigma(\bold Z^{(1)})
\\ \bold Z^{(1)} = \bold X\bold W^{(1)} + \bold b^{(1)}\]

<p><strong>[math 2-2. n층으로 이루어진  다중신경망의 합성함수]</strong></p>

<ul>
  <li>l = 1,…,L까지 순차적인 신경망 계산을 순전파(forward propagation)이라 부른다.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210127212725927.png" alt="" /></p>

<p><strong>[img 2-3. 다층 신경망 구조]</strong></p>

<ul>
  <li>이론적으로 2층 정도의 신경망으로도 임의의 연속함수를 근사(universal approximation theorem)할 수 있다.</li>
  <li>층이 깊을 수록 필요한 뉴런(텐서를 가지고 있는 노드), 파라미터의 숫자가 기하급수적으로 줄어들어 좀 더 효율적이다.
    <ul>
      <li>즉, 층을 깊이 하면 넓이를 얇게 해도 된다.</li>
      <li>물론 최적화는 여전히 어렵다.(CNN에서 깊게 설명)</li>
    </ul>
  </li>
</ul>

<h3 id='역전파-backpropagation-알고리즘'>역전파(backpropagation) 알고리즘</h3>

<ul>
  <li><em>각 층에 사용된 패러미터 <strong>{W^{l}^,b^{l}^}^L^ ~l=1~</strong>을 역순으로 학습</em>하는데 사용된다.</li>
  <li>합성미분의 <em>연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)</em>을 이용하여 역순으로 구한다.</li>
</ul>

\[z = (x+y)^2의\ 그레디언트\ 벡터,\ \dfrac{\partial z}{\partial x} =\ ?\\
z = w^2 \rightarrow \dfrac{\partial z}{\partial w} = 2w 
w = x + y \rightarrow \dfrac{\partial w}{\partial x} = 1,\  \dfrac{\partial w}{\partial y} = 1\\
\dfrac{\partial z}{\partial x} = \dfrac{\partial z}{\partial w}\dfrac{\partial w}{\partial x} = 2w \cdot 1 = 2(x+y)\]

<p><strong>[math 3. 편미분을 이용한 역전파 알고리즘 예시]</strong></p>

<ul>
  <li>먼저 윗층의 그레디언트 벡터를 구한 뒤, 그 벡터를 이용해 그 아래 그레디언트 벡터를 구한다.
    <ul>
      <li>포워드 프로파 게이션과 달리, 각 뉴런 또는 노드의 텐서 값을 메모리에 넣어야 하므로, 메모리를 많이 먹는다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210127222227474.png" alt="" /></p>

<p><strong>[img 3. 2층 신경망 어려운 예제]</strong></p>

<ul>
  <li>파란색 : forward propagation</li>
  <li>빨간색 : back propagation</li>
</ul>

<h2 id='확률론'>확률론</h2>

<ul>
  <li>딥러닝은 <strong>확률론 기반의 기계학습 이론에 바탕</strong>을 두고 있으며, 통계적 해석은 손실함수 들의 기본 작동원리이다.
    <ul>
      <li>회귀 분석의 L~2~노름은 예측오차 분산을 최소화하는 방향으로 학습하며</li>
      <li>분류 문제의 교차 엔트로피는 모델 예측의 불확실성을 최소화하는 방향으로 학습한다.</li>
    </ul>
  </li>
</ul>

<h3 id='확률분포'>확률분포</h3>

<ul>
  <li>
    <p>확률분포란, <em>확률 변수가 특정한 값을 가질 확률을 나타내는 함수</em>를 의미한다.</p>
  </li>
  <li>
    <p>확률분포는 데이터공간 (x,y) 에서 데이터를 추출하는 분포이다.</p>
  </li>
  <li>
    <p>확률 변수는 이산형(discrete)과 연속형(continuous)으로 구분된다.</p>

    <ul>
      <li>이산형 확률 변수 : 확률 변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해 모델링<br />
\(\mathbb{P}(X \in A) = \sum_{x\in A}{P(X= x)}\\
\mathbb{P} = 확률변수\)</li>
    </ul>

    <p><strong>[math 4. 이산형 확률변수]</strong></p>

    <ul>
      <li>연속형 확률 변수 : 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다.</li>
    </ul>

\[\mathbb{P}(X \in A) = \int_{A}{P(x)dx}\\
P(x)=\lim_{h\rightarrow0}\frac{\mathbb{P}(x - h \leq X \leq x + h)}{2h}= 확률변수의\ 밀도\]

    <p><strong>[math 4-1. 연속형 확률변수]</strong></p>
  </li>
  <li>
    <p>P(x)는 <em>입력 x에 대한 주변확률 분포</em>로 y에 대한 정보를 주진 않음</p>
  </li>
</ul>

<h3 id='기대값'>기대값</h3>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>조건부 확률 P(x</td>
          <td>y)는 <em>데이터 공간에서 입력 x와 출력 y 사이의 관계를 모델링하며, 입력 변수 x에 대해 정답이 y일 확률</em>을 의미함.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>softmax(WΦ  + b )은 데이터 x으로부터 추출된 특징패턴 Φ(x)과 가중치행렬 W을 통해 조건부확률 P(y</td>
          <td>x)을 계산</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>기대값(expectation)은 <em>데이터를 대표하는 통계량, 확률 분포를 통해 다른 통계적 범함수를 계산하는데 사용</em></p>

    <ul>
      <li>회귀 문제의 경우 조건부 기대값을 추정하며 이는 각 확률분포에 따라 다음과 같이 구한다.</li>
    </ul>

\[\mathbb{E}_{x\sim P(x)}[f(x)] = \int_\chi f(x)P(x)dx \rightarrow연속확률분포\\
\mathbb{E}_{x\sim P(x)}[f(x)] = \sum_{x\in\chi} f(x)P(x) \rightarrow 이산확률분포\]

    <p><strong>[math 5. 기대값 구하기]</strong></p>

    <ul>
      <li>분산, 첨도 공분산 등의 통계량을 계산하는데 사용함.</li>
    </ul>
  </li>
</ul>

\[분산	 : \mathbb{V}(x) = \mathbb{E}_{x\sim P(x)}[(x-\mathbb{E}[x])^2]\\
비대칭도 : Skewness(x) = \mathbb{E}\left[\left(\frac{x-\mathbb{E}[x]}{\sqrt{\mathbb{V}(x)}}\right)^3\right]\\
공분산 : Cov(x_1,x_2) = \mathbb{E}_{x_1,x_2\sim P(x_1,x_2)}(x_1 -\mathbb{E}[x_1])(x_2 - \mathbb{E}[x_2])\]

<p>​	<strong>[math 5-1. 기대값의 사용]</strong></p>

<h3 id='몬테카를로-샘플링-Monte-Carlo-sampling'>몬테카를로 샘플링(Monte Carlo sampling)</h3>

<ul>
  <li><em>확률 분포를 모를 때 데이터를 이용하여 기대값을 계산하는 방법</em>.</li>
  <li>이산형, 연속형이든 관계없이 다음과 같이 표현함.</li>
</ul>

\[\mathbb{E}_{x\sim P(x)}[f(x)] \approx \frac{1}{N}\sum^N_{i=1}f(x^{(i)}),\ x^{(i)}\stackrel{\text{i.i.d.}}{\sim} P(x)\]

<p><strong>[math 6.몬테카를로 샘플링]</strong></p>

<ul>
  <li><em>독립 추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장</em></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1">#f(x) = e^(-x^2), [-1, 1]
</span><span class="k">def</span> <span class="nf">mc_int</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">int_len</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span>
    <span class="n">stat</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
        <span class="n">fun_x</span> <span class="o">=</span> <span class="n">fun</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">int_val</span> <span class="o">=</span> <span class="n">int_len</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fun_x</span><span class="p">)</span>
        <span class="n">stat</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">int_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">stat</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">stat</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">mc_int</span><span class="p">(</span><span class="n">f_x</span><span class="p">,</span> <span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>

</code></pre></div></div>
<p><strong>[code 6. f(x), [-1, 1] 몬테카를로 코드구현]</strong></p>

<h2 id='통계학'>통계학</h2>

<ul>
  <li><em>통계적 모델링은 적절한 가정 위에서 확률 분포를 추정(inference)하는 것이 목표</em>이며, 이는 기계학습이 결과를 예측하는 것과 같은 목표이다.</li>
  <li>유한한 개수의 데이터로는 모집단의 분포를 정확하게 알아낼 수 없으므로 근사적으로 확률분포를 추정하여 불확실성을 최소화</li>
</ul>

<h3 id='모수'>모수</h3>

<ul>
  <li><em>데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적(parametirc) 방법론</em>이라고 함.
    <ul>
      <li>이와 반대로 데이터에 따라 모델의 주조 및 개수가 유연하게 바뀌면 비모수(nonparametric) 방법론이라 부르며 기계학습의 많은 방범론이 이를 따르기도 함.</li>
    </ul>
  </li>
</ul>

<h3 id='데이터-모수-추정'>데이터 모수 추정</h3>

<h4 id='확률-분포-가정'>확률 분포 가정</h4>

<ul>
  <li>히스토그램의 모양을 관찰하여 확률분포를 가정할 수 도 있다.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210130173640696.png" alt="" /></p>

<p><strong>[img 7. 여러가지 모양의 확률 분포]</strong></p>

<table>
  <thead>
    <tr>
      <th>확률 분포명</th>
      <th>데이터 모양</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>베르누이 분포</td>
      <td>데이터가 2개의 값(0 또는 1)만 가짐</td>
    </tr>
    <tr>
      <td>카테고리 분포</td>
      <td>데이터가 n개의 이산적인 값만을 가짐</td>
    </tr>
    <tr>
      <td>베타 분포</td>
      <td>데이터가 [0, 1] 사이에서 값을 가짐</td>
    </tr>
    <tr>
      <td>감마 분포, 로그 정규 분포</td>
      <td>데이터가 0 이상의 값을 가짐</td>
    </tr>
    <tr>
      <td>정규 분포, 라플라스 분포</td>
      <td>데이터가 $\mathbb{R}$(실수) 전체에서 값을 가짐</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig 7.  확률 분포의 예시]</strong></p>

<ul>
  <li>하지만 이런식으로 가정하는 것보다 <em>데이터를 생성하는 원리를 고려한 뒤, 각 분포마다 모수를 추정 후  각 확률분포의 검정방벙으로 검정하는 방식이 원칙</em>이다.</li>
</ul>

<h4 id='데이터-모수-추정'>데이터 모수 추정</h4>

<ul>
  <li>
    <p>데이터 확률분포를 가정한 수 데이터 모수를 추정한다.</p>
  </li>
  <li>
    <p>평균 $\mu$와 분산 $\sigma^2$으로 이를 추정하는 통계량(statistic)은 다음과 같다.<br />
\(\stackrel{표본 평균}{\bar{X} = \frac{1}{N}\sum^N_{i=1}X_i},\ \ \ \stackrel{표본 분산}{S^2 = \frac{1}{N-1}\sum^N_{i=1}(X_i-\bar{X})^2}\\
\mathbb{E}[\bar{X}] = \mu,\ \ \ \mathbb{E}[S^2] = \sigma^2,\ 표본 표준 편차 = \sqrt{표본분산} = \sqrt{S^2} = S\)<br />
<strong>[math 7. 통계량 계산]</strong></p>

    <ul>
      <li>표본분산을 구할 때 N이 아닌 N-1로 나누는 이유는 불편(unbiased) 추정량을 구하기 위해서라고 하며, 고급 통계학 내용이므로 일단 넘어가겠다.</li>
    </ul>
  </li>
  <li>
    <p><em>통계량의 확률 분포를 표집분포(Sampling distribution)</em>이라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포 $\mathcal{N}(\mu,\sigma^2/N )$를 따릅니다.</p>

    <ul>
      <li>이를 중심 극한 정리(Central Limit Theorem)이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립합니다.</li>
    </ul>
  </li>
</ul>

<h3 id='최대-가능도-maximum-likelihood-estimation-MLE-추정'>최대 가능도(maximum likelihood estimation, MLE) 추정</h3>

<ul>
  <li>
    <p>통계량을 측정하는 적절한 방법은 확률분포마다 다르다.</p>
  </li>
  <li>
    <p>이론적으로 가장 <em>가능성이 높은 모수 측정 방법은 최대 가능도 추정법(maximum likelihood estimation, MLE)</em>입니다.<br />
\(\hat{\theta}_{MLE} = argmax\ L(\theta;x) = argmax\ P(x|\theta)\)<br />
<strong>[math 8. 최대가능도 추정법]</strong></p>
  </li>
  <li>
    <p>데이터 집합 <strong>X</strong>가 <em>독립적으로 추출되었을 경우 로그가능도를 최적화</em>합니다.</p>

    <ul>
      <li>이때 모수 $\theta$는 가능도를 최적화하는 MLE가 됩니다.</li>
    </ul>
  </li>
</ul>

\[L(\theta;X) = \prod^n_{i=1}P(x_i|\theta) \Rightarrow log\ L(\theta; X) = \sum^n_{i=1}logP(x_i|\theta)\]

<p>​	<strong>[math 8. 독립 추출 시의 추정법 최대가능도 추정법]</strong></p>

<ul>
  <li>데이터의 숫자가 수억단위가 되면 컴퓨터의 연산으로 계산 불가능하므로, 데이터가 독립일 시, 로그 가능도의 덧셈으로 바꾸면 컴퓨터로 연산이 가능해짐.
    <ul>
      <li>경사하강법으로 가능도 최적화시, 미분연산을 사용하며, 음의 로그가능도(negative log-likelihood)를 사용하면 연산량이 O(n^2^)에서 O(n)으로 줄여준다.</li>
      <li>불편 추정량을 보장하진 않음.</li>
    </ul>
  </li>
</ul>

<h4 id='최대-가능도-추정법-예제'>최대 가능도 추정법 예제</h4>

<h5 id='정규분포'>정규분포</h5>

\[\hat{\theta}_{MLE}= argmax\ L(\theta; x) = argmax\ P(x|\theta)\\
log\ L(\theta;X) = \sum^n_{i=1}logP(x_i|\theta) = \sum^n_{i=1}log\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{{|x_i-\mu|^2}}{2\mu^2}} = -\frac{n}{2}log2\pi\sigma^2 - \sum^n_{i=1}\frac{{|x_i-\mu|}^2}{2\sigma^2}\]

<p><strong>[math 8-1. 모수 추정을 위한 로그 가능도 계산]</strong><br />
\(0 = \frac{\partial logL}{\partial\mu}= -\sum^n_{i=1}\frac{x_i - \mu}{\sigma^2} \Rightarrow \hat{\mu}_{MLE}=\frac{1}{n}\sum^N_{i=1}x_i\\
0 = \frac{\partial logL}{\partial\sigma}= -\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum^n_{i=1}|{x_i - \mu}|^2 \Rightarrow \hat{\sigma}_{MLE}^2=\frac{1}{n}\sum^N_{i=1}(x_i -\mu)^2\\\)<br />
<strong>[math 8-2. 미분을 통한 모수 추정]</strong></p>

<h5 id='카테고리-분포'>카테고리 분포</h5>

\[\hat{\theta}_{MLE}= argmax\ P(x_i|\theta) = argmax\ log(\prod^n_{i=1}\prod^d_{k=1}p_k^{x_i,k})\\
log(\prod^n_{i=1}\prod^d_{k=1}p_k^{x_i,k})=\sum^d_{k=1}(\sum^n_{i=1}x_{i,k})logp_k = \sum^d_{k=1}n_klogp_k\ with\ \sum^d_{k=1}p_k=1\\
\Rightarrow \mathcal{L}(p_1,\dots,p_k,\lambda) = \sum^d_{k=1}n_k logp_k + \lambda(1-\sum_kp_k) (라그랑주\ 승수법)\\
0 = \frac{\partial \mathcal{L}}{\partial p_k} = \frac{n_k}{p_k} - \lambda,\ \ \ 0=\frac{\partial \mathcal{L}}{\partial \lambda} = 1 - \sum^d_{k=1}p_k \rightarrow\ p_k =\frac{n_k}{\sum^d_{k=1}n_k}\]

<p>**[math 8-3. 모수 추정] **</p>

<h4 id='딥-러닝에서-최대가능도-추정법'>딥 러닝에서 최대가능도 추정법</h4>

<ul>
  <li>
    <p>딥러닝 모델의 가중치를 $\theta=(W^{(1)},\dots,W^{(L)})$라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리 분포의 모수 $(p_1,\dots,p_k)$를 모델링합니다.</p>
  </li>
  <li>
    <p>원핫벡터로 표현한 정답레이블 $y= (y_1, \dots,y_k)$을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있습니다.<br />
\(\hat{\theta}_{MLE} = argmax \frac{1}{n}\sum^n_{i=1}\sum^K_{k=1}y_{i,k}log(MLP_\theta(x_i)_k)\)<br />
 <strong>[math 9. 분류 문제 최대가능도 추정]</strong></p>
  </li>
</ul>

<h4 id='확률분포의-거리-구하기-쿨백-라이블러-발산'>확률분포의 거리 구하기 - 쿨백-라이블러 발산</h4>

<ul>
  <li><em>기계학습에서 사용되는 함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도</em>됩니다.</li>
  <li>두 개의 확률분포 P(x), Q(x)가 있을 경우 두 확률분포 사이의 거리(distance)를 계산할 때 여러 함수를 이용
    <ul>
      <li>총변동 거리 (Total Variation Distance, TV)</li>
      <li>쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)</li>
      <li>바슈타인 거리 (Wasserstein Distance)</li>
    </ul>
  </li>
  <li>이 중 쿨백-라이블러 발산(KL Divergence)은 다음과 같이 정의.<br />
\(\mathbb{KL}(P\|Q) = -\mathbb{E}_{x\sim P(x)}[logQ(x)] + \mathbb{E}_{x\sim P(x)}[logP(x)]\\
 -\mathbb{E}_{x\sim P(x)}[logQ(x)] = 크로스\ 엔트로피,\ \mathbb{E}_{x\sim P(x)}[logP(x)] =  엔트로피\)</li>
</ul>

<p><strong>[math 10. 쿨백-라이블러 발산 구하기]</strong></p>

<ul>
  <li><em>분류 문제에서 정답레이블을 P, 모델 예측을 Q라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화</em> 하는 것과 같음.</li>
</ul>

<h2 id='베이즈-통계학'>베이즈 통계학</h2>

<ul>
  <li>베이즈 통계학이란, 모수 추정에 사용되는 베이즈 정리에 대한 내용, 데이터 추가시 데이터 업데이트 방법에 대한 이론</li>
  <li>베이즈 정리란, 조건부확률을 이용해 정보를 갱신하는 방법을 알려줌, 예측 모형의 방법론</li>
</ul>

<h3 id='조건부-확률'>조건부 확률</h3>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>*조건부 확률 $P(A</td>
          <td>B)$는 사건 B가 일어난 상황에서 사건 A가 발생할 확률*</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>이를 통해 $P(A</td>
          <td>B)$ 또한 구할 수 있다.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

\[P(A\cap B) = P(B)P(A|B)\\
P(B|A) = \frac {P(A\cap B)}{P(A)} = P(B)\frac {P(A|B)}{P(A)}\]

<p><strong>[math 11. 조건부확률에 대한 식]</strong><br />
\(P(\theta|\mathcal{D}) = P(\theta)\frac{P(\mathcal{D}|\theta)}{P(\mathcal{D})}\\
P(\theta|\mathcal{D}) : 사후확률(posterior),\ P(\theta):사전확률(prior), \ P(\mathcal{D}|\theta): 가능도(likelihood),\ P(\mathcal{D}):Evidence\)<br />
<strong>[math 11-1. 베이즈 정리 용어 정리]</strong></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>예시를 들어보자면 코로나 발병률이 10%(사전확률 $P(\theta)$:0.1), 실제로 걸려서 확진될 확률 99%, 안걸렸는데 오진될 확률 1% (가능도, $P(\mathcal{D}</td>
          <td>\theta)$: 0.99, 0.01 )라 할때 질병에 걸린 사람의 검진결과가 나왔을 때 정말로 코로나에 감염되었을 확률?</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

\[P(\theta) = 0.1,\ P(\neg\theta) = 0.9,\ P(\mathcal{D}|\theta)=0.99,\ p(\mathcal{D}|\neg\theta)=0.01\\
P(\mathcal{D}) = \sum_\theta P(\mathcal{D}|\theta)P(\theta) = 0.99 \times 0.1 + 0.01 \times0.9 = 0.108\\
P(\theta|\mathcal{D}) = 0.1 \times\frac{0.99}{0.108} \approx 0.916 \rightarrow 정답\]

<p><strong>[math 11-2. 사후확률 계산]</strong></p>

<ul>
  <li>$\theta : 코로나\ 발병사건으로\ 정의(관찰 불가),\ \mathcal{D}: 테스트\ 결과로\ 정의(관찰 가능), \neg\theta : ~가\ 아닐\ 확률  $</li>
  <li>오탐율(False alarm)이 오르면 테스트의 정밀도(Precision)가 떨어진다. (0.1로 10배 오를시 0.524까지 떨어짐)</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210201103716368.png" alt="" /></p>

<p><strong>[img 11. 조건부 확률의 시각화]</strong></p>

<h3 id='베이즈-정리를-통한-정보의-갱신'>베이즈 정리를 통한 정보의 갱신</h3>

<ul>
  <li>베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있음.</li>
</ul>

\[new\ P(\theta|\mathcal{D}) = P(\theta|\mathcal{D})\frac{P(\mathcal{D}|\theta)}{P(\mathcal{D})}\]

<p><strong>[math 12. 갱신된 사후확률 구하기]</strong><br />
\(new\ P(\theta|\mathcal{D}) = 0.1 \times \frac {0.99}{0.189} \approx 0.524,\ P(\theta|\mathcal{D}) = 0.99,\ P(\theta|\neg\mathcal{D}) = 0.1 \\ 
P(\mathcal{D}^*)=0.99\times0.524+0.1\times0.476 \approx0.566\\
갱신된\ 사후확률\ P(\theta|\mathcal{D}^*) = 0.524 \times\frac{0.99}{0.566}\approx0.917\)<br />
<strong>[math 12-1. 갱신된 사후확률 계산]</strong></p>

<ul>
  <li>코로나 확정을 받은 사람이 오진율이 10%일시 두번째 검진이 양성일 시에도 확진일 확률?</li>
</ul>

<h3 id='조건부확률과-인과관계의-차이'>조건부확률과 인과관계의 차이</h3>

<ul>
  <li>
    <p>데이터가 많아도 조건부 확률은 인과관계(causality)와 다르다</p>
  </li>
  <li><em>인과관계는 데이터 분포의 변화에 강건한 예측 모형을 만들 때 고려함</em>.
    <ul>
      <li>인과 관계를 고려하지 않으면 시나리오나 바뀐 데이터에 따라 정확도가 크게 떨어짐</li>
    </ul>
  </li>
  <li>인과관계는 <em>중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산</em> 해야함.
    <ul>
      <li>키가 클수록 지능지수가 높다? =&gt; 연령이 클수록 나이와 지능이 높아서 생기는 인과관계</li>
      <li>여기서 중첩요인은 연령</li>
    </ul>
  </li>
</ul>

<h4 id='simpson’s-paradox에-의한-인과관계-오류'>simpson’s paradox에 의한 인과관계 오류</h4>

<p><img src="/assets/img/AI 수학 기본/image-20210201110731430.png" alt="" /></p>

<p><strong>[img 13. 신장 결석 치료법에 따른 치료율]</strong></p>

<ul>
  <li>실제로는 작은 결석 또, 큰 결석 관계없이 외과수술이 치료율이 높지만, 약물 치료법이 성공율이 높은 작은 결석 치료에 많이 사용되므로 전체 치료율은 높아보인다.</li>
  <li>do(T=a)라는 조정 효과를 통해 Z의 개입을 제거해야한다.</li>
</ul>

\[P_a(R=1) = \sum_{z\in \{0,1\}}P(R=1|T=b,Z=z)P(Z=z) = \frac{81}{87}\times\frac{(87+270)}{700} + \frac{192}{263}\times\frac{263+80}{700}\approx 0.8325 \\

P_b(R=1) = \sum_{z\in \{0,1\}}P(R=1|T=b,Z=z)P(Z=z) = \frac{234}{270}\times\frac{(87+270)}{700} + \frac{55}{80}\times\frac{263+80}{700}\approx 0.7789\]

<p><strong>[math 13. 조정 효과를 통한 Z(중첩요인) 개입 제거]</strong></p>

<h2 id='CNN'>CNN</h2>

<h3 id='CNN-Convolution-Neural-Network-의-이해'>CNN(Convolution Neural Network)의 이해</h3>

<ul>
  <li>기존의 모델들은 뉴런이 모두 연결된 (fully connected) 구조였지만 <em>CNN은 동일한 고정된 가중치 값을 가진 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조</em>임.</li>
  <li>선형의 변환의 한 종류임은 같음,</li>
  <li>커널 사이즈는 고정되므로 parameter 사이즈가 작다.</li>
</ul>

\[h_i =\sigma\left(\sum^k_{j=1}V_jx_{i+j-1}\right)\]

<p><strong>[math 14.  Convolution 연산]</strong></p>

<p><img src="/assets/img/AI 수학 기본/image-20210202212714627.png" alt="" /></p>

<p><strong>[img 14. Convolution 연산 그림]</strong></p>

<ul>
  <li>CNN의 수학적 의미는 신호(signal)를 커널을 이용해 국소적으로 증폭 또는 감소시켜 정보를 추출 또는 필터링하는 것이며, 이는 크게 2가지 정의 할 수 있다.
    <ul>
      <li>정의역이 연속적(continuous)인 공간 : 적분으로 표현</li>
      <li>정의역이 이상(discrete) 공간 : 급수로 표현</li>
    </ul>
  </li>
</ul>

\[continuous\ \  [f*g](){: .wikilink}{:target=\"_blank\"} = \int_{\mathbb{R}^d}f(z)g(x-z)dz=\int_{\mathbb{R}^d}f(x-z)g(z)dz=[g*f](){: .wikilink}{:target=\"_blank\"}\\
discrete\ \  [f*g](){: .wikilink}{:target=\"_blank\"} = \sum_{a \in \mathbb{Z}^d}f(a)g(i-a)=\sum_{a \in \mathbb{Z}^d}f(i-a)g(a)=[g*f](){: .wikilink}{:target=\"_blank\"}: kernal\ term\]

<p><strong>[math 14-1.  Convolution 연산 수식]</strong></p>

<ul>
  <li>z 또는 a만 움직이는 형태로 연산</li>
  <li>사실 x-z, i-a가아니라 x+z, i+a 이며 cross-correlation 이다.
    <ul>
      <li>전체 공간에서는 +,-가 차이가 크지않으므로 convolution이라 불러왔음</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Convolution 연산 그래픽적 이해</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/img/AI 수학 기본/Convolution_of_spiky_function_with_box2.gif" alt="" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/AI 수학 기본/Convolution_of_box_signal_with_itself2.gif" alt="" /></td>
    </tr>
  </tbody>
</table>

<p><strong>[fig 14-1. Convolution 연산 그래픽적 이해]</strong></p>

<ul>
  <li>커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)로 적용.</li>
</ul>

\[1D-conv\ \  [f*g](){: .wikilink}{:target=\"_blank\"} = \sum^d_{p=1}f(p)g(i+p)\\
2D-conv\ \  [f*g](){: .wikilink}{:target=\"_blank\"}\\
3D-conv\ \  [f*g](){: .wikilink}{:target=\"_blank\"}\\\]

<p><strong>[math 14-2.  Convolution 여러 차원 연산]</strong></p>

<ul>
  <li>1차원 뿐만 아니라 다양한 차원에서 계산 가능</li>
  <li>1차원(text), 2차원(흑백), 3차원(컬러)별로 적용 가능</li>
  <li>앞의 f항은 바뀌지 않는다.</li>
</ul>

<h3 id='다차원-CNN-Convolution-Neural-Network-의-이해'>다차원 CNN(Convolution Neural Network)의 이해</h3>

<p><img src="/assets/img/AI 수학 기본/image-20210202223807091.png" alt="" /></p>

<p><strong>[img 14-2. 2차원 Convolution 연산 그래픽적 이해-1]</strong></p>

<p><img src="/assets/img/AI 수학 기본/image-20210202225109490.png" alt="" /></p>

<p><strong>[img 14-3. 2차원 Convolution 연산 그래픽적 이해-2]</strong><br />
\(O_H=H-K_H+1\\
O_W=W-K_w+1\)</p>

<p><strong>[math 14-3.  Convolution 출력 크기 계산]</strong></p>

<ul>
  <li>예를 들어 28x28 입력을 3x3 커널로 연산시 26x26이 된다.</li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210202225605899.png" alt="" /></p>

<p><strong>[img 14-4. 3차원 Convolution 연산 그래픽적 이해]</strong></p>

<ul>
  <li>3차원의 경우 2차원 Convolution을 3번 적용하는 것이s다.
    <ul>
      <li>커널의 갯수도 늘어남</li>
    </ul>
  </li>
  <li>3차원 부터는 입력을 텐서라고 말한다.</li>
</ul>

<h3 id='CNN의-역전파'>CNN의 역전파</h3>

<ul>
  <li>커널이 모든 입력데이터에 공통으로 적용되므로 역전파 계산시 convolution 연산을 함</li>
</ul>

\[\frac \partial {\partial x}[f*g](){: .wikilink}{:target=\"_blank\"} = \frac \partial {\partial x}\int_{\mathbb{R}^d}f(y)g(x-y)dy =\int_{\mathbb{R}^d}f(y)\frac {\partial g}{\partial x}(x-y)dy =[f*g'](){: .wikilink}{:target=\"_blank\"}\]

<p><strong>[math 14-4.  Convolution 연산 연속시 역전파]</strong></p>

<ul>
  <li>Discrete 구조에도 마찬가지로 성립한다.</li>
</ul>

\[\frac {\partial \mathcal{L}}{\partial w_i} = \sum_j \delta_jx_i+j-1, \\
ex) \frac {\partial \mathcal{L}}{\partial w_1}= \delta_ix_i + \delta_2x_2+\delta_3x_3\]

<p><strong>[math 14-5.  Convolution 연산]</strong></p>

<h2 id='RNN'>RNN</h2>

<h3 id='시퀀스-sequence-데이터'>시퀀스(sequence) 데이터</h3>

<ul>
  <li>소리, 문자열, 주가 추이 등, 순차적으로 들어오는 데이터
    <ul>
      <li>시계열(time-series) 데이터는 시간 순서에 따라 나열된 데이터로, 시퀀스 데이터에 속함.</li>
    </ul>
  </li>
  <li>독립 동등 분포 (i.i.d)가정을 위배하기 때문에 순서를 바꾸거나 과거 정보이 변형되면 데이터의 확률 분포도 바뀜.
    <ul>
      <li>ex) 개가 사람을 물었다. $\leftrightarrow$ 사람이 개를 물었다. $\rightarrow$ 위치를 바꾼것 만으로, 데이터의 확률, 의미가 달라짐.</li>
    </ul>
  </li>
</ul>

<h4 id='sequence-data-handling'>sequence data handling</h4>

<ul>
  <li>이전 시퀀스의 정보로 앞으로의 데이터의 확률 분포를 다루기 위해 조건부확률 결합법칙 이용</li>
</ul>

\[P(X_1,\dots,X_t) = P(X_t|X_1,\dots,X_{t-1})P(X_1,\dots,X_{t-1})\\
= P(X_t|X_1,\dots,X_{t-1})P(X_{t-1}|X_1,\dots,X_{t-2})\times P(X_1,\dots,X_{t-2})\\
=\prod^t_{s=1}P(X_s|X_{s-1},\dots,X_1)\\
\prod_{s=1}^t = s =1,\dots,t 까지\ 전부\ 곱하라\\
즉,\ X_t \sim P(X_t|X_{t-1},\dots,X_1), \\
X_{t+1} \sim P(x_{t+1}|X_t,X_{t-1},\dots,X_1)\]

<p><strong>[math. 베이즈 법칙에 의한 P(X~s~) 추론 ]</strong></p>

<ul>
  <li>과거 정보를 사용하지만 0부터 t-1까지 모든 데이터가 필요한건 아님, 오히려 지나친 과거 정보는 제외
    <ul>
      <li>시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있어야 한다.</li>
      <li>예를 들어, 시퀀스 데이터 $X_t$ 예측 시 전부가 아닌, $X_{t-1}\sim X_{t-\tau}$개 만큼만 사용하는 모델을 AR($\tau$), 즉 자기 회귀 모델(Autoregressive Model)이라고 부름.</li>
    </ul>
  </li>
  <li>
    <p>위의 자기 회귀 모델의 경우 $\tau$를 파라메터로 하는데 이 값을 짐작하기 힘들거나 $\tau$ 이상의 과거 정보가 필요할지도 모른다.</p>

    <ul>
      <li>이를 보완하기 위한 모델이 잠재 자기 회귀 모델(Latent autoregressive Model, 잠재 AR 모델)이라고 부르며 RNN의 기본 모델이다.</li>
    </ul>
  </li>
</ul>

\[X_t \sim P(X_t|X_{t-1},H_t), \\
X_{t+1} \sim P(x_{t+1}|X_t,X_{t-1},H_{t+1})\\
잠재변수\ H_t=X_{t-2},\dots,X_{1}=Net_\theta(H_{t-1},X_{t-1})\]

<p><strong>[math. 잠재변수 H~t~를 신경망을 통해 반복 사용해 시퀀스 데이터의 패턴을 학습하는 것이 RNN]</strong></p>

<h3 id='RNN의-이해와-BPTT'>RNN의 이해와 BPTT</h3>

<p><img src="/assets/img/AI 수학 기본/image-20210213105548929.png" alt="" /></p>

<p><strong>[img. 기본 RNN 모형, 순전파와 역전파 화살표 포함]</strong></p>

<ul>
  <li>기본적인 RNN 모형은 Multi Layer Perceptron과 유사하다.</li>
  <li>이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링</li>
</ul>

\[O_t = HW^{(2)}+b^{(2)}\\
H_t = \sigma(X_tW_X^{(1)}+H_{t-1}W^{(1)}_H+b^{(1)})\\
O_t = 출력,\ H_t=잠재변수,\ \sigma=활성화함수,\ X_tW^{(1)}=가중치행렬,\ b^{(1)}=bias\\
O,H와\ 달리\ 가중치\ 행렬\ W\ 들은\ t(시간)에\ 따라\ 변하지\ 않음.\]

<p><strong>[math. 잠재변수 H~t~의 생성에서 이전 잠재변수인 H~t-1~ 활용]</strong></p>

<ul>
  <li>RNN의 역전파는 잠재변수의 연결 그래프에 따라 순차적으로 계산되며 이를 Backpropagtion Through Time(BPTT)라고 한다.</li>
</ul>

<p>\(L(x,y,w_h,w_o)=\sum^T_{t=1}l(y_t,o_t)\\
\partial_{w_h}L(x,y,w_h,w_o)=\sum^T_{t=1}\partial_{w_h}l(y_t,o_t)=\sum^T_{t=1}\partial_{o_t}l(y_t,o_t)\part_{h_t}g(h_t,w_h)[\part_{w_h}h_t],\\
\part_{w_h}h_t=\part_{w_h}f(x_t,h_{t-1},w_h)+\sum^{t-1}_{i=1}\left(\prod^t_{j=t+1}\part_{h_{j-1}}f(x_j,h_{j-1},w_h)\right)\part_{w_h}f(x_i,h_{i-1},w_h)\\
while\ h_t=f(x_t,h_{t-1},w_h)\ and\ o_t =g(h_t,w_o).\)<br />
<strong>[math. BPTT 역전파 계산 과정]</strong></p>

<ul>
  <li>RNN의 가중치행렬의 미분을 계산해보면 미분의 곱으로 이루어진 항이 계산됨.
    <ul>
      <li>이 미분의 곱은 시퀀스의 길이가 길어질 수록 값이 불안정해진다.(무한대로 수렴 또는 0으로, 값이 크게 바뀜 등)</li>
      <li>이를 막기 위해 적절한 길이 시점에서 끊어 준다.(truncated BPTT 기술)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/AI 수학 기본/image-20210213113745291.png" alt="" /></p>

<p><strong>[img. LSTM과 GPU 그림]</strong></p>

<ul>
  <li>최근에는 길이가 긴 시퀀스를 처리하기 위해 다른 RNN unit을 사용함.</li>
</ul>

</div>

  </div><a class="u-url" href="/articles/AI/MATH/AI%20%EC%88%98%ED%95%99%20%EA%B8%B0%EB%B3%B8.html" hidden></a>
  <p class="u-path" hidden>_articles/AI/MATH/AI 수학 기본.md</p>
  <script type="module" src="/assets/scripts/utils/update_recents.js"></script>
</article>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Digital garden of Nurgle.</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="a-name">The Digital garden of Nurgle.</li><li><a class="u-email" href="mailto:roadvirushn@gmail.com">roadvirushn@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li>
    <a href="https://github.com/RoadVirusHN"><svg class="svg-icon">
        <use xlink:href="/assets/svg/social-icons.svg#github"></use>
      </svg>
      <span class="username">RoadVirusHN</span></a>
  </li><!---->
</ul></div>

      <div class="footer-col footer-col-3">
        <p>이것이 디지털 동물의 숲이다!! 파멸편 (This is the Digital Animal Crossing!! Bad Ending.01)</p>
      </div>
    </div>

  </div>

</footer>
</body>

<script src="/assets/scripts/bundle/common.bundle.js"></script>

</html>